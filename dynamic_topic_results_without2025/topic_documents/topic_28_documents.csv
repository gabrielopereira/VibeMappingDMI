"year","title","abstract","journal","doi"
"2014","Likely and Unlikely Stories: Conspiracy Theories in an Age of Propaganda","The relationships between dominant practices of mass communication and widely accepted “conspiracy theories” require closer attention. The tendency of conspiracy adherents to selectively employ alternative information and communication resources while rejecting the “good information” readily available to the public has frequently been cited. Largely overlooked has been the basic character of an overall media environment wherein most information accessible to citizens is structured in accordance with commercial and/or state interests. Some conspiracy theories may appear plausible due to ongoing public exposure to  integration propaganda  pervasive within the mainstream media and a corresponding receptiveness to compatible expressions of  agitation propaganda . Other conspiracy theories may gain appeal as “credible alternatives” to mainstream accounts, once longstanding media frames and narratives have been subjected to critical scrutiny.","",""
"2016","Social bots distort the 2016 U.S. Presidential election online discussion","Social media have been extensively praised for increasing democratic discussion on social issues related to policy and politics. However, what happens when this powerful communication tools are exploited to manipulate online discussion, to change the public perception of political entities, or even to try affecting the outcome of political elections? In this study we investigated how the presence of social media bots, algorithmically driven entities that on the surface appear as legitimate users, affect political discussion around the 2016 U.S. Presidential election. By leveraging state-of-the-art social bot detection algorithms, we uncovered a large fraction of user population that may not be human, accounting for a significant portion of generated content (about one-fifth of the entire conversation). We inferred political partisanships from hashtag adoption, for both humans and bots, and studied spatio-temporal communication, political support dynamics, and influence mechanisms by discovering the level of network embeddedness of the bots. Our findings suggest that the presence of social media bots can indeed negatively affect democratic political discussion rather than improving it, which in turn can potentially alter public opinion and endanger the integrity of the Presidential election.","",""
"2017","Disinformation and social bot operations in the run up to the 2017 French presidential election","Recent accounts from researchers, journalists, as well as federal investigators, reached a unanimous conclusion: social media are systematically exploited to manipulate and alter public opinion. Some disinformation campaigns have been coordinated by means of bots, social media accounts controlled by computer scripts that try to disguise themselves as legitimate human users. In this study, we describe one such operation that occurred in the run up to the 2017 French presidential election. We collected a massive Twitter dataset of nearly 17 million posts, posted between 27 April and 7 May 2017 (Election Day). We then set to study the MacronLeaks disinformation campaign: By leveraging a mix of machine learning and cognitive behavioral modeling techniques, we separated humans from bots, and then studied the activities of the two groups independently, as well as their interplay. We provide a characterization of both the bots and the users who engaged with them, and oppose it to those users who didn’t. Prior interests of disinformation adopters pinpoint to the reasons of scarce success of this campaign: the users who engaged with MacronLeaks are mostly foreigners with pre-existing interest in alt-right topics and alternative news media, rather than French users with diverse political views. Concluding, anomalous account usage patterns suggest the possible existence of a black market for reusable political disinformation bots.","",""
"2018","A simulated cyberattack on Twitter: Assessing partisan vulnerability to spear phishing and disinformation ahead of the 2018 U.S. midterm elections","State-sponsored “bad actors” increasingly weaponize social media platforms to launch cyberattacks and disinformation campaigns during elections. Social media companies, due to their rapid growth and scale, struggle to prevent the weaponization of their platforms. This study conducts an automated spear phishing and disinformation campaign on Twitter ahead of the 2018 United States midterm elections. A fake news bot account — the @DCNewsReport — was created and programmed to automatically send customized tweets with a “breaking news” link to 138 Twitter users, before being restricted by Twitter.Overall, one in five users clicked the link, which could have potentially led to the downloading of ransomware or the theft of private information. However, the link in this experiment was non-malicious and redirected users to a Google Forms survey. In predicting users’ likelihood to click the link on Twitter, no statistically significant differences were observed between right-wing and left-wing partisans, or between Web users and mobile users. The findings signal that politically expressive Americans on Twitter, regardless of their party preferences or the devices they use to access the platform, are at risk of being spear phished on social media.","",""
"2019","Mapping the anti-vaccination movement on Facebook","ABSTRACT Over the past decade, anti-vaccination rhetoric has become part of the mainstream discourse regarding the public health practice of childhood vaccination. These utilise social media to foster online spaces that strengthen and popularise anti-vaccination discourses. In this paper, we examine the characteristics of and the discourses present within six popular anti-vaccination Facebook pages. We examine these large-scale datasets using a range of methods, including social network analysis, gender prediction using historical census data, and generative statistical models for topic analysis (Latent Dirichlet allocation). We find that present-day discourses centre around moral outrage and structural oppression by institutional government and the media, suggesting a strong logic of ‘conspiracy-style’ beliefs and thinking. Furthermore, anti-vaccination pages on Facebook reflect a highly ‘feminised’ movement ‒ the vast majority of participants are women. Although anti-vaccination networks on Facebook are large and global in scope, the comment activity sub-networks appear to be ‘small world’. This suggests that social media may have a role in spreading anti-vaccination ideas and making the movement durable on a global scale.","",""
"2019","How knowledge contributors are legitimizing their posts on controversial scientific topics: A case of the measles, mumps, and rubella (MMR) vaccine","Traditionally, journalists, government agencies, and medical professionals have acted as mediators, facilitating the transfer of scientific knowledge from scientists to the general public. More recently, however, ordinary citizens are circumventing top-down mediation and contributing directly to discussions about scientific topics online. For the present study, we examined how these emerging mediators of online scientific information are shaping the discussion of hotly debated (at least within the public sphere) scientific topics, specifically, the alleged link between autism and the measles, mumps, and rubella (MMR) vaccine. Using content analysis, we have identified the resources that lay pro- and anti-vaccination knowledge contributors most often cite when making knowledge claims. Additionally, we examined how these contributors 1) use citations to legitimize their arguments; and, 2) take on particular roles in such arguments. Our results shed light on an emerging form of online science communication and the process by which knowledge contributed by ordinary citizens is shaping these online discussions. These findings have implications for online health information and health decision-making.","",""
"2019","Evolution of bot and human behavior during elections","Online social media have become one of the main communication platforms for political discussion. The online ecosystem, however, does not only include human users but has given a space to an increasing number of automated accounts, referred to as bots, extensively used to spread messages and manipulate the narratives others are exposed to. Although social media service providers put increasing efforts to protect their platforms, malicious bot accounts continuously evolve to escape detection. In this work, we monitored the activity of almost 245K accounts engaged in the Twitter political discussion during the last two U.S. voting events. We identified approximately 31K bots and characterized their activity in contrast with humans. We show that, in the 2018 midterms, bots changed the volume and the temporal dynamics of their online activity to better mimic humans and avoid detection. Our findings highlight the mutable nature of bots and illustrate the challenges to forecast their evolution.","",""
"2019","Examining Emergent Communities and Social Bots Within the Polarized Online Vaccination Debate in Twitter"," Many states in the United States allow a “belief exemption” for measles, mumps, and rubella (MMR) vaccines. People’s opinion on whether or not to take the vaccine can have direct consequences in public health. Social media has been one of the dominant communication channels for people to express their opinions of vaccination. Despite governmental organizations’ efforts of disseminating information of vaccination benefits, anti-vaccine sentiment is still gaining momentum. Studies have shown that bots on social media (i.e., social bots) can influence opinion trends by posting a substantial number of automated messages. The research presented here investigates the communication patterns of anti- and pro-vaccine users and the role of bots in Twitter by studying a retweet network related to MMR vaccine after the 2015 California Disneyland measles outbreak. We first classified the users into anti-vaccination, neutral to vaccination, and pro-vaccination groups using supervised machine learning. We discovered that pro- and anti-vaccine users retweet predominantly from their own opinion group. In addition, our bot analysis discovers that 1.45% of the corpus users were identified as likely bots which produced 4.59% of all tweets within our dataset. We further found that bots display hyper-social tendencies by initiating retweets at higher frequencies with users within the same opinion group. The article concludes that highly clustered anti-vaccine Twitter users make it difficult for health organizations to penetrate and counter opinionated information while social bots may be deepening this trend. We believe that these findings can be useful in developing strategies for health communication of vaccination. ","",""
"2020","ILLUMINATI(NG) THE SEARCH PROCESS: THEORIZING THE RESEARCH PRACTICES   OF """"ALTERNATIVE"""" OR """"CONTROVERSIAL"""" RESEARCH","Previous scholarship has examined how conspiracy theories spread online; addressed the question of what conspiracy theorists believe and why; asked whether or not conspiracy theorizing is a reasonable form of sense-making; and characterized the socio-cultural effects of conspiracy theories. Yet, the research practices of those who explore topics that have been labeled “conspiracy theories,” remain under-examined and under-theorized. Existing at the convergence of three interdisciplinary areas of scholarship—the study of conspiracy theory/ies, information seeking and behavior (e.g., research practices), and archival studies--this project will present preliminary dissertation research. The data will come from in-depth, qualitative interviews with individuals who regularly conduct research into one of three topics: the assassination of John F. Kennedy, the 1947 incident in Roswell, New Mexico, and the Missing 411 phenomenon. Interviews are semi-structured, addressing participants’ experiences of different modes of research: within a research community, in isolation (by themselves), and with the help of a reference archivist or librarian, among others that may emerge. This paper will also feature reflexive grounded theory analysis of my own feminist standpoint as a researcher and interviewer.","",""
"2020","VACCINE INFORMATION SEEKING AND SHARING: HOW PRIVATE FACEBOOK GROUPS   CONTRIBUTED TO THE ANTI-VACCINE MOVEMENT ONLINE","Although vaccines are supported by decades of research demonstrating their efficacy and safety, many parents still decide not to vaccinate their children due to the perceived risks. One major factor in vaccine dissent is the proliferation of vaccine opposed content online. Online content has become an integral part of how people make health and science-related decisions. This study explores how members of anti-vaccine Facebook groups use the platform to seek and share vaccine information. Using interviews and Facebook posts shared in vaccine opposed groups, this study was able to identify the information seeking and sharing behaviors on Facebook and how Facebook’s platform changes are affecting the vaccine opposed movement. Findings from this study will provide further insights into the relationships among social media use, values, and trust in the vaccine debate. In addition, results may be applicable to other scientific controversies, online misinformation, and the development of public health interventions.","",""
"2020","Going viral: How a single tweet spawned a COVID-19 conspiracy theory on Twitter"," In late March of 2020, a new hashtag, #FilmYourHospital, made its first appearance on social media. The hashtag encouraged people to visit local hospitals to take pictures and videos of empty hospitals to help “prove” that the COVID-19 pandemic is an elaborate hoax. Using techniques from Social Network Analysis, this case study examines how this conspiracy theory propagated on Twitter and whether the hashtag virality was aided by the use of automation or coordination among Twitter users. We found that while much of the content came from users with limited reach, the oxygen that fueled this conspiracy in its early days came from a handful of prominent conservative politicians and far right political activists on Twitter. These power users used this hashtag to build awareness about the campaign and to encourage their followers to break quarantine and film what is happening at their local hospitals. After the initial boost by a few prominent accounts, the campaign was mostly sustained by pro-Trump accounts, followed by a secondary wave of propagation outside the U.S. The rise of the #FilmYourHospital conspiracy from a single tweet demonstrates the ongoing challenge of addressing false, viral information during the COVID-19 pandemic. While the spread of misinformation can be potentially mitigated by fact-checking and directing people to credible sources of information from public health agencies, false and misleading claims that are driven by politics and supported by strong convictions and not science are much harder to root out. ","",""
"2020","Infodemic amid the COVID-19 pandemic","The unfortunate arrival of the COVID-19 pandemic has also brought along with it a tsunami of information that can be both authentic and important as well as non-reliable and misguiding. The World Health Organization (WHO) coins this outburst of information in this era of pandemic as an infodemic. It becomes essential for societies to consume and act on trusted information in these times of uncertainty and grief. In this article, we describe and assess the role of blockchain technology and its features to establish an environment of a trusted information ecosystem. We present an equivalence mapping of these important parameters to curb an infodemic with blockchain technology features and applications. This equivalence mapping provides a directional sense to stakeholders, decision-makers, policy-makers and investors to gauge and synthesize the potential of blockchain technology for tackling an infodemic.","",""
"2020","What types of COVID-19 conspiracies are populated by Twitter bots?","With people moving out of physical public spaces due to containment measures to tackle the novel coronavirus (COVID-19) pandemic, online platforms become even more prominent tools to understand social discussion. Studying social media can be informative to assess how we are collectively coping with this unprecedented global crisis. However, social media platforms are also populated by bots, automated accounts that can amplify certain topics of discussion at the expense of others. In this paper, we study 43.3M English tweets about COVID-19 and provide early evidence of the use of bots to promote political conspiracies in the United States, in stark contrast with humans who focus on public health concerns.","",""
"2020","Fighting the ‘Infodemic’: Legal Responses to COVID-19 Disinformation"," Online disinformation has been on the rise in recent years. A digital outbreak of disinformation has spread around the COVID-19 pandemic, often referred to as an “infodemic.” Since January 2020, digital media have been both the culprits of and antidotes to misinformation. The first months of the pandemic have shown that countering disinformation online has become as important as ensuring much needed medical equipment and supplies for health workers. For many governments around the world, priority COVID-19 actions included measures such as (a) providing guidance to social media companies on taking down contentious pandemic content (e.g., India); (b) establishing special units to combat disinformation (e.g., EU, UK); and (c) criminalizing malicious coronavirus falsehood, including in relation to public health measures. This article explores the short and potential long-term effects of newly passed legislation in various countries directly targeting COVID-19 disinformation on the media, whether traditional or digital. The early actions enacted under the state-of-emergency carve new directions in negotiating the delicate balance between freedom of expression and online censorship, in particular by imposing limitations on access to information and inducing self-restraint in reporting. Based on comparative legal analysis, this article provides a timely discussion of intended and unintended consequences of such legal responses to the “infodemic,” reflecting on a basic set of safeguards needed to preserve trust in online information. ","",""
"2020","A Relationship-Centered and Culturally Informed Approach to Studying Misinformation on COVID-19"," The panic and anxiety that accompanies a global pandemic is only exacerbated by the spread of misinformation. For COVID-19, in many parts of the world, such misinformation is circulating through globally popular mobile instant messaging services (MIMS) like WhatsApp and Telegram. Compared to more public social media platforms like Facebook and Twitter, these services offer private, intimate, and often encrypted spaces for users to chat with family members and friends, making it difficult for the platform to moderate misinformation on them. Thus, there is an enhanced onus on users of MIMS to curb misinformation by correcting their family and friends within these spaces. Research on understanding how such relational correction occurs in different parts of the globe will need to attend to how the nature of these interpersonal relationships and the cultural dynamics that influence them shape the correction process. Thus, as people increasingly use MIMS to connect with close relations to make sense of this global crisis, studying the issue of misinformation on these services requires us to adopt a relationship-centered and culturally informed approach. ","",""
"2020","Social Media and Trust in Scientific Expertise: Debating the Covid-19 Pandemic in The Netherlands"," This article examines the role of social media dynamics in the public exchange of information between scientists (experts), government (policy-makers), mass media (journalists), and citizens (nonexperts) during the first 4 months after the Covid-19 outbreak in the Netherlands. Over the past decade, the institutional model of science communication, based on linear vectors of information flows between institutions, has gradually converted into a networked model where social media propel information flows circulating between all actors involved. The question driving our research is, “How are social media deployed to both undermine and enhance public trust in scientific expertise during a health crisis?” Analyzing the public debate during the period of the corona outbreak in the Netherlands, we investigate two stages: the emergency response phase and the “smart exit strategy” phase, discussing how scientific experts, policy-makers, journalists, and citizens appropriate social media logic to steer information and to control the debate. We conclude by outlining the potential risks and benefits of adopting social media dynamics in institutional contexts of science communication. ","",""
"2020","“Coronavirus EXPLAINED”: YouTube, COVID-19, and the Socio-Technical Mediation of Expertise"," Since the coronavirus outbreak, YouTube has altered its content moderation policies to surface more “authoritative information” while removing videos that contain “medically unsubstantiated claims.” This was made urgent by incidents like a live-stream interview of renowned British conspiracy theorist David Icke—in which he falsely linked the spread of the coronavirus to 5G technology—that gained substantial traction online. Behind these events, however, lies a tension between the need for authoritative medical information and the socio-technical mediation that enables multiple, competing voices to lay claim to such authority on YouTube; a tension exacerbated by the current pandemic. Following an investigation into the sources and types of video content average users were likely to see when searching for information about the coronavirus on the site, we suggest that through its incentive structure and participatory affordances, YouTube may have subordinated expertise to a logic of likability—leaving institutional experts trailing behind. ","",""
"2020","Online Temptations: COVID-19 and Religious Misinformation in the MENA Region"," During the coronavirus pandemic, religious misinformation has been found on social media platforms causing fear, confusion, and polluting the Middle East and North Africa (MENA) region’s online sphere. Exploring cases of religious clickbait in the form of false hadiths and viral religious advice from religious figures entrenched in the MENA’s political elite, this essay discusses how new dynamics for religion in the age of the Internet are contributing to a uniquely regional and religious form of misinformation. This essay looks at how the phenomenon of religious misinformation is a defining characteristic of the MENA’s online sphere, becoming even more acute during the COVID-19 pandemic. ","",""
"2021","How the Mainstream Media Help to Spread Disinformation about Covid-19","Introduction In this article, we hypothesise how mainstream media coverage can promote the spread of disinformation about Covid-19. Mainstream media are often discussed as opposed to disinformation (Glasser; Benkler et al.). While the disinformation phenomenon is related to the intentional production and spread of misleading and false information to influence public opinion (Fallis; Benkler et al.), mainstream media news is expected to be based on facts and investigation and focussed on values such as authenticity, accountability, and autonomy (Hayes et al.). However, journalists might contribute to the spread of disinformation when they skip some stage of information processing and reproduce false or misleading information (Himma-Kadakas). Besides, even when the purpose of the news is to correct disinformation, media coverage might contribute to its dissemination by amplifying it (Tsfati et al.). This could be particularly problematic in the context of social media, as users often just read headlines while scrolling through their timelines (Newman et al.; Ofcom). Thus, some users might share news from the mainstream media to legitimate disinformation about Covid-19. The pandemic creates a delicate context, as journalists are often pressured to produce more information and, therefore, are more susceptible to errors. In this research, we focussed on the hypothesis that legitimate news can contribute to the spread of disinformation on social media through headlines that reinforce disinformation discourses, even though the actual piece may frame the story differently. The research questions that guide this research are: are URLs with headlines that reinforce disinformation discourses and other mainstream media links shared into the same Facebook groups? Are the headlines that support disinformation discourses shared by Facebook users to reinforce disinformation narratives?  As a case study, we look at the Brazilian disinformation context on Covid-19. The discussion about the disease in the country has been highly polarised and politically framed, often with government agents and scientists disputing the truth about facts on the disease (Araújo and Oliveira; Recuero and Soares; Recuero et al.). Particularly, the social media ecosystem seems to play an important role in these disputes, as Brazilian President Jair Bolsonaro and his supporters use it as a key channel to spread disinformation about the virus (Lisboa et al.; Soares et al.). We use data from public groups on Facebook collected through CrowdTangle and a combination of social network analysis and content analysis to analyse the spread and the content of URLs and posts.  Theoretical Background Disinformation has been central to the Covid-19 “infodemic”, created by the overabundance of information about the pandemic, which makes it hard for people to find reliable guidance and exacerbates the outbreak (Tangcharoensathien et al.). We consider disinformation as distorted, manipulated, or false information intentionally created to mislead someone (Fallis; Benkler et al.). Disinformation is often used to strengthen radical political ideologies (Benkler et al.). Around the world, political actors politically framed the discussion about the pandemic, which created a polarised public debate about Covid-19 (Allcott et al., Gruzd and Mai; Recuero and Soares). On social media, contexts of polarisation between two different political views often present opposed narratives about the same fact that dispute public attention (Soares et al.). This polarisation creates a suitable environment for disinformation to thrive (Benkler et al.) The polarised discussions are often associated with the idea of “bubbles”, as the different political groups tend to share and legitimate only discourses that are aligned with the group's ideological views. Consequently, these groups might turn into ideological bubbles (Pariser). In these cases, content shared within one group is not shared within the other and vice versa. Pariser argues that users within the bubbles are exposed exclusively to content with which they tend to agree. However, research has shown that Pariser’s concept of bubbles has limitations (Bruns), as most social media users are exposed to a variety of sources of information (Guess et al.). Nevertheless, polarisation might lead to different media diets and disinformation consumption (Benkler et al.). That is, users would have contact with different types of information, but they would choose to share certain content over others because of their political alignment (Bruns). Therefore, we understand that bubbles are created by the action of social media users who give preference to circulate (through retweets, likes, comments, or shares) content that supports their political views, including disinformation (Recuero et al.). Thus, bubbles are ephemeral structures (created by users’ actions in the context of a particular political discussion) with permeable boundaries (users are exposed to content from the outside) in discussions on social media. This type of ephemeral bubble might use disinformation as a tool to create a unique discourse that supports its views. However, it does not mean that actors within a “disinformation bubble” do not have access to other content, such as the news from the mainstream media. It means that the group acts to discredit and to overlap this content with an “alternative” story (Larsson). In addition, the mainstream media might disseminate false or inaccurate disinformation (Tsfati et al.). Particularly, we focus on inaccurate headlines that reinforce disinformation narratives, as social media users often only read news headlines (Newman et al.; Ofcom). This is especially problematic because a large number of social media users are exposed to mainstream media content, while exposure to disinformation websites is heavily concentrated on only a few users (Guess et al.; Tsfati et al.). Therefore, when the mainstream media disseminate disinformation, it is more likely that a larger number of social media users will be exposed to this content and share it into ideological bubbles. Based on this discussion, we aim to understand how the mainstream media contribute to the spread of disinformation discourses about Covid-19. Methods This study is about how mainstream media coverage might contribute to the spread of disinformation about Covid-19 on Facebook. We propose two hypotheses, as follows: H1: When mainstream media headlines frame the information in a way that reinforces the disinformation narrative, the links go into a “disinformation bubble”. H2: In these cases, Facebook users might use mainstream media coverage to legitimate disinformation narratives. We selected three case studies based on events that created both political debate and high media coverage in Brazil. We chose them based on the hypothesis that part of the mainstream media links could have produced headlines that support disinformation discourses, as the political debate was high. The events are:   On 24 March 2020, Brazilian President Jair Bolsonaro made a public pronouncement on live television. In the week before the pronouncement, Brazilian governors decided to follow World Health Organisation (WHO) protocols and closed non-essential business. In his speech, Bolsonaro criticised social distancing measures. The mainstream media reproduced some of his claims and claims from other public personalities, such as entrepreneurs who also said the protocols would harm the economy.  On 8 June 2020, a WHO official said that it “seems to be rare that an asymptomatic person transmits [Covid-19] onward to a secondary individual”. Part of the mainstream media reproduced the claim out of context, which could promote the misperception that both asymptomatic and pre-symptomatic persons (early stages of an illness, before the first symptoms) do not transmit Covid-19 at all.  On 9 November 2020, Brazil’s national sanitary watchdog Anvisa reported that they had halted the clinical studies on the CoronaVac vaccine, developed by the Chinese company Sinovac. Bolsonaro often criticised CoronaVac because it was being produced in partnership with São Paulo’s Butantan Institute and became the subject of a political dispute between Bolsonaro and the Governor of São Paulo, João Dória. Bolsonaro said the halt of the CoronaVac trial was """"another victory for Jair Bolsonaro"""". Anvisa halted the trail after a """"severe adverse event"""". The mainstream media rapidly reverberated the decision. Later, it was revealed that the incident was a death that had nothing to do with the vaccine.   Before we created our final dataset that includes links from the three events together, we explored the most shared URLs in each event. We used keywords to collect posts shared in the public groups monitored by CrowdTangle, a tool owned by Facebook that tracks publicly available posts on the platform. We collected posts in a timeframe of three days for each event to prevent the collection of links unrelated to the cases. We collected only posts containing URLs. Table 1 summarises the data collected. Table 1: Data collected     Dates   March 24-26 2020   June 8-10 2020   November 9-11 2020     Keywords   “Covid-19” or “coronavirus” and “isolation” or “economy”   “Covid-19” or “coronavirus” and “asymptomatic”   “vaccine” and “Anvisa” or “CoronaVac”     Number of posts   4780   2060   3273     From this original dataset, we selected the 60 most shared links from each period (n=180). We then filtered for those which sources were mainstream media outlets (n=74). We used content analysis (Krippendorff) to observe which of these URLs headlines could reinforce disinformation narratives (two independent coders, Krippendorff’s Alpha = 0.76). We focussed on headlines because when these links are shared on Facebook, often it is the headline that appears to other users. We considered that a headlined reinforced disinformation discourses only when it was flagged by both coders (n=21 – some examples are provided in Table 3 in the Results section). Table 2 provides a breakdown of this analysis. Table 2: Content analysis     Event   Mainstream media links   Headlines that support disinformation discourses     Number of links   Number of posts     Economy and quarantine   24   7   112     Asymptomatic   22   7   163     Vaccine trial   28   7   120     Total   74   21   395     As the number of posts that shared URLs with headlines that supported disinformation was low (n=395), we conducted another CrowdTangle search to create our final dataset. We used a sample of the links we classified to create a “balanced” dataset. Out of the 21 links with headlines that reinforced disinformation, we collected the 10 most shared in public groups monitored by CrowdTangle (this time, without any particular timeframe) (n=1346 posts). In addition, we created a “control group” with the 10 most shared links that neither of the coders considered could reinforce disinformation (n=1416 posts). The purpose of the “control group” was to identify which Facebook groups tend to share mainstream media links without headlines that reinforce disinformation narratives. Therefore, our final dataset comprises 20 links and 2762 posts.  We then used social network analysis (Wasserman and Faust) to map the spread of the 20 links. We created a bipartite network, in which nodes are (1) Facebook groups and (2) URLs; and edges represent when a post within a group includes a URL from our dataset. We applied a modularity metric (Blondel et al.) to identify clusters. The modularity metric allows us to identify “communities” that share the same or similar links in the network map. Thus, it helped us to identify if there was a “bubble” that only shares the links with headlines that support disinformation (H1).  To understand if the disinformation was supporting a larger narrative shared by the groups, we explored the political alignments of each cluster (H2). We used Textometrica (Lindgreen and Palm) to create word clouds with the most frequent words in the names of the cluster groups (at least five mentions) and their connections. Finally, we also analysed the posts that shared each of the 10 links with headlines that reinforced disinformation. This also helped us to identify how the mainstream media links could legitimate disinformation narratives (H2). Out of the 1346 posts, only 373 included some message (the other 973 posts only shared the link). We used content analysis to see if these posts reinforced the disinformation (two independent coders – Krippendorff’s Alpha = 0.723). There were disagreements in the categorisation of 27 posts. The two coders reviewed and discussed the classification of these posts to reach an agreement. Results Bubbles of information  In the graph (Figure 1), red nodes are links with headlines that support disinformation discourses, blue nodes are the other mainstream media links, and black nodes are Facebook groups. Our first finding is that groups that shared headlines that support disinformation rarely shared the other mainstream media links. Out of the 1623 groups in the network, only 174 (10.7%) shared both a headline that supports disinformation discourse, and another mainstream media link; 712 groups (43.8%) only shared headlines that support disinformation; and 739 groups (45.5%) only shared other links from the mainstream media. Therefore, users’ actions created two bubbles of information.  Figure 1: Network graph The modularity metric confirmed this tendency of two “bubbles” in the network (Figure 2). The purple cluster includes seven URLs with headlines that support disinformation discourse. The green cluster includes three headlines that support disinformation discourse and the other 10 links from the mainstream media. This result partially supports H1: When mainstream media headlines frame the information in a way that reinforces the disinformation narrative, the links go into a “disinformation bubble”. As we identified, most of the headlines that support disinformation discourse went into a separate “bubble”, as users within the groups of this bubble did not share the other links from the mainstream media.  Figure 2: Network graph with modularity This result shows that users’ actions boost the creation of bubbles (Bakshy et al.), as they choose to share one type of content over the other. The mainstream media are the source of all the URLs we analysed. However, users from the purple cluster chose to share only links with headlines that supported disinformation discourses. This result is also related to the political framing of the discussions, as we explore below.  Disinformation and Political Discourse We used word clouds (Lindgreen and Palm) to analyse the Facebook groups’ names to explore the ideological affiliation of the bubbles. The purple bubble is strongly related to Bolsonaro and his discourse (Figure 3). Bolsonaro is the most frequent word. Other prevalent words are Brazil, patriots (both related to his nationalist discourse), right-wing, conservative, military (three words related to his conservative discourse and his support of the military dictatorship that ruled Brazil from 1964 to 1985), President, support, and Alliance [for Brazil] (the name of his party). Some of the most active groups within the purple bubble are “Alliance for Brazil”, “Bolsonaro 2022 [next presidential election]”, “Bolsonaro’s nation 2022”, and “I am right-wing with pride”.  Figure 3: Purple cluster word cloud Bolsonaro is also a central word in the green cluster word cloud (Figure 4). However, it is connected to other words such as “against” and “out”, as many groups are anti-Bolsonaro. Furthermore, words such as left-wing, Workers’ Party (centre-left party), Lula and Dilma Rousseff (two Workers’ Party ex-presidents) show another ideological alignment in general. In addition, there are many local groups (related to locations such as Rio de Janeiro, São Paulo, Rio Grande do Sul, Minas Gerais, and others), and groups to share news (news, newspaper, radio, portal). “We are 70 per cent [anti-Bolsonaro movement]”, “Union of the Left”, “Lula president”, and “Anti-Bolsonaro” are some of the most active groups within the green cluster.  Figure 4: Green cluster word cloud Then, we analysed how users shared the mainstream media links with headlines that support disinformation discourses. In total, we found that 81.8% of the messages in the posts that shared these links also reproduced disinformation narratives. The frequency was higher (86.2%) when considering only posts that shared one of the seven links from the purple cluster (based on the modularity metric). Consequently, it was lower (64%) in the messages that shared one of the other three links.  The messages often showed support for Bolsonaro; criticised other political and health authorities (the WHO, São Paulo Governor João Dória, and others), China, and the “leftists” (all opposition to Bolsonaro); claimed that quarantine and social distancing measures were unnecessary; and framed vaccines as dangerous. We provide some examples of headlines and posts in Table 3 (we selected the most-shared URL for each event to illustrate). This result supports H2 as we found that users shared mainstream media headlines that reinforce disinformation discourse to legitimate the disinformation narrative; and that it was more prevalent in the purple bubble.  Table 3: Examples of headlines and posts     Headline   Post     """"Unemployment is a crisis much worse than coronavirus"""", says Bolsonaro   Go to social media to support the President. Unemployment kills. More than any virus... hunger, depression, despair and everything     UNEMPLOYMENT, THE DEPUTIES CHAMBER, THE SENATE AND THE SUPREME COURT KILL MORE THAN COVID19     Asymptomatic patients do not boost coronavirus, says WHO   QUARANTINE IS FAKE #StayHome, the lie of the century!     THIS GOES TO THE PUPPETS OF THE COMMUNIST PARTIES THE AND FUNERARY MEDIA     Anvisa halts Coronavac vaccine trial after """"serious adverse event""""   [The event] is adverse and serious, so the vaccine killed the person by covid     And Doria [Governor of São Paulo and political adversary of Bolsonaro] wants to force you to take this shit     This result shows that mainstream media headlines that support disinformation narratives may be used to reinforce disinformation discourses when shared on Facebook, making journalists potential agents of disinformation (Himma-Kadakas; Tsfati et al.). In particular, the credibility of mainstream news is used to support an opposing discourse, that is, a disinformation discourse. This is especially problematic in the context of Covid-19 because the mainstream media end up fuelling the infodemic (Tangcharoensathien et al.) by sharing inaccurate information or reverberating false claims from political actors.  Conclusion In this article, we analysed how the mainstream media contribute to the spread of disinformation about Covid-19. In particular, we looked at how links from the mainstream media with headlines that support disinformation discourse spread on Facebook, compared to other links from the mainstream media. Two research questions guided this study: Are URLs with headlines that reinforce disinformation discourses and other mainstream media links shared into the same Facebook groups? Are the headlines that support disinformation discourses shared by Facebook users to reinforce disinformation narratives?  We identified that (1) some Facebook groups only shared links with headlines that support disinformation narratives. This created a “disinformation bubble”. In this bubble, (2) Facebook users shared mainstream media links to reinforce disinformation – in particular, pro-Bolsonaro disinformation, as many of these groups had a pro-Bolsonaro alignment. In these cases, the mainstream media contributed to the spread of disinformation. Consequently, journalists ought to take extra care when producing news, especially headlines, which will be the most visible part of the stories on social media. This study has limitations. We analysed only a sample of links (n=20) based on three events in Brazil. Other events and other political contexts might result in different outcomes. Furthermore, we used CrowdTangle for data collection. CrowdTangle only provides information about public posts in groups monitored by the tool. Therefore, our result does not represent the entire Facebook. References Allcott, Hunt, et al. “Polarization and Public Health: Partisan Differences in Social Distancing during the Coronavirus Pandemic.” National Bureau of Economic Research, Working Paper No. 26946 (2020). 6 Jan. 2021 &lt;https://doi.org/10.3386/w26946&gt;. Araújo, Ronaldo Ferreira, and Thaiane Moreira Oliveira. “Desinformação e Mensagens Sobre a Hidroxicloroquina no Twitter: Da Pressão Política à Disputa Científica.” Atoz – Novas Práticas em Informação e Conhecimento 9.2 (2020). 6 Jan. 2021 &lt;http://dx.doi.org/10.5380/atoz.v9i2.75929&gt;. Bakshy, Eytan, et al. “Exposure to Ideologically Diverse News and Opinion on Facebook.” Science 348.6239 (2015). 6 Jan. 2021 &lt;https://science.sciencemag.org/content/348/6239/1130&gt;. Benkler, Yochai, et al. Network Propaganda: Manipulation, Disinformation, and Radicalization in American Politics. New York: Oxford University Press, 2018. Blondel, Vincent D., et al. “Fast Unfolding of Communities in Large Networks.” Physics.soc-ph (2008). 6 Jan. 2021 &lt;http://lanl.arxiv.org/abs/0803.0476&gt;. Bruns, Axel. Are Filter Bubbles Real?. Cambridge: Polity Press, 2019. CrowdTangle Team. CrowdTangle. Menlo Park, Calif.: Facebook, 2020. &lt;https://apps.crowdtangle.com/search/&gt;. Fallis, Don. “What Is Disinformation?” Library Trends 63.3 (2015): 401-426. Glasser, Susan B. “Covering Politics in a ‘Post-Truth’ America.” Brookings Institution Press, 2 Dec. 2016. 22 Feb. 2021 &lt;https://www.brookings.edu/essay/covering-politics-in-a-post-truth-america/&gt;. Gruzd, Anatoliy, and Philip Mai. “Going Viral: How a Single Tweet Spawned a COVID-19 Conspiracy Theory on Twitter.” Big Data &amp; Society, 7.2 (2020). 6 Jan. 2021 &lt;https://doi.org/10.1177/2053951720938405&gt;. Guess, Andrew, et al. Avoiding the Echo Chamber about Echo Chambers: Why Selective Exposure to Like-Minded Political News Is Less Prevalent than You Think. Miami: John S. and James L. Knight Foundation, 2018. Hayes, Arthur S., et al. “Shifting Roles, Enduring Values: The Credible Journalist in a Digital Age.” Journal of Mass Media Ethics 22.4 (2007): 262-279. 22 Feb.2021 &lt;https://doi.org/10.1080/08900520701583545&gt;. Himma-Kadakas, Marju. “Alternative Facts and Fake News Entering Journalistic Content Production Cycle”. Cosmopolitan Civil Societies: An Interdisciplinary Journal 9.2 (2017). 6 Jan. 2021 &lt;https://doi.org/10.5130/ccs.v9i2.5469&gt;. Kripendorff, Klaus. Content Analysis: An Introduction to Its Methodology. California: Sage Publications, 2013. Larsson, Anders Olof. “News Use as Amplification – Norwegian National, Regional and Hyperpartisan Media on Facebook.” Journalism &amp; Mass Communication Quarterly 96 (2019). 6 Jan. 2021 &lt;https://doi.org/10.1177/1077699019831439&gt;. Lindgreen, Simon, and Fredrik Palm. Textometrica Service Package (2011). 6 Jan. 2021 &lt;http://textometrica.humlab.umu.se&gt;. Lisboa, Lucas A., et al. “A Disseminação da Desinformação Promovida por Líderes Estatais na Pandemia da COVID-19.” Proceedings of the Workshop Sobre as Implicações da Computação na Sociedade (WICS), Porto Alegre: Sociedade Brasileira de Computação, 2020. 6 Jan. 2021 &lt;https://doi.org/10.5753/wics.2020.11042&gt;. Newman, Nic, et al. Reuters Institute Digital News Report 2018. Oxford: Oxford University, 2018. Ofcom. “Scrolling News: The Changing Face of Online News Consumption.” 2016. 23 Feb. 2021 &lt;https://www.ofcom.org.uk/__data/assets/pdf_file/0022/115915/Scrolling-News.pdf&gt;. Pariser, Eli. The Filter Bubble. New York: Penguin, 2011. Recuero, Raquel, and Felipe Soares. “O Discurso Desinformativo sobre a Cura do COVID-19 no Twitter: Estudo de Caso.” E-Compós (2020). 23 Feb. 2021 &lt;https://doi.org/10.30962/ec.2127&gt;. Recuero, Raquel, et al. “Polarization, Hyperpartisanship, and Echo Chambers: How the Disinformation about COVID-19 Circulates on Twitter.” Contracampo (2021, in press). 23 Feb. 2021 &lt;https://doi.org/10.1590/SciELOPreprints.1154&gt;. Soares, Felipe Bonow, et al. “Disputas discursivas e desinformação no Instagram sobre o uso da hidroxicloroquina como tratamento para o Covid-19.” Proceedings of the 43º Congresso Brasileiro de Ciências da Comunicação, Salvador: Intercom, 2020. 23 Feb. 2021 &lt;http://www.intercom.org.br/sis/eventos/2020/resumos/R15-0550-1.pdf&gt;. Tangcharoensathien, Viroj, et al. “Framework for Managing the COVID-19 Infodemic: Methods and Results of an Online Crowdsourced WHO Technical Consultation.” J Med Internet Res 22.6 (2020). 6 Jan. 2021 &lt;https://doi.org/10.2196/19659&gt;. Tsfati, Yariv, et al. “Causes and Consequences of Mainstream Media Dissemination of Fake News: Literature Review and Synthesis.” Annals of the International Communication Association 44.2 (2020): 157-173. 22 Feb. 2021 &lt;https://doi.org/10.1080/23808985.2020.1759443&gt;. Wasserman, Stanley, and Katherine Faust. Social Network Analysis: Methods and Applications. Cambridge: Cambridge UP, 1994.","",""
"2021","Anti-Vaccine Beliefs and COVID-19 Information Seeking on Social Media: Examining Processes Influencing COVID-19 Beliefs and Preventative Actions","This study explored how anti-vaccine beliefs and social media use operate as interrelated factors contributing to COVID-19–specific beliefs and actions. Results show that those harboring greater anti-vaccine sentiments rely strongly on social media sources for COVID-19 information. Tests of indirect effects show that COVID-19 information seeking on social media mediates the relationship between anti-vaccine beliefs and COVID-19 conspiracy beliefs. Furthermore, results support a three-step model linking anti-vaccine beliefs to reduced COVID-19 preventative actions through social media use and conspiracy beliefs. Although anti-vaccine beliefs and information seeking contribute to reduced prevention action, the results also indicate these factors have differing relationships with anti-vaccine intentions. Whereas anti-vaccine beliefs predict more vaccine resistance, COVID-19 information seeking on social media contributes to higher levels of vaccine efficacy and intentions.","",""
"2021","Conspiratorial Discourses on Social Media: Agendamelding Explorations and COVID-19","This article examines a recent trend of popular conspiracism advancing in social media settings around the world. Drawing evidence from a national survey conducted in Cyprus, this study scrutinizes people’s melding tendencies with other individuals along with various social, technological, ideological, and demographic factors as predictors of conspiracism. While social media platforms constitute fertile environments that encourage alternative ideations, multiple factors encompassing ideology, education, income, and especially people’s distrust of institutions constitute significant predictors of conspiratorial tendencies.","",""
"2021","Media framing of COVID-19 pandemic in the transitional regime of Serbia: Exploring discourses and strategies"," The subject of this study is media discourse on the Covid-19 pandemic in the Republic of Serbia. The study seeks to contribute to the understanding of the communication aspects of the current public health crisis within the transitional (hybrid) regime of Serbia. One of the paper’s objectives is to explore how Serbian media frame the discourse on the Covid-19 pandemic. The second objective is to examine whether journalists produce investigative and analytical contents on this pressing topic independently or just mediate to the public patterns created by the public health crisis management. By applying language analysis and intertextual analysis methods to a sample of 230 media texts, we point to the incoherence of media discourse on the Covid-19 pandemic, as well as to the lack of media independence in an environment of the permanent political campaign. ","",""
"2021","THE DISINFECTANT DIVERSION: FRAMING STRATEGIES OF PARTISAN MEDIA IN         INTERPRETING THE COVID-19 PANDEMIC","Following the rise of Donald Trump leading up to the 2016 US         presidential election, political communication scholars have turned a critical eye towards         the role of conservative media outlets in the construction of an overarching meta-narrative,         largely referred to in the existing literature as the “deep story” (Hochschild, 2016). The         aim of the present study to extend this seminal work to analyze how mainstream,         conservative, and liberal outlets rely on meta-narratives to construct meaning in their         coverage of the COVID-19 pandemic. Employing qualitative methods, we analyze the coverage         that six American news outlets afforded the April 23rd 2020 Coronavirus Task Force news         briefing, where President Trump insinuated injections of disinfectant could be a useful way         to fight COVID-19. Our analysis includes 115 news articles, 41 Facebook posts and 87         television clips from these outlets. Our results reveal that both the left and right wing         media systems employed overarching narratives in their coverage. The left-wing media heavily         emphasized the tendency to deny or argue scientific fact among conservatives. In contrast,         we observed that the right-wing media constantly used similar framing strategies in an         attempt to vilify the left-wing media and liberals. Considering the existing literature         (Kreiss, 2018: Poletta &amp; Callahan, 2019), we observed many instances where right-wing         pundits and journalists relied on previously established heuristics, cuing audiences to         perceive the left-wing media as elitist out to discredit Trump. Our findings provide an         in-depth analysis of how partisan media relies on meta-narratives to convey meaning to their         audiences.","",""
"2021","MASK NARRATIVES PROMOTED BY ANTI-VACCINATION ACCOUNTS ON INSTAGRAM PRIOR         TO THE COVID-19 PANDEMIC","The COVID-19 pandemic has sparked passionate debate worldwide on         matters of public health. A portion of this debate has been dedicated to the efficacy of         masks to prevent the spread of COVID-19. While the majority of health officials agree that         wearing a mask is efficacious, there has been a widespread movement against masks. The         “anti-mask” movement is often characterized for spreading misinformation about masks and for         its overlap with the anti-vaccine movement. This paper focuses on the mask sentiments of the         anti-vaccination community prior to the COVID-19 pandemic. The goal of this paper is to         identify if the anti-vaccination movement held prior beliefs about masks to prevent the         spread of respiratory diseases and if those beliefs differ from their mask sentiment today.         Through thematic analysis of 44 Instagram posts prior to the onset of the COVID-19 pandemic,         we find that online vaccine safety communities have, in the past, regarded mask-wearing as a         viable alternative to vaccines. Notably, posts supported the efficacy of mask-wearing while         criticizing the mandates to wear masks in healthcare settings. In this paper, we elaborate         on these mask narratives, as well as their implications in how the anti-vaccination group         had a dramatic shift in mask sentiment during the pandemic.","",""
"2021","INTO THE BELLY OF THE BEAST: THE RESEARCH ON SOCIAL MEDIA AND COVID-19         MISINFORMATION IN 2020","In this paper we present some preliminary findings from an ongoing         research on a comprehensive corpus of 378 interdisciplinary studies about misinformation and         COVID-19 published in 2020, focusing on the role of social media platforms in spreading and         countering mis- and disinformation. We followed the PRISMA guidelines for systematic reviews         to collect and screen results, and a coding scheme based on methodological and substantive         questions to analyze them. The preliminary results show, among others, that research on         COVID-19 misinformation reproduces a well-known trend of differentiated attention to social         media platforms based on both popularity among users and ease of access to data by scholars,         that online survey distributed via social media has been a very popular approach, and the         presence of a wide range of perspectives, and sometimes diverging point of views, on         problematic information, in terms of prevalence of misinformation, countermeasures, and the         role of social media communication. During the conference presentation, comprehensive         results from the research will be presented.","",""
"2021","REJECTING SCIENCE WITH SCIENCE: BOUNDARY-WORK IN ANTI-MASK TWITTER REPLY        THREADS DURING COVID-19","The COVID-19 pandemic has been marked by a controversy in the United States over the public health benefits of mask-wearing, especially on social media. Many have contested the scientific consensus that masks are an effective method to prevent and slow the spread of COVID-19 infections, often along explicitly political lines. Here, we investigate specifically how Twitter users engaging in arguments about mask-wearing invoke scientific principles to argue against masks. We further analyze the sources that these users cite to support their claims. Using a qualitative approach drawing from constructivist grounded theory, we show how these users work to defend the legitimacy of their claims and their external sources by selectively exploiting rhetorical values of scientific endeavour. We analogize their work to the process of scientific boundary-work, in which actors consciously manipulate the boundary between science and not-science for personal and political gain.","",""
"2021","Communicating public health during COVID-19, implications for vaccine rollout"," A large body of information and opinion related to COVID-19 is being shared via social media platforms. Recent reports have raised concerns about the reliability and verifiability of said information being disseminated and the way systems, processes and design of the platforms facilitates such spread. This, alongside other areas of concern, has resulted in several social media platforms taking steps towards tackling the spread of mis- and dis-information. Here we discuss approaches to online public health messaging from a range of sources during COVID-19, with a focus on official and non-official sources in the United Kingdom (UK). We highlight issues for ongoing public health decisions, and the potential impact for the future course of the pandemic. ","",""
"2021","Studying the COVID-19 infodemic at scale"," This special theme issue of Big Data &amp; Society presents leading-edge, interdisciplinary research that focuses on examining how health-related (mis-)information is circulating on social media. In particular, we are focusing on how computational and Big Data approaches can help to provide a better understanding of the ongoing COVID-19 infodemic (overexposure to both accurate and misleading information on a health topic) and to develop effective strategies to combat it. ","",""
"2021","Identifying how COVID-19-related misinformation reacts to the announcement of the UK national lockdown: An interrupted time-series study"," COVID-19 is unique in that it is the first global pandemic occurring amidst a crowded information environment that has facilitated the proliferation of misinformation on social media. Dangerous misleading narratives have the potential to disrupt ‘official’ information sharing at major government announcements. Using an interrupted time-series design, we test the impact of the announcement of the first UK lockdown (8–8.30 p.m. 23 March 2020) on short-term trends of misinformation on Twitter. We utilise a novel dataset of all COVID-19-related social media posts on Twitter from the UK 48 hours before and 48 hours after the announcement (n = 2,531,888). We find that while the number of tweets increased immediately post announcement, there was no evidence of an increase in misinformation-related tweets. We found an increase in COVID-19-related bot activity post-announcement. Topic modelling of misinformation tweets revealed four distinct clusters: ‘government and policy’, ‘symptoms’, ‘pushing back against misinformation’ and ‘cures and treatments’. ","",""
"2021","Identifying and characterizing scientific authority-related misinformation discourse about hydroxychloroquine on twitter using unsupervised machine learning"," This study investigates the types of misinformation spread on Twitter that evokes scientific authority or evidence when making false claims about the antimalarial drug hydroxychloroquine as a treatment for COVID-19. Specifically, we examined tweets generated after former U.S. President Donald Trump retweeted misinformation about the drug using an unsupervised machine learning approach called the biterm topic model that is used to cluster tweets into misinformation topics based on textual similarity. The top 10 tweets from each topic cluster were content coded for three types of misinformation categories related to scientific authority: medical endorsements of hydroxychloroquine, scientific information used to support hydroxychloroquine’s use, and a comparison group that included scientific evidence opposing hydroxychloroquine’s use. Results show a much higher volume of tweets featuring medical endorsements and use of supportive scientific information compared to accurate and updated scientific evidence, that misinformation-related tweets propagated for a longer time frame, and the majority of hydroxychloroquine Twitter discourse expressed positive views about the drug. Metadata from Twitter accounts found that prominent users within misinformation discourse were more likely to have media or political affiliation and explicitly expressed support for President Trump. Conversely, prominent accounts within the scientific opposition discourse primarily consisted of medical doctors or scientists but had far less influence in the Twitter discourse. Implications of these findings and connections to related social media research are discussed, as well as cognitive mechanisms for understanding susceptibility to misinformation and strategies to combat misinformation spread via online platforms. ","",""
"2021","Different types of COVID-19 misinformation have different emotional valence on Twitter"," The spreading of COVID-19 misinformation on social media could have severe consequences on people's behavior. In this paper, we investigated the emotional expression of misinformation related to the COVID-19 crisis on Twitter and whether emotional valence differed depending on the type of misinformation. We collected 17,463,220 English tweets with 76 COVID-19-related hashtags for March 2020. Using Google Fact Check Explorer API we identified 226 unique COVID-19 false stories for March 2020. These were clustered into six types of misinformation (cures, virus, vaccine, politics, conspiracy theories, and other). Applying the 226 classifiers to the Twitter sample we identified 690,004 tweets. Instead of running the sentiment on all tweets we manually coded a random subset of 100 tweets for each classifier to increase the validity, reducing the dataset to 2,097 tweets. We found that only a minor part of the entire dataset was related to misinformation. Also, misinformation in general does not lean towards a certain emotional valence. However, looking at comparisons of emotional valence for different types of misinformation uncovered that misinformation related to “virus” and “conspiracy” had a more negative valence than “cures,” “vaccine,” “politics,” and “other.” Knowing from existing studies that negative misinformation spreads faster, this demonstrates that filtering for misinformation type is fruitful and indicates that a focus on “virus” and “conspiracy” could be one strategy in combating misinformation. As emotional contexts affect misinformation spreading, the knowledge about emotional valence for different types of misinformation will help to better understand the spreading and consequences of misinformation. ","",""
"2021","The COVID-19 Infodemic: Twitter versus Facebook"," The global spread of the novel coronavirus is affected by the spread of related misinformation—the so-called COVID-19 Infodemic—that makes populations more vulnerable to the disease through resistance to mitigation efforts. Here, we analyze the prevalence and diffusion of links to low-credibility content about the pandemic across two major social media platforms, Twitter and Facebook. We characterize cross-platform similarities and differences in popular sources, diffusion patterns, influencers, coordination, and automation. Comparing the two platforms, we find divergence among the prevalence of popular low-credibility sources and suspicious videos. A minority of accounts and pages exert a strong influence on each platform. These misinformation “superspreaders” are often associated with the low-credibility sources and tend to be verified by the platforms. On both platforms, there is evidence of coordinated sharing of Infodemic content. The overt nature of this manipulation points to the need for societal-level solutions in addition to mitigation strategies within the platforms. However, we highlight limits imposed by inconsistent data-access policies on our capability to study harmful manipulations of information ecosystems. ","",""
"2021","Characterizing QAnon: Analysis of YouTube comments presents new conclusions about a popular conservative conspiracy","QAnon has become an important phenomenon in American politics due to both its relative popularity as well as its adoption/endorsement by political elites. However, this conspiracy theory/social movement has received sparse investigation in the social sciences. This gap is particularly noticeable in regards to the QAnon movement’s overall beliefs and perceptions of global affairs. This piece addresses these research gaps by using repeatable inductive computational social science methods to analyze a sample of comments from YouTube, a platform popular with QAnon followers. This investigation affirms previous observations regarding QAnon’s narratives connecting the U.S. government (particularly prominent Democrats) and alleged sexual violence against children, anti-semitism/fundamentalist Christian theology, and pro-Trump sentiments, and also reaveals several novel conclusions regarding QAnon. These novel observations include: [1] that the QAnon community sustains substantial discussion of international affairs, largely revolving around China, Russia and Israel (in order of prominence); [2] that discussion of China in QAnon comments received more “likes” than other international topics; and [3] that a nexus of conjectures tying former presidential candidate, Senator, and Secretary of State Hillary Clinton to the Chinese party-state dominate these China-centric comments. Aside from these novel conclusions regarding QAnon, this paper also seeks to make a contribution to repeatable social science analysis of YouTube comments more generally.","",""
"2021","Friends get vaccinated: The power of social media groups in the COVID-19 vaccination campaign","In times of crisis the power of social media is reflected in its ability to influence social behavior and act quickly without bureaucratic mechanisms. During the Israeli COVID-19 vaccination campaign, social media groups were formed to collect, verify, and disseminate information about leftover vaccine doses. Masses of people quickly joined these groups, rushed to the vaccine locations, and shared real-time information with other group members. Based on 15 semi-structured interviews with group members and admins, we identified three motives for creating groups: making information accessible, the struggle against vaccine opponents, and a desire to return to life as it was before the pandemic. Rapid group joining has been described in terms of collective behavior and contagion theory.","",""
"2021","QAnon and the information dark age","We are entering a dark age for information literacy, an age predicated on a strange reversal of accepted wisdom. Whereas early Internet advocates predicted a utopian age of information access and literacy, the twenty-first century has witnessed a paradoxical technological expansion of communications technologies and, at the same, the growth and spread of bizarre, vast, complex conspiracies. Although many argue that belief in conspiracies is the mark of a “crippled epistemology” (Sunstein and Vermeule, 2009), I argue that this particular fusion of information access and ignorance is emblematic of what Chun (2015) has described as the combination of individual content creation within a mass medium. It is our incredible access to information, when combined with anonymized mass communications platforms, which has exacerbated networked conspiratorial thinking and given rise to the most complex example of this problematic: QAnon. In this article, I analyze QAnon through the lens of a theoretical frame I call the information dark age, and I argue that QAnon represents a new paranoid permutation, which takes advantage of information technology to spread its shadow across the Internet. The power of the QAnon conspiracy is its protean nature, its ability to grow quickly through crowd-sourced contributions to the overarching theory. Perhaps even more disturbing is that QAnon has weaponized this network in an effort to derail the 2020 presidential election in favor of President Trump and spread misinformation about the COVID-19 pandemic. Without a dramatic evolution in our current media infrastructure, we are facing the increasing spread and worsening effect of this information dark age.","",""
"2021","Fascist cross-pollination of Australian conspiracist Telegram channels","The COVID-19 pandemic has brought about trauma and uncertainty for vast swathes of the world population, including in Australia. One effect of this has been the growth of COVID-19 conspiracy theories, and general conspiracism. This article explores efforts by fascists and neo-Nazis to exploit the rise in conspiratorial thinking for recruitment and dissemination of their ideas. Five Australian conspiracist Telegram channels are studied for signs of fascist cross-pollination, and it is found that users with fascist sympathies attempt to influence the channels’ discourse through appeals to purported ideological and situational commonalities.","",""
"2021","The Hydroxychloroquine Twitter War: A case study examining polarization in science communication","The COVID-19 pandemic has created communication challenges exacerbated by the circulation of misinformation and the politicization of science. The case of hydroxychloroquine is an illustrative example, with the drug being aggressively promoted as a cure even while emerging evidence demonstrated the contrary. This research analyzed how hydroxychloroquine discussions took place on Twitter from 21 to 28 April 2020, a key period in developments around the drug. We collected, in real time, tweets with “hydroxychloroquine” over this period, which resulted in a dataset of nearly one million tweets from over 350,000 Twitter accounts. Our content analysis provides specific details of how hydroxychloroquine was promoted and critiqued, and which accounts were tweeting. Findings showed a highly polarized environment with active bots and conspiracy propagators, where political perspectives dominated the Twittersphere in the place of science-focused discussions.","",""
"2021","Shouting Into the Wind: Medical Science versus “B.S.” in the Twitter Maelstrom of Politics and Misinformation About Hydroxychloroquine"," In the social media marketplace of ideas during the 2020 coronavirus pandemic, epidemiologists and other scientific and medical experts competed for attention with news media, government agencies, politicians, celebrities, and rank conspiracy theorists. However, everyone with a Twitter account was not equally qualified to speak knowledgeably about critical issues related to the outbreak, such as prevention and treatment. And, accurate information from informed sources can mean the difference between life and death. Our exploratory study addresses a simple, but important question: whose messages about the efficacy of hydroxychloroquine as a treatment for coronavirus were getting the most attention on Twitter? We provide a data visualization of Twitter activity for the period of 21 January through 21 May 2020 that shows users who tweeted about hydroxychloroquine, as well as who interacted with each of them (through likes, comments, retweets, etc.) to determine who were the most prominent voices on the network during a critical juncture of the outbreak. From our analysis, it appears that President Donald Trump’s handle ( @realDonaldTrump ) and other pro-Trump related accounts were the most influential voices on Twitter during this time of the crisis, rather than those from relevant experts, such as the Centers for Disease Control and Prevention ( @CDCgov ) or the National Institute of Allergy and Infectious Diseases ( @NIAIDnews ). ","",""
"2021","Demystifying the COVID-19 Infodemic: Conspiracies, Context, and the Agency of Users"," This article presents new empirical insights into what people do with conspiracy theories during crises. By suppressing the impulse to distinguish between truth and falsehood, which has characterized most scholarship on the COVID-19 “infodemic,” and engaging with claims surrounding two popular COVID-19 conspiracies—on 5G and on Bill Gates—in South Africa and Nigeria, we illustrate how conspiracies morph as they interact with different socio-political contexts. Drawing on a mixed-method analysis of more than 6 million tweets, we examine how, in each country, conspiracies have uniquely intersected with longer-term discourses and political projects. In Nigeria, the two conspiracies were both seized as opportunities to extend criticism to the ruling party. In South Africa, they produced distinctive responses: while the 5G conspiracy had limited buy-in, the Gates conspiracy resonated with deep-rooted resentment toward the West, corporate interests, and what is seen as a paternalistic attitude of some external actors toward Africa. These findings stress the importance of taking conspiracy theories seriously, rather than dismissing them simply as negative externalities of digital ecosystems. Situating conspiracies in specific dynamics of trust and mistrust can make an important difference when designing responses that are not limited to broadcasting truthful information, but can also enable interventions that account for deeply rooted sentiments of suspicion toward specific issues and actors, which can vary significantly across communities. ","",""
"2021","Polarization Over Vaccination: Ideological Differences in Twitter Expression About COVID-19 Vaccine Favorability and Specific Hesitancy Concerns"," Vaccine hesitancy has been a growing public health issue, but during COVID-19, understanding vaccine hesitancy and promote vaccine favorability takes on a troubling immediacy. With the growing political polarization on scientific issues, the COVID-19 vaccine-related sentiment has recently been divided across ideological lines. This study aims to understand how vaccine favorability and specific vaccine-related concerns including possible side effects, distrust in medical professionals, and conspiratorial beliefs concerning COVID-19 vaccines were articulated and transmitted by Twitter users from opposing ideological camps and with different follower scopes. Using a combination of computational approaches, including supervised machine-learning and structural topic modeling, we examined tweets surrounding COVID-19 vaccination ( N = 16,959) from 1 March to 30 June 2020. Results from linear mixed-effects models suggested that Twitter users high on conservative ideology and with a standard instead of large follower scope tend to express less favorable vaccine-related sentiments and talk more about vaccine side effects, distrust of medical professionals, and conspiracy theories. There is also an interaction effect where liberals with large follower scope expressed the least amount of distrust of medical professionals, whereas extreme conservatives expressed greater distrust for health professionals, regardless of their follower scope. Finally, structural topic modeling revealed distinct topical focuses among liberal and conservative users. Theoretical and practical implications for leveraging social media in effective health communication practice were discussed. ","",""
"2021","Sustained Online Amplification of COVID-19 Elites in the United States"," In the absence of clear, consistent guidelines about the COVID-19 pandemic in the United States, many people use social media to learn about the virus, public health directives, vaccine distribution, and other health information. As people individually sift through a flood of information online, they collectively curate a small set of accounts, known as crowdsourced elites, that receive disproportionate attention for their COVID-19 content. However, these elites are not all created equal: not all accounts have received the same attention during the pandemic, and various demographic and ideological groups have crowdsourced their own elites. Using a mixed-methods approach with a panel of Twitter users in the United States over the first year of the COVID-19 pandemic, we identify COVID-19 crowdsourced elites. We distinguish sustained amplification from episodic amplification and demonstrate that crowdsourced elites vary across demographics with respect to race, geography, and political alignment. Specifically, we show that different subpopulations preferentially amplify elites that are demographically similar to them, and that they crowdsource different types of elite accounts, such as journalists, elected officials, and medical professionals, in different proportions. In light of this variation, we discuss the potential for using the disproportionate online voice of crowdsourced COVID-19 elites to equitably promote public health information and mitigate misinformation across networked publics. ","",""
"2021","Online Social Endorsement and Covid-19 Vaccine Hesitancy in the United Kingdom"," We explore the implications of online social endorsement for the Covid-19 vaccination program in the United Kingdom. Vaccine hesitancy is a long-standing problem, but it has assumed great urgency due to the pandemic. By early 2021, the United Kingdom had the world’s highest Covid-19 mortality per million of population. Our survey of a nationally representative sample of UK adults ( N = 5,114) measured socio-demographics, social and political attitudes, media diet for getting news about Covid-19, and intention to use social media and personal messaging apps to encourage or discourage vaccination against Covid-19. Cluster analysis identified six distinct media diet groups: news avoiders, mainstream/official news samplers, super seekers, omnivores, the social media dependent, and the TV dependent. We assessed whether these media diets, together with key attitudes, including Covid-19 vaccine hesitancy, conspiracy mentality, and the news-finds-me attitude (meaning giving less priority to active monitoring of news and relying more on one’s online networks of friends for information), predict the intention to encourage or discourage vaccination. Overall, super-seeker and omnivorous media diets are more likely than other media diets to be associated with the online encouragement of vaccination. Combinations of (a) news avoidance and high levels of the news-finds-me attitude and (b) social media dependence and high levels of conspiracy mentality are most likely to be associated with online discouragement of vaccination. In the direct statistical model, a TV-dependent media diet is more likely to be associated with online discouragement of vaccination, but the moderation model shows that a TV-dependent diet most strongly attenuates the relationship between vaccine hesitancy and discouraging vaccination. Our findings support public health communication based on four main methods. First, direct contact, through the post, workplace, or community structures, and through phone counseling via local health services, could reach the news avoiders. Second, TV public information advertisements should point to authoritative information sources, such as National Health Service (NHS) and other public health websites, which should then feature clear and simple ways for people to share material among their online social networks. Third, informative social media campaigns will provide super seekers with good resources to share, while also encouraging the social media dependent to browse away from social media platforms and visit reliable and authoritative online sources. Fourth, social media companies should expand and intensify their removal of vaccine disinformation and anti-vax accounts, and such efforts should be monitored by well-resourced, independent organizations. ","",""
"2021","From “Nasa Lies” to “Reptilian Eyes”: Mapping Communication About 10 Conspiracy Theories, Their Communities, and Main Propagators on Twitter"," In recent years, conspiracy theories have pervaded mainstream discourse. Social media, in particular, reinforce their visibility and propagation. However, most prior studies on the dissemination of conspiracy theories in digital environments have focused on individual cases or conspiracy theories as a generic phenomenon. Our research addresses this gap by comparing the 10 most prominent conspiracy theories on Twitter, the communities supporting them, and their main propagators. Drawing on a dataset of 106,807 tweets published over 6 weeks from 2018 to 2019, we combine large-scale network analysis and in-depth qualitative analysis of user profiles. Our findings illustrate which conspiracy theories are prevalent on Twitter, and how different conspiracy theories are separated or interconnected within communities. In addition, our study provides empirical support for previous assertions that extremist accounts are being “deplatformed” by leading social media companies. We also discuss how the implications of these findings elucidate the role of societal and political contexts in propagating conspiracy theories on social media. ","",""
"2021","A Conspiracy of Data: QAnon, Social Media, and Information Visualization"," Seeing is believing, so goes the cliché. In our extremely online world, the particular nexus between visual information and political belief has become one of the thorniest challenges to truth. We live in an extremely visual world in which we navigate social media, search engines, platforms, interfaces, icons, memes, and smartphones. Despite the fact that we navigate visual information at an astounding rate, we have not nationally developed literacies to debunk bad information. I argue that we are witnessing a confluence between extremely online, crowd-sourced conspiracies, whose adherents possess a high capacity for online information gathering, and visualization, meant to communicate data about our world effectively and accurately through optical means which has been co-opted for information warfare. Deploying such informatics further legitimates bizarre, unhinged theories about political reality. QAnon, the extremely online conspiracy theory that has cast its shadow over the Internet, relies exclusively on information visualization to communicate its message and is symptomatic of our inability to combat misinformation that mimics the methods of data analysis and information literacy. I argue that QAnon’s success—indeed, its very existence—relies on (at least) two principal factors: (1) QAnon relies, intentionally or no, on a slippage between data and information that obscures the interventions by Q and Q’s anons in leveraging information warfare, and (2) QAnon supports such a slippage with complex and interactive visualizations of bad information, thereby accelerating apophenia, the tendency to see linkages between random events and data points. ","",""
"2021","Hunting Conspiracy Theories During the COVID-19 Pandemic"," The fear of the unknown combined with the isolation generated by COVID-19 has created a fertile environment for strong disinformation, otherwise known as conspiracy theories, to flourish. Because conspiracy theories often contain a kernel of truth and feature a strong adversarial “other,” they serve as the perfect vehicle for maligned actors to use in influence campaigns. To explore the importance of conspiracies in the spread of dis-/mis-information, we propose the usage of state-of-the-art, tuned language models to classify tweets as conspiratorial or not. This model is based on the Bidirectional Encoder Representations from Transformers (BERT) model developed by Google researchers. The classification method expedites analysis by automating a process that is currently done manually (identifying tweets that promote conspiracy theories). We identified COVID-19 origin conspiracy theory tweets using this method and then used social cybersecurity methods to analyze communities, spreaders, and characteristics of the different origin-related conspiracy theory narratives. We found that tweets about conspiracy theories were supported by news sites with low fact-checking scores and amplified by bots who were more likely to link to prominent Twitter users than in non-conspiracy tweets. We also found different patterns in conspiracy vs. non-conspiracy conversations in terms of hashtag usage, identity, and country of origin. This analysis shows how we can better understand who spreads conspiracy theories and how they are spreading them. ","",""
"2021","#VACHINA: How Politicians Help to Spread Disinformation About COVID-19 Vaccines","This paper focuses on how Brazilian politicians helped to spread disinformation about Covid-19 vaccines, discussing legitimation strategies and actors that played a significant role on Twitter and Facebook. Based on data gathered through CrowdTangle and Twitter API, we selected the 250 most shared/retweeted posts for each dataset (n=500) and examined if they contained disinformation, who posted it, and what strategy was used to legitimize this discourse. Our findings indicate that politicians and hyperpartisan accounts have a key influence in validating the Brazilian president’s populist discourse through rationalization (pseudo-science) and denunciation (against the vaccine). The political frame also plays an important role in disinformation messages.","",""
"2022","Telegram and Digital Methods","Introduction The study of online conspiracy theory communities presents unique methodological challenges. Online conspiracy theorists often adhere to an individualistic knowledge culture of “doing one’s own research” (Fenster 158). This results in a decentralised landscape of theories, narratives and communities that challenges conventional top-down approaches to analysis. Moreover, conspiracy theories tend to be discussed on the fringes of the online ecosystem, in chat groups, small subcultural Web forums and away from mainstream social media platforms such as Facebook and Twitter (Frenkel; see also De Zeeuw et al.). In this context, the messaging app Telegram has developed into a particularly prominent space (Rogers, “Deplatforming”; Urman and Katz). On the one hand, this platform is not quite part of the same mainstream as Facebook or Twitter, owing in part to its emphasis on security, “social privacy”, and lack of central moderation (Rogers, “Deplatforming”). But it is also not quite an “alternative social medium” (Gehl), as it does not position itself in opposition to mainstream platforms per se, nor does its business model centered around investor funding and advertisements present a break from the “dominant political economy” (ibid.). This ambiguous position might account for Telegram’s wide adoption, as well as its status as a relatively safe haven for communities deplatformed elsewhere – including a lively ecosystem of conspiracy theory communities (La Morgia et al.). Because Telegram communities are distributed over a wide range of channels and chat groups, they cannot always be investigated using existing analytical approaches for social media research. Confronting this challenge, we propose and discuss a method for studying Telegram communities that repurposes the “methods of the medium” (Rogers, Digital Methods). Specifically, our method appropriates Telegram’s feature of forwarding messages from one group to another to discover interlinked distributed communities, collect data from these communities for close reading, and map their information sharing practices.  In this article, we will first present this approach and illustrate the types of analyses the collected data might afford in relation to a brief case study on Dutch-speaking conspiracy theories. In this short illustration, we map the convergence of right-wing and conspiratorial communities, both structurally and discursively. As Vieten discusses, “digital pandemic populism during lockdown might have pushed further the mobilisation of the far right, also on the streets”. In the Dutch context there has been a demonstrated connection between the two. Because of this connection, we were drawn to the questions of what these entanglements might look like in a relatively unmoderated Telegram environment. We then proceed to discuss some strengths and limitations, identify avenues for future research, and conclude with some ethical, methodological and epistemological reflections. Overview of Method Our method first combines expert knowledge, and the affordances of the Telegram app’s ‘search’ function to retrieve a set of channels mentioning specific politicians or political parties, as well as other marked terms that might point towards far-right or conspiratorial content. This includes wakker (awakened), variations of batavier and geus (nationalist demonyms), names of known far right politicians (such as The Netherlands’ Thierry Baudet and Flanders’ Dries Van Langenhove) and conspiracy theory activists, and volk (a term meaning roughly “our people”). As this approach precludes discovery of related groups that do not match the queries exactly, this initial curated list is then supplemented with channels advertised elsewhere, such as those featured on the Websites of far-right politicians and organisations, as well as channels covered in mainstream news media. This yields an initial expert list of channels, in our sample case of Dutch-speaking right-wing and conspiracist actors comprising 50 items. One might stop here, and collect data for this manually curated list of groups, as in Nikkhah et al.’s study of Telegram use among the Iranian diaspora in the United States, or Davey and Weinberg’s analysis of far-right groups used in the US military. But this would exclude any groups not known by the researchers; and groups are not always easy to naively discover on Telegram. Because of this, in a subsequent step, we expanded the initial set of relevant Telegram channels by crawling posts in these channels that were forwarded from other channels, constituting links between these channels. We used a custom crawler based on the open source library Selenium, which allows one to control a browser programmatically. The browser was then made to scroll through the Web-based view of the selected channels (e.g. https://t.me/s/durov). In principle, all messages ever posted in a channel are available in this view. We then follow those links, and store the names of the linked channels. Overall, this method thus presumes that if a channel forwards a message from another channel there will be some overlap in terms of topic of discussion between both, making the newly discovered channel similarly relevant to the analysis. This results in a network-like representation of connections between channels. In the context of our case study, this process expands our initial seed list to a list of over 215 relevant public channels, after discarding groups that are not germane to the case study, i.e. those that were not related to far-right or conspiracy theory-related communities. To verify this, channels were inductively coded by a team of four researchers after capture. We then repeat the data collection for this new list of channels, retrieving forwarded messages from over 370,000 total messages spanning the period 2017-2021. This dataset then serves as a starting point for structural analysis of the wider context of the community, aspects of which will be illustrated in the next section.  Illustration  Emphasising the value of a “quali-quanti” (Venturini and Latour) approach we offer a tentative analysis of the decentralised Dutch-speaking conspiracist narratives and communities on Telegram, and in a broader sense observe what such a distributed community may look like on the platform. This then suggests the various affordances which a dataset collected with this method can offer.  Fig. 1: Network visualisation of collected channels (depth: 1) including channels forwarded from (4354 nodes). Nodes are sized and coloured by degree (amount of connections to other channels).  A first observation that can be made concerns the topology of the network of channels we found (see fig. 1). A network analysis is a suitable distant reading approach for this kind of data, because it “maps and measures formal and informal relationships … viz., who knows whom, and who shares what information and knowledge with whom” (Serrat 40). It is a type of analysis that can reveal the relevance and positioning of actors and narratives within the data. In our network visualisation, we use the ForceAtlas2 algorithm (Jacomy et al.) to position the nodes. This algorithm makes more connected nodes “gravitate” towards each other; the more central a node is, the more connected it is to the rest of the network, roughly speaking (Figure 1). Highlighting the channels representing political parties shows, for example, that while the Dutch far right party FvD (FVDNL) is quite central (connected), this is not the case for the Flemish far right politician Dries Van Langenhove (kiesdries). This suggests that compared to a similar Flemish politician, the Dutch FvD is a more prominent part of the general conspiracist discussion – which is then perhaps more overtly politicised in the Dutch context. We can additionally discern channels that we might label “content aggregators”, which forward large numbers of messages from other channels but post comparatively little original content, occupying a relatively central place in the wider network. These content aggregators play an important structural role in the network, as other channels might forward messages from these collections on a “pick and choose” basis. More abstractly, they also serve to confirm thematic similarity between the channels messages are forwarded from, with the owners of the aggregator channels playing the role of a curator that collects interesting content about a certain topic of interest.  Furthermore, our data reveal that the network is highly dynamic. As forwarded messages are timestamped, we can plot the graph at different moments in time. When comparing changes over a year, we can observe a significant growth in the number of channels that connect to the network particularly between 2020 and 2021 (see fig. 2). This growth, and the associated diversity of the network, can be attributed to Telegram’s role as a haven for actors that were deplatformed (or present themselves as targets of deplatforming and censorship) from other social media; a “platform of last resort”. Previous research has for instance indicated that a number of alt-right fringe actors moved to Telegram after being deplatformed from Facebook or Twitter (Rogers, “Deplatforming”). It can be hypothesised that events such as Donald Trump’s removal from Twitter around the time of the January 2021 Capitol riots might similarly have inspired other actors to move to Telegram in response to the platform policies of mainstream social media.  Fig. 2: The channel network based on messages sent before June 2020 (334 nodes). Same layout and parameters as in Figure 1 (not to scale). Nodes also appearing in Figure 1 are highlighted. The structure of the network (in fig. 1) can also be used to discern ‘sub-communities’, which forward messages mutually but have relatively few links to the broader network. These can then be analysed qualitatively. This may then reveal that Flemish groups that oppose COVID-19 policies are less connected with the far right, whereas such groups that can be identified as Dutch seem to merge more easily with far-right channels. As discussed, this is also suggested by the position of FVDNL, the Dutch far-right political party Forum voor democratie (FvD) which is central in the network. On this level, this suggests that structurally, Dutch far-right parties are more explicitly affiliating themselves with conspiracy-related channels than Flemish parties. Actual textual analyses of the channels’ posts and images, however, could offer a more nuanced picture, whereby structurally unconnected channels might still share common harmful narratives, spanning anti-progressive discourse, anti-mainstream sentiments, anti-government discourse, and evocations of prominent conspiracy theories such as QAnon and The Great Reset. These structural analyses then present a number of possibilities for further content analysis, where one might for example “zoom in” on Dutch far-right groups in particular, and qualitatively study images posted therein to identify salient narratives and positions. Discussion  Methodological Gains Variations of the proposed approach have been used in other work (e.g. Hashemi and Chahooki; La Morgia et al.). Most prominently, the Pushshift Telegram Dataset (Baumgartner et al.) comprises a large dataset of channel metadata, author metadata and messages. This dataset was collected by discovering new channels from an initial seed list of approximately 300 channels using forwarded messages, and then collecting messages from these channels. While there is great archival value in the resulting datasets, our approach differs from these earlier approaches in a number of instructive ways. Like other work we appropriate an affordance of Telegram – forwarded messages – for our own research purposes, but we purposely limit the extent of “following” these forwarded messages. Though one could keep following links indefinitely, not every link is a link that is structural to the distributed community of users that is of interest here. Though more extensive crawling might reveal ever more channels and associated data, these are also increasingly unlikely to be related to the initial topic of interest, and are in any case further removed from the users of the initial seed groups. For this reason, we use a relatively shallow crawl depth and only retain links up to two “hops” away from the initial seed channels. This trades a higher number of crawled channels for a higher likelihood that the captured channels are indeed relevant to the case study. The suitable crawl depth would differ from case to case. In our case, it was established empirically through pilot crawls, which were stopped once collected groups appeared to no longer be strongly connected to the initial seed groups by topic.  Datasets of this type are often also difficult to reproduce or qualify. For example for the datasets compiled by La Morgia et al. as well as for Baumgartner et al., the original seed list is not provided. Because of this, it is impossible to see where the network of found groups originates and how it might be biased one way or another. We suggest that where possible, this seed list is documented and shared. In our case, this would be particularly important as the seeds represent an intentional and explicit bias; that is, Dutch-speaking conspiracy-themed and far-right groups. If the starting point of the crawl is documented, one could potentially re-collect the data later from the same starting points and compare results to those found initially, allowing for longitudinal analyses of the topology of these communities. Ethics The method described here does not deal with personally identifiable information (PII) per se; one can map the channel network on a structural level without collecting user data or analysing specific messages, when purely tracing the origin of forwarded messages. It should be noted, however, that in the process of collecting these structural data, one can potentially go further. For example, it is possible to scrape the full content of messages. When also including chat groups, user details including (user-provided) full names are also available. Their inclusion in (public) datasets should be subject to closer scrutiny than that of public channels, as the former may represent conversations had under the assumption that this conversation was more or less off the record; while the latter are explicitly intended for information broadcast. Even if many of these chat groups are technically public, we should consider that """"even if users are aware of being observed by others, they do not consider the possibility that their actions and interactions may be documented and analysed in detail at a later occasion"""" (Sveningsson Elm 77). In many cases, a (structural) analysis of only channels strikes a good balance between collecting representative data and respecting the privacy of those who produce the collected data. Avenues for Future Research The method may be expanded in a number of ways. One could, as discussed, increase the amount of crawl iterations, which would expand the network at the potential cost of case-specificity. A larger seed list could also increase the quantity of the data, though the effect of this can often be limited, as a relatively limited amount of channels forward messages from other channels. Links between channels could be collected not only from forwarded messages, as we do here, but also via other repurposed Telegram features such as channel invitation links or simple hyperlinks to other channels found in message content. The latter would require more fine-grained parsing of the message texts through natural language processing, for example, as a hyperlink can suggest a wider range of connections than an intentionally forwarded message. Additionally, and as previously mentioned, one could include not only public channels but also public chat groups, which are often linked to these channels and offer a space for people to discuss the content posted in them. While this can be an attractive way of acquiring extra data, we forego this in our example. As discussed, there are ethical trade-offs to consider when deciding to work with data from groups; but, it can be argued that Telegram channels represent an explicit “broadcast” style of communication (Shehabat et al.). Because the channel owner(s) decide what is worthy of sharing, one can reasonably assume that if one “follows the medium” here, all content retrieved from a channel will be somewhat relevant to the channel's purported theme. Conversely, discourse in chat groups might be expected to meander into a variety of directions and can easily include many (forwarded) messages only tangentially related to the case study of interest.  Conclusion In this article we have sought to present one methodological approach to studying communities on Telegram. Rather than presenting a thorough case study or a definitive analysis of the Telegram-based community we discuss, our goal was to demonstrate the method's benefits but also its potential shortcomings, avenues of further development, and what types of analysis data collected with it might afford. A cursory analysis of the fringe community we studied here shows how with such data one can map a given community or set of communities on a structural level, which may then be used to demarcate areas of interest for further content analysis. The observations presented in this article are far from a complete picture of the data collected, but can serve as suggestions for analytical avenues one might venture down in a more substantive analysis.  Beyond these observations, our repurposing of “the methods of the medium” (Rogers, Digital Methods) through forwarded messages allows us to contribute an empirically informed reflection on the possibilities and limitations of studying conspiracist information sharing practices on Telegram. Our method for instance highlights tensions between public and private knowledge, whereby we only consider information from public channels, and for technical and ethical reasons omit Telegram’s closed-off, private chat groups from our analysis. Our method of sourcing channels through forwarded messages does not preclude the existence of isolated channels or clusters of channels that, for a lack of forwarded messages from channels that were already identified, elude the scope of such snowballing efforts. Along the same lines, one could imagine that a deeper, more far-reaching crawl would reveal some strange bedfellows for the initial seed that were not part of our a priori understanding and hypotheses concerning the communities of interest. All of these considerations represent choices that may be taken differently depending on the case study at hand.   Statement on Data and Ethics  The analysis on offer in this article is limited to names of public channels on Telegram and we purposely refrain from citing channel names or analysing specific messages so they cannot be traced back to single persons. Our analysis does not comprise live subjects or PII, and thus did not require ERB clearance from our respective institutions. The anonymised dataset described above is available upon request via Zenodo at https://doi.org/10.5281/zenodo.6344795. Acknowledgements We would like to thank Nathalie van Raemdonck (Vrije Universiteit Brussel) and Jasmin Seijbel (Erasmus Universiteit Rotterdam) for their contributions to the empirical work underlying this article. References Baumgartner, Jason, et al. “The Pushshift Telegram Dataset.” Proceedings of the International AAAI Conference on Web and Social Media 14 (2020): 840–847. Blondel, Vincent D., et al. “Fast Unfolding of Communities in Large Networks.” Journal of Statistical Mechanics: Theory and Experiment 2008.10 (2008): P10008. Davey, Jacob, and Dana Weinberg. Influence: Discussions of the US Military in Extreme Right-Wing Telegram Channels. ISD Global, 2021. De Zeeuw, Daniel et al. “Tracing Normiefication: A Cross-Platform Analysis of the QAnon Conspiracy Theory.” First Monday 25.1 (2020).  Fenster, Mark. Conspiracy Theories. Minneapolis: U of Minnesota P, 2008.  Frenkel, Sheera. “Facebook Amps Up Its Crackdown on QAnon.” The New York Times 6 Oct. 2020, sec. Technology. &lt;https://www.nytimes.com/2020/10/06/technology/facebook-qanon-crackdown.html&gt;.  Gehl, Robert W. “The Case for Alternative Social Media.” Social Media + Society 1.2 (2015). Hashemi, Ali, and Mohammad Ali Zare Chahooki. “Telegram Group Quality Measurement by User Behavior Analysis.” Social Network Analysis and Mining 9.1 (2019): 33. Jacomy, Mathieu, et al. “ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software.” PLOS ONE 9.6 (2014): e98679. La Morgia, Massimo et al. “Uncovering the Dark Side of Telegram: Fakes, Clones, Scams, and Conspiracy Movements.” arXiv:2111.13530 [cs] (2021). &lt;http://arxiv.org/abs/2111.13530&gt;.  Nikkhah, Sarah, et al. “Coming to America: Iranians’ Use of Telegram for Immigration Information Seeking.” Aslib Journal of Information Management 72.4 (2020): 561–585. Rogers, Richard. “Deplatforming: Following Extreme Internet Celebrities to Telegram and Alternative Social Media.” European Journal of Communication 35.3 (2020): 213–229. ———. Digital Methods. Cambridge: MIT P, 2013. Serrat, Olivier. “Social Network Analysis.” In Knowledge Solutions: Tools, Methods, and Approaches to Drive Organizational Performance. Ed. Olivier Serrat. Singapore: Springer, 2017. 39–43. &lt;https://doi.org/10.1007/978-981-10-0983-9_9&gt;.  Shehabat, Ahmad, Teodor Mitew, and Yahia Alzoubi. “Encrypted Jihad: Investigating the Role of Telegram App in Lone Wolf Attacks in the West.” Journal of Strategic Security 10.3 (2017): 27–53. Sveningsson Elm, Malin. “How Do Various Notions of Privacy Influence Decisions in Qualitative Internet Research?” In Internet Inquiry: Conversations about Method. Eds. Annette Markham and Nancy Baym. SAGE, 2009. 69–97. Urman, Aleksandra, and Stefan Katz. “What They Do in the Shadows: Examining the Far-Right Networks on Telegram.” Information, Communication &amp; Society (2020). Venturini, Tommaso, and Bruno Latour. “The Social Fabric: Digital Footprints and Quali-quantitative Methods.” In Proceedings of Futur en Seine 2009: The Digital Future of the City. Festival for Digital Life and Creativity, 2010. 87-101. Vieten, Ulrike M. “The ‘New Normal’ and ‘Pandemic Populism’: The COVID-19 Crisis and Anti-Hygienic Mobilisation of the Far-Right.” Social Sciences 9.9 [165] (2020).","",""
"2022","From COVID-19 Treatment to Miracle Cure","Introduction Medical misinformation and conspiracies have thrived during the current infodemic as a result of the volume of information people have been exposed to during the disease outbreak. Given that SARS-CoV-2 (COVID-19) is a novel coronavirus discovered in 2019, much remains unknown about the disease. Moreover, a considerable amount of what was originally thought to be known has turned out to be inaccurate, incomplete, or based on an obsolete knowledge of the virus. It is in this context of uncertainty and confusion that conspiracies flourish. Michael Golebiewski and danah boyd’s work on ‘data voids’ highlights the ways that actors can work quickly to produce conspiratorial content to fill a void. The data void absent of high-quality data surrounding COVID-19 provides a fertile information environment for conspiracies to prosper (Chou et al.). Conspiracism is the belief that society and social institutions are secretly controlled by a powerful group of corrupt elites (Douglas et al.). Michael Barkun’s typology of conspiracy reveals three components: 1) the belief that nothing happens by accident or coincidence; 2) nothing is as it seems: the """"appearance of innocence"""" is to be suspected; 3) the belief that everything is connected through a hidden pattern. At the heart of conspiracy theories is narrative storytelling, in particular plots involving influential elites secretly colluding to control society (Fenster). Conspiracies following this narrative playbook have flourished during the pandemic. Pharmaceutical corporations profiting from national vaccine rollouts, and the emergency powers given to governments around the world to curb the spread of coronavirus, have led some to cast these powerful commercial and State organisations as nefarious actors – 'big evil' drug companies and the ‘Deep State’ – in conspiratorial narratives. Several drugs believed to be potential treatments for COVID-19 have become entangled with conspiracy. At the start of the pandemic scientists experimented with repurposing existing drugs as potential treatments for COVID-19 because safe and effective vaccines were not yet available. A series of antimicrobials with potential activity against SARS-CoV-2 were tested in clinical trials, including lopinavir/ritonavir, favipiravir and remdesivir (Smith et al.). Only hydroxychloroquine and ivermectin transformed from potential COVID treatments into conspiracy objects. This article traces how the hydroxychloroquine and ivermectin conspiracy theories were amplified in the news media and online. It highlights how debunking processes contribute to amplification effects due to audience segmentation in the current media ecology. We conceive of these amplification and debunking processes as key components of a ‘Conspiracy Course’ (Baker and Maddox), identifying the interrelations and tensions between amplification and debunking practices as a conspiracy develops, particularly through mainstream news, social media and alternative media spaces. We do this in order to understand how medical claims about potential treatments for COVID-19 succumb to conspiracism and how we can intervene in their development and dissemination. In this article we present a commentary on how public discourse and actors surrounding two potential treatments for COVID-19: the anti-malarial drug hydroxychloroquine and the anti-parasitic drug ivermectin became embroiled in conspiracy. We examine public discourse and events surrounding these treatments over a 24-month period from January 2020, when the virus gained global attention, to January 2022, the time this article was submitted. Our analysis is contextually informed by an extended digital ethnography into medical misinformation, which has included social media monitoring and observational digital field work of social media sites, news media, and digital media such as blogs, podcasts, and newsletters. Our analysis focusses on the role that public figures and influencers play in amplifying these conspiracies, as well as their amplification by some wellness influencers, referred to as “alt.health influencers” (Baker), and those affiliated with the Intellectual Dark Web, many of whom occupy status in alternative media spaces. The Intellectual Dark Web (IDW) is a term used to describe an alternative influence network comprised of public intellectuals including the Canadian psychologist Jordan Peterson and the British political commentator Douglas Murray. The term was coined by the American mathematician and podcast host Eric Weinstein, who described the IDW as a group opposed to “the gated institutional narrative” of the mainstream media and the political establishment (Kelsey). As a consequence, many associated with the IDW use alternative media, including podcasts and newsletters, as an """"eclectic conversational space"""" where those intellectual thinkers excluded from mainstream conversational spaces in media, politics, and academia can “have a much easier time talking amongst ourselves” (Kelsey). In his analysis of the IDW, Parks describes these figures as """"organic"""" intellectuals who build identification with their audiences by branding themselves as """"reasonable thinkers"""" and reinforcing dominant narratives of polarisation. Hence, while these influential figures are influencers in so far as they cultivate an online audience as a vocation in exchange for social, economic and political gain, they are distinct from earlier forms of micro-celebrity (Senft; Marwick) in that they do not merely achieve fame on social media among a niche community of followers, but appeal to those disillusioned with the mainstream media and politics. The IDW are contrasted not with mainstream celebrities, as is the case with earlier forms of micro-celebrity (Abidin Internet Celebrity), but with the mainstream media and politics. A public figure, on the other hand, is a “famous person” broadcast in the media. While celebrities are public figures, public figures are not necessarily celebrities; a public figure is ‘a person of great public interest or familiarity’, such as a government official, politician, entrepreneur, celebrity, or athlete. Analysis In what follows we explore the role of influencers and public figures in amplifying the hydroxychloroquine and ivermectin conspiracy theories during the pandemic. As part of this analysis, we consider how debunking processes can further amplify these conspiracies, raising important questions about how to most effectively respond to conspiracies in the current media ecology. Discussions around hydroxychloroquine and ivermectin as potential treatments for COVID-19 emerged in early 2020 at the start of the pandemic when people were desperate for a cure, and safe and effective vaccines for the virus were not yet publicly available. While claims concerning the promising effects of both treatments emerged in the mainstream, the drugs remained experimental COVID treatments and had not yet received widespread acceptance among scientific and medical professionals. Much of the hype around these drugs as COVID “cures” emerged from preprints not yet subject to peer review and scientific studies based on unreliable data, which were retracted due to quality issues (Mehra et al.). Public figures, influencers, and news media organisations played a key role in amplifying these narratives in the mainstream, thereby extending the audience reach of these claims. However, their transformation into conspiracy objects followed different amplification processes for each drug. Hydroxychloroquine, the “Game Changer” Hydroxychloroquine gained public attention on 17 March 2020 when the US tech entrepreneur Elon Musk shared a Google Doc with his 40 million followers on Twitter, proposing “maybe worth considering chloroquine for C19”. Musk’s tweet was liked over 50,200 times and received more than 13,500 retweets. The tweet was followed by several other tweets that day in which Musk shared a series of graphs and a paper alluding to the “potential benefit” of hydroxychloroquine in in vitro and early clinical data. Although Musk is not a medical expert, he is a public figure with status and large online following, which contributed to the hype around hydroxychloroquine as a potential treatment for COVID-19. Following Musk’s comments, search interest in chloroquine soared and mainstream media outlets covered his apparent endorsement of the drug. On 19 March 2020, the Fox News programme Tucker Carlson Tonight cited a study declaring hydroxychloroquine to have a “100% cure rate against coronavirus” (Gautret et al.). Within hours another public figure, the then-US President Donald Trump, announced at a White House Coronavirus Task Force briefing that the FDA would fast-track approval of hydroxychloroquine, a drug used to treat malaria and arthritis, which he said had, “tremendous promise based on the results and other tests”. Despite the Chief Medical Advisor to the President, Dr Anthony Fauci, disputing claims concerning the efficacy of hydroxychloroquine as a potential therapy for coronavirus as “anecdotal evidence”, Trump continued to endorse hydroxychloroquine describing the drug as a “game changer”:  HYDROXYCHLOROQUINE &amp; AZITHROMYCIN, taken together, have a real chance to be one of the biggest game changers in the history of medicine.  He said that the drugs should be  put in use IMMEDIATELY. PEOPLE ARE DYING, MOVE FAST, and GOD BLESS EVERYONE!  Trump’s tweet was shared over 102,800 times and liked over 384,800 times. His statements correlated with a 2000% increase in prescriptions for the anti-malarial drugs hydroxychloroquine and chloroquine in the US between 15 and 21 March 2020, resulting in many lupus patients unable to source the drug. There were also reports of overdoses as individuals sought to self-medicate with the drug to treat the virus. Once Trump declared himself a proponent of hydroxychloroquine, scientific inquiry into the drug was eclipsed by an overtly partisan debate. An analysis by Media Matters found that Fox News had promoted the drug 109 times between 23 and 25 March 2020, with other right wing media outlets following suit. The drug was further amplified and politicised by conservative public figures including Trump’s attorney Rudy Giuliani, who claimed on 27 March 2020 that “hydroxychloroquine has been shown to have a 100% effective rate in treating COVID-19”, and Brazil’s President, Jair Bolsonaro, who shared a Facebook post on 8 July 2020 admitting to taking the drug to treat the virus: “I’m one more person for whom this is working. So I trust hydroxychloroquine”. In addition to these conservative political figures endorsing hydroxychloroquine, on 27 July 2020 the right-wing syndicated news outlet Breitbart livestreamed a video depicting America’s Frontline Doctors – a group of physicians backed by the Tea Party Patriots, a conservative political organisation supportive of Trump – at a press conference outside the US Supreme Court in Washington. In the video, Stella Immanuel, a primary care physician in Texas, said “You don’t need masks…There is prevention and there is a cure!”, explaining that Americans could resume their normal lives by preemptively taking hydroxychloroquine. The video was retweeted by public figures including President Trump and Trump’s son Donald Trump Jr., before going viral reaching over 20 million users on Facebook. The video explicitly framed hydroxychloroquine as an effective “cure” for COVID-19 suppressed by “fake doctors”, thereby transferring it from potential treatment to a conspiracy object. These examples not only demonstrate the role of prominent public figures in amplifying conspiratorial claims about hydroxychloroquine as an effective cure for COVID-19, they reveal how these figures converted the drug into an “article of faith” divorced from scientific evidence. Consequently, to believe in its efficacy as a cure for COVID-19 demonstrated support for Trump and ideological skepticism of the scientific and medical establishment. Ivermectin, the “Miracle Cure” Ivermectin followed a different amplification trajectory. The amplifying process was primarily led by influencers in alternative media spaces and those associated with the IDW, many of whom position themselves in contrast to the mainstream media and politics. Despite scientists conducting clinical trials for ivermectin in early 2020, the ivermectin conspiracy peaked much later that year. On 8 December 2020, the pulmonary and ICU specialist Dr. Pierre Kory testified to the US Senate Committee about I-MASK: a prevention and early outpatient treatment protocol for COVID-19. During the hearing, Kory claimed that “ivermectin is effectively a ‘miracle drug’ against COVID-19”, which could end the pandemic. Kory’s depiction of ivermectin as a panacea, and the subsequent media hype, elevated him as a public figure and led to an increase in public demand for ivermectin in early 2021. This resulted in supply issues and led some people to seek formulations of the drug designed for animals, which were in greater supply and easier to access. Several months later in June 2021, Kory’s description of ivermectin as a “miracle cure” was amplified by a series of influencers, including Bret Weinstein and Joe Rogan, both of whom featured Kory on their podcasts as a key public figure in the fight against COVID Conspiratorial associations with ivermectin were further amplified on 9 July 2021 when Bret Weinstein appeared on Fox Nation's Tucker Carlson Today claiming he had “been censored for raising concerns about the shots and the medical establishment's opposition to alternative treatments”. The drug was embroiled in further controversy on 1 September 2021 when Joe Rogan shared an Instagram post explaining that he had taken ivermectin as one of many drugs to treat the virus. In the months that followed, Rogan featured several controversial scientists on his podcast who implied that ivermectin was an effective COVID “cure” suppressed as part of a global agenda to promote vaccine uptake. These public figures included Dr Robert Malone, an American physician who contributed to the development of mRNA technology, and Dr Peter McCullough, an American cardiologist with expertise in vaccines. As McCullough explained to Rogan in December 2021:  it seemed to me early on that there was an intentional very comprehensive suppression of early treatment in order to promote fear, suffering, isolation, hospitalisation and death and it seemed to be completely organised and intentional in order to create acceptance for and then promote mass vaccination.  McCullough went on to imply that the pandemic was planned and that vaccine manufacturers were engaged in a coordinated response to profit from mass vaccination. Consequently, whereas conservative public figures, such as Trump and Bolsonaro, played a primary role in amplifying the hype around hydroxychloroquine as a COVID cure and embroiling it in a political and conspiratorial narrative of collusion, influencers, especially those associated with alternative media and the IDW, were crucial in amplifying the ivermectin conspiracy online by platforming controversial scientists who espoused the drug as a “miracle cure”, which could allegedly end the pandemic but was being suppressed by the government and medical establishment. Debunking  Debunking processes refuting the efficacy of these drugs as COVID “cures” contributed to the amplification of these conspiracies. In April 2020 the paper endorsing hydroxychloroquine that Trump tweeted about a week earlier was debunked. The debunking process for hydroxychloroquine involved a series of statements, papers, randomised clinical trials and retractions not only rejecting the efficacy of hydroxychloroquine, but suggesting it was unsafe and had the potential to cause harm (Boulware et al.; Mehra; Voss). In April 2020, the FDA released a statement cautioning against the use of hydroxychloroquine for COVID-19 outside of a hospital setting or a clinical trial due to risk of heart rhythm problems, and in June the FDA revoked its emergency use authorisation to treat COVID-19 in certain hospitalised patients. The debunking process was not limited to fact-based claims, it also involved satire and ridicule of those endorsing the drug as a treatment for COVID-19. Given the politicisation of the drug, much of this criticism was directed at Trump, as a key proponent of the drug, and Republicans in general, both of whom were cast as scientifically illiterate. The debunking process for ivermectin was similarly initiated by scientific and medical authorities who questioned the efficacy of ivermectin as a COVID-19 treatment due to reliability issues with trials and the quality of evidence (Lawrence). In response to claims that supply issues led people to seek formulations of the drug designed for animals, in April 2021 the FDA released a statement cautioning people not to take ivermectin to prevent or treat COVID-19:  While there are approved uses for ivermectin in people and animals, it is not approved for the prevention or treatment of COVID-19 … . People should never take animal drugs … . Using these products in humans could cause serious harm.  The CDC echoed this warning, claiming that “veterinary formulations intended for use in large animals such as horses, sheep, and cattle can be highly concentrated and result in overdoses when used by humans”. Many journalists and Internet users involved in debunking ivermectin reduced the drug to horse paste. Social media feeds debunking ivermectin were filled with memes ridiculing those consuming “horse dewormer”. Mockery of those endorsing ivermectin extended beyond social media, with the popular US sketch comedy show Saturday Night Live featuring a skit mocking Joe Rogan for consuming “horse medicine” to treat the virus. The skit circulated on social media in the following days, further deriding advocates of the drug as a COVID cure as not only irresponsible, but stupid. This type of ridicule, visually expressed in videos and Internet memes, fuelled polarisation. This polarisation was then weaponised by influencers associated with the IDW to sell ivermectin as a “miracle drug” suppressed by the medical and political establishment, thereby embroiling the drug further in conspiracy (Baker and Maddox). This type of opportunistic marketing is not intended for a mass audience. Instead, audiences are taking advantage of what Crystal Abidin refers to as “silosociality”, wherein content is tailored for specific subcommunities, which are not necessarily “accessible” or “legible” to outsiders (Abidin Refracted Publics 4). This dynamic both reflects and reinforces the audience segmentation that occurs in the current media ecology by virtue of alternative media with mockery and ridicule strengthening in- and out-group dynamics.   Conclusion In this article we have traced how hydroxychloroquine and ivermectin moved from promising potential COVID-19 treatments to objects tainted by conspiracy. Despite common associations of conspiracy theories with the fringe, both the hydroxychloroquine and ivermectin conspiracy theories emerged in the mainstream, amplified across mainstream social networks with the help of influencers and public figures whose claims were further amplified by the news media commenting on their apparent endorsement of these drugs as COVID cures. Whereas hydroxychloroquine was politicised as a result of controversial public figures and right-wing media outlets endorsing the drug and the conspiratorial narrative espoused by America’s Frontline Doctors, notably much of the conspiracy around ivermectin shifted to alternative media spaces amplified by influencers disillusioned with the mainstream media. We have demonstrated how debunking processes, which sought to discredit these drugs as potential treatments for COVID-19, often ridiculed those who endorsed them, further polarising discussions involving these treatments and pushing advocates to the extreme. By encouraging proponents of these treatments to retreat to alternative media spaces, such as podcasts and newsletters, polarisation strengthened in-group dynamics, assisting the ability for opportunistic influencers to weaponise these conspiracies for social, economic, and political gain. These findings raise important questions about how to effectively counter conspiracies. When debunking not only refutes claims but ridicules advocates, debunking can have unintended consequences by strengthening in-group dynamics and fuelling the legitimacy of conspiratorial narratives. References Abidin, Crystal. Internet Celebrity: Understanding Fame Online. Emerald Group Publishing, 2018. Abidin, Crystal. """"From ‘Networked Publics’ to ‘Refracted Publics’: A Companion Framework for Researching ‘below the Radar’ Studies."""" Social Media + Society 7.1 (2021). Baker, Stephanie Alice. """"Alt.Health Influencers: How Wellness Culture and Web Culture Have Been Weaponised to Promote Conspiracy Theories and Far-Right Extremism during the COVID-19 Pandemic."""" European Journal of Cultural Studies 25.1 (2022): 3-24. Baker, Stephanie Alice, and Alexia Maddox. “COVID-19 Treatment or Miracle 'Cure'?: Tracking the Hydroxychloroquine, Remdesivir and Ivermectin Conspiracies on Social Media.” Paper presented at the BSA Annual Conference 2022: Building Equality and Justice Now, 20-22 April 2022. &lt;https://www.britsoc.co.uk/media/25695/ac2022_draft_conf_prog.pdf&gt;. Barkun, Michael. A Culture of Conspiracy. University of California Press, 2013. Boulware, David R., et al. """"A Randomized Trial of Hydroxychloroquine as Postexposure Prophylaxis for Covid-19."""" New England Journal of Medicine 383.6 (2020): 517-525. Chou, Wen-Ying Sylvia, Anna Gaysynsky, and Robin C. Vanderpool. """"The COVID-19 Misinfodemic: Moving beyond Fact-Checking."""" Health Education &amp; Behavior 48.1 (2021): 9-13. Douglas, Karen M., et al. """"Understanding Conspiracy Theories."""" Political Psychology 40 (2019): 3-35. Fenster, Mark. Conspiracy Theories: Secrecy and Power in American Culture. University of Minnesota Press, 1999. Gautret, Philippe, et al. """"Hydroxychloroquine and Azithromycin as a Treatment of COVID-19: Results of an Open-Label Non-Randomized Clinical Trial."""" International Journal of Antimicrobial Agents 56.1 (2020): 105949. Golebiewski, Michael, and danah boyd. """"Data Voids: Where Missing Data Can Easily Be Exploited."""" Data &amp; Society (2019). Kelsey, Darren. """"Archetypal Populism: The ‘Intellectual Dark Web’ and the ‘Peterson Paradox’."""" Discursive Approaches to Populism across Disciplines. Cham: Palgrave Macmillan, 2020. 171-198. Lawrence, Jack M., et al. """"The Lesson of Ivermectin: Meta-Analyses Based on Summary Data Alone Are Inherently Unreliable."""" Nature Medicine 27.11 (2021): 1853-1854. Marwick, Alice E. Status Update. Yale University Press, 2013. Mehra, Mandeep R., et al. """"RETRACTED: Hydroxychloroquine or Chloroquine with or without a Macrolide for Treatment of COVID-19: A Multinational Registry Analysis."""" (2020). Parks, Gabriel. """"Considering the Purpose of ‘an Alternative Sense-Making Collective’: A Rhetorical Analysis of the Intellectual Dark Web."""" Southern Communication Journal 85.3 (2020): 178-190. Senft, Theresa M. Camgirls: Celebrity and Community in the Age of Social Networks. Peter Lang, 2008. Smith, Tim, et al. """"COVID-19 Drug Therapy."""" Elsevier (2020). Voss, Andreas. “Official Statement from International Society of Antimicrobial Chemotherapy (ISAC).” International Society of Antimicrobial Chemotherapy 3 Apr. 2020. &lt;https://www.isac.world/news-and-publications/official-isac-statement&gt;.","",""
"2022","#FreeBritney and the Pleasures of Conspiracy","Introduction  There are many competing explanations for why people are drawn to conspiracy theories. Increasingly, conspiracy theories are mainstream sites of cultural engagement (Barkun). Conspiracy theorising, then, is part of, or at least brushes up against, people’s daily sense-making practices. However, many still think of conspiracy theorising and the communities that form around them as deviant, pathological or deficient (Swami et al.). In this article, we argue that we need to shift from a model of a deficient and deviant understanding of conspiracy theorising to understand these practices as part of our everyday behavioural and social repertoires. We argue that part of this shift means attending to the sensory and felt experience of conspiracy thinking, as a bodily and affective experience, as a site of pleasure. Centring pleasure as an explanatory framework for conspiracy theorising does not foreclose other explanations. Rather we argue that pleasure operates as a broader explanatory framework within which these competing explanations can also offer insight. We do not aim to provide an empirical account of the #FreeBritney movement in this article, but instead use it as an example through which we can begin to develop pleasure as a potential explanatory framework for understanding conspiracy theorising. To argue for the centrality of ‘pleasure’ in conspiracy theories, we draw on scholarship from fandom studies to ask, “What can the ‘Free Britney’ movement tell us about the pleasures of conspiracy?” We pay particular attention to how conspiracy theorising can be understood as a site of pleasure and, at times, hope, which in turn transform conspiracy theories into ‘sticky’ cultural sites (Ahmed). The centring of pleasure as a driver of conspiracy theorising also points to possible alternative approaches to countering the affective pull of conspiracy theories. Why #FreeBritney?  This article focusses on the #FreeBritney community as an example for several reasons. #FreeBritney sits outside many of the political concerns that often characterise conspiracy theories; that is, it is neither left nor right in its orientation. Additionally, #FreeBritney was initially written off as nonsense by mainstream media outlets and commentators. For example, in the first version of TikToker Abbie Richards’s viral chart that categorises conspiracy theories, #FreeBritney is in the same category as UFOs and not something that ‘actually happened’ (Richards), meaning Richards did not believe the central claim of the #FreeBritney movement, that Britney wished to end an abusive conservatorship, was real. Similar coverage was evident in other press, including by Maria Sherman for Jezebel, which describes the #FreeBritney theory as “dubiously sourced” and as “mak[ing] gargantuan assumptions about mental health without much concrete evidence” (Sherman). Despite the derision, #FreeBritney persisted, and the claims made in the initial, instigating episode of Britney’s Gram (a fan-created podcast) have been borne in court, affirmed by Spears herself, and in numerous pieces of investigative reporting (Stark and Day). The #FreeBritney Context So, how did we get to #FreeBritney? In early 2008, after a string of increasingly erratic public appearances, Britney Spears was placed into a conservatorship arrangement. Conservatorships are typically reserved for the elderly and mentally ill, or those without the capacity to care for them themselves. Spears’s conservatorship meant that she could not make any personal or financial decisions for herself. Spears’s conservatorship was overseen by her father and court-appointed lawyers who benefited financially by allegedly exploiting the arrangement (Day and Abrams). Until 2021, Spears remained under the conservatorship, while continuing to work. These working arrangements included world tours, TV appearances and a long-running Las Vegas residency where she performed a 90-100 minute show several times per week (Jacobs). Rumours marked the beginning of Spears’s conservatorship that it was an attempt to exploit Spears financially while keeping her under parental control (Jacobs). This is evidenced by her thwarted attempt to acquire legal representation, where the court ultimately ruled that she was too unwell to retain her own counsel (Coscarelli et al.). Rumours of a broader conspiracy designed to entrap Spears in the conservatorship only gained widespread traction in 2019, resulting in the birth of the #FreeBritney movement. The growth of #FreeBritney discourse can be traced to an April 2019 episode of the podcast Britney’s Gram (Barker and Babs). Britney’s Gram was initially a ‘close reading’ of Spears’s Instagram focussed on parsing her captions, images, and emoticon use. In the podcast's special ‘emergency’ episode, episode 75, titled “#FreeBritney”, the nature of the conspiracy regarding Spears’s conservatorship took shape. The ‘emergency’ episode of the podcast responded to a tip called into the Britney’s Gram hotline. The anonymous source claims to be a paralegal who worked on legal documents related to the conservatorship throughout their employment.  The paralegal claims that the conservatorship is “disturbing to say the least”. The show goes on to lay out a timeline of key events that support their assertion that Spears is being kept in the conservative against her will. Their claims are supported by a ‘close reading’ of Spears’s output, including her Instagram account and her public appearances, both official and unofficial. The hosts assemble their theory from a diverse range of sources, but their iterative theory building is underscored by the hosts’ empathetic reading, “what if it were me?” Fandom and the Collective Feelings of Conspiracy The #FreeBritney movement offers an opportunity to reflect on the parallels and intersections between fandom culture and conspiracy. It also allows us to consider what contemporary fan practices might tell us about the appeal of engaging in conspiracy. While #FreeBritney as a movement has extended far beyond the reach of the Britney Spears fandom, its roots began in the everyday fan practices that are not unique to the singer's supporters. Identifying as a ‘fan’ of a celebrity, a band, television show, film franchise, or other popular cultural texts has become a mainstream activity in recent decades, moving from a more subcultural or fringe practice (Gray et al.). Fan practices often include developing a repertoire of knowledge of their chosen fandom. This repertoire allows them to conduct close readings of these ‘texts’, which include relevant images and social media content (Hills), and look for patterns, consistencies and inconsistencies — what Jason Mittell (52) calls ‘forensic fandom’. Fans also create their own paratexts drawing on their fandom-specific knowledge to create work such as fanfiction, fan videos (fanvids), blogs, dedicated social media accounts, podcasts (such as Britney’s Gram) and other texts that fans may also analyse (Geraghty). Much like engaging in conspiracy, participating in fandom is also a broad continuum in terms of commitment, and depth of engagement. Some fans are more peripheral to the fandom, casually engaged, and only broadly aware of close reading practices that may be normalised for those within the more engaged inner circle of the fandom. However, these more casual fans may also draw on and consume paratext created by more avid fans. Creators of popular and well-made paratexts can even become renowned in social media spaces within fan communities for their creations (Hills). This mirrors conspiracy thinking, where believers range from curious about the conspiracy to committed and embedded in the conspiracy community. Like fandoms, the more active participants in the conspiracy can become established and well-known in the community for disseminating information and knowledge. For example, many followers of the QAnon conspiracy receive most of their information through secondary QAnon social media influencers who interpret ‘Qdrops’ rather than interpret the cryptic message board posts themselves (Conner and MacMurray). Scholarship examining fandom and fan experiences has emphasised the key role of pleasure for fans in developing this fan expertise (McCann and Southerton). In particular, the practices of close textual reading and familiarity with the fandom's texts, symbols, and key players offer a sense of community and collective feeling. As McCann and Southerton report in their study on queer shipping among One Direction fans (when fans invest emotional energy in the relationship, the ‘ship’, between two characters or celebrities), pleasure is collective rather than individual and emerges from a sense of belonging and shared investment. While, as we have discussed, the differing levels of involvement and investment can create hierarchy, and therefore potential conflict within fandom, scholarship on fandom has argued that fans primarily take pleasure in the feeling of community, support and belonging (McCann and Southerton; Geraghty; Pearson). Fan spaces are spaces in which collective feelings can be heightened, as participants take pleasure in experiencing something that thousands of others are feeling simultaneously — whether it be in person at a concert or, increasingly, in social media communities. The pleasures of fandom also go beyond momentous occasions like a singer's album launch or a celebrity scandal. Fans can cultivate pleasure in the mundane practices of fandom by building a sense of building and momentum, by using their close reading to predict imminent events (e.g. attempting to discern what Instagram posts might be hinting that a popstar is going to put out a new album) or undertaking rereading of old material to reinterpret meanings in new contemporary light. The pleasures of anticipation are central to these fan practices, with close reading offering endless rewards. Conspiracy theorists operate similarly, even when an anticipated event does not come to fruition. When the predictions of the mysterious Q that tell of mass arrests of prominent enemies of the movement fail to eventuate, rather than lose belief in Q’s prophetic power, the believers find explanation and new events to anticipate (Butler and Martin). Is #FreeBritney a Conspiracy? While it is tempting to situate #FreeBritney firmly within the domain of fan studies, we argue that while later borne out by facts, it can also be understood as a conspiracy theory. Conspiracy theories are united by a focus on and fear of a larger malevolent actor, who uses the power vested in institutions to control the narrative about the conspiracy, and indeed the conspiracy itself (Melley). In #FreeBritney, the stakes are a little lower, with the clearest villains being Spears’s immediate family, who appear to have financially benefited from her conservatorship. Nevertheless, the conspiracy involves elements of control, not only over Spears herself but the media, the criminal justice system, and the medical professionals diagnosing and treating Spears, as well as any close friends and staff. As with other conspiracies, power is exercised through social institutions to ‘cover up’ the conspiracy itself and any damage it is causing (Barkun; Melley). If conspiracies are secret, how then are they detected? Key to conspiracy theorising is the ‘close reading’ or ‘forensic’ examination (Mittell) of various texts to spot inconsistencies and gaps in authenticity that disrupt the dominant narrative. This is a hallmark of conspiracy theorising, which relies on “the interpretation of half-hidden cIues, tell-tale signs, and secret messages” (Melley 16). Within #FreeBritney, close reading is most obviously applied to her Instagram account and extends to various court courts, interviews, and media reporting. This analysis allows for these inconsistencies to build an alternative explanation while using a corpus of evidence available to everyone. Where Is the Pleasure?  Where can we locate the sources of pleasure in #FreeBritney? To be clear, we are arguing for an understanding of pleasure that is not eroticised but rather found in the arguably mundane practises of conspiracy. The close, detailed sifting through evidence required to build a conspiracy theory is pleasurable in a number of ways. These practices are pleasurable in and of themselves — developing deep knowledge assembling the threads in the conspiracy theory holds the individual in a continual site of possibility and potential. The space of ‘what if’ where nothing is certain and outcomes can be constantly refigured allows conspiracy theorists to exist in expectation, in ‘looking forward to’ as one would a long-awaited holiday. The pleasure is in anticipating the event, but not necessarily in the resolution of the conspiracy itself. The momentum and anticipation in fan communities are remarkably similar to those of conspiracy theory communities, creating a pleasurable affective atmosphere (Anderson) that circulates in and through digital practices. The ‘close reading’ practice we describe is also pleasurable through proximity and intimacy. Close reading allows for a point of entry and connection to the broader #Free Britney community, where close readings are contributed, the readings of others are affirmed, and these individual contributions are incorporated into the fabric of the community. Close reading also provides proximity and a sense of intimate familiarity with Spears herself. Close reading is only made possible through deep knowledge, through being able to understand Spears’s self-presentation, mediated through digital platforms like Instagram, as authentic or forced. The Internet also makes close reading more accessible and immediate. Instagram posts can be saved for later perusal, comments screenshotted, and deleted comments captured before they vanish. This work of understanding, interpreting, and building happens both in real time (as soon as content is posted) and retrospectively, using what is now known or agreed upon to go back and reinterpret old material, hunting for clues and signs previously missed. This is evident in a number of TikToks where fans closely interpret Britney’s movement to confirm their theories. In one video, Spears discusses the LGBTQIA+ community. The video is not particularly coherent, and in the comments, a fan writes, “If you need help, wear yellow and blink twice”, and “If you need help do two spins” (ABC News). In her next video, Spears appears wearing a yellow top and holding flowers; she blinks twice, then does two spins for the camera. Given what we now know about Spears’s situation at the time, it seems likely she was in dialogue with her fans, counting on their close reading, attention to detail, and emotional investment. While Spears’s abusive conservatorship was obviously of concern to fans, there is also pleasure in the moments of reading, knowing, and dialoguing with Spears, creating a parasocial intimacy (ABC News). These compounding pleasures are overlapping and mutually reinforcing and create what Ahmed would call a ‘sticky’ site of affective engagement. Ahmed’s conceptualisation of ‘stickiness’ often refers to negative affects, but we argue can apply to positive or pleasurable affectivities. Conclusion #FreeBritney began as a fringe fan concern. It was mocked, derided and dismissed, before being ultimately vindicated through legal action and the removal of the conservatorship. Legal action addressing the financial exploitation of Spears is underway (Day).  In a video after the end of her conservatorship, Spears speaks to her fans through an Instagram video detailing her next steps (Sky News). She also thanks the #FreeBritney movement, saying,  the Free Britney Movement, you guys rock! Honestly, my voice was muted and threatened for so long, and um I wasn’t able to speak up or say anything, and um because of you guys’ awareness and kind of knowing what was going on and delivering that news to the public for so long ... because of you, I honestly think you guys saved my life.  Examining the #FreeBritney movement allows us to consider the role of pleasure in conspiracy theorising. Through this reading, we can also begin to understand conspiracy theorists in a more nuanced way. Those who believe in conspiracy theories are often characterised as fearful, anxious, and paranoid. However, there are pleasurable affectivities also associated with conspiracy theorising. While conspiracy theories most often circulate through and coalesce in online spaces, #FreeBritney demonstrates that theories also drive practice with fans protesting outside of Spears’s court hearings and taking steps to dismantle the conservatorship system more generally (Rolling Stone). Focussing on pleasure can also explain the derision directed towards conspiracy theories and their subscribers. Anti-fan communities provide a language to discuss the gleeful debunking and mocking of conspiracy theories. Pleasure is also a core part of anti-fandom, that is groups mobilised around their hate of something or someone (usually a celebrity with a fan following), and this anti-fandom mirrors many core fan practices (Pinkowitz). The anti-fan is smarter and more discerning than the fan and has the ‘right’ way of thinking, reasoning, and appreciating. The rational anti-fan understands that any clue in Spears’s videos is coincidental and that fans are over-involved, overreacting and out of touch. However, the pleasure of anti-fandom, and debunking more generally, cannot exist without the fan and the conspiracy theory. Thus, the pleasure of the anti-fan only exists in dialogue with the fan, or in this case, the perceived conspiracy theorist. Attending to conspiracy theories as a site of pleasure allows us to construct a deeper and more nuanced understanding of the seemingly magnetic pull of conspiracy theories. References ABC News. “Britney Spears’s Fans Claim She Is Pleading for Help through Her Social Media Videos.” 24 July 2020. &lt;https://www.abc.net.au/news/2020-07-24/britney-spears-fans-claim-she-is-pleading-for-help/12488754&gt;. Ahmed, Sara. Cultural Politics of Emotion. Edinburgh UP, 2014. Anderson, Ben. “Affective Atmospheres.” Emotion, Space and Society 2.2 (2009): 77–81. Barker, Tess, and Grey Babs. “75 #FREEBRITNEY.” Britney’s Gram, podcast, 75 (16 Apr. 2019). &lt;https://soundcloud.com/user-405122914-411166228/74-freebritney&gt;. Barkun, Michael. “Conspiracy Theories as Stigmatized Knowledge.” Diogenes 62.3-4 (2015): 114–20. Butler, Josh, and Sarah Martin. “Australian Online Anti-Vaccine Groups Switch to Putin Praise and Ukraine Conspiracies.” The Guardian 1 Mar. 2022. &lt;https://www.theguardian.com/australia-news/2022/mar/02/australias-anti-vaccine-groups-switch-focus-to-putin-praise-and-ukraine-conspiracies&gt;. Conner, Christopher T., and Nicholas MacMurray. “The Perfect Storm: A Subcultural Analysis of the QAnon Movement.” Critical Sociology (Nov. 2021). &lt;http://dx.doi.org/10.1177/08969205211055863&gt;. Coscarelli, Joe, et al. “Britney Spears Can Hire a New Lawyer of Her Choice, Judge Rules.” The New York Times 14 July 2021. &lt;https://www.nytimes.com/2021/07/14/arts/music/britney-spears-conservatorship-lawyer.html&gt;. Day, Liz. “Britney Spears Fights Father’s Fee Claim, Alleging Financial Misconduct.” The New York Times 19 Jan. 2022. &lt;https://www.nytimes.com/2022/01/18/business/britney-spears-father-fees.html&gt;. Day, Liz, and Rachel Abrams. “Investigation into Britney Spears Conservatorship Will Look into Her Finances.” The New York Times 2 Nov. 2021. &lt;https://www.nytimes.com/2021/11/02/us/britney-spears-father-deposition.html&gt;. Geraghty, Lincoln. “Introduction: Fans and Paratexts.” Popular Media Cultures: Fans, Audiences and Paratexts, ed. Lincoln Geraghty. Palgrave Macmillan UK, 2015. 1–14. Gray, Jonathan, et al. “Why Still Study Fans?” Fandom, Second Edition: Identities and Communities in a Mediated World, ed. Jonathan Gray et al. NYU P, 2017. 1–27. Hills, Matt. “Fiske’s ‘Textual Productivity’ and Digital Fandom: Web 2.0 Democratization versus Fan Distinction.” Participations 10.1 (2013): 130–53. Jacobs, Julia. “What Is Actually Happening with Britney Spears?” The New York Times 17 May 2019. &lt;https://www.nytimes.com/2019/05/17/arts/music/britney-spears-conservatorship-mental-health.html&gt;. McCann, Hannah, and Clare Southerton. “Repetitions of Desire: Queering the One Direction Fangirl.” Girlhood Studies 12.1 (2019): 49–65. Melley, Timothy. Empire of Conspiracy. Cornell UP, 2016. Mittell, Jason. Complex TV: The Poetics of Contemporary Television Storytelling. NYU P, 2015. Pearson, Roberta. “Fandom in the Digital Era.” Popular Communication 8.1 (2010): 84–95. Pinkowitz, Jacqueline M. “‘The Rabid Fans That Take [Twilight] Much Too Seriously’: The Construction and Rejection of Excess in Twilight Antifandom.” Transformative Works and Cultures 7 (2011): 1–17. Richards, Abbie. “The Conspiracy Chart.” Twitter 3 Oct. 2020. &lt;https://twitter.com/abbieasr/status/1312512066071060480&gt;. Rolling Stone. “#FreeBritney Rallies around the World.” 14 July 2021. &lt;https://www.rollingstone.com/culture/culture-pictures/freebritney-rallies-britney-spears-conservatorship-photos-1197458/buk_1491/&gt;. Sherman, Maria. “A Guide to the #FreeBritney Theory That Britney Spears Is Being Held against Her Will.” Jezebel 23 Apr. 2019. &lt;https://jezebel.com/a-guide-to-the-freebritney-theory-that-britney-spears-1834216480&gt;. Sky News. “Britney Spears Thanks Fans in Instagram Video after Conservatorship Ends.” 17 Nov. 2021. &lt;https://news.sky.com/video/video-im-not-here-to-be-a-victim-britney-spears-speaks-after-end-of-conservatorship-12470545&gt;. Stark, Samatha, and Liz Day. “‘Controlling Britney Spears’ Reveals Details of Her Life under Conservatorship.” The New York Times 2 Nov. 2021. &lt;https://www.nytimes.com/article/controlling-britney-spears.html&gt;. Swami, Viren, et al. “Associations between Belief in Conspiracy Theories and the Maladaptive Personality Traits of the Personality Inventory for DSM-5.” Psychiatry Research 236 (2016): 86–90.","",""
"2022","Playing Conspiracy","Introduction Scholars, journalists, conspiracists, and public-facing groups have employed a variety of analogies to discuss the role that misleading content (conspiracy theory, disinformation, malinformation, and misinformation), plays in our everyday lives. Terms like the “disinformation war” (Hwang) or the “Infodemic” (United Nations) attempt to summarise the issues of misleading content to aide public understanding. This project studies the effectiveness of these analogies in conveying the movement of online conspiracy theory in social media networks by simulating them in a game. Building from growing comparisons likening conspiracy theories to game systems (Berkowitz; Kaminska), we used game design as a research tool to test these analogies against theory. This article focusses on the design process, rather than implementation, to explore where the analogies succeed and fail in replication. Background and Literature Review Conspiracy Theories and Games Online conspiracy theories reside in the milieu of misinformation (unintentionally incorrect), disinformation (intentionally incorrect), and malinformation (intentionally harmful) (Wardle and Derakhshan 45). They are puzzled together through the vast amount of information available online (Hannah 1) creating a “hunt” for truth (Berkowitz) that refracts information through deeply personal narratives that create paradoxical interpretations (Hochschild xi). Modern social media networks offer curated but fragmented content distribution where information discovery involves content finding users through biased sources (Toff and Nielsen 639). This puzzling together of theories gives conspiracy theorists agency in ‘finding the story’, giving them agency in a process with underlining goals (Kaminska). A contemporary example is QAnon, where the narrative of a “secret global cabal”, large-scale pedophile rings, and overstepping government power is pieced together through Q-drops or cryptic clues that users decipher (Bloom and Moskalenko 5). This puzzle paints a seemingly hidden reality for players to uncover (Berkowitz) and offers gripping engagement which connects “disparate data” into a visualised conspiracy (Hannah 3). Despite their harmful impacts, conspiracy theories are playful (Sobo). They can be likened to playful acts of make-belief (Sobo), reality-adjacent narratives that create puzzles for exploration (Berkowitz), and community building through playful discovery (Bloom and Moskalenko 169). Not only do conspiracies “game the algorithm” to promote content, but they put players into in a self-made digital puzzle (Bloom and Moskalenko 17, 18). This array of human and nonhuman actors allows for truth-spinning that can push people towards conspiracy through social bonds (Moskalenko). Mainstream media and academic institutions are seen as biased and flawed information sources, prompting these users to “do their own research” within these spaces (Ballantyne and Dunning). However, users are in fragmented worldviews, not binaries of right and wrong, which leaves journalism and fact-checkers in a digital world that requires complex intervention (De Maeyer 22). Analogies Analogies are one method of intervention. They offer explanation for the impact conspiracy has had on society, such as the polarisation of families (Andrews). Both conspiracists and public-facing groups have commonly used an analogy of war. The recent pandemic has also introduced analogies of virality (Hwang; Tardáguila et al.). A war analogy places truth on a battleground against lies and fiction. “Doing your own research” is a combat maneuver for conspiracy proliferation through community engagement (Ballantyne and Dunning). Similarly, those fighting digital conspiracies have embraced the analogy to explain the challenges and repercussions of content. War suggests hardened battlelines, the need for public mobilisation, and a victory where truth prevails, or defeat where fallacy reigns (Shackelford). Comparatively, a viral analogy, or “Infodemic” (United Nations), suggests misleading content as moving through a network like an infectious system; spreading through paths of least resistance or effective contamination (Scales et al. 678; Graham et al. 22). Battlelines are replaced with paths or invasion, where the goal is to infect the system or construct a rapid response vaccine that can stymie the ever-growing disease (Tardáguila et al.). In both cases, victorious battles or curative vaccinations frame conspiracy and disinformation as temporary problems. The idea of the rise and falls of a conspiracy’s prominence as link to current events emulates Byung-Chung Han’s notion of the digital swarm, or fragmented communities that coalesce, bubble up into volatile noise, and then dissipate without addressing the “dominant power relations” (Han 12). For Han, swarms arise in digital networks with intensive support before disappearing, holding an influential but ephemeral life. Recently, scholarship has applied a media ecology lens to recognise the interconnection of actors that contribute to these swarms. The digital-as-ecosystem approach suggests a network that needs to be  actively managed (Milner and Phillips 8). Tangherlini et al.’s work on conspiracy pipelines highlights the various actors that move information through them to make the digital ecosystem healthy or unhealthy (Tangherlini et al.). Seeing the Internet, and the movement of information on it, as an ecology posits a consideration of processes that are visible (i.e., conspiracy theorists) and invisible (i.e., algorithms etc.) and is inclusive of human and non-human actors (Milner and Phillips). With these analogies as frames, we answer Sobo’s call for a playful lens towards conspiracy alongside De Maeyer’s request for serious interventions by using serious play. If we can recognise both conspiracy and its formation as game-like and understand these analogies as explanatory narratives, we can use simulation game design to ask: how are these systems of conspiracy propagation being framed? What gaps in understanding arise when we frame conspiracy theory through the analogies used to describe it?   Method Research-Creation and Simulation Gaming Our use of game design methods reframed analogies through “gaming literacy”, which considers the knowledge put into design and positions the game as a set of practices relating to the everyday (Zimmerman 24). This process requires constant reflection. In both the play of the game and the construction of its parts we employed Khaled’s critical design framework (10-11). From March to December 2021 we kept reflective logs, notes from bi-weekly team meetings, playtest observations, and archives of our visual design to consistently review and reassess our progression. We asked how the visuals, mechanics, and narratives point to the affordances and drawbacks of these analogies. Visual and Mechanical Design Before designing the details of the analogies, we had to visualise their environment – networked social media. We took inspiration from existing visual representations of the Internet and social media under the hypothesis that employing a familiar conceptual model could improve the intelligibility of the game (figs. 1 and 2). In usability design, this is referred to as """"Jakob's law"""" (Nielsen), in which, by following familiar patterns, the user can focus better on content, or in our case, play.  Fig. 1: “My Twitter Social Ego Networks” by David Sousa-Rodrigues. A visual representation of Sousa-Rodrigues’s social media network. &lt;https://www.flickr.com/photos/11452351@N00/2048034334&gt;. We focussed on the networked publics (Itō) that coalesce around information and content disclosure. We prioritised data practices that influence community construction through content (Bloom and Moskalenko 57), and the larger conspiracy pipelines of fragmented data (Tangherlini et al. 30).  Fig. 2: """"The Internet Map"""" by Ruslan Enikeev. A visual, 2D, interactive representation of the Internet. &lt;http://internet-map.net/&gt;. Our query focusses on how play reciprocated, or failed to reciprocate, these analogies. Sharp et al.’s suggestion that obvious and simple models are intuitively understood allowed us to employ simplification in design in the hopes of parsing down complex social media systems. Fig. 3 highlights this initial attempt where social media platforms became “networks” that formed proximity to specific groups or “nodes”.  Fig. 3: Early version of the game board, with a representation of nodes and networks as simplified visualisations for social networks. This simplification process guided the scaling of design as we tried to make the seemingly boundless online networks accessible. Colourful tokens represented users, placed on the nodes (fig. 4). Tokens represented portions of the user base, allowing players to see the proliferation of conspiracy through the network. Unfortunately, this simplification ignores the individual acts of users and their ability to bypass these pipelines as well as the discovery-driven collegiality within these communities (Bloom and Moskalenko 57). To help offset this, we designed an overarching scenario and included “flavour text” on cards (fig. 5) which offered narrative vignettes that grounded player actions in dynamic story.   Fig. 4: The first version for the printed playtest for the board, with the representation of “networks” formed by a clustering of """"nodes"""". The movement of conspiracy was indicated by colour-coded tokens.  Fig. 5: Playing cards. They reference a particular action which typically adds or removes token. They also reference a theory and offer text to narrativise the action. Design demonstrates that information transmission is not entirely static. In the most recent version (fig. 6), this meant having the connections between nodes become subverted through player actions. Game mechanics, such as playing cards (fig. 5), make these pipelines interactive and visible by allowing players to place and move content throughout the space in response to each other’s actions.  Fig. 6: The most updated version of the board, now named """"Lizards and Lies"""". Red regions are initial starting points for conspiracy to enter mainstream social media (purple). Design adaptations focussed on making conspiracy theory dynamic. Player choice (i.e. where to add conspiracy) had to consider a continuously changing board created by other actors to reflect the adaptive nature of conspiracy theories. In this way, analogies came alive or died through the actions of players within a visually responsive system. This meant that each game had different swarms of conspiracy, where player decisions “wrote” a narrative through play. By selecting how and where conspiracy might be placed or removed, players created a narrative distinct to their game. For example, a conspiracy theorist player (one playable character) might explain their placing of conspiracy theory within the Chrpr/Twitter network as a community response to fact-checking (second playable character) in the neighbouring Shreddit/Reddit community. Results War Analogy Initial design took inspiration from wargaming to consider battlelines, various combatants, and a simulated conflict. Two player characters were made. Conspiracy theorists were posited against fact-checkers, where nodes and networks functioned as battlelines of intervention. The war narrative was immediately challenged by the end-state. Either conspiracy overtook networks or the fact checkers completely stymied conspiracy’s ability to exist. Both end-states seemed wrong for players. Battle consistently felt futile as conspiracists could always add more content, and fact-checkers could always remove something. Simply put, war fell flat. While the game could depict communities and spaces of combat, it struggled to represent how fragmented conspiracy theories are. In play, conspiracy theory became stagnant, the flow of information felt compelled, and the actors entered uneven dynamics. Utopia was never achieved, and war always raged on. Even when players did overtake a network, the victory condition (needing to control the most networks) made this task, which would normally be compelling, feel lacklustre. To address this, we made changes. We altered the win condition to offer points at the end of each turn depending on what the player did (i.e., spreading conspiracy into networks). We expanded the number of networks and connections between them (fig. 3 and fig. 6) to include more fluid and fragmented pipelines of conspiracy dissemination. We included round-end events which shifted the state of the game based on other actors, and we pushed players to focus on their own actions more than those of the others on the board. These changes naturally shifted the battleground from hardened battle lines to a fragmented amorphous spread of disinformation; it moved war to virality. Viral Analogy As we transitioned towards the viral, we prioritised the reflexive, ephemeral movements of conspiracy proliferating through networks. We focussed less on adding and removing content and shifted to the movement of actors through the space. Some communities became more susceptible to conspiracy content, fact-checkers relied on flagging systems, and conspiracy theories followed a natural, but unexpected pipeline of content dissemination. These changes allowed players to feel like individual actors with specific goals rather than competing forces. Fact-checkers relied on mitigation and response while conspiracists evaluated the susceptibility of specific communities to conspiracy content. This change illuminated a core issue with fact-checking; it is entirely responsive, endless, and too slow to stop content from having an impact. While conspiracists could play one card to add content, fact-checkers had to flag content, move their token, and use a player card to eliminate content – all of which exacerbated this issue. In this manner, the viral approach rearticulated how systems themselves afford the spread of conspiracy, where truly effective means to stop the spread relied on additional system actors, such as training algorithms to help remove and flag content. While a more effective simulation, the viral analogy struggled in its presentation of conspiracy theory within social media. Play had a tipping point, where given enough resources, those stopping the spread of conspiracy could “vaccinate” it and clean the board. To alter this, our design began to consider actions and reactions, creating a push and pull of play focussed on balancing or offsetting the system. This transition naturally made us consider a media ecology analogy. Media Ecology Replacing utopic end-states with a need to maintain network health reframed the nature of engagement within this simulation. An ecological model recognises that harmful content will exist in a system and aims not at elimination, but at maintaining a sustainable balance. It is responsive. It considers the various human and non-human actors at play and focusses on varied actor goals. As our game shifted to an ecological model, homogenous actors of conspiracists or fact-checkers were expanded. We transitioned a two-player game into a four-player variant, testing options like literacy educators, content recommending algorithms, and ‘edgelords'. Rather than defeating or saving social media, play becomes focussed on actors in the system. Play and design demonstrated how actions would shape play decisions. Characters were seen as network actors rather than enemies, changing interaction. Those spreading conspiracy began to focus less on “viral paths”, or lines of battle, and instead on where or how they could impact system health. In some cases, conspiracists would build one network of support, in others they created pockets around the board from which they could run campaigns. Those stopping the spread came to see their job as management. Rather than try and eliminate all conspiracy, they determined which sites to engage with, what content held the greatest threat, and which tools would be most effective. Media ecology play focussed less on outsmarting opponents and instead on managing an actor’s, and other players’, goals within an evolving system. Challenging Swarms and a Turn to Digital Ecology Using games to evaluate analogies illuminates clear gaps in their use, and the value of a media ecology lens. A key issue across the two main analogies (war and virality) was a utopic endstate. The idea that conspiracy can be beaten back, or vaccinated, fails to consider the endless amount of conspiracy possible to be made, or the impossibility of vaccinating the entire system. As our transitionary design process shows, the notion of winners and losers misplaces the intent of various actors groups where conspiracy is better framed as community-building rather than “controlling” a space (Bloom and Moskalenko 57). In design, while Han’s notion of the swarm was helpful, it struggled to play out in our simulations because fragments of conspiracy always remained on the board. This lingering content suggests that fact-checking does not actually remove ideological support. Swarms could quickly regrow around lingering support presenting them not as ephemeral as Han argued. As design transitioned towards ecology, these “fragments” were seen as part of a system of actors. Gameplay shows a deep interplay between the removal of content and its spread, arguing that removing conspiracy is a band-aid solution to a larger problem. Our own simplification of analogy into a game is not without limitations. Importantly, the impact of user specific acts for interpreting a movement (Toff and Nielsen 640), and the underlying set of networks that create “dark platforms” (Zeng and Schäfer 122) were lost in the game’s translation. Despite this, our work provides directions for scholarship and those engaging with the public on these issues to consider. Reframing our lens to understand online conspiracy as an aspect of digital ecological health, asks us to move away from utopic solutions and instead focus on distinct actors as they relate to the larger system. Conclusion Employing serious play as a lens to our framing of digital conspiracy, this project emphasises a turn towards media ecology models. Game design functioned as a tool to consider the actors, behaviours, and interactions of a system. Our methodological approach for visualising war and viral analogies demonstrates how playful responses can prompt questions and considerations of theory. Playing in this way, offers new insights for how we think about and grapple with the various actors associated with conspiracy theory and scholarship should continue to embrace ecological models to weigh the assemblage of actors. References Andrews, Travis. “QAnon Is Tearing Families Apart.” Washington Post, 2020. &lt;https://www.washingtonpost.com/technology/2020/09/14/qanon-families-support-group/&gt;. Ballantyne, Nathan, and David Dunning. “Skeptics Say, ‘Do Your Own Research.’ It’s Not That Simple.” The New York Times, 3 Jan. 2022. &lt;https://www.nytimes.com/2022/01/03/opinion/dyor-do-your-own-research.html&gt;. Berkowitz, Reed. “QAnon Resembles the Games I Design. But for Believers, There Is No Winning.” Washington Post, 2021. &lt;https://www.washingtonpost.com/outlook/qanon-game-plays-believers/2021/05/10/31d8ea46-928b-11eb-a74e-1f4cf89fd948_story.html&gt;. Bloom, Mia, and Sophia Moskalenko. Pastels and Pedophiles: Inside the Mind of QAnon. Stanford University Press, 2021. De Maeyer, Juliette. “Taking Conspiracy Culture Seriously: Journalism Needs to Face Its Epistemological Trouble.” Journalism 20.1 (2019): 21–23. &lt;https://doi.org/10.1177/1464884918807037&gt;. Graham, Timothy, et al. Like a Virus: The Coordinated Spread of Coronavirus Disinformation. The Australia Institute, 2020. &lt;https://apo.org.au/node/305864&gt;. Han, Byung-Chul. In the Swarm: Digital Prospects. Trans. Erik Butler. MIT Press, 2017. Hannah, Matthew N. “A Conspiracy of Data: QAnon, Social Media, and Information Visualization.” Social Media + Society, 7.3 (2021). &lt;https://doi.org/10.1177/20563051211036064&gt;. Hochschild, Arlie Russell. Strangers in Their Own Land: Anger and Mourning on the American Right. The New Press, 2016. Hwang, Tim. “Deconstructing the Disinformation War.” MediaWell, Social Science Research Council 1 June 2020. &lt;https://mediawell.ssrc.org/expert-reflections/deconstructing-the-disinformation-war/&gt;. Itō, Mizuko. “Introduction.” Networked Publics. Ed. Kazys Varnelis. MIT Press, 2008. Kaminska, Izabella. “The ‘Game Theory’ in the Qanon Conspiracy Theory.” Financial Times 16 Oct. 2020. &lt;https://www.ft.com/content/74f9d20f-9ff9-4fad-808f-c7e4245a1725&gt;. Khaled, Rilla. “Questions over Answers: Reflective Game Design.” Playful Disruption of Digital Media. Ed. Daniel Cermak-Sassenrath. Singapore: Springer, 2018. 3–27. &lt;https://doi.org/10.1007/978-981-10-1891-6_1&gt;. Milner, Ryan M., and Whitney Phillips. You Are Here. MIT Press, 2020. &lt;https://you-are-here.pubpub.org/&gt;. Moskalenko, Sophia. “Evolution of QAnon &amp; Radicalization by Conspiracy Theories.” The Journal of Intelligence, Conflict, and Warfare 4.2 (2021): 109–14. &lt;https://doi.org/10.21810/jicw.v4i2.3756&gt;. Nielsen, Jakob. “End of Web Design.” Nielsen Norman Group, 2000. &lt;https://www.nngroup.com/articles/end-of-web-design/&gt;. Scales, David, et al. “The Covid-19 Infodemic — Applying the Epidemiologic Model to Counter Misinformation.” New England Journal of Medicine 385.8  (2021): 678–81. &lt;https://doi.org/10.1056/NEJMp2103798&gt;. Shackelford, Scott. “The Battle against Disinformation Is Global.” The Conversation 2020. &lt;http://theconversation.com/the-battle-against-disinformation-is-global-129212&gt;. Sharp, Helen, et al. Interaction Design: Beyond Human-Computer Interaction. 5th ed. Wiley, 2019. Sobo, Elisa Janine. “Playing with Conspiracy Theories.” Anthropology News 31 July 2019. &lt;https://www.anthropology-news.org/articles/playing-with-conspiracy-theories/&gt;. Tangherlini, Timothy R., et al. “An Automated Pipeline for the Discovery of Conspiracy and Conspiracy Theory Narrative Frameworks: Bridgegate, Pizzagate and Storytelling on the Web.” PLoS ONE 15.6 (2020). &lt;https://doi.org/10.1371/journal.pone.0233879&gt;. Tardáguila, Cristina, et al. “Taking an Ecological Approach to Misinformation.” Poynter 5 Dec. 2019. &lt;https://www.poynter.org/fact-checking/2019/taking-an-ecological-approach-to-misinformation/&gt;. Toff, Benjamin, and Rasmus Kleis Nielsen. “‘I Just Google It’: Folk Theories of Distributed Discovery.” Journal of Communication 68.3 (2018): 636–57. &lt;https://doi.org/10.1093/joc/jqy009&gt;. United Nations. “UN Tackles ‘Infodemic’ of Misinformation and Cybercrime in COVID-19 Crisis.” 2020. &lt;https://www.un.org/en/un-coronavirus-communications-team/un-tackling-%E2%80%98infodemic%E2%80%99-misinformation-and-cybercrime-covid-19&gt;. Wardle, Claire, and Hossein Derakhshan. “Thinking about ‘Information Disorder’: Formats of Misinformation, Disinformation, and Mal-Information.” Journalism, ‘Fake News’ &amp; Disinformation. Eds. Cherilyn Ireton and Julie Posetti. Paris: Unesco, 2018. 43–54. Zeng, Jing, and Mike S. Schäfer. “Conceptualizing ‘Dark Platforms’. Covid-19-Related Conspiracy Theories on 8kun and Gab.” Digital Journalism 9.9 (2021): 1321–43. &lt;https://doi.org/10.1080/21670811.2021.1938165&gt;. Zimmerman, Eric. “Gaming Literacy: Game Design as a Model for Literacy in the Twenty-First Century.” The Video Game Theory Reader 2. 2008. 9.","",""
"2022","Page Not Found","One cannot use the Internet for long without encountering its many dead ends. Despite the adage that everything posted online stays there forever, users quickly discover how fleeting Web content can be. Whether it be the result of missing files, platform moderation, or simply bad code, the Internet constantly displaces its archival contents. Eventual decay is the fate of all digital media, as Wendy Hui Kyong Chun observed in a 2008 article. “Digital media is not always there”, she writes. “We suffer daily frustrations with digital sources that just disappear” (160). When the media content we seek is something trivial like a digitised vacation photo, our inability to retrieve it may merely disappoint us. But what happens when we lose access to Web content about significant cultural events, like viral misinformation about a school shooting? This question takes on great urgency as conspiracy content spreads online at baffling scale and unprecedented speed. Although conspiracy theories have long been a fixture of American culture, the contemporary Internet enables all manner of “information disorder” (Wardle and Derakhshan) to warp media coverage, sway public opinion, and even disrupt the function of government—as seen in the harrowing “Stop the Steal” attack on the U.S. Capitol on 6 January 2021, when rioters attempted to prevent Congress from verifying the results of the 2020 Presidential Election. Scholars across disciplines have sought to understand how conspiracy theories function within our current information ecosystem (Marwick and Lewis; Muirhead and Rosenblum; Phillips and Milner). Much contemporary research focusses on circulation, tracking how conspiracy theories and other types of misinformation travel from fringe Websites to mainstream news outlets such as the New York Times. While undoubtedly valuable, this emphasis on circulation provides an incomplete picture of online conspiracy theories’ lifecycle. How should scholars account for the afterlife of conspiracy content, such as links to conspiracy videos that get taken down for violating YouTube’s Community Guidelines? This and related questions about the dead ends of online conspiracy theorising are underexplored in the existing scholarly literature. This essay contends that the Internet’s tendency to decay ought to factor into our models of digital conspiracy theories. I focus on the phenomenon of malfunctional hyperlinks, one of the most common types of disrepair to which the Internet is prone. The product of so-called “link rot”, broken links would appear to signal an archival failure for online conspiracy theories. Yet recent work from rhetorical theorist Jenny Rice suggests that these broken hyperlinks instead function as a rhetorically potent archive in their own right. To understand this uncanny persuasive work, I draw from rhetorical theory to analyse broken links to conspiracy content on Reddit, the popular social news platform, surrounding the 2018 school shooting in Parkland, Florida, the worst high school shooting in American history. I show that broken links on the subreddit r/conspiracy, by virtue of their dysfunction, persuade conspiracy theorists that they possess “stigmatized knowledge” (Barkun 26) about the shooting that is being suppressed. Ultimately, I argue that link rot functions as a powerful source of evidence within digital conspiracy theories, imbuing broken links with enduring rhetorical force to validate marginalised belief systems. Link Rot—Archival Failure or Archival Possibility? As is suggested by the prefix ‘inter-’, connectivity has always been one of the Internet’s core functionalities. Indeed, the ability to hyperlink two different texts—and now images, videos, and other media—is so fundamental to navigating the Web that we often take these links for granted until they malfunction. In popular parlance, we then say we have clicked on a “broken” or “dead” link, and without proper care to prevent its occurrence, all URLs are susceptible to dying eventually (much like us mortals). This slow process of decay is known as “link rot”. The precise extent of link rot on the Internet is unknown—and likely unknowable, in practice if not principle—but multiple studies have been conducted to assess the degree of link rot in specific archives. One study from 2015 found that nearly 50% of the URLs cited in 406 library and information science journal articles published between 2008-2012 were no longer accessible (Kumar et al. 59). In the context of governmental Webpages, a 2010 study determined that while only 8% of the URLs sampled in 2008 had link rot, that number more than tripled to 28% of URLs with link rot when sampled only two years later (Rhodes 589-90). More recently, scholars from Harvard’s Berkman Klein Center for Internet and Society uncovered an alarming amount of link rot in the online archive of the New York Times, perhaps the most prominent newspaper in the United States: “25% of all links were completely inaccessible, with linkrot becoming more common over time – 6% of links from 2018 had rotted, as compared to 43% of links from 2008 and 72% of links from 1998” (Zittrain et al. 4). Taken together, these data indicate that link rot worsens over time, creating a serious obstacle for the study of Web-based phenomena. Link rot is particularly worrisome for researchers who study online misinformation (including digital conspiracy theories), because the associated links are often more vulnerable to removal due to content moderation or threats of legal action. How should scholars understand the function of link rot within digital conspiracy theories? If our academic focus is on how conspiracy theories circulate, these broken links might seem at best a roadblock to scholarly inquiry or at worst as totally insignificant or irrelevant. After all, users cannot access the material in question; they reach a dead end. Yet recent work by rhetoric scholar Jenny Rice suggests these dead ends might have enduring persuasive power. In her book Awful Archives: Conspiracy Rhetoric and Acts of Evidence, Rice argues that evidence is an “act rather than a thing” and that as a result, we ought to recalibrate what we consider an archive (12, original emphasis). For Rice, archives are more than simple aggregates of documents; instead, they are “ordinary and extraordinary experiences in public life that leave lasting, palpable residues, which then become our sources—our resources—for public discourse” (16-17). These “lasting, palpable residues” are deeply embodied, Rice maintains, for the evidence we gather is “always real in its reference, which is to a felt experience of proximities” (118). For conspiracy theorists in particular, an archive might evoke a profound sense of what Rice memorably describes as “Something intense, something real. Something off. Something fucked up. Something anomalous” (12, original emphasis). This is no less true when an archive fails to function as designed. Hence, for the remainder of this essay, I pivot to analysing how link rot functions within digital conspiracy theories about the 2018 school shooting in Parkland, Florida. As we will see, the shooting galvanised meaningful gun control activism via the March for Our Lives movement, but the event also quickly became fodder for proliferating conspiracy content. From Crisis to Crisis Actors: The Parkland Shooting and Its Aftermath On the afternoon of 14 February 2018, Nikolas Cruz entered his former high school, Marjory Stoneman Douglas, and murdered 17 people, including 14 students (Albright). While a horrific event, the Parkland shooting unfortunately marked merely the latest in a long line of similar tragedies in the United States, which has been punctuated by school shootings for decades. But the Parkland shooting stands out among the gruesome lineage of similar tragedies due to the profound resolve of its student-survivors, who agitated for gun policy reform through the March for Our Lives movement. In the weeks following the shooting, a group of Parkland students partnered with Everytown for Gun Safety, a non-profit organisation advocating for gun control, to coordinate a youth-led demonstration against gun violence. Held in the U.S. capitol of Washington, D.C. on 24 March 2018, the March for Our Lives protest was the largest demonstration against gun violence in American history (March for Our Lives). The protest drew around 200,000 participants to Washington; hundreds of thousands of protestors attended an estimated 800 smaller rallies held across the United States (CBS News). Furthermore, likeminded protestors across Europe, Asia, Africa, and Australia held allied events to show support for these American students’ cause (Russo). The broader March for Our Lives organisation developed out of the political demonstrations on 24 March 2018; four years later, March for Our Lives continues to be a major force in debates about gun violence in the United States. Although the Parkland shooting inspired meaningful gun control activism, it also quickly provoked a deluge of online conspiracy theories about the tragedy and the people involved, including the student-activists who survived the shooting and spearheaded March for Our Lives. This conspiracy content arrived at breakneck pace: according to an analysis by the Washington Post, the first conspiracy posts appeared on the platform 8chan a mere 47 minutes after the first news reports aired about the shooting (Timberg and Harwell). Later that day, Parkland conspiracy theories migrated from fringe haunts like 8chan to InfoWars, a mainstay of the conspiracy media circuit, where host/founder Alex Jones insinuated that the shooting could be a “false flag” event orchestrated by the Democratic Party (Media Matters Staff). Over the ensuing hours, days, weeks, and months, Parkland conspiracies continued to circulate, receiving mainstream news coverage when conversative activists and politicians publicly espoused conspiracy claims about the shooting (Arkin and Popken). Ultimately, the conspiracist backlash was so persistent and virulent throughout 2018 that PolitiFact, a fact-checking site run by the Poynter Institute, declared the Parkland conspiracy theories their 2018 “Lie of the Year” (Drobnic Holan and Sherman). As with many conspiracy theories, the Parkland conspiracies remixed novel information with longstanding conspiracist tropes. Predominantly, these theories alleged that the Parkland student-activists who founded March for Our Lives were being controlled by outside forces to do their bidding. Although conspiracy theorists diverged in who they named as the shadowy puppet master pulling the strings—was it the Democratic Party? George Soros? Someone else?—all agreed that a secretive agenda was afoot. The most extreme version of this theory held that David Hogg, X González, and other prominent March for Our Lives activists were “crisis actors”. This account envisions Hogg et al. as paid performers playing the part of angry and traumatised students for media coverage about a school shooting that either did not occur as reported or did not occur at all (Yglesias). While unnerving and callous, these crisis actor allegations are not new ideas; rather, they draw from a long history of loosely antisemitic “New World Order” conspiracy theories that see an ulterior motive behind significant historical events (Barkun 39-65). Parkland conspiracy theorists circulated a wide variety of media artifacts—anti-March for Our Lives memes, obscure blog posts, and manipulated video footage of the Parkland students, among other content—to propagate their crisis actor claims. But whether due to platform moderation, threat of legal action, or simply public pressure, much of this conspiracy material is now inaccessible, leaving behind only broken links to conspiracy content that once was. By closely examining these broken links through a rhetorical lens, we can trace the “lasting, palpable residues” (Rice 16) link rot leaves in its wake. “All part of the purge”: Parkland Link Rot on r/conspiracy In this final section, I use the tools of rhetorical analysis to demonstrate how link rot can function as a form of evidence for conspiracy theorists. Rhetorical analysis, when applied to digital infrastructure, requires that we expand our notion of rhetoric beyond intentional human persuasion. As James J. Brown, Jr. argues, digital infrastructure is rhetorical because it determines “what’s possible in a given space”, which may or may not involve human beings (99). Human intentionality still matters in many contexts, of course, but seeing digital infrastructure as a “possibility space” opens up productive new avenues for rhetorical inquiry (Brown, Jr. 72-99). This rhetorical perspective aligns with the method of “affordance analysis” derived from Science and Technology Studies and related fields, which investigates how technologies facilitate certain outcomes for users (Curinga). Much like an affordance analysis, my goal is to illustrate how broken links produce certain rhetorical effects, not to make broader empirical claims about the extent of link rot within Parkland conspiracy theories. The r/conspiracy page on Reddit, the popular social news platform, serves as an ideal site for conducting a rhetorical analysis of broken links. The r/conspiracy subreddit is a preeminent hub for digital conspiracy content, with nearly 1.7 million members as of March 2022 and thousands of active users viewing the site at any given time (r/conspiracy). Beyond its popularity, Reddit’s platform design makes link rot a common feature on r/conspiracy. As a forum-based social media platform, Reddit consists entirely of subreddits dedicated to various topics. In each subreddit, users generate and contribute to threads with relevant content, which often entails posting links to materials hosted elsewhere on the Internet. Importantly, Reddit allows each subreddit to set its own specific community rules for content moderation (so long as these rules themselves abide by Reddit’s general Content Policy), and unlike other profile-based social media platforms, Reddit allows anonymity through the use of pseudonyms. For all of these reasons, one finds a high frequency of link rot on r/conspiracy, as posts linking to external conspiracy media stay up even when the linked content itself disappears from the Web. Consider the following screenshot of an r/conspiracy Parkland post from 23 February 2018, a mere nine days after the Parkland shooting, which demonstrates what conspiracist link rot looks like on Reddit (fig. 1). Titling their thread “A compilation of anomalies from the Parkland shooting that the media won't address. The media wants to control the narrative. Feel free to use this if you find it helpful”, this unknown Redditor frames their post as an intervention against media suppression of suspicious details (“A compilation of anomalies”). Yet the archive this poster hoped to share with likeminded users has all but disintegrated—the poster’s account has been deleted (whether by will or force), and the promised “compilation of anomalies” no longer exists. Instead, the link under the headline sends users to a blank screen with the generic message “If you are looking for an image, it was probably deleted” (fig. 2). Fittingly, the links that the sole commenter assembled to support the original poster are also rife with link rot. Of the five links in the comment, only the first one works as intended; the other four videos have been removed from Google and YouTube, with corresponding error messages informing users that the linked content is inaccessible.  Fig. 1: Parkland Link Rot on r/conspiracy. (As a precaution, I have blacked out the commenter’s username.)  Fig. 2: Error message received when clicking on the primary link in Figure 1. Returning to Jenny Rice’s theory of “evidentiary acts” (173), how might the broken links in Figure 1 be persuasive despite their inability to transport users to the archive in question? For conspiracy theorists who believe they possess “stigmatized knowledge” (Barkun 26) about the Parkland shooting, link rot paradoxically serves as powerful validation of their beliefs. The unknown user who posted this thread alleges a media blackout of sorts, one in which “the media wants to control the narrative”. This claim, if true, would be difficult to verify. Interested users would have to scour media coverage of Parkland to assess whether the media have ignored the “compilation of anomalies” the poster insists they have uncovered and then evaluate the significance of those oddities. But link rot here produces a powerful evidentiary shortcut: the alleged “compilation of anomalies” cannot be accessed, seemingly confirming the poster’s claims to have secretive information about the Parkland shooting that the media wish to suppress. Indeed, what better proof of media censorship than seeing links to professed evidence deteriorate before your very eyes? In a strange way, then, it is through objective archival failure that broken links function as potent subjective evidence for Parkland conspiracy theories. Comments about Parkland link rot elsewhere on r/conspiracy further showcase how broken links can validate conspiracy theorists’ marginalised belief systems. For example, in a thread titled “Searching for video of Parkland shooting on bitchute”, a Redditor observes, “Once someone gives the link watch it go poof”, implying that links to conspiracy content disappear due to censorship by an unnamed force (“Searching for video”). That nearly everything else on this particular thread suffers from link rot—the original poster, the content of their post, and most of the other comments have since been deleted—seems only to confirm the commentor’s ominous prediction. In another thread about a since-deleted YouTube video supposedly “exposing” Parkland students as crisis actors, a user notes, “You can tell there’s an agenda with how quickly this video was removed by YouTube” (“Video Exposing”). Finally, in a thread dedicated to an alleged “Social Media Purge”, Redditors share strategies for combating link rot, such as downloading conspiracy materials and backing them up on external hard drives. The original poster warns their fellow users that even r/conspiracy is not safe from censorship, for removal of content about Parkland and other conspiracies is “all part of the purge” (“the coming Social Media Purge”). In sum, these comments suggest that link rot on r/conspiracy persuades users that their ideas and their communities are under threat, further entrenching their conspiratorial worldviews. I have argued in this article that link rot has a counterintuitive rhetorical effect: in generating untold numbers of broken links, link rot supplies conspiracy theorists with persuasive evidence for the validity of their beliefs. These and other dead ends on the Internet are significant yet understudied components of digital conspiracy theories that merit greater scholarly attention. Needless to say, I can only gesture here to the sheer scale of dead ends within online conspiracy communities on Reddit and elsewhere. Future research ought to trace other permutations of these dead ends, unearthing how they persuade users from beyond the Internet’s grave. References “A compilation of anomalies from the Parkland shooting that the media won't address. The media wants to control the narrative. Feel free to use this if you find it helpful.” Reddit. &lt;https://www.reddit.com/r/conspiracy/comments/7ztc9l/a_compilation_of_anomalies_from_the_parkland/&gt;. Albright, Aaron. “The 17 Lives Lost at Douglas High.” Miami Herald 21 Feb. 2018.&lt;https://www.miamiherald.com/news/local/community/broward/article201139254.html&gt;. Arkin, Daniel, and Ben Popken. “How the Internet’s Conspiracy Theorists Turned Parkland Students into ‘Crisis Actors’.” NBC News 21 Feb. 2018. &lt;https://www.nbcnews.com/news/us-news/how-internet-s-conspiracy-theorists-turned-parkland-students-crisis-actors-n849921&gt;. Barkun, Michael. A Culture of Conspiracy: Apocalyptic Visions in Contemporary America. 2nd ed. Berkeley: University of California Press, 2013. Brown, Jr., James J. Ethical Programs: Hospitality and the Rhetorics of Software. Ann Arbor: University of Michigan Press, 2015. CBS News. “How Many People Attended March for Our Lives? Crowd in D.C. Estimated at 200,000.” CBS News 25 Mar. 2018. &lt;https://www.cbsnews.com/news/march-for-our-lives-crowd-size-estimated-200000-people-attended-d-c-march/&gt;. Chun, Wendy Hui Kyong. “The Enduring Ephemeral, or the Future Is a Memory.” Critical Inquiry 35.1 (2008): 148-71. &lt;https://www.jstor.org/stable/10.1086/595632&gt;. Curinga, Matthew X. “Critical Analysis of Interactive Media with Software Affordances.” First Monday 19.9 (2014). &lt;https://journals.uic.edu/ojs/index.php/fm/article/view/4757/4116&gt;. Drobnic Holan, Angie, and Amy Sherman. “PolitiFact’s Lie of the Year: Online Smear Machine Tries to Take Down Parkland Students.” PolitiFact 11 Dec. 2018. &lt;http://www.politifact.com/article/2018/dec/11/politifacts-lie-year-parkland-student-conspiracies/&gt;. Kumar, D. Vinay, et al. “URLs Link Rot: Implications for Electronic Publishing.” World Digital Libraries 8.1 (2015): 59-66. March for Our Lives. “Mission and Story.” &lt;https://marchforourlives.com/mission-story/&gt;. Marwick, Alice, and Becca Lewis. Media Manipulation and Misinformation Online. Data &amp; Society Research Institute, 2017. &lt;https://datasociety.net/library/media-manipulation-and-disinfo-online/&gt;. Media Matters Staff. “Alex Jones on Florida High School Shooting: It May Be a False Flag, and Democrats Are Suspects.” Media Matters for America 14 Feb. 2018. &lt;https://www.mediamatters.org/alex-jones/alex-jones-florida-high-school-shooting-it-may-be-false-flag-and-democrats-are-suspects&gt;. Muirhead, Russell, and Nancy L. Rosenblum. A Lot of People Are Saying: The New Conspiracism and the Assault on Democracy. Princeton: Princeton University Press, 2019. “I Posted A 4Chan Link a few days ago, that got deleted here, that mentions the coming Social Media Purge by a YouTube insider. Now we are seeing it happen.” Reddit. &lt;https://www.reddit.com/r/conspiracy/comments/7zqria/i_posted_a_4chan_link_a_few_days_ago_that_got/&gt;. Phillips, Whitney, and Ryan M. Milner. You Are Here: A Field Guide for Navigating Polarized Speech, Conspiracy Theories, and Our Polluted Media Landscape. Cambridge: MIT Press, 2021. r/conspiracy. Reddit. &lt;https://www.reddit.com/r/conspiracy/&gt;. Rhodes, Sarah. “Breaking Down Link Rot: The Chesapeake Project Legal Information Archive's Examination of URL Stability.” Law Library Journal 102. 4 (2010): 581-97. Rice, Jenny. Awful Archives: Conspiracy Theory, Rhetoric, and Acts of Evidence. Columbus: Ohio State UP, 2020. Russo, Carla Herreria. “The Rest of the World Showed Up to March for Our Lives.” Huffington Post 25 Mar. 2018. &lt;https://www.huffpost.com/entry/world-protests-march-for-our-lives_n_5ab717f2e4b008c9e5f7eeca&gt;. “Searching for video of Parkland shooting on bitchute.” Reddit. &lt;https://www.reddit.com/r/conspiracy/comments/ddl1s8/searching_for_video_of_parkland_shooting_on/&gt;. Timberg, Craig, and Drew Harwell. “We Studied Thousands of Anonymous Posts about the Parkland Attack – and Found a Conspiracy in the Making.” Washington Post 27 Feb. 2018. &lt;https://www.washingtonpost.com/business/economy/we-studied-thousands-of-anonymous-posts-about-the-parkland-attack---and-found-a-conspiracy-in-the-making/2018/02/27/04a856be-1b20-11e8-b2d9-08e748f892c0_story.html&gt;. “Video exposing David Hogg and Emma Gonzalez as crisis actors and other strange anomalies involving the parkland shooting.” Reddit. &lt;https://www.reddit.com/r/conspiracy/comments/ae3xxp/video_exposing_david_hogg_and_emma_gonzalez_as/&gt;. Wardle, Claire, and Hossein Derakhshan. Information Disorder: Toward and Interdisciplinary Framework for Research and Policymaking. Council of Europe, 2017. &lt;https://rm.coe.int/information-disorder-toward-an-interdisciplinary-framework-for-researc/168076277c&gt;. Yglesias, Matthew. “The Parkland Conspiracy Theories, Explained.” Vox 22 Feb. 2018. &lt;https://www.vox.com/policy-and-politics/2018/2/22/17036018/parkland-conspiracy-theories&gt;. Zittrain, Jonathan, et al. “The Paper of Record Meets an Ephemeral Web: An Examination of Linkrot and Content Drift within The New York Times.” Social Science Research Network 27 Apr. 2021. &lt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3833133&gt;.","",""
"2022","Conspiracy","Conspiracies have been a cultural mainstay for decades (Melley). While often framed as an American problem (Melley), social media has contributed to their global reach (Gerts et al.). Bruns, Harrington, and Hurcombe have traced the contemporary movement of conspiracy theories into the cultural mainstream from fringe conspiracist groups on social media platforms such as Facebook through their greater uptake in more diverse communities and to substantial amplification by celebrities, sports stars, and media outlets. Consequently, conspiracy theories that were once the product of subcultural groups have increasingly mixed into popular and authoritative media (Marwick and Lewis) and entertainment (Hyzen and van den Bulck; van den Bulck and Hyzen). Over the past five years conspiracy theories, whether they be anti-vaccination, politically motivated, or pop-cultural artefacts, have found their way into mainstream cultural discourse. Increasingly, conspiracy theories, once regarded as the domain of largely harmless eccentrics, are having real, material effects. These real-world harms are evident across a number of domains of social life, from the storming of the US Capitol on 6 January 2021 (Moskalenko and McCauley) to the effects of vaccine refusal and resistance which continue to stymie attempts to control the global COVID-19 pandemic (Baker, Wade, and Walsh). Digital spaces and communities have made conspiracy theories more accessible and transmissible. Conspiracies are persistent, resistant, and pervasive. The illusion of neat segmentation between the sites of conspiracy theorising and mainstream media content generation has vanished. However, our understanding of what motivates those engaging with and disseminating conspiracy theories is still partial and incomplete. While there is a large corpus of social psychological research into conspiracies, much of this research is focused on deficits in logic, reasoning, and/or personality traits. The focus of the ‘deficits’ of those draw to conspiracy theories is also reflected in popular discourse, where those believing in conspiracy theories are described within a variety of synonyms for the word ‘stupid’ (Chu, Yuan, and Liu). In this issue, we approach the topic of conspiracy from a different standpoint, exploring the sociological conditions that enable conspiracies to flourish. We have assembled a variety of articles, both empirical and conceptual, from which a more complex social picture of conspiracy emerges. To begin examining the complex social life of conspiracy theories, our feature article by Brownwyn Fredericks, Abraham Bradfield, Sue McAvoy, James Ward, Shea Spierings, Troy Combo, and Agnes Toth-Peter cuts through the conspiracy frame to a very real world example of the consequences of conspiracy. They examine the specific social contexts and media ecologies through which COVID-19 conspiracies have flourished in some (not all) Indigenous communities in Australia. Their analysis highlights the detrimental impacts of unresolved elements of settler colonialism that propagate conspiracist thinking within these communities. Through research conducted with stakeholder participants from the Indigenous health sector (both Indigenous and non-Indigenous) they outline a series of recommendations for how we can constructively address the demonstrated impact of circulating misinformation upon Indigenous communities in Australia. In their recommendations they reinforce the need to centralise Indigenous voices and expertise in our social and political life. Other articles in the issue explore how to theorise conspiracism, present examples of contemporary conspiracism in digital media, unpack methods for how to conduct research in this socially contentious space, and highlight the consequences of conspiracies. They draw examples of communities entangled with conspiracy theories and media environments across the world. Absence and presence (of evidence) are both important elements in conspiracy theorising. In contrast to scholarship that focusses on the spread of conspiracy-style misinformation, Tyler Easterbrook’s examination of dead links or ‘link rot’ online demonstrates how the absence and removal of information can be a powerful motivator of conspiracy rhetoric. Easterbrook’s work demonstrates the potential complexities of moderation models that emphasise the removal of conspiratorial content. The absence of content can be as powerful as its presence. Scott DeJong’s and Alex Bustamante’s article uses novel methods to interrogate the analogies we frequently use when discussing the spread of conspiracy theories online. In designing their own board system to model how conspiracy theories might spread, they speak to a growing body of work that likens conspiracy theories to game systems. DeJong’s and Bustamante’s article highlighted the powerful capacity of creative methods to speak to social problems. Echoing Easterbrook’s warning about the power of content removal to fuel conspiracy theorising, in their simulating DeJong and Bustamante found that there is an “interplay between the removal of content and its spread” and argue that “removing conspiracy is a band-aid solution to a larger problem”. With current attention focussed on the problem of moderating conspiracy and misinformation in digital ecologies, these articles are important considerations about the relative success of such a strategy. In their commentary examining so-called COVID-19 ‘cures’, Stephanie Alice Baker and Alexia Maddox explore how hydroxychloroquine and ivermectin shifted from potential COVID treatments to objects embroiled in conspiracy during the pandemic. Baker and Maddox highlight the interwoven nature of the conspiracy landscape illustrating the roles that public figures and influencers played in amplifying conspiratorial discourse and knowledge about these drugs. Importantly, as with DeJong and Bustamante, and as also highlighted by Easterbrook, they highlight how tackling conspiracy theories is not as simple as providing “accurate” facts to counter false and misleading information. Baker and Maddox argue that, paradoxically, the process of debunking which included mockery and derision “reinforces the audience segmentation that occurs in the current media ecology by virtue of alternative media with mockery and ridicule strengthening in and out group dynamics”. When debunking succumbs to ridicule, they suggest that critics may be strengthening people’s commitment to conspiratorial narratives and alternative influence networks. Tresa LeClerc’s article explores the increasing entanglement of health and wellness with alternative right (or alt-right) conspiracies, focussing on underlying themes of white nationalism within these communities. LeClerc’s piece compellingly traces the ideological underpinnings of purity within the paleo diet that already blend pseudoscience and conspiracy, highlighting the ways wellness spaces have cultivated modes of thinking that are conducive to alt-right conspiracies. Also delving into the intersections of wellness and conspiracy, Marie Heřmanová  explores conspirituality and the politicisation of spiritual influencers during the COVID-19 pandemic, focusing on the case of prominent Czech lifestyle Instagrammer Helena Houdová who became an outspoken anti-vaxxer and COVID denialist. In a rich case study, Heřmanová  examines the ways Helena blends her feminine aesthetic and aspirational and individualistic take on spirituality with conspiracy messages informed by QAnon and political messaging that speaks to both national history and global trends. Heřmanová astutely observes that the rise of conspirituality reveals the capacity of these influencers to bridge the gap between the everyday and personal, and the collective narratives of conspiracies such as QAnon.     Continuing to explore how conspiracy theories intersect with embodied and digital environments, in her article on ‘Coronaconspiracies’ Merlyna Lim examines the role algorithms and users play in facilitating conspiracy theories during the pandemic. Lim contends that social media provides a fertile environment for conspiracies to flourish, while maintaining that “social media algorithms do not have an absolute hegemony in translating the high visibility or even the virality of conspiracy theories into the beliefs in them”. As Lim explains, human users retain their agency online; it is their “choices” and “preferences” that are informed by the algorithmic dynamics of these technologies. Extending research into the relationship between conspiracy and algorithms, the impacts of labelling are foregrounded in the work of Ahmed Al-Rawi, Carmen Celestini, Nicole Stewart, and Nathan Worku. Their article presents a reverse-engineering approach to understanding how Google’s autocomplete feature assigns subtitles to widely known conspiracists. Google’s algorithmic approach to labelling actors is proprietary knowledge, which blackboxes this process to researchers and the wider public. This article provides a technical peek into how this may work, but also raises the concern that these labels do not reflect what is publicly known about these actors. Their work provides an insight into the ways that the Google autocomplete subtitling feature may further contribute to the negative real-world impacts that these conspiracists, and other such toxic actors, have. Stijn Peeters and Tom Willaert take us into the fringes of the online ecosystem to explore ways to research conspiracist communities on Telegram. They extrapolate on Richard Rogers‘s edict to ​​repurpose the methods of the medium and take us through a case-based examination of how to conduct a structural analysis of forwarded messages to identify conspiracy communities. In weighing up the results of applying this technique to Dutch-speaking conspiracist narratives and communities on Telegram they highlight the methodological gains of such a technique and the ethical considerations that doing this style of data gathering and analysis can raise. Moving away from the fringes, Naomi Smith and Clare Southerton take us into the belly of popular culture with their examination of the #FreeBritney movement and raise the proposition of conspiracy as a site of pleasure. They turn on its head the assumption that conspiracy thinking is because of a deficient and deviant understanding and point to the appeal and pleasure of engaging in the chase of partial threads and leads found in social media that could be woven into an explanation, or conspiracy. Drawing from fan studies, they highlight that pleasure is not a new site of motivation and that a lot can be learned by applying it as an explanatory frame for why people engage with conspiracies. The diverse body of scholarship assembled in this special issue illustrates the complex nature of contemporary conspiracies as they find expression in digital spaces and media. There are a variety of approaches to understanding this phenomenon that highlight how strategies of control and technological intervention may not be straightforwardly successful. The contributions to this issue demonstrate, from a range of perspectives, the importance of understanding how and why conspiracy theories matter to the communities that embrace them if we are to address their social consequences. References Baker, Stephanie Alice, Matthew Wade, and Michael James Walsh. """"The Challenges of Responding to Misinformation during a Pandemic: Content Moderation and the Limitations of the Concept of Harm."""" Media International Australia 177 (2020): 103-07. Bruns, Axel, Stephen Harrington, and Edward Hurcombe. “‘Corona? 5G? Or Both?’: The Dynamics of COVID-19/5G Conspiracy Theories on Facebook."""" Media International Australia 177 (2020): 12-29. Chu, Haoran, Shupei Yuan, and Sixiao Liu. """"Call Them Covidiots: Exploring the Effects of Aggressive Communication Style and Psychological Distance in the Communication of Covid-19."""" Public Understanding of Science 30.3 (2021): 240-57. Gerts, Dax, et al. “‘Thought I’d Share First’ and Other Conspiracy Theory Tweets from the Covid-19 Infodemic: Exploratory Study."""" JMIR Public Health Surveill 7.4 (2021): e26527. Hyzen, Aaron, and Hilde van den Bulck. """"Conspiracies, Ideological Entrepreneurs, and Digital Popular Culture."""" Media and Communication 9 (2021): 179–88. Marwick, Alice, and Rebecca Lewis. """"Media Manipulation and Disinformation Online."""" New York: Data &amp; Society Research Institute, 2017. 7-19. Melley, Timothy. Empire of Conspiracy: The Culture of Paranoia in Postwar America. Cornell University Press, 2016. Moskalenko, Sophia, and Clark McCauley. """"QAnon: Radical Opinion Versus Radical Action."""" Perspectives on Terrorism 15.2 (2021): 142-46. Van den Bulck, Hilde, and Aaron Hyzen. """"Of Lizards and Ideological Entrepreneurs: Alex Jones and Infowars in the Relationship between Populist Nationalism and the Post-Global Media Ecology."""" International Communication Gazette 82.1 (2020): 42-59.","",""
"2022","Cloaked science: the Yan reports","ABSTRACT This paper describes a 2020 disinformation campaign promoting the unsubstantiated claim that the novel coronavirus is the product of a Chinese bioweapons program. Exploiting a vulnerability in open-access scientific publishing, the campaign was based on papers posted to an online preprint repository designed to accelerate the diffusion of scientific knowledge. This provided the campaign with an air of scientific legitimacy, helped it reach millions of Americans, and muddied public discourse over the origins of SARS-CoV-2. This case study offers insights into the tactics and practices of media manipulation, the contested nature of modern epistemic systems, the interplay of technical and social systems, and the vulnerability of open systems to manipulation.","",""
"2022","Media and information literacy for developing resistance to ‘infodemic’: lessons to be learnt from the binge of misinformation during COVID-19 pandemic"," The COVID-19 pandemic was accompanied by the spurt of misinformation, which was termed as ‘infodemic’ and ‘disinfodemic’, swaying the health decisions of the populace. There was the binge of bizarre information which putatively intensified the coronavirus and consequent fatalities due to relying on false information. The overview provides essence of infodemic during COVID-19 situation, mainly actuated through social media platforms. The absence of au courant media and information literacy skills amongst masses as they were unable to extricate the trustworthy information from the substantial available information they were accessing on their gadgets, underpins the need for immediate action to curtail any further infodemic. Literature accessed from the Internet was documented, analyzed, and compiled. The splurge of misinformation during COVID-19 pandemic, bizarre instances of infodemic, efforts of social media platforms to curb it, need for strengthening media and information literacy of folks and role of libraries and educational institutions in accomplishing this have been discussed. The prevalent milieu necessitates the need for empowering folks with media and information literacy skills for developing critical thinking skills amongst them for managing any future outflow of misinformation. ","",""
"2022","Ideological variation in preferred content and source credibility on Reddit during  the COVID-19 pandemic"," In this exploratory study, we examine political polarization regarding the online discussion of the COVID-19 pandemic. We use data from Reddit to explore the differences in the topics emphasized by different subreddits according to political ideology. We also examine whether there are systematic differences in the credibility of sources shared by the subscribers of subreddits that vary by ideology, and in the tendency to share information from sources implicated in spreading COVID-19 misinformation. Our results show polarization in topics of discussion: the Trump, White House, and economic relief topics are statistically more prominent in liberal subreddits, and China and deaths topics are more prominent in conservative subreddits. There are also significant differences between liberal and conservative subreddits in their preferences for news sources. Liberal subreddits share and discuss articles from more credible news sources than conservative subreddits, and conservative subreddits are more likely than liberal subreddits to share articles from sites flagged for publishing COVID-19 misinformation. ","",""
"2022","Conspiracy theories in digital environments: Moving the research field forward","In the past few years, the discussion of conspiracy theories has embroiled researchers, politicians and the public alike. During the COVID-19 pandemic in particular, the term ‘conspiracy theory’ became a buzzword in the news media, public communication and everyday discussions. The pandemic also demonstrated that conspiratorial narratives disseminated online are not benign, obscure and eventually harmless ideas, but can mislead policy making, hinder crisis relief and public health efforts, or undermine trust in institutions and science. Factors contributing to the prevalence of conspiracy theories are complex and include psychological as well as socio-political factors. This special issue focuses specifically on the role of digital media and how they shape the dissemination and mitigation of, as well as research on, conspiracy theories. The special issue includes 13 research articles by authors from 11 countries and regions, which provide timely insights into the phenomenon of conspiracy theories with cross-cultural and cross-platform advances.","",""
"2022","<b>To Convince, to Provoke or to Entertain? A Study on Individual Motivations behind Engaging with Conspiracy Theories Online</b>","The growing dissemination of conspiracy theories on social media has challenged the well-being of societies. This study aims to understand why individuals would engage with conspiracy theories and what role specific beliefs, but also individual factors such as personality traits play. To answer these questions, we conducted surveys in six countries (Belgium, Switzerland, Germany, France, the UK and the U.S.) and investigate three motivations (conviction, entertainment and reaction provocation) behind the dissemination of conspiracy content on social media. Our findings demonstrate that across issues, individuals who indicated they would engage with conspiracy theories do it mainly because they are convinced by the message. Political orientation and issue attitudes proof to be connected to individual engagement with conspiracy theories out of conviction, while dark personality traits such as narcissism and psychopathy are valid predictors for why individuals would disseminate conspiracy theories out of entertainment reasons or to provoke reactions.","",""
"2022","You the readers will complete the list. The Castrochavismo conspiracy theory","The study of conspiracy theories has often taken a normative perspective. Recently, sociology and cultural studies have argued for a neutral view of conspiracy theories, urging for a nuanced understanding of conspiracy theories. Building on a growing body of scholarship addressing conspiracy theories in relational terms, this article considers the advantages of analyzing conspiracy theories as controversies or issues, as formulated in Science and Technology Studies. Our primary focus is on the political uses of castrochavismo and how the digital objects are shared and reconfigured, shaping the dynamics of the issue. The core conceptual premise is that conspiracy theories are formed as particular cases of controversies, where the factions in dispute deploy repertories, and we identify the empiricist and ironizing ones. Drawing on a data collection from Twitter, we present the case of study of castrochavismo, with a particular focus in the Colombian electoral cycle. The case highlights the importance and challenges when analyzing a conspiracy theory from the lens of controversy studies, promoting a symetrical, non-normative, research of the topic.","",""
"2022","Coronavirus meets the clash of civilizations","Conspiracy Theories (CTs) are a global phenomenon, but some societies are better equipped than others to resist them. This article discusses the characteristics of the China-related COVID-19 CTs in the Brazilian Facebook, based on 28,312 posts published from January 2020 to June 2021. We argue that, in Brazil, the spread of CTs was facilitated by a widespread political and knowledge institutions’ legitimacy crisis. The rise of the extremist politician Jair Bolsonaro to the Presidency provides evidence in this regard. In consequence, the boundaries between fringe and mainstream politics become porous. This article discusses which agents disseminate China-related COVID-19 CTs, and which topics receive more attention. We found a significant presence of actors belonging to mainstream politics and the media among the CTs’ main disseminators. Additionally, the CTs circulating in the Brazilian social media environment reproduce concerns about China’s growing presence in the global arena, which originate elsewhere. Still, they add a specific emphasis on the Communist threat. We sustain that this emphasis relates as much to Brazil’s internal politics as to China itself.","",""
"2022","Mindsets of conspiracy: A typology of affinities towards conspiracy myths in digital environments","In times of crisis, the spread of conspiracy myths increases since people seek answers to complex questions. Besides societal aspects, social media platforms, especially messenger services, have been identified as a positive driver for spreading conspiracy myths. Much research focused on whether right-wing populist attitudes correlate with belief in conspiracy myths resulting in inconsistent findings. We show that different anti-system attitudes and corresponding digital media usage can promote the affinity towards conspiracy myths apart from right-wing attitudes. With this paper, we first want to sharpen the terminology on ‘conspiracy myths’ and develop a scale to measure affinity towards conspiracy myths in different dimensions. We second use this scale to investigate different mindsets of conspiracy in the Swiss population. Third, we want to find out how the dimensions correlate with messenger usage. Based on data from a representative population survey in Switzerland from November to December 2020, we investigated different affinities towards conspiracy myths, represented by far-left, far-right, populist, anti-elitism, general anti-system attitudes and science skepticism. We then used the six dimensions in a cluster analysis and identified five typological mindsets. About 30% of the population accordingly have higher affinities towards conspiracy myths than the rest. Our study also highlights the potential role of messenger services in spreading conspiracy myths. To a certain extent, Facebook Messenger and Telegram usage show a robust correlation with the different dimensions of the affinity towards conspiracy myths. In contrast, WhatsApp usage does not show a robust correlation.","",""
"2022","WeChat users’ debunking strategies in response to COVID-19 conspiracy theories: A mixed-methods study"," The current study adopted a mixed-methods approach to examine both qualitative and quantitative data of Chinese WeChat users’ strategies in response to COVID-19 conspiracy theories disseminated in WeChat. Thematic analysis based on 30 interviewees suggested interesting patterns about how such conspiracy theories were disseminated based on relationship types within WeChat groups and how different types of debunking strategies were used to counter conspiracy theories based on the relational outcomes and contexts. Quantitative data based on 588 participants suggested COVID-19 information exposures from different sources, conspiracy beliefs, exposures of conspiracy beliefs and face concerns influence WeChat users’ responses to address COVID-19-related information in WeChat platform. ","",""
"2022","Deep state phobia: Narrative convergence in coronavirus conspiracism on Instagram","Recent scholarship has established that conspiracist narratives proliferated in mainstream online discourse during the coronavirus pandemic. This proliferation has been provocatively characterized as a ‘conspiracy singularity’ in which previously divergent conspiracy narratives converged into a single, overarching narrative. Yet while the idea of narrative convergence has long figured in conspiracy theory research, empirical evidence has been scarce. The present article aims to address this gap by means of an investigation of an archive containing over 470,000 conspiracy-related Instagram posts from 2020. Given the size and conceptual complexity of the dataset, the paper introduces a ‘digital hermeneutics’ approach, which combines data science methods with qualitative interpretation and theorization. Operating across three levels of observation (hashtag analysis, text analysis, and image analysis) we identify patterns of convergence among different conspiracy narratives (including anti-vax, QAnon, anti-5G, and ‘The Great Reset’) over the year 2020 as well as the apparent role of protagonists and antagonists (notably Donald Trump and Bill Gates) in creating connections. In interpreting these findings we focus on the concept of ‘the Deep State’ as a bridge between various conspiracist narratives, which seems to cut diagonally across political ideologies.","",""
"2022","Tracing a historical development of conspiracy theory networks on the web: The hyperlink network of vaccine hesitancy on the Danish web 2006–2015"," This article investigates the vast field of conspiracy theories by focusing on the example of conspiracy theories related to vaccine hesitancy. Conspiracy theories have been with us for a long time, and as any other type of semantic content they spread by travelling through media. Therefore, if one wants to understand how conspiracy theories proliferate, it is relevant to investigate the media roads by which they travel and that each offer different opportunities for establishing connections. It is obvious that within the last three decades the advent of digital media has opened up new road systems to support conspiracy theories’ getting around. This article focuses on one such road system, the World Wide Web, and how the hyperlink networks on the Danish web related to conspiracy theories and vaccine hesitancy have developed from 2006 to 2015. The article aims at (1) contributing to the development of methods that enables such a study, and (2) providing results about how these hyperlink networks have developed. The network analysis reveals that the potential exchange of ideas about vaccination between experts and non-experts is not facilitated by the media material structures in either of the years, since almost no links exist between the two actor types, at least not on the physical performative level of hyperlinks. Experts are connected, but non-experts as a whole tend to function as an archipelago of isolated islands—isolated from the experts, and by and large isolated from each other. This tendency has remained almost the same throughout the investigated period. News media that one could expect to function as brokers connecting experts and non-experts are not particularly well-connected in the network and apparently do not mediate between actor types. ","",""
"2022","Measuring the diffusion of conspiracy theories in digital information ecologies","Digital platforms and media are fertile breeding grounds for disinformation and conspirational views. They provide a variety of communication venues for a mixed set of actors and foster the diffusion of content between actor groups, across platforms and media, and across languages and geographical spaces. Understanding those diffusion processes requires approaches to measure the prevalence and spread of communicative acts within and across digital platforms. Given the increasing access to digital data, computational methods provide new possibilities to capture this spread and do justice to the interrelated nature and hybridity of online communication. Against this background, the paper focuses on the spread of conspiracy theories in digital information ecologies. It provides a review of recent methodological approaches to measuring conspiracy-related content online regarding the (a) prevalence and (b) diffusion of conspiracy theories. To that end, the paper differentiates between social network analysis approaches and computational techniques of automated text classification. It further discusses how far these and related computational approaches could facilitate studying the diffusion of conspiracy theories across different actor types, languages, topics and platforms. In doing so, it takes the specific nature of online communication and challenges in the field of conspiracy-related content into account.","",""
"2022","Authority-led conspiracy theories in China during the COVID-19 pandemic – Exploring the thematic features and rhetoric strategies"," The COVID-19 pandemic has witnessed the flourish of various conspiracy theories globally, where governmental authorities have often played an essential role in spreading and promoting such misleading information. This study examines the authority-led conspiracy theories in China by analysing 44,068 conspiracy theory relevant Weibo posts. The sample was collected by 46 sets of keywords representing popular conspiracy theories circulating online in the pandemic. Through structural topic modelling and textual analysis, we revealed the thematic features and rhetorical strategies of authority-led conspiracy posts. Authorities were found to employ conspiratorial narrative as a regular tactic in the political discourse, and often phrased them in a connotative manner with specific rhetorical strategies. And non-authority users would often elaborate relevant topics endorsed by authorities to apparent conspiracy theories. This study enriches our understanding of authority’s role in spreading conspiracy theories during uncertain times in authoritarian countries. ","",""
"2022","At the onset of an infodemic: Geographic and disciplinary boundaries in researching problematic COVID-19 information","This paper analyzes the literature on problematic COVID-19 information published at the onset of the pandemic in 2020. It explores how scientific research has addressed this issue from a disciplinary, methodological and substantive perspective in different world regions. Three hundred seventy-eight articles were analyzed using content analysis and computational methods, including social network analysis and text mining. The study revealed a multidisciplinary field characterized by substantial contributions from medicine and social sciences and with a certain degree of interdisciplinarity and international collaborations. Research devoted particular attention to infodemic and conspiracy theories and their impact on compliance with health-protective behaviors, and showed a general preference for quantitative methods such as surveys. Most contributions focused on European and Americas regions and were from authors working in the same areas. Attention to various topics was also geographically differentiated. For example, conspiracy theories and informational factors that may influence COVID-compliant behaviors characterize the whole corpus, but Western research did the lion’s share. Similarly, the attention paid to different social media platforms differed geographically. Worldwide attention was dedicated to global social media platforms such as Twitter, WhatsApp, Facebook and Instagram. In contrast, geographically specific platforms (e.g., VKontakte or WeChat) have been studied less and mainly in regions where they are used the most. Focusing on a crucial period of the COVID-19 infodemic such as the first year of the pandemic, this study contributes to the literature on problematic information by highlighting how misinformation and disinformation research was carried out within and at the crossroads of disciplines and geographic regions. It suggests under-explored areas by contrast with the most relevant trends and establishes a foundation for benchmarking with post-2020 research on this topic.  ","",""
"2022","Conspiracy, anxiety, ontology: Theorising QAnon","The rise of QAnon presents researchers with a number of important questions. While emerging literature provides insights into how QAnon exists online, there is a dearth of theoretical engagement with the questions of why it exists, and what conditions brought it into being. This paper seeks to address this gap by contextualizing QAnon as an ontological phenomenon underpinned by anxiety, and inquiring into the identity formation strategies employed by the movement. Applying the basic precepts of discourse theory and discourse analysis to a representative canon of QAnon content, it finds that, like other formations of collective identity, QAnon is premised on interconnected dynamics of ontological fulfillment that cannot be explained away by pointing to ‘the algorithm’ or ‘madness’. Nor can it be tackled effectively by the content takedowns and de-platforming strategies currently employed. The paper concludes with a call to explore more empathetic engagement with conspiracy adherents, arguing that until we (re)discover a more inclusive, agonistic politics, QAnon and other fantastical conspiracy movements will continue to arise and some may metastasize into violent action. New forms of resilience to (online) polarization can be built on this principle.","",""
"2022","Combating misinformation in times of COVID-19: A comparison of the social network strategies of the Spanish government and the autonomous communities","Social networks offer excellent opportunities for healthcare organizations to disseminate information and communicate with individuals during a health crisis, since they can influence health-related decisions and perspectives. In this paper we compare strategies carried out by the Spanish government and the autonomous community authorities on several social networks (Facebook, Instagram, and Twitter). In spite of offering two-way communications, actual use of such platforms was one-way. In addition, some agencies did not have profiles on these platforms; in fact, two agenices were not present on Instagram, the platform with the youngest users. Finally, there was an increasing use of images on social network postings. Results demonstrate that in times of health crisis, some governmental agencies employ social networks mainly as a tool to disseminate information.","",""
"2022","Understanding the #plandemic: Core framings on Twitter and what this tells us about countering online far right COVID-19 conspiracies","This paper examines the need and possibility for developing online resilience-based approaches in response to COVID-19 vaccine conspiracies, often linked to the far right. Examining three datasets collected between December 2020 and April 2021, this paper details conspiracy narratives that have developed around COVID-19 vaccines, with specific focus on understanding the deployment of the idea of a planned pandemic or so-called ‘#plandemic’. This is then used to consider where existing resilience-based approaches to countering off-line polarisation and extremism might posit an appropriate online response. The article identifies four key #plandemic framings of COVID-19 vaccines — as control, as reset, as unnecessary and as unsafe — and analyses how these themes are constructed, to find that they are often created through hostile and confrontational interaction with other users. Based on these findings, the conclusion suggests companies shift their focus away from ‘negative’ approaches to content moderation (e.g., content removal) and towards resilience-building responses that cultivate flexible individual identities, build community support networks, and/or engage users with national and supranational democratic structures, as a more effective response to the sharing of online conspiracies.","",""
"2022","COVID-19 Conspiracy Theories Discussion on Twitter"," The coronavirus disease 2019 (COVID-19) pandemic was an unexpected event and resulted in catastrophic consequences with long-lasting behavioral effects. People began to seek explanations for different aspects of COVID-19 and resorted to conspiracy narratives. The objective of this article is to analyze the changes on the discussion of different COVID-19 conspiracy theories throughout the pandemic on Twitter. We have collected a data set of 1.269 million tweets associated with the discussion on conspiracy theories between January 2020 and November 2021. The data set includes tweets related to eight conspiracy theories: the 5G, Big Pharma, Bill Gates, biological weapon, exaggeration, FilmYourHospital, genetically modified organism (GMO), and the vaccines conspiracy. The analysis highlights several behaviors in the discussion of conspiracy theories and allows categorizing them into four groups. The first group are conspiracy theories that peaked at the beginning of the pandemic and sharply declined afterwards, including the 5G and FilmYourHospital conspiracies. The second group associated with the Big Pharma and vaccination-related conspiracy whose role increased as the pandemic progressed. The third are conspiracies that remained persistent throughout the pandemic such as exaggeration and Bill Gates conspiracies. The fourth are those that had multiple peaks at different times of the pandemic including the GMO and biological weapon conspiracies. In addition, the number of COVID-19 new cases was found to be a significant predictor for the next week tweet frequency for most of the conspiracies. ","",""
"2022","Folk Theories of Avoiding Content Moderation: How Vaccine-Opposed Influencers Amplify Vaccine Opposition on Instagram"," This study analyzes how vaccine-opposed users on Instagram share anti-vaccine content despite facing growing moderation attempts by the platform. Through a thematic analysis of Instagram content (in-feed and ephemeral “stories”) of a sample of vaccine-opposed Instagram users, we explore the observable tactics deployed by vaccine-opposed users in their attempts to avoid content moderation and amplify anti-vaccination content. Tactics range from lexical variations to encode vaccine-related keywords, to the creative use of Instagram features and affordances. The emergence of such tactics exists as a type of “folk theorization”—the cultivation of non-professional knowledge of how visibility on the platform works. Findings highlight the complications of content moderation as a route to minimizing misinformation, the consequences of algorithmic opacity and knowledge-building within problematic online communities. ","",""
"2022","Picturing Opaque Power: How Conspiracy Theorists Construct Oppositional Videos on YouTube"," Conspiracy theories were once perceived as delusions of individuals on the fringes of society, but have become commonplace in mainstream culture. Today, they are produced, consumed, and circulated on various online media environments. From memes on 4chan, QAnon influencers on Instagram, to flat earth or antivaxx videos on YouTube, modern-day conspiracy culture embodies compelling mediated images and narratives that are composed of various audiovisual materials. Building on Stuart Hall’s encoding/decoding model, and Henry Jenkins’ notion of “participatory culture,” we analyze these audiovisual conspiracy theories as “oppositional readings” of hegemonic truths. More concretely, we analyze how conspiracy theorists reconstruct various audiovisual (mass-media) materials into streamlined narratives on YouTube videos to picture opaque power. Based on an in-depth qualitative analysis of 24 conspiracy theory videos, strategically selected from a larger sample of 200, we present three major categories of audiovisual narrative construction in conspiracy videos on YouTube: (1) Simulating: using fiction, religious and cultural images and narratives to render images of events otherwise invisible; (2) Deciphering: decoding hidden messages by “closely reading” images and looking for hidden symbolism; (3) Exhibiting: exposing information, research, and images that are “hidden in plain sight” but point to conspiracy. This article contributes to the growing body of literature on conspiracy theories by showing how they are not just texts, but should better be seen as media practices involving the recontextualizing of (mass)media material into new audiovisual conspiracy theory narratives. This shapes not just their content and form, but also their place in public discourse. ","",""
"2023","Taxonomizing Information           Practices in a Large           Conspiracy Movement:           Using Early QAnon as a           Case Study","abstract:This article presents a taxonomy of the information practices apparent in an imageboard discussion thread that was influential in jump-starting the worldwide QAnon movement. After introducing QAnon with a review of literature, the author examines 4Chan /pol/ thread #147547939 (key in introducing multiple key elements of the QAnon narrative) to enumerate and classify the information practices deployed by discussion participants. In conclusion, the article expands beyond existing research's previous focus on outright fabrication, showing that early QAnon participants' information practices are also defined in large part by suspicious and idiosyncratic modes of reading authentic sources, not simply the propagation of falsehoods.","",""
"2023","‘A mother’s intuition: it’s real and we have to believe in it’: how the maternal is used to promote vaccine refusal on Instagram","ABSTRACT In this article we examine the proliferation of anti-vaccine content on social media during the COVID-19 pandemic. We employ a case study approach to analyse the techniques used by 13 anti-vaccine influencers to promote vaccine refusal on Instagram for 19 months from January 2020 to July 2021. Our findings reveal that the maternal is strategically invoked in anti-vaccine content by appealing to three interrelated ideal types: the protective mother; the intuitive mother and the doting mother. These portrayals of the maternal are used to encourage vaccine refusal by presenting hegemonic ideals of the ‘good mother’ as one who is natural, holistic and authentic; depicting anti-vaccination as a feminine ideal to which mothers ought to aspire. Authenticity is framed here as a form of embodied expertise, uncorrupted by culture, politics and the medical establishment. Our findings question the pejorative portrayal of suburban mothers in popular media as critical actors in the anti-vaccine movement by revealing the ways anti-vaccine influencers strategically target mothers on social media to achieve visibility, attention and to support their cause.","",""
"2023","Digital inequalities and public health during COVID-19: media dependency and vaccination","ABSTRACT During the COVID-19 pandemic information about the transmission of the virus came out slowly and recommended practices changed over time. This made communication media, like the Internet, especially important. Few prior studies have considered how digital inequalities influence information flows. Building on three research streams – vaccine hesitancy, information-seeking, and digital inequalities – we examine how digital inequalities, health media, and mass media affect COVID-19 vaccine hesitancy. Using representative survey data of US Internet users, our structural equation model demonstrates the importance of digital inequalities and media use for vaccine hesitancy. Digital inclusion plays an important role in public health. It leads to increased health information-seeking, which reduces vaccine hesitancy. Our model presents evidence supporting a comprehensive policy approach to vaccine hesitancy beyond factors like socio-demographics and prior health beliefs to include broader factors like digital equity measures and sources of health information. Where and how people find information on public health issues seems to be as important as demographics.","",""
"2023","Who are the plotters behind the pandemic? Comparing Covid-19 conspiracy theories in Google search results across five key target countries of Russia’s foreign communication","ABSTRACT This article advances extant research that has audited search algorithms for misinformation in four respects. Firstly, this is the first misinformation audit not to implement a national but a cross-national research design. Secondly, it retrieves results not in response to the most popular query terms. Instead, it theorizes two semantic dimensions of search terms and illustrates how they impact the number of misinformative results returned. Furthermore, the analysis not only captures the mere presence of misinformative content but in addition whether the source websites are affiliated with a key misinformation actor (Russia’s ruling elites) and whom the conspiracy narratives cast as the malicious plotters. Empirically, the audit compares Covid-19 conspiracy theories in Google search results across 5 key target countries of Russia’s foreign communication (Belarus, Estonia, Germany, Ukraine, and the US) and Russia as of November 2020 (N = 5280 search results). It finds that, across all countries, primarily content published by mass media organizations rendered conspiracy theories visible in search results. Conspiratorial content published on websites affiliated with Russia’s ruling elites was retrieved in the Belarusian, German and Russian contexts. Across all countries, the majority of conspiracy narratives suspected plotters from China. Malicious actors from the US were insinuated exclusively by sources affiliated with Russia’s elites. Overall, conspiracy narratives did not primarily deepen divides within but between national communities, since – across all countries – only plotters from beyond the national borders were blamed. To conclude, the article discusses methodological advice and promising paths of research for future cross-national search engine audits.","",""
"2023","Mapping the connections of health professionals to COVID-19 myths and facts in the Australian Twittersphere","ABSTRACT The spread of COVID-19 misinformation on social media has elicited concern amongst scholars, health agencies and governments owing to its potential harms to public health. This article addresses the question of how networks of Australian health professionals engaged with COVID-19 facts and myths on Twitter between August and October 2020. After reviewing selected literature on COVID-19 misinformation, we present our analytical choices and the methodology we used to constitute datasets of COVID-19 factual and mythical hashtags and of verified Australian health professional accounts (N:377). The article distinguishes between the capacities of ‘actor-actants’ and ‘issue-actants,’ and between the adoption of ‘field’ and ‘contested’ hashtags during a controversy. We identify categories of Australian health professional Twitter accounts such as GPs, nurses, specialists, public health professionals and researchers, and analyse the patterns of connections between these actor-actants and COVID-19 facts and myths. We find that these categories exhibit clearly distinct behaviour when tweeting or retweeting factual and mythical hashtags. Even though the rate of Australian health professionals’ connection with myths in comparison to facts on Twitter is low, hashtags such as #hydroxychloroquine attracted significant engagement. We examine these hashtags’ context and find that they were mainly being debunked, though a minority of accounts endorsed them. We analyse these adoption patterns, and critically assess the ‘echo chamber effect.’ We also consider public health and privacy implications for the dissemination of accurate information, for trust in health professionals during a pandemic, and for combatting misinformation.","",""
"2023","Clones and zombies: rethinking conspiracy theories and the digital public sphere through a (post)-colonial perspective","ABSTRACT This article investigates what is at stake in decolonising the study of conspiracy theories online. It challenges the confidence with which conspiracy theories are often dismissed as aberrations and negative externalities of digital ecosystems. Without reifying conspiracy theories, we identify as problematic how alternative forms of knowledge production are dismissed and colonial tropes reproduced. Contributing to conversations around ‘decolonising the internet’, we offer additional and sharper tools to understand the role and implications of conspiracy theorising for communicative and political practices in different societies globally. Empirically, we analyse a conspiracy theory circulating in Nigeria between 2018 and 2019 purporting that Nigerian President Buhari had died and the man in office was his ‘clone’. Conceptually, our analysis intersects with Achille Mbembe’s work on power in the postcolony, to illustrate how it is possible to adopt alternative forms of normativity that eschew the stigmatisation and exclusion that has prevailed, but still offer evaluative frameworks to locate conspiracy theories in contemporary digital environments. We engage with Mbembe’s ideas about how humorous and grotesque forms of communication can result in the zombification of both the ‘dominant’ and those ‘apparently dominated’. We argue that zombification as a theoretical intervention provides a useful addition to the conceptual and normative repertoire of those studying conspiracy theories, between the poles of dismissal/condemnation and pure curiosity/acceptance of what is said.","",""
"2023","De-platforming disinformation: conspiracy theories and their control","ABSTRACT Informed by two case studies of de-platforming interventions performed by Facebook against two high profile conspiracy theorists who had been messaging about Covid-19, this article investigates how de-platforming functions as an instrument of social control, illuminating the intended and unintended effects it induces. To help interpret the patterns in the data, two novel conceptual innovations are introduced. The concept of ‘minion accounts’ captures how following a de-platforming intervention, a series of secondary accounts are set up to continue the mission. Such accounts are part of a wider retinue of ‘re-platforming’ behaviours. Overall, the empirical evidence reviewed suggests that whilst de-platforming can constrain transmission of conspiratorial disinformation, it does not eradicate it.","",""
"2023","The establishing of subject positions in Swedish news media discourses during the first year of the COVID-19 pandemic"," The COVID-19 pandemic has dominated the global media since 2020. To a large extent, it is via the news media that the public has learned about the risks, levels of danger, governmental regulations and mandatory actions. This article highlights the subject positions constructed by the Swedish news media from January 2020 to February 2021 in reports about the pandemic. The result shows that citizens can be active-passive or solitary solidarity, these positions appeal to individual accountability, thus potentially shaping and fostering citizens in line with the Swedish government’s wider response to the pandemic. The news media’s images are of self-regulated citizens who govern and discipline themselves and others according to the current discourses, all of which simultaneously evoke fear, togetherness and hope. The ideological dilemmas for citizens are whether to be active-passive or, if necessary, switch to the solitary solidarity subject position. ","",""
"2023","COVID ONLY ON THURSDAYS: DEALING WITH AN INFODEMIC IN LANGUAGE- AND REGION-BASED ONLINE COMMUNITIES","Facing the ‘infodemic’ accompanying the coronavirus pandemic, many online communities were faced with questions of how to organize and moderate a deluge of information regarding the virus. On the social media platform Reddit, new communities (subreddits) about the virus quickly emerged. Accompanied by some negotiation and contestation, r/coronavirus eventually became the official platform-wide information channel about the pandemic. However, many existing subreddits nevertheless had to decide how to deal with the topic. Should they ban it from their community and refer to the official subreddit, allow the topic without restrictions, or adapt existing moderation mechanisms to the evolving situation? New localized communities emerged for discussing the course of the pandemic in particular countries or regions. Additionally, communities in languages other than English were facing challenges in dealing with covid-19 content. In this submission, I explore moderation strategies in several region- and language-based subreddits to examine how rules in existing online communities were changed in the light of pandemic information. ","",""
"2023","CONTESTING THE COVID-19 CONSENSUS: CONNECTIVE ACTION OF PSEUDOANONYMOUS ACCOUNTS ON FINNISH TWITTER","Recent years have witnessed invigorated debate about the contradictions of identity-concealing public participation and its influence on democratic deliberation and policy-making. Some have documented the exploitation of misrepresented identities for manipulation, including the manufacturing and disrupting of political discourses. Others argue that pseudonymous speech fosters less conformist and more secure public spaces for critical social commentary and political activism. In this study, we extend this dichotomy by approaching pseudoanonymous online action as connective action; as focused and communicative political activism aimed to contest the authorities’ Covid-19 response during the pandemic in Finland.  Our empirical analysis uses the historical Twitter data, over 4.2 million tweets, by pseudoanonymous Twitter accounts predominantly tweeting about Covid-19 (n = 229). Using mention and retweet networks combined with profile metrics and qualitative reading of tweets filtered through the network analysis, we explore the networked activity of these accounts, the strategies they have employed to contest authorities, and their connections to political actors and other more organized forms of mobilization around Covid-19.  The preliminary analysis highlights the prominence of these accounts in Covid-19 related Twitter discussions, along with their distinct behavioural features. Pseudoanonymous accounts average shorter lifespans and higher deactivation rates. Some accounts have transitioned from named accounts towards identity-concealing action during the observation period. Further, our analysis reveals a symbiotic relationship between the pseudoanonymous accounts and the established Covid-19 movements. We argue that pseudoanonymous participation is an increasingly emblematic form of connective action afforded by social media platforms, particularly fitted to undermine the legitimacy of authorities.","",""
"2023","PLATFORMIZATION OF CONSPIRACISM: INTRODUCING A THEORETICAL FRAMEWORK FOR INVESTIGATING CONSPIRACY THEORIES ON """"ALTERNATIVE"""" PLATFORMS USING A CASE STUDY OF BITCHUTE AND GAB","Digital platforms have modified conspiracy theory communication as they enable users to publicly share their support, to circumvent traditional gatekeepers, and to form like-minded communities. However, the impacts of conspiracy discourses differ greatly between platforms. By drawing on critical platform studies, this article introduces a theoretical and analytical framework to investigate the interplay and mutual shaping between “alternative” platforms and conspiracy theory communication, which we describe as the _platformization of conspiracism_. Along the four interconnected dimensions of our framework – _infrastructure_, _economic model_, _governance_, and _user culture_ of platforms – we examine _BitChute_ and _Gab_ in the context of conspiracy theory communication. To investigate the platforms' technological features, business model, and governance practices, we conduct a documentation analysis of media reports and the platforms own news updates alongside an in-depth examination of each platform's functionality and interface. To gain insights into the user culture, i.e., the key characteristics and monetizing strategies of conspiracy theory propagators, we analyze 20 prominent conspiracy theory channels and profiles from BitChute and Gab, respectively. Findings from our study shed light on how both platforms have positioned themselves as technological equivalents to their “mainstream” counterparts by offering similar features and how they differ from their counterparts by presenting themselves as defenders of free speech. At the user level, our findings suggest that both platforms provide conspiracy propagators a fertile refuge through which they can maintain their presence and connection with their followers – which also allows them to profit from their visibility by receiving monetary rewards.","",""
"2023","VIRAL HEALTH MISINFORMATION FROM GEOCITIES TO COVID-19","Although often discussed in the public discourse as a phenomenon newly exacerbated by social media, the use of the Internet to spread health-related misinformation is as old as the Internet itself. Techniques, networks, and narratives from prior novel health outbreaks such as HIV or Ebola continue to circulate and are repurposed in the current COVID-19 pandemic. We examine and compare two case studies of health misinformation — HIV mis/disinformation in from the mid-1990s to early 2000s circulating in GeoCities and the role of official COVID-19 Dashboards in present-day COVID-19 mis/disinformation. This contributes to our understanding of current and historical health misinformation as well as the connections between them.","",""
"2023","Clickbait or conspiracy? How Twitter users address the epistemic uncertainty of a controversial preprint"," Many scientists share preprints on social media platforms to gain attention from academic peers, policy-makers, and journalists. In this study we shed light on an unintended but highly consequential effect of sharing preprints: Their contribution to conspiracy theories. Although the scientific community might quickly dismiss a preprint as insubstantial and ‘clickbaity’, its uncertain epistemic status nevertheless allows conspiracy theorists to mobilize the text as scientific support for their own narratives. To better understand the epistemic politics of preprints on social media platforms, we studied the case of a biomedical preprint, which was shared widely and discussed controversially on Twitter in the wake of the coronavirus disease 2019 pandemic. Using a combination of social network analysis and qualitative content analysis, we compared the structures of engagement with the preprint and the discursive practices of scientists and conspiracy theorists. We found that despite substantial engagement, scientists were unable to dampen the conspiracy theorists’ enthusiasm for the preprint. We further found that members from both groups not only tried to reduce the preprint's epistemic uncertainty but sometimes deliberately maintained it. The maintenance of epistemic uncertainty helped conspiracy theorists to reinforce their group's identity as skeptics and allowed scientists to express concerns with the state of their profession. Our study contributes to research on the intricate relations between scientific knowledge and conspiracy theories online, as well as the role of social media platforms for new genres of scholarly communication. ","",""
"2023","Race/ethnicity, online information and COVID-19 vaccination: Study of minority immigrants’ internet use for health-related information"," The COVID-19 pandemic aggravated existing challenges for racial/ethnic minority immigrants in the U.S. in obtaining health information and seeking health care. Based on in-depth interviews with 49 racial/ethnic minority immigrants in the U.S. Midwest, this study examines how they navigated online health information related to general health issues and in particular COVID-19, how they encounter online misinformation related to COVID-19 vaccination and their willingness to get vaccinated. Results show that participants use online health information from both the U.S. and their home country to stay informed about the pandemic, but often encounter misinformation and hate speech online. Further, participants are hesitant to correct misinformation due to contentious online environment. Additionally, findings revealed that younger participants tended to be less willing to get vaccinated due to low perceived benefits. The study suggests scholarly and practical implications for those who work in the area of health communication, digital media messaging and minority communication. ","",""
"2023","The COVID-19 infodemic in social media: Political exaggeration and communicative autonomy","This study aims to assess the difficulty of maintaining the interpretative autonomy of communication professionals and citizens, in the face of information about the global pandemic. At the same time, this research analyzes critically the World Health Organization’s accusation of an ’infodemic’; was it confirmed or should it be regarded as political exaggeration? An analysis was made of 15,000 tweets around the world, with more than 1,000 RTs for each one, that circulated from 6 February to 18 March 2020. The results demonstrate that it is not so much possible to speak of infodemic but of a remarkable difficulty in interpreting information, together with a preponderant weight of opinion and emotionality. Academia is responsible for disseminating concepts; corporations, for filtering ethically their content; the political class, for not hiding behind the infodemic to lower the challenge of managing the pandemic.","",""
"2023","Does the platform matter? Social media and COVID-19 conspiracy theory beliefs in 17 countries"," While the role of social media in the spread of conspiracy theories has received much attention, a key deficit in previous research is the lack of distinction between different types of platforms. This study places the role of social media affordances in facilitating the spread of conspiracy beliefs at the center of its enquiry. We examine the relationship between platform use and conspiracy theory beliefs related to the COVID-19 pandemic. Relying on the concept of technological affordances, we theorize that variation across key features make some platforms more fertile places for conspiracy beliefs than others. Using data from a crossnational dataset based on a two-wave online survey conducted in 17 countries before and after the onset of the COVID-19 pandemic, we show that Twitter has a negative effect on conspiracy beliefs—as opposed to all other platforms under examination which are found to have a positive effect. ","",""
"2023","Autopsy of a metaphor: The origins, use and blind spots of the ‘infodemic’"," In 2020, the term ‘infodemic’ rose from relative obscurity to becoming a popular catch-all metaphor, representing the perils of fast, wide-spreading (false) information about the coronavirus pandemic. It featured in thousands of academic publications and received widespread attention from policymakers and the media. In this article, we trace the origins and use of the ‘infodemic’ metaphor and examine the blind spots inherent in this seemingly intuitive term. Drawing from literature in the cognitive sciences and communication studies, we show why information does not spread like a virus and point out how the ‘infodemic’ metaphor can be misleading, as it conflates multiple forms of social behaviour, oversimplifies a complex situation and helps constitute a phenomenon for which concrete evidence remains patchy. We point out the existing tension between the usefulness of the widespread use of the term ‘infodemic’ and its uncritical adoption, which we argue can do more harm than good, potentially diluting the quality of academic work, public discourse and contributing to state overreach in policymaking. ","",""
"2023","Evolution of the plandemic communication network among serial participants on Twitter"," The coronavirus pandemic has been accompanied by the spread of misinformation on social media. The Plandemic conspiracy theory holds that the pandemic outbreak was planned to create a new social order. This study examines the evolution of this popular conspiracy theory from a dynamic network perspective. Guided by the analytical framework of network evolution, the current study explores drivers of tie changes in the Plandemic communication network among serial participants over a 4-month period. Results show that tie changes are explained by degree-based and closure-based structural features (i.e. tendencies toward transitive closure and shared popularity and tendencies against in-degree activity and transitive reciprocated triplet) and nodal attributes (i.e. bot probability and political preference). However, a participant’s level of anger expression does not predict the evolution of the observed network. ","",""
"2023","Digital news media as a social resilience proxy: A computational political economy perspective"," This article investigates how the COVID-19 pandemic was framed in public, private and non-profit media production. It conceptualises digital news as an indicator of social resilience and the interaction between social and biological/natural systems. We analysed news articles published in 2020/2021 on 21 Croatian websites using natural language processing. We collected 985,850 articles and manually coded samples to train different classifiers. The first classifier was developed to determine which articles relate to COVID-19. The second classifier was used for articles’ topic classification; the third classifier was used to classify articles into resilience classes. A limited discussion of transformative (long-term) resilience, especially in private media, contributed to the most significant content share. The debate focused on keeping the status quo through coping or returning to pre-pandemic conditions through adaptive mechanisms. The news media contributed to how public issues were framed and how science and scientific research were discussed. ","",""
"2023","Conspiracy theories in online environments: An interdisciplinary literature review and agenda for future research"," Research on conspiracy theories in digital media has grown considerably in recent years. As a result, the field of research has become more multidisciplinary and diverse. To bridge disciplinary boundaries, identify foci of analysis and research gaps, this study provides an interdisciplinary systematic literature review (2007–2020), analyzing current research on conspiracy theorizing online, both quantitatively and qualitatively. Findings show that the majority of studies lack a definition of conspiracy theories and fail to conceptually delineate conspiracy theories from other forms of deceptive content. We also found that while the field employs a variety of methodological approaches, most studies have focused on individual, “mainstream” social media platforms, “Western” countries, English-language communication, and single conspiracy theories. We use the findings of our review to remedy conceptual and empirical shortcomings and to provide suggestions on how to move forward in research on conspiracy theories online. ","",""
"2023","Information, identity, and action: The messages of the Dutch anti-vaccination community on Telegram"," The anti-vaccination movement has successfully spread its views on social media. This study examined how community building emerges in the messages of Dutch anti-vaccination Telegram groups. Particularly, we investigated the extent to which these groups provide a platform for sharing information, perpetuating a shared identity, and promoting action. As negative emotions are considered a prime driver of collective action, we examined to what extent the messages had a negative valence. We used a mixed-method approach through a quantitative content analysis of 4654 text messages from five Telegram groups, while also examining the nature of the content through a qualitative analysis. The results suggest that most messages contained a form of shared identity (ingroup favoritism and outgroup hostility) or (mis)information, and, to a lesser extent, encouragements for (online) action. Moreover, most content had a negative valence. These findings illustrate how online groups might be sources of (mis)information, polarization, and intergroup hostility. ","",""
"2023","Did COVID-19 Blur Partisan Boundaries? A Comparison of Partisan Affinity and Source Heterophily in Online Alternative News-Sharing Networks Before and During the COVID-19 Pandemic"," This study explores partisan and group heterophily within cross-platform online communities that share alternative news media content in Denmark, Sweden, Germany, and Austria. The analysis is related to the emergence of anti-systemic cross-partisan counter-publics in Europe that have gained momentum with the outbreak of COVID-19 and the subsequent resistance against government restrictions. Comparing two periods (before and after the outbreak of COVID-19), we investigate whether these developments foster cross-partisan information sharing in online communities that form around right-wing, left-wing, and anti-systemic alternative news media content. Drawing on a network-analytical approach, we study networks formed around URL sharing of alternative news content across Facebook, Instagram, Twitter, Reddit, Telegram, TikTok, YouTube, and VKontakte. Data include 30 million social media posts from January 2019 to September 2021. The results show that overall source heterophily in online alternative news networks increases slightly with the COVID-19 pandemic, mainly due to the increased proliferation of anti-system news. This increase is, however, not an expression of a more profound collapse of bi-partisan, left-right cleavages and is contingent on country contexts. Except for the time of the initial outbreak, the overall sharing of COVID-19-related content tends to increase rather than decrease partisan homophily. Finally, the results show that non-bi-partisan, anti-system media have had a significant effect on alternative media information ecosystems during the COVID-19 pandemic. ","",""
"2023","Trustworthiness Over Time on Twitter: Three Critical Periods for the Norwegian Health Authorities and Political Leadership During the COVID-19 Pandemic"," Public health authorities and political leaders need to come across as trustworthy in their handling of a crisis like the COVID-19 pandemic. There is, however, little knowledge about how the affordances and dynamics of social media influence perceptions of trustworthiness, especially during a protracted crisis. In this article, we study how Twitter users were discussing the trustworthiness of the Norwegian health authorities and political leadership throughout three periods of partial lockdown during the COVID-19 pandemic. Across all the periods, there was a substantial number of positive comments, but these were outweighed by negative ones. Ability was clearly the most discussed factor for trustworthiness, and many users offered up their lay expertise. Discussions of integrity and benevolence were less frequent and mostly negative when they occurred. An increase in negative comments during the last period might be read as an expression of fatigue, and there was a noted dissatisfaction with the ability of the political leadership. Taken together, the study suggests Twitter to be an arena where users are exposed to arguments and counterarguments in negotiations over ability in particular. Such discussions can intensify as a crisis drags on and are important to grasp for health authorities and political leadership alike. Thus, the study sheds light on the contribution that a socio-technical platform like Twitter makes to the discursive formation of trustworthiness over time, which in turn might function to strengthen or erode public trust in public authorities and political leadership. ","",""
"2023","A God-Tier LARP? QAnon as Conspiracy Fictioning"," The QAnon movement, which gained a lot of traction in recent years, defies categorization: is it a conspiracy theory, a new mythology, a social movement, a religious cult, or an alternate reality game? How did the posts of a (supposedly) anonymous government insider named Q on an obscure online imageboard in October 2017 instigate a serious conspiracy movement taking part in the storming of the US Capitol in early 2021? Returning to the origins of QAnon on 4chan’s Politically Incorrect board and its initial reception as a potential LARP, we analyze it as an instance of participatory online play that fosters deep engagement above all. Drawing on concepts from play and performance studies, we theorize the dynamics by which QAnon developed into an influential conspiracy narrative as instances of “conspiracy fictioning.” In particular, we revive the notion of hyperstition to make sense of how such conspiracy fictionings work to recursively “bootstrap” their own alternate realities into existence. By thus exploring the participatory and playful engagement mechanisms that drive today’s conspiracy movements, we aim to elucidate the epistemological and socio-political dynamics that mark the growing entanglement of play and politics, fact and fiction in society. ","",""
"2023","“As Reliable as a Kalashnikov Rifle”: How Sputnik News Promotes Russian Vaccine Technologies in the Turkish Twittersphere"," Established in 2014, SputnikTR (a localized version of Sputnik News) is the most popular pro-Russian media outlet active in Turkey. The news content published by SputnikTR’s Twitter account currently attracts the highest engagement rates among the international public broadcasters active in Turkey. SputnikTR’s official Twitter account has more followers (1M) than Sputnik News English (326K). This article argues that SputnikTR’s Twitter account is used to promote Russian vaccine technologies in Turkey. We believe that it is also a conduit for the dissemination of pro-Russian as well as anti-Western narratives to the Turkish online public. Using a computational methodology, we collected 2,782 vaccine-related tweets posted by SputnikTR’s Twitter account between April 2019 and April 2021. We deployed framing as well as critical discourse analysis to study the contents of our dataset. Our findings suggest that SputnikTR uses (a) disinformation as well as misinformation in vaccine-related news and (b) unethical communication techniques to maximize engagement with content posted on Twitter. Our findings are significant insofar as they are the first documented instances of Russian propaganda efforts on Turkish Twitter. These efforts seem to be focused on promoting the Russian vaccine while encouraging public hesitancy toward Western vaccine technologies. ","",""
"2023","From Facebook to YouTube: The Potential Exposure to COVID-19 Anti-Vaccine Videos on Social Media"," This article examines the role of Facebook and YouTube in potentially exposing people to COVID-19 vaccine–related misinformation. Specifically, to study the potential level of exposure, the article models a uni-directional information-sharing pathway beginning when a Facebook user encounters a vaccine-related post with a YouTube video, follows this video to YouTube, and then sees a list of related videos automatically recommended by YouTube. The results demonstrate that despite the efforts by Facebook and YouTube, COVID-19 vaccine–related misinformation in the form of anti-vaccine videos propagates on both platforms. Because of these apparent gaps in platform-led initiatives to combat misinformation, public health agencies must be proactive in creating vaccine promotion campaigns that are highly visible on social media to overtake anti-vaccine videos’ prominence in the network. By examining related videos that a user potentially encounters, the article also contributes practical insights to identify influential YouTube channels for public health agencies to collaborate with on their public service announcements about the importance of vaccination programs and vaccine safety. ","",""
"2023","One biologist, one million deaths: Expertise between science, social media, and politics during the COVID-19 pandemic in Brazil","The article discusses the multiple forms of expertise articulated by a specific kind of digital influencer - online science communicators - during the COVID-19 pandemic in Brazil. Our case study focuses on the performance of Atila Iamarino, a PhD in Microbiology that achieved an unprecedented public recognition after predicting, in a YouTube live transmission, that more than a million people could die in the country due to the coronavirus. Assuming the relational and networked dimension of expertise, the article discusses how Atila combined and interchanged academic, affective, and sociotechnical abilities in his performances on social media and on other (media) institutions during a public health crisis marked by the lack of coordination and the political instrumentalization of science by the Brazilian federal government. The case study is based on a systematic observation of Atila’s accounts on YouTube and Twitter, and on additional material published from March to August 2020. In the conclusions, based on how the Brazilian science influencer managed his visibility, alliances, and scientific background during the radical uncertainty period, we highlight how the expertise was built based on conditions of possibility that emerged in Brazil during the pandemic, which reveals contemporary tensions between science, politics, media, and other epistemic institutions.","",""
"2023","Alternative Credibility, Phenomenological Empathy, and the Plandemic: Trust in Consipiracy Theories During the COVID-19 Pandemic","Plandemic: The Hidden Agenda behind COVID-19 is a twenty-six-minute film that went viral during the spring of 2020. The film invited controversy for sowing doubt in the official account of the COVID-19 pandemic by presenting an alternate perspective on several key issues such as masking, vaccines, and COVID-19 control measures. The film also vilified public health institutions and officials like Antony Fauci, among others. This paper aims to evaluate how conspiracy theories like the Plandemic find fertile ground during moments of crisis like the COVID-19 pandemic. To accomplish this the paper has two aims: (i) highlight the crucial role played by ‘alternative credibility’ and ‘empathy’ in garnering trust; (ii) identify how both concepts operate in the opening segment of the Plandemic, when the film’s protagonist Judy Mikovits is introduced in a manner that commentators claim played a crucial role in gaining the audience’s trust.","",""
"2023","COVID-19 Vaccine Hesitancy : A Mixed Methods Investigation of Matters of Life and Death","In this article, hesitancy towards COVID-19 vaccinations is investigated as a phenomenon touching upon existential questions. We argue that it encompasses ideas of illness and health, and also of dying and fear of suffering. Building on a specific strand within anti-vaccination studies, we conjecture that vaccine hesitancy is, to some extent, reasonable, and that this scepticism should be studied with compassion. Through a mixed methods approach, vaccine hesitancy, as it is being expressed in a Swedish digital open forum, is investigated and understood as, on the one hand, a perceived need of protecting one’s body from techno-scientific experiments, and thus the risk of becoming a victim of medicine itself. On the other hand, the community members express what we call a tacit belief in modern medicine by demonstrating their own “expert” pandemic knowledge. The analysis also shows how the COVID-19 pandemic triggers memories of another pandemic, namely the swine flu in 2009–2010, and what we term a medical crisis that occurred then, due to a vaccine that caused a rare but severe side effect in Sweden and elsewhere.","",""
"2023","Light At The End Of The Tunnel? The Staging of Expertise During the COVID-19 Vaccination Campaign","In this paper, we compare the governmental and public framings of expertise in the Dutch Covid-19 vaccination campaign in the period between January 1st and April 30th, 2021. Specifically, we collected all statements regarding vaccination on three interrelated stages: (1) the official press conferences; (2) Twitter, for responses to government policies; and (3) political motions that were put forward by Members of Parliament in the days following the press conferences. We combine an interactional framing approach with a discursive psychological perspective to get insights into how framings between stages modify, contest, or build upon each other. We argue that the press conferences show a persistent technocratic framing, in the sense that a direct line between science and policy is assumed and promoted. Unlike the first period of the COVID-19 crisis in 2020, experts are not often quoted initially, but key political actors themselves act as responsible for the message that there is light at the end of the tunnel, if only citizens will get vaccinated. Once the AstraZeneca vaccine comes under fire, however, experts are again held accountable for the policy message. Throughout, governmental policies are disputed on Twitter and in Parliament, albeit in different ways, by making hidden moralities relevant, such as the government’s assumed complacency, rigidity, and inability to explain policies with the available evidence.","",""
"2023","Distrusting Consensus: How a Uniform Corona Pandemic Narrative Fostered Suspicion and Conspiracy Theories","Although the institutional model of science communication operated well during the corona-pandemic, and relevant public institutions (media, science, politics) garnered higher levels of trust following “rally-around-the-flag” dynamics, other people would develop distrusts towards those institutions and the emerging orthodox corona narrative. Their ideas are often framed as conspiracy theories, and today’s globalized media eco-system enables their proliferation. This looming “infodemic” became a prime object of concern. In this article I agnostically study those distrusts from a cultural sociological perspective to better understand how and why people (came to) disbelieve official knowledge and their producers. To do so, I draw on my ethnographic fieldwork in the off- and online worlds of people labeled as conspiracy theorists in the Netherlands, which includes the media they consume, share and produce. Based on an inductive analysis of people’s own sense-making, I present three dominant reasons: media’s panicky narrative of fear and mayhem; governments sole focus on lockdowns and vaccines; and the exclusion of heterodox scientific perspectives in the public sphere. Each of these reasons problematize a perceived orthodoxy in media, politics and science, and this uniformity bred suspicion about possible conspiracies between these public institutions. Too much consensus gets distrusted. While we can discard those ideas as irrational conspiracy theories, I conclude that these findings have important implications for the way we deal with and communicate about complex societal problems. Next to keeping things simple and clear, as crisis/risk/science communication holds, we need to allow for uncertainty, critique and epistemic diversity as well.","",""
"2023","Trust, media, and science in the context of the Covid-19 pandemic","The first global pandemic of the information age has revealed how the coordinated spread of accurate information and the communication of relevant expert knowledge rely on functioning media channels, platforms, and institutions. As such, the coronavirus pandemic has exposed, and sometimes even catalyzed, longer-running societal processes through which traditional gatekeepers of scientific truth and expertise have been challenged or side-stepped, as alternative actors and institutions have taken the media stage and influenced policymaking spheres. To what extent has the changing media landscape contributed to (dis)trust in expertise? How do different political contexts shape the dynamics between science, policy, and diverse media publics? And in which ways does the contemporary spread of (mis/dis)information take shape? The articles in this collection address these questions by presenting original empirical analyses from a range of geographic and disciplinary vantage points.","",""
"2023","Looking back to look forward: 5G/COVID-19 conspiracies and the long history of infrastructural fears"," Almost as soon as the COVID-19 pandemic began spreading throughout much of the world, conspiracies arose that blamed the virus on the deployment of fifth-generation cellular networks (5G) infrastructure. These conspiracies had significant consequences, including protests against 5G and the destruction of 5G infrastructure. This article uses a media genealogy approach to place the 5G/COVID-19 conspiracies within the long and recurring cycle of conspiracies focused on mobile infrastructure. Placed within that broader history, this article argues that the 5G/COVID-19 conspiracies should have been unsurprising, and these types of infrastructural conspiracies should be a more significant part of mobile media and communication (MMC) research because infrastructures are an often invisible, yet crucial, part of the mobile practices studied within MMC research. The article concludes by theorizing about why mobile infrastructures are such a frequent target for conspiracy theories and argues that researchers should begin planning now for combatting the conspiracies that will almost inevitably arise when the next generation of mobile infrastructure gets linked to fears about public health. ","",""
"2024","Natural language processing analysis applied to COVID-19 open-text opinions using a distilBERT model for sentiment categorization","AbstractCOVID-19 is a disease that affects the quality of life in all aspects. However, the government policy applied in 2020 impacted the lifestyle of the whole world. In this sense, the study of sentiments of people in different countries is a very important task to face future challenges related to lockdown caused by a virus. To contribute to this objective, we have proposed a natural language processing model with the aim to detect positive and negative feelings in open-text answers obtained from a survey in pandemic times. We have proposed a distilBERT transformer model to carry out this task. We have used three approaches to perform a comparison, obtaining for our best model the following average metrics: Accuracy: 0.823, Precision: 0.826, Recall: 0.793 and F1 Score: 0.803. ","",""
"2024","Doctors for the truth: Latin American antivaccine oppositional cultures on Telegram"," The antivaccine hesitancy movement represents a challenge to public policy and platform regulations. During COVID-19, various Latin American antivaccine groups clashed with official sanitary initiatives. Despite many responses, little progress has been made in reaching these groups to transform their perceptions about the benefits of the COVID-19 vaccine. During the pandemic in Latin America, the antivaccine network Médicos por la Verdad (Doctors for the Truth) gained prominence in various countries. Finding itself limited by legal and technical restrictions, this network used alternative media such as Telegram to disseminate messages. This study argues that such groups may be considered an antivaccination culture that opposes government measures. This focus emphasizes narrative construction and allows us to understand the phenomenon from the collective meaning-making perspective. This study analyzed 232,638 Telegram messages from 14 public channels associated with the Médicos por la Verdad network. Our findings indicate that this antivaccine network builds an oppositional culture expressed and reinforced through multimodal, trans-media, fragmented narratives and suspends disbelief that constructs a world where the community enacts a truth pact. These narrative methods foster building a resilient network of oppositional cultures, decreasing the effectiveness of policies. We conclude that research beyond the framework of misinformation and the analysis of conventional platforms is needed to understand the antivaccine oppositional cultures. ","",""
"2024","The different worlds of Google – A comparison of search results on conspiracy theories in 12 countries"," Search engines play an important role in the spread of disinformation and conspiracy theories, accentuating the power of global platform companies such as Google to contribute to the digital (information) divide by providing search results of lesser quality in certain countries. We investigated this phenomenon by asking what kind of results users see when they search for information on eleven popular conspiracy theories (CTs) via Google. We analysed links from Google search results ( N = 1259) in 12 Western and non-Western countries and 10 languages. Overall, users are more likely to encounter neutral or debunking content when using Google to search for prominent CTs. However, for some CTs, strong country differences in the quality of search results emerge, showing clear correlations between categorical inequalities and unequal access to reliable information. In countries where journalists enjoy less freedom, people enjoy fewer democratic rights and are less able to rely on social elites, Google also provides less enlightening content on CTs than in developed and prosperous democracies. The countries thus disadvantaged are precisely those countries where there is a high propensity to believe in CTs according to comparative survey research. However, in countries where a global language is spoken, for example, English or Portuguese, there is no correlation between structural, country-specific factors and the quality of search results. In this sense, structurally disadvantaged countries seem to benefit from belonging to a larger language community. ","",""
"2024","Participatory conspiracy culture: Believing, doubting and playing with conspiracy theories on Reddit"," The popularization and normalization of conspiracy theories over the last decade are accompanied by concerns over conspiracy theories as irrational beliefs, on the one hand; and their advocates as radical and extremist believers on the other hand. Building on studies emphasizing that such accounts are one-sided at best, and pars pro toto stigmatizations at worst; we propose to study what we call “ participatory conspiracy culture”—the everyday, mundane online debates about conspiracy theories. Based on a 6-month ethnography on Reddit’s r/conspiracy subreddit, an analysis of 242 selected discussions, and supported by digital methods tool 4CAT, this article addresses the question of how people participate in online conspiracy culture. It shows that among the plethora of conspiracy theories discussed online, discussions are heterogeneous, and their participants relate to each other primarily through conflict. Three epistemological positions occur: belief (particularly leading to constant discreditation of others’ beliefs), doubt (particularly as opposed to belief), and play (particularly with the fun of entertaining conspiracy theories without taking them too seriously). We conclude that the participatory conspiracy culture of r/conspiracy is not a homogenous echo chamber of radical belief, but a heterogeneous participatory culture in which belief is fundamentally contested, rather than embraced. ","",""
"2024","Combating contamination and contagion: Embodied and environmental metaphors of misinformation","In recent years, government agencies, information institutions, educators and researchers have paid increasing attention to issues of misinformation, disinformation and conspiracy theorizing. This has prompted a seemingly endless supply of guides, frameworks and approaches to ‘combating’ the problem. In studies of mis- and disinformation, a constellation of analogous concepts are defined in multiple ways across multidisciplinary literatures and institutional contexts. Misinformation, disinformation and conspiracy theory are often conflated, lacking specific, portable definitions across fields of study. Linguistic metaphors are often leveraged in place of this definitional work. The larger conceptual metaphors that they connote contain normative assumptions that often impose values and moral imperatives, imply deficiencies, assume intent, and foreground individual agency or lack thereof. Metaphors are as restrictive as they are illuminating; once used, a metaphor also applies constraints to the way in which a phenomenon can be understood. Metaphors not only shape the ways in which science is communicated to the public, but also the kinds of questions that are asked, the theories and methods used, and the parameters of the research design. By analyzing instances of linguistic metaphor, this exploratory study identifies and develops two conceptual metaphors that are frequently evoked to discuss mis- and disinformation: embodied health metaphors and environmental health metaphors. The former includes linguistic metaphors like viral/virality, infodemic, infobesity, information hygiene, information dysfunction, and information pathology. The latter includes linguistic metaphors like information pollution, infollution, and digital wildfires. Uncritically invoking such metaphors adopts tacit arguments deriving from the original field of study (e.g., public health’s tendency to equate individual embodied health with virtue), or the image of the metaphor itself ( digital wildfires implies quick spread and immediate danger), or both. Widespread and uncritical use of such metaphors, we argue, rewards speed and epistemic homogeneity in mis- and disinformation research – ultimately discouraging in-depth inquiry.","",""
"2024","The online structure and development of posting behaviour in Dutch anti-vaccination groups on Telegram"," Online communities play an important role in spreading public discontent and could contribute to polarization. This study focuses on anti-vaccination views in the Netherlands, which have intensified during the COVID-19 pandemic. We examined the structure and development of five Dutch anti-vaccination Telegram groups and studied their interactivity and posting behaviour. Using group-based trajectory modelling, we examined the development of users’ posting behaviour in these groups. We find four posting trajectories across all five groups. A small group of users contributes the majority of posts. Overall, posting frequency declines over time and our results do not show evidence for a group of users whose posting frequency increases. This is taken to indicate that only a small group of users spread their anti-vaccination views through Telegram groups. While social media can reach a broad audience, most users are not necessarily engaged to also actively contribute to the online anti-vaccination community. ","",""
"2024","Setting the misinformation agenda: Modeling COVID-19 narratives in Twitter communities"," This research investigates the dynamics of COVID-19 misinformation spread on Twitter within the unique context of Finland. Employing cutting-edge methodologies including text classification, topic modeling, social network analysis, and correspondence analysis (CA), the study analyzes 1.6 million Finnish tweets from December 2019 to October 2022. Misinformation tweets are identified through text classification and grouped into topics using BERTopic modeling. Applying the Leiden algorithm, the analysis uncovers retweet and mention networks, delineating distinct communities within each. CA determines these communities’ topical focuses, revealing how various groups prioritized different misinformation narratives throughout the pandemic. The findings demonstrate that influential, diverse communities introduce new misinformation, which then spreads to niche groups. This agenda-setting effect is amplified by social media algorithms optimized for engagement. The results provide valuable insights into how online communities shape public discourse during crises through the strategic dissemination of misinformation. ","",""
"2024","‘Conspiracy theories should be called spoiler alerts’: Conspiracy, coronavirus and affective community on Russell Brand’s YouTube comment section"," This article examines how conspiracy theories anchor affective communities through an analysis of the YouTube comment section for the actor and comedian turned political influencer, Russell Brand. Comparing videos before and after Brand’s shift to covid scepticism, I explore like counts, reply networks and other commenting patterns in a dataset of 217,157 comments and conduct an in-depth analysis of 2000 top comments. The findings show first, a shift towards right-wing viewpoints; second, a reduction in comment length and comment replies alongside an increase in likes; third, a sharp rise in proclamations of Brand fandom; and fourth, a steep increase in references to conspiracy. The in-depth analysis reveals that comments focused not on narrating the content of conspiracies but on celebrating conspiracy as the basis of a political community and as a defence against accusations of paranoia. I argue that conspiracy theories can function as formal categories that anchor affective communities. ","",""
"2024","Folk theories of false information: A mixed-methods study in the context of Covid-19 in Turkey"," This study explores how media users define false information in the daily flow of their lives against a backdrop of sociopolitical contexts. We focus on the vernacular definitions of false information through the concept of folk theories, which are the intuitive explanatory tools users develop to make sense of and act in the world around them. Based on mixed-method research conducted in Turkey during the Covid-19 pandemic, we identify three prevailing folk theories of false information. First, users consider text-based characteristics, such as the presence of evidence as a flag of accuracy/inaccuracy. Second, users assume that people in their social networks distinguish between the accurate and the inaccurate, and thus the information coming from these circles is accurate. Finally, users imagine that people whose worldviews conflict with theirs spread inaccurate information. Despite users’ overarching references to textual traits of news, it appears that the latter two folk theories drive users’ information processing practices in daily life. ","",""
"2024","Conceptualizing platformed conspiracism: Analytical framework and empirical case study of BitChute and Gab"," This article introduces the notion of platformed conspiracism to conceptualize reconfigured forms of conspiracy theory communication as a result of the mutual shaping between platform specificities and emergent user practices. To investigate this relational socio-technological shaping, we propose a conceptual platform-sensitive framework that systematically guides the study of platformed conspiracism. To illustrate the application of the framework, we examine how platformed conspiracism unfolds on BitChute and Gab during the COVID-19 pandemic. Our findings show that both platforms have positioned themselves as technological equivalents to their “mainstream” counterparts, YouTube and Twitter, by offering similar interfaces and features. However, given their specific services, community-based and politically marketed business models, and minimalist approaches to content moderation, both platforms provide conspiracy propagators a fertile refuge through which they can diversify their presence and profit monetarily from their supply of conspiracy theories and active connection with their followers. ","",""
"2024","Pseudo-scientific versus anti-scientific online conspiracism: A comparison of the Flat Earth Society’s Internet forum and Reddit","Attitudes of distrust and paranoia toward scientific and political institutions are increasingly identified as major troubles in online communication and often lumped together under the umbrella term of conspiracy theories. However, this term encompasses two distinct communication practices that deserve to be distinguished. Traditional conspiratorial thinking adopts pseudo-scientific arguments, while newer manifestations lack coherent theories, promoting trolling, and antagonism. We argue that these strands align with different types of digital communications and are supported by different technical infrastructure and cultures of use, with classic conspiracy theories prevalent in early online venues and “conspiracies-without-theory” more common on social media. By comparing the Flat Earth Society’s Internet forum and its subreddit, we highlight their stark differences. The forum prioritizes pseudo-scientific discourse, while the subreddit fosters confrontational antagonism and unmoderated escalation. Recognizing these distinctions is vital for understanding their communicative profoundly different nature and developing targeted strategies to address them effectively.","",""
"2024","The use of emotions in conspiracy and debunking videos to engage publics on YouTube"," With the rise of digital media, conspiracy theories infamous for their emotional manipulation have challenged science epistemology and democratic discourse. Despite extensive literature on misinformation and the role of emotion in persuasion, less is understood about how emotion is used in conspiracy and debunking messages on video platforms and the impact of emotional framing on public engagement with science on social media. Our article fills this gap by analyzing thousands of YouTube videos that propagate or debunk COVID-19 conspiracy theories from March to May 2020. We found that conspiracy and debunking videos used the emotions of trust and fear differently depending on the issue framing of the conspiracy. Our article also reveals a dilemma facing debunking messages—when debunking videos used more trust-related emotions, these videos received more likes yet fewer views. These findings shed new light on the role of emotion on user engagement with misinformation and its correction on digital platforms. ","",""
"2024","What do 5G networks, Bill Gates, Agenda 21, and QAnon have in common? Sources, distribution, and characteristics"," Mounting uncertainties regarding the coronavirus disease (COVID-19) pandemic and the popularity of social media created fertile grounds for conspiracy theories to flourish, leading to a global “infodemic.” We examine information sources used to support five popular COVID-19-related conspiracy theories on Twitter to identify (1) their primary building blocks, (2) similarities and dissimilarities across COVID-19 conspiracy theories, and (3) the relationship between type of message content and content distribution. Findings show that statements of belief and of malicious purpose were most popular, followed by conspirators, authentication, and secretive actions. However, only malicious purposes and secretive actions messages successfully predicted higher distribution of content, while, for instance, content authentication showed a negative relation. Furthermore, the type of conspiracy theories matters. Mega-theories, such as Agenda 21 and QAnon, incorporated less statements of Belief. COVID-19 vaccine–related theories focused more on authentication, while QAnon highlighted the conspirators behind the pandemic. Conceptual and practical implications are discussed. ","",""
"2024","Managing the Pandemic in Digitized Spaces: Assessing the Social Media Approaches of Scandinavian Public Health Authorities"," In response to the COVID-19 pandemic, health- and civil-contingency agencies—referred to here as public health authorities (PHAs)—in Sweden, Norway, and Denmark turned to social media to disseminate pandemic recommendations and information. This study explores the social media crisis management strategies employed by Scandinavian PHAs. Specifically, we apply a multiplatform research approach to assess communication objectives (Instruct, Support, Manage Reputation, and Solicit Interaction) across three social media platforms—Facebook, Instagram, and Twitter (currently known as X). Introducing a series of hypotheses based on previous scholarship, we detail the prevalence of different objectives across platforms and countries. The results indicate prominent use of reputational management, particularly on Twitter, while instructive information emerged as a highly used communication objective in Sweden and Denmark. Overall, the communicative trends remained parallel across nations, despite Sweden implementing a more relaxed crisis management strategy. The main distinction in Sweden’s approach manifested in a relatively lower emphasis on the pandemic by its PHAs compared to Denmark and Norway. National differences in crisis communication objectives indicate that Norwegian PHAs stand out in terms of using reputational management, while Sweden stands out in employing more supportive information on Instagram. ","",""
"2024","Mechanisms Driving Online Vaccine Debate During the COVID-19 Pandemic"," The prevalence of the anti-vaccine movement in today’s society has become a pressing concern, largely amplified by the dissemination of vaccine skepticism. During the early stages of the COVID-19 pandemic, the vaccination debate sparked controversial debates on social media platforms such as Twitter, which can lead to serious consequences for public health. What determines anti-vax attitudes is an important question for understanding the source of the campaigns and mitigating the misinformation spread. Compared with other countries, Türkiye differentiates itself with high vaccination rates and lack of political support for anti-vaxxers despite its highly polarized political system. Analyzing Turkish Twittersphere, we explore several mechanisms capturing content production and behaviors of accounts within the pro- and anti-vax segments in online vaccine-related discussions. Our findings indicate there is no relation between political stance and anti-vaccine attitude. Both supporters of vaccination (pro-vaxxers) and opponents (anti-vaxxers) can be found across the political spectrum. Moreover, linguistic differences reveal that anti-vaxxers employ more emotional language, while pro-vaxxers express more skepticism. Notably, automated accounts are less prevalent leading to difficulty in assessing genuine support for vaccines, while anti-vaccine bots produce slightly more content. These findings have crucial implications for vaccine policy, emphasizing the importance of understanding diverse language patterns and beliefs among anti-vaxxers and pro-vaxxers to develop effective communication strategies at the national level. ","",""
"2024","“Memes Save Lives”: Stigma and the Production of Antivaccination Memes During the COVID-19 Pandemic"," Disinformation research is increasingly concerned with the hierarchies and conditions that enable the strategic production of false and misleading content online. During the COVID-19 pandemic, it was revealed that 12 influencers were responsible for a significant volume of antivaccine disinformation. This article examines how influencers use antivaccination memes for commercial and political gain. Drawing on a 12-month digital ethnography of three disinformation producers on Instagram and Telegram, we conceptualize their strategy of meme warfare in terms of the logics of spoiled identity, demonstrating how stigma is used to galvanize and recast the antivaccination movement around themes of persecution and moral superiority. Dispensing with the idea that content moderation has forced disinformation “underground,” we find that disinformation producers configure memes to adapt to specific platforms by directing mainstream audiences to less regulated platforms, personal newsletters, and sites. By examining the tactics and techniques disinformation producers use to spread antivaccination messaging online, we question the effectiveness of content moderation policies as a solution to regulate influencers whose visibility and status strategically straddle multiple sites in the broader information ecosystem. ","",""
"2024","The Making of #CovidTwitter: Who Were the Loudest “Covid Influencers” and What Did They Say About the COVID-19 Pandemic?"," This study explores COVID-19 communications disseminated by the top 100 most followed Twitter profiles—what we call the Twitter influencing elite. Focusing on a critical period from January to July 2020, we conducted a quantitative and qualitative analysis of 6,602 tweets about COVID-19 produced by these Covid Influencers. The findings reveal that approximately two-thirds of the COVID-19 tweets in our sample originated from established global media organizations. While these sources were prominent, they were not the “loudest” in terms of engagement and virality. That belonged to powerful politicians like Trump and Obama, popular singers such as Harry Styles and Taylor Swift, and business personalities like Elon Musk. What is more, our qualitative analysis highlights how the affordances of the digital space and the context of the pandemic were leveraged by these influential Twitter users to advance their own agendas. For instance, some blended health information and caring narratives with promotional appeals, while others, such as Elon Musk and Donald Trump, engaged in political agitation and/or anti-care discourses creating a staccato of conflicting messaging and mis/dis-information. These findings demonstrate that the Twitter space is as political and politicized as it is commercial and commercialized. We conclude that digital influencers and celebrities cannot just simply be used to produce communications during times of crisis as many across the study of health and medical communication have argued. The involvement by digital influencers and celebrities—much like the Covid Influencers we examined here—in spreading information must be critically scrutinized, considering the potential for mixed motives, agendas, and real-world outcomes. ","",""
"2024","Business as Usual? Assessing Amplified Political Posts Across Social Media Platforms During the COVID-19 Pandemic"," Social media platforms, such as Facebook, Instagram, and Twitter (now X), play a crucial role in facilitating connections between politicians and citizens, particularly during a crisis like the COVID-19 pandemic. This article examines the characteristics of viral social media posts in Norway and Sweden during the initial wave of the pandemic. Despite their geographical proximity and cultural similarities, Norway and Sweden adopted different approaches to the pandemic, providing a compelling basis for comparative analysis. Employing a visual computational approach, this study maps viral posts by analyzing engagement metrics such as likes, reactions, shares, and comments. A close reading of popular posts investigates the communication strategies employed across platforms and national contexts. The findings reveal that political criticism on Twitter attracted substantial engagement, while Instagram posts leaned toward self-promotion. On Facebook, popular posts exhibited a more varied use of communication strategies, reflecting a nuanced approach to engagement across different social media environments. ","",""

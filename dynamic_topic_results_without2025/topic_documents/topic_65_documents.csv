"year","title","abstract","journal","doi"
"2000","Cyberhate and Performative Speech in Accelerated Time(s)","       In foregrounding the performative character of hate speech, legal scholars and activists have sought to demonstrate why hate speech should be prohibited not only on the basis of what it says but on the basis of what it does. In this article I examine the conditions upon which hate speech has been posited as performative speech in order to consider how virtual environments trouble existing understandings of hate speech. In particular, this article seeks to show how cyberspace may in fact create the conditions for a more immediate and radical recontextualisation and recirculation of hate speech where speed operates as a potential source of resistance.        In foregrounding the performative character of hate speech, legal scholars and activists have sought to demonstrate why hate speech should be prohibited not only on the basis of what it says but on the basis of what it does. In this article I examine the conditions upon which hate speech has been posited as performative speech in order to consider how virtual environments trouble existing understandings of hate speech. In particular, this article seeks to show how cyberspace may in fact create the conditions for a more immediate and radical recontextualisation and recirculation of hate speech where speed operates as a potential source of resistance.        Judith Butler maintains that """"speech is always in some ways out of our control"""" (15). This is not to suggest that our speech is bound to misfire. Instead, Butler's claim draws attention to the range of possibilities located precisely within the failed speech act, revealing how such misfires offer the possibility that certain words which carry the potential to injure may eventually become """"disjointed from their power to injure and recontextualised in more affirmative modes"""" (15). Whereas Langston suggests that such speech events inevitably work to silence intended victims, Butler recognises how the subject may also be inaugurated through such linguistic injuries. She maintains that to be injured through language is to """"suffer a loss of context, that is, not to know where you are"""" (4). The """"loss of context"""" or disorientation she claims might result from hate speech not only suggests that hate speech is context-specific but also that hate speech, understood as a performance, might transform or even produce a speaker's social location. In this view, the very words used to """"put someone in their place"""" may also enable people to speak up from their position on the margins of power. Thus, the repetition of hate speech does not necessarily reinforce certain words' power to injure but may result in a pattern of """"slippages"""" which enables the meaning and force of such speech to eventually come undone.        In order to understand the impact of hate speech in virtual environments, it is useful to consider how the spatial and temporal character of cyberspace affects the repetitions to which Butler refers. While we typically speak of cyber-space, the emergence of cyberspace has arguably resulted in an increased preoccupation with time. For Virilio, the shift from space to time appears to be most visible in cyberspace, which he maintains does not represent a space so much as a particular temporal dimension where speed, not territory, holds the most strategic value. However, he further maintains that we have also reached a moment when humans, who have already surpassed the sound and heat barriers, are left with nothing to race against but speed of light -- something which can never be surpassed. And, as he warns, like other technological revolutions, this one is bound to result in an accident, this time not a physical one but instead one which will throw history itself into a disarray. """"Cyberspace looms up like a transfer accident in substantial reality"""", he warns, """"what gets damaged is no longer the substance, the materiality of the tangible world, it is the whole of its constitution"""" (Open Sky 131). Thus, cyberspace not only appears to give rise to an accelerated sort of time but to create the conditions under which we run the risk of suffering a """"fundamental loss of orientation"""" (Speed and Information). I would like to consider how the speed and subsequent loss of orientation Virilio associates with virtual environments may in fact be the very condition which opens up the possibility for a more immediate and radical recontextualisation of hate speech in such spaces.        In positing cyberspace as a radically discontinuous space, Virilio implies that cyberspace may represent a break with history itself. One need only consider how quickly existing forms of inequity have become entrenched in cyberspace to understand the extent to which it is not an entirely new or autonomous space. It follows then that the pattern of perlocutionary effects which apparently enables certain words to injure is not disrupted simply because such words are circulating in a virtual space. However, this is not to suggest that subordinate speech necessarily acts identically on line and off. If to be injured through language is to """"suffer a loss of context, that is, not to know where you are"""" (Butler 4), what might it mean to suffer a loss of context in cyberspace? What might it mean to become disoriented in a space where one's context is always and already destabilised? And, what sorts of possibilities are created in cyberspace when one is thrust into an ill-fitting or undesirable social location? I maintain that in cyberspace the potential to suffer a loss of context as a result of a linguistic injury is always partially foreclosed by the fact that one never knows precisely where one is to begin with. It follows that in cyberspace the intended victim of a verbal assault is also at least less likely to become disarmed, debilitated, and silenced. Without overemphasising the agency one gains in virtual environments, is it not possible that one's ability to """"talk back"""", while not guaranteed, may be made significantly more likely, if only due to the fact that one's rhetorical skills are unlikely to be rendered worthless in the face of physical threats?        To illustrate the extent to which the illocutionary force of hate speech may be undermined by the spatial and temporal character of virtual environments, I draw attention to Reverend Phelps's """"godhatesfags.com"""" site. Once you move past the yellow construction sign reading """"Warning -- Gospel Preaching Ahead"""", you discover a list of Bible Passages which apparently confirm the site's claim that """"god hates fags"""". The site also contains a list of so-called """"fag churches"""" and a variety of news items that draw attention to the supposed dangers of the growing global gay agenda. However, while the Website is clearly offensive, it also seems to produce the possibility for politically promising misinterpretations. The Website's editorials on subjects as diverse as Canada's """"gay Mafia"""" and Finland's allegedly lesbian Prime Minister and news about Phelps's intention to carry out """"missionary work"""" in both of these demonic nations seems more likely to amuse than offend many of the site's visitors. The site's tendency to misinterpellate everyone from lesbians to P-FLAG mothers as """"fags"""" may be read as another amusing misfire, another failed attempt on the part of Phelps to demonise gays, lesbians, and their supporters.        While Phelps's claims are bound to misfire in any context, the ability to find Phelps's claims immediately humourous appears to be at least partially linked to their context. People who seek to prohibit hate speech not only on the basis of what it says but also on the basis of what it does typically maintain that the effects of hate speech are immediate, final, and ultimately, debilitating (Langston; MacKinnon; Matsuda et al.). In cyberspace, such claims appear to be even less easily established than in the material world. As previously argued, the speed associated with virtual environments seems to produce a disorienting effect, making the potential to suffer a """"loss of context"""" in the face of a linguistic """"attack"""" at least less likely. In addition, in contrast to the material world, where an encounter with hate speech is likely to lead to a feeling of entrapment if not complete debilitation, cyberspace invests people with an unprecedented degree of mobility. As a result, in sharp contrast to the experience one might have encountering Phelps's message in a public space near their home, the person who encounters Phelps's message online, where neither proximity nor distance hold their traditional values, can escape both quickly and with little effort.        However, it is also important to consider the extent to which the speed associated with virtual spaces may also affect the pace at which potentially injurious words, images, and ideas are recirculated and recontextualised. Building on Butler, I have emphasised that hate speech is always repeatable speech. If we take for granted the fact that cyberspace not only increases the amount of speech generated but also the rate at which such speech is recontextualised and redeployed, it would appear as if the potential exists both for the amount of hate speech in circulation to increase and for such speech to be repeated more often and more rapidly. If we further accept the claim that the repetition of hate speech is always an imperfect one, bound to result in some loss of meaning or minor transgression, it becomes possible to see how this highly indeterminable context may also enable hate speech to be reclaimed more quickly. Once again, """"godhatesfags.com"""" serves as a useful example.       Shortly after Phelps's Website appeared, online monitoring organisations, including Hate Watch, established direct links to the site. The link between Hate Watch and """"godhatesfags.com"""" not only serves to place Phelps's online activities under surveillance but also to recontextualise the site. Viewed through their link, Phelps's site is quite literally framed by the Hate Watch site, and subsequently, recontextualised by their anti-hate discourse in a surprisingly direct manner. In addition, various features of Phelps's site have also been parodied and appropriated. """"Godhatesfigs.com"""", which among other features includes a list of bible passages which allegedly confirm the Website's claim that """"god hates figs"""", is one such example. The creators of """"Godlovesfags.com"""" gained notoriety when they managed to steal Phelps's domain name and redirect all """"godhatesfags"""" visitors to their counter site for seventy-two hours. """"Godhatesphelps"""" is another site that seeks to parody and repeat aspects of Phelps's message in an effort to reveal the absurd nature of Phelps's claims.               Shortly after Phelps's Website appeared, online monitoring organisations, including Hate Watch, established direct links to the site. The link between Hate Watch and """"godhatesfags.com"""" not only serves to place Phelps's online activities under surveillance but also to recontextualise the site. Viewed through their link, Phelps's site is quite literally framed by the Hate Watch site, and subsequently, recontextualised by their anti-hate discourse in a surprisingly direct manner. In addition, various features of Phelps's site have also been parodied and appropriated. """"Godhatesfigs.com"""", which among other features includes a list of bible passages which allegedly confirm the Website's claim that """"god hates figs"""", is one such example. The creators of """"Godlovesfags.com"""" gained notoriety when they managed to steal Phelps's domain name and redirect all """"godhatesfags"""" visitors to their counter site for seventy-two hours. """"Godhatesphelps"""" is another site that seeks to parody and repeat aspects of Phelps's message in an effort to reveal the absurd nature of Phelps's claims.                   References          Austin, J.L. How to Do Things with Words. 15th ed. Cambridge: Harvard UP, 1997. (Original work published in 1962.)               Butler, Judith. Excited Speech: A Politics of the Performative. New York: Routledge, 1997.         Langston, Rae. Speech Acts and Unspeakable Acts. Philosophy and Public Affairs 22 (1993): 293-330.                 Matsuda, M.J., et al. Words That Wound: Critical Race Theory, Assaultive Speech, and the First Amendment. San Francisco: Westview Press, 1993.                   MacKinnon, C. Only Words. Cambridge: Harvard UP, 1993.                   Virilio, Paul. """"Global Algorithm 1.7: The Silence of the Lambs: Paul Virilio in Conversation (with C.Oliveira)."""" CTHEORY 1995. 18 June 1999 &lt;http://www.ctheory.com/ga1.7-silence.php&gt;.                   Virilio, Paul. Open Sky. Trans. J. Rose. New York: Verso, 1997.                   Virilio, Paul. """"Speed and Information: Cyberspace Alarm!"""" CTHEORY 18.3 (1995). 18 June 1999 &lt;http://www.dds.nl/~n5m/texts/virilio.htm&gt;.                   Virilio, Paul. Speed and Politics: An Essay on Dromololgy. Trans. M. Polizzotti. New York: Semiotext(e), 1986.         Citation reference for this article         MLA style:         Kate Eichhorn. """"Cyberhate and Performative Speech in Accelerated Time(s)."""" M/C: A Journal of Media and Culture 3.3 (2000). [your date of access] &lt;http://www.api-network.com/mc/0006/cyberhate.php&gt;.         Chicago style:         Kate Eichhorn, """"Cyberhate and Performative Speech in Accelerated Time(s),"""" M/C: A Journal of Media and Culture 3, no. 3 (2000), &lt;http://www.api-network.com/mc/0006/cyberhate.php&gt; ([your date of access]).          APA style:         Kate Eichhorn. (2000) Cyberhate and performative speech in accelerated time(s). M/C: A Journal of Media and Culture 3(3). &lt;http://www.api-network.com/mc/0006/cyberhate.php&gt; ([your date of access]).           ","",""
"2001","Recruitment by extremist groups on the Internet","Recruitment efforts of white extremist groups on the Internet were examined using qualitative methods. There is no evidence of a concerted effort to target children at popular locations where children are active on the Internet. Most propaganda writings are not well written and persuasive. White extremists attempt to attract children with text, MP3 music, and games, but their efforts are not well organized and are more reactive than aggressive. Findings are reported for the uses of the Internet, methods employed, and how technology of the Internet contributes to these activities.","",""
"2002","Digital Representation: Racism on the World Wide Web"," This paper argues that the various rhetorical modes in which hate is expressed on the Web are tailored to the types of messages offered. The unique technologies of the Web, that differentiate it from the earlier media of communication, facilitate the various rhetorical modes. The Web, as an unregulated medium, fosters the worldwide dissemination of both 'actionable' and 'non-actionable' hate messages. The actionable hate messages, regardless of their intensity and potential to excite violent actions, are not legally restricted through any international censorship regulations; the power of restricting such messages is national, if such messages counter national laws and conventions. The questions explored here are: Does the Internet and the Web facilitate the spreading of hate messages? Should Internet hate materials be regulated? If so, how might that be done? What criteria should be used to differentiate between hate and non-hate materials? Is it possible to draw and enforce a line between hate and non-hate messages? What input would measures against hate messages have on the Internet culture itself? ","",""
"2013","Racist comments at online news sites: a methodological dilemma for discourse analysis"," In 2004, awash with the hope for a public sphere reinvigorated by the popular internet, the online arms of many U.S. newspapers opened their websites for comments. Now, nine years into this experiment, many newspapers have abandoned the practice of allowing comments. Online news sites have adopted a variety of strategies to deal with offensive comments, including turning “comments off,” not archiving comments, and adopting aggressive comment moderation policies. These strategies present researchers who wish to understand how racism operates in the new public sphere of mainstream news sites with a set of methodological dilemmas. In this article we (1) lay out the methodological pitfalls for the systematic investigation of the prevalent pattern of racism in online comments in the public sphere and (2) suggest steps by which scholars may deal with these methodological intricacies. We conclude by pointing to the broader implications of online content moderation. ","",""
"2015","#Hashtagging hate: Using Twitter to track racism online","This paper considers three different projects that have used Twitter to track racist language: 1) Racist Tweets in Canada (the author’s original work); 2) Anti-social media (a 2014 study by U.K. think tank DEMOS); and, 3) The Geography of Hate Map (created by researchers at Humboldt University) in order to showcase the ability to track racism online using Twitter. As each of these projects collected racist language on Twitter using very different methods, a discussion of each data collection method used as well as the strengths and challenges of each method is provided. More importantly, however, this paper highlights why Twitter is an important data collection tool for researchers interested in studying race and racism.","",""
"2015","Histories of Hating"," This roundtable discussion presents a dialogue between digital culture scholars on the seemingly increased presence of hating and hate speech online. Revolving primarily around the recent #GamerGate campaign of intensely misogynistic discourse aimed at women in video games, the discussion suggests that the current moment for hate online needs to be situated historically. From the perspective of intersecting cultural histories of hate speech, discrimination, and networked communication, we interrogate the ontological specificity of online hating before going on to explore potential responses to the harmful consequences of hateful speech. Finally, a research agenda for furthering the historical understandings of contemporary online hating is suggested in order to address the urgent need for scholarly interventions into the exclusionary cultures of networked media. ","",""
"2016","Hate Speech and Covert Discrimination on Social Media: Monitoring the Facebook Pages of Extreme-Right Political Parties in Spain","This study considers the ways that overt hate speech and covert discriminatory practices circulate on Facebook despite its official policy that prohibits hate speech. We argue that hate speech and discriminatory practices are not only explained by users’ motivations and actions, but are also formed by a network of ties between the platform’s policy, its technological affordances, and the communicative acts of its users. Our argument is supported with longitudinal multimodal content and network analyses of data extracted from official Facebook pages of seven extreme-right political parties in Spain between 2009 and 2013. We found that the Spanish extreme-right political parties primarily implicate discrimination, which is then taken up by their followers who use overt hate speech in the comment space.","",""
"2017","Your Privilege Is Trending: Confronting Whiteness on Social Media"," Social media activism provides an important space for dialogue and consciousness-raising. Racism, privilege, and inequalities have received considerable attention in social media discussions. #WhiteProverbs was one attempt to confront this issue, focusing particularly on White privilege. The tweets show how social media is a site where “serious games” are played, as agents are constrained by the “rules” but still able to make choices and push boundaries. This article explores the #WhiteProverbs tweets that came from Australian users to better understand how Australian social media users understand and confront whiteness. Through the use of humor, specifically irony and sarcasm, Twitter users identify a number of key ways that White privilege is reproduced, including justifications for racial inequality, questioning claims to racial differences, and constructing an exclusively White national identity. ","",""
"2018","Inciting anger through Facebook reactions in Belgium: The use of emoji and related vernacular expressions in racist discourse","This article uses the concept of ‘platformed racism‘ to explore the tension between how platforms afford and govern emoji, and how users appropriate them to engage in racist discourse. The paper takes the example of Belgian far-right political party Vlaams Belang’s use of Facebook Reactions to spread anger, and how audiences responded to this call by posting more emoji to express rage and engage in long-running racist tropes. Emoji are central elements of social media and its practices, and represent an opportunity to investigate the material politics of platforms and to explore their role in racist discourse.","",""
"2018","#GirlsLikeUs: Trans advocacy and community building online"," In this research, we examine the advocacy and community building of transgender women on Twitter through methods of network and discourse analysis and the theory of networked counterpublics. By highlighting the network structure and discursive meaning making of the #GirlsLikeUs network, we argue that the digital labor of trans women, especially trans women of color, represents the vanguard of struggles over self-definition. We find that trans women on Twitter, led by Janet Mock and Laverne Cox, and in response to histories of misrepresentation and ongoing marginalization and violence, deliberately curate an intersectional networked counterpublic that works to legitimize and support trans identities and advocate for trans autonomy in larger publics and counterpublics. ","",""
"2018","“One tweet to make so much noise”: Connected celebrity activism in the case of Marlee Matlin"," Celebrity activism, online celebrity, and online activism are all growing areas of research, but have received relatively little integration. This article argues that connected celebrity activism deploys social media to forge a variety of connections, enabling activist values to pervade a celebrity persona, reinforcing perceptions of authenticity and recirculating those values to disparate audiences. In the case of Deaf American actor Marlee Matlin, media reform activism serves as a unifying feature, expressed via technologically-facilitated connections between her acting, activist, and online activities, creating a cohesive star text that is seemingly authentic in respect to both Deaf and celebrity identities without being stereotypical. Such centrality and unification via connected celebrity activism stands in contrast to more traditional celebrity activism, and draws upon the specific dynamics of digital media, online activism, and contemporary celebrity culture. ","",""
"2019","From Twitter to Charlottesville: Analyzing the Fighting Words Between the Alt-Right and Antifa","This study examines the Twitter rivalry of two groups of the alt-Right and antifascist movement to understand how certain appeals, launched through social media, may promote material violence. Several studies have explored the impact of extreme political rhetoric in motivating hostile responses, such as the one that erupted at the 2017 Unite the Right rally in Charlottesville, Virginia. The present study contributes to this literature by examining how Twitter can offer a staging ground for political hostilities to swell, circulate, and sometimes activate the call for confrontation. A textual analysis deconstructs the Twitter accounts of the Proud Boys and Oath Keepers and Antifa over a six-week-period culminating in the violent Charlottesville rally. A focus on the groups’ framing of the opposition and use of persuasive appeals offers insight into the priming nature of political extremism happening on Twitter today.","",""
"2019","Extreme Speech and Global Digital Cultures — Introduction","In this article, we introduce the Special Section on Extreme Speech and Global Digital Cultures by developing the concept of “extreme speech.” In addressing the growing cultures of online vitriol and extremism, this concept advances a critical ethnographic sensibility to situated online speech cultures and a comparative global conversation that moves beyond the legal-normative debates that have been dominant in North America and Europe. We demonstrate this intervention by highlighting three interlinked arguments: Extreme speech inhabits a spectrum of practices rather than a binary opposition between acceptable and unacceptable speech; the sociotechnological aspects of new media embody a context in itself; and the violence of extreme speech acts is productive of identity in historically specific ways. This approach entails a methodological move that takes account of the meanings online users attach to vitriol as historical actors. It thus allows for critical frameworks to emerge from emic terms of action rather than moral concepts superimposed from the outside. Ethnographic explorations of extreme speech, we suggest, open up a new avenue to critique the contemporary global conjuncture of exclusionary politics.","",""
"2019","Extreme Speech| Defining Online Hate and Its “Public Lives”: What is the Place for “Extreme Speech”?","Following Sahana Udupa and Matti Pohjonen's invitation to move the debate beyond a normative understanding of hate speech, this article seeks to build a foundation for conceptual and empirical inquiry of speech commonly considered deviant and disturbing. It develops in three stages. It first maps the public lives of terms that refer to online vitriol and how they have been used by different communities of researchers, politicians, advocacy groups, and national organizations. Second, it shows how different types of “haters” have been interpreted as parts of “swarms” or “armies,” depending on whether their violent potential emerges around critical incidents or whether they respond to longer-term strategies through which communities and their leaders tie their speech acts to explicit narratives. The article concludes by locating “extreme speech” within this broader conceptual tapestry, arguing that the paternalistic gaze that characterizes a lot of research on online hate speech is tied to what Chantal Mouffe has referred to as the “moralization of politics,” a phenomenon that cannot be matched by responses that are themselves moral.","",""
"2019","Extreme Speech| The Digital Traces of #whitegenocide and Alt-Right Affective Economies of Transgression","This article explores how the notion of “extreme speech” can advocate a context-specific, practice-oriented approach to alt-right digital culture while also foregrounding its imbrication in larger histories of racial formation. Designating the popular White-nationalist hashtag #whitegenocide as an alt-right structure of feeling, it uses a data-critical discourse on “digital traces” to support a form of social media ethnography that traces affective communication practices online. Bringing this framework to the analysis of top #whitegenocide retweets, it elaborates the functioning of alt-right affective economies of transgression, which, driven by reactionary irony and a sense of race-based threat, contribute to shaping civil discourse and defining Whiteness in digital spaces. Finally, it investigates how the locative and corporal traces left by individual #whitegenocide retweeters both shape and are shaped by larger affective economies of transgression.","",""
"2019","Extreme Speech| A Comparative Approach to Social Media Extreme Speech: Online Hate Speech as Media Commentary","By exploring lessons learned from Ethiopia and Finland, this article challenges two assumptions about online hate speech research. First, it challenges the assumption that the best way to understand controversial concepts such as online hate speech is to determine how closely they represent or mirror some underlying set of facts or state of affairs online or in social media. Second, it challenges the assumption that academic research should be seen as separate from the many controversies that surround online hate speech debates globally. In its place, the article proposes the theory of “commentary” as a comparative research framework aimed at explaining how the messy and complex world of online and social media practices is articulated as hate speech over other ways of imagining this growing problem in global digital media environments.","",""
"2019","Elites and foreign actors among the alt-right: The Gab social media platform","Content regulation and censorship of social media platforms is increasingly discussed by governments and the platforms themselves. To date, there has been little data-driven analysis of the effects of regulated content deemed inappropriate on online user behavior. We therefore compared Twitter — a popular social media platform that occasionally removes content in violation of its Terms of Service — to Gab — a platform that markets itself as completely unregulated. Launched in mid-2016, Gab is, in practice, dominated by individuals who associate with the “alt-right” political movement in the United States. Despite its billing as “The Free Speech Social Network,” Gab users display more extreme social hierarchy and elitism when compared to Twitter. Although the framing of the site welcomes all people, Gab users’ content is more homogeneous, preferentially sharing material from sites traditionally associated with the extremes of American political discourse, especially the far right. Furthermore, many of these sites are associated with state-sponsored propaganda from foreign governments. Finally, we discovered a significant presence of German language posts on Gab, with several topics focusing on German domestic politics, yet sharing significant amounts of content from U.S. and Russian sources. These results indicate possible emergent linkages between domestic politics in European and American far right political movements. Implications for regulation of social media platforms are discussed.","",""
"2019","Alt-right pipeline: Individual journeys to extremism online","The rise of the alt-right as a potent and sometimes violent political force has been well documented. Yet the journey of an individual towards upholding these ideologies is less well understood. Alt-righters are not instantly converted, but rather incrementally nudged along a particular medial pathway. Drawing on video testimonies, chat logs, and other studies, this paper explores the interaction between this alt-right “pipeline” and the psyche of a user. It suggests three overlapping cognitive phases that occur within this journey: normalization, acclimation, and dehumanization. Finally, the article examines the individual who has reached the end of this journey, an extremist who nevertheless remains largely unregistered within traditional terrorist classifications.","",""
"2019","Collective identity changes in far-right online communities: The role of offline intergroup conflict"," Despite the increasing citizen engagement with socio-political online communities, little is known about how such communities are affected by significant offline events. Thus, we investigate here the ways in which the collective identity of a far-right online community is affected by offline intergroup conflict. We examine over 14 years of online communication between members of Stormfront Downunder, the Australian sub-forum of the global white supremacist community Stormfront.org . We analyse members’ language use and discourse before and after significant intergroup conflict in 2015, culminating in local racist riots in Sydney, Australia. We found that the riots were associated with significant changes in the collective beliefs of the community (as captured by members’ most salient concerns and group norms), emotions and consensus within the community. Overall, the effects of the local riots were manifest in a reinvigorated sense of purpose for the far-right community with a stronger anti-Muslim agenda. ","",""
"2019","Visualizing YouTube’s comment space: online hostility as a networked phenomena"," This study examines YouTube’s comment space. By focusing on responses to the provocative musical group, Das Racist, we offer an innovative analysis of online racialized expression as a networked phenomenon. A blend of social network analysis, qualitative coding, and thick data descriptive methods are used to interpret comments posted on the five most viewed Das Racist videos. Given the dearth of literature exploring YouTube’s comment space, this study serves as a critical means to further understand race and the production and consumption of YouTube comments in everyday online encounters. We visualized networked antagonisms, which were found to be significantly racialized, and entangled with other expressions of hostility. YouTube comments are often perceived as individual, random insults or only generalized expressions of “hate.” Our study probes deeper and discovers that racialized expressions also involved networked interactions, where hostile ideas, passed through multiple parts of the comment network, both intra-/inter-video. ","",""
"2020","Report and repeat: Investigating Facebook’s hate speech removal process","Social media is rife with hate speech. Although Facebook prohibits this content on its site, little is known about how much of the hate speech reported by users is actually removed by the company. Given the enormous power Facebook has to shape the universe of discourse, this study sought to determine what proportion of reported hate speech is removed from the platform and whether patterns exist in Facebook’s decision-making process. To understand how the company is interpreting and applying its own Community Standards regarding hate speech, the authors identified and reported hundreds of comments, posts, and images featuring hate speech to the company (n=311) and recorded Facebook’s decision regarding whether or not to remove the reported content. A qualitative content analysis was then performed on the content that was and was not removed to identify trends in Facebook’s content moderation decisions about hate speech. Of particular interest was whether the company’s 2018 policy update resulted in any meaningful change.Our results indicated that only about half of reported content containing hate speech was removed. The 2018 policy change also appeared to have little impact on the company’s decision-making. The results suggest that Facebook also had substantial issues including: removing misogynistic hate speech, establishing consistency in removing attacks and threats, an inability to consider context in removal decisions, and a general lack of transparency within the hate speech removal processes. Facebook’s failure to effectively remove reported hate speech allows misethnic discourses to spread and perpetuates stereotypes. The paper concludes with recommendations for Facebook and other social media organizations to consider to minimize the amount and impact of hate speech on their platforms.","",""
"2020","Manufacturing Hate 4.0: Can Media Studies Rise to the Challenge?"," This article reflects on the growing scourge of hate speech and its propagation via digital social media networks. It discusses how media studies has drawn attention to salient aspects of online hate speech including technological affordances, communication tactics, representational tropes, and audience response. It argues that insights from media studies are vital for unpacking the societal impact of the media and indeed for tackling a destructive force such as online hate speech. It further encourages media studies scholars to engage vigorously with colleagues in and across other disciplines to forge interdisciplinary research collaborations to address pressing societal issues. It urges media studies scholars to connect with the realms of industry, policy making, and civic society to ensure that the public discourse on the challenges of digitalization and mediatization is academically informed, evidence-based, and finely balanced. ","",""
"2021","Zoom-ing in on White Supremacy","The Alt Right Are Not Alright  Academic explorations complicating both the Internet and whiteness have often focussed on the rise of the “alt-right” to examine the co-option of digital technologies to extend white supremacy (Daniels, “Cyber Racism”; Daniels, “Algorithmic Rise”; Nagle). The term “alt-right” refers to media organisations, personalities, and sarcastic Internet users who promote the “alternative right”, understood as extremely conservative, political views online. The alt-right, in all of their online variations and inter-grouping, are infamous for supporting white supremacy online, “characterized by heavy use of social media and online memes. Alt-righters eschew ‘establishment’ conservatism, skew young, and embrace white ethnonationalism as a fundamental value” (Southern Poverty Law Center). Theoretical studies of the alt-right have largely focussed on its growing presence across social media and websites such as Twitter, Reddit, and notoriously “chan” sites 4chan and 8chan, through the political discussions referred to as “threads” on the site (Nagle; Daniels, “Algorithmic Rise”; Hawley). As well, the ability of online users to surpass national boundaries and spread global white supremacy through the Internet has also been studied (Back et al.). The alt-right have found a home on the Internet, using its features to cunningly recruit members and to establish a growing community that mainstream politically extreme views (Daniels, “Cyber Racism”; Daniels, “Algorithmic Rise; Munn). This body of knowledge shows that academics have been able to produce critically relevant literature regarding the alt-right despite the online anonymity of the majority of its members. For example, Conway et al., in their analysis of the history and social media patterns of the alt-right, follow the unique nature of the Christchurch Massacre, encompassing the use and development of message boards, fringe websites, and social media sites to champion white supremacy online. Positioning my research in this literature, I am interested in contributing further knowledge regarding the alt-right, white supremacy, and the Internet by exploring the sinister conducting of Zoom-bombing anti-racist events. Here, I will investigate how white supremacy through the Internet can lead to violence, abuse, and fear that “transcends the virtual world to damage real, live humans beings” via Zoom-bombing, an act that is situated in a larger co-option of the Internet by the alt-right and white supremacists, but has been under theorised as a hate crime (Daniels; “Cyber Racism” 7). Shitposting  I want to preface this chapter by acknowledging that while I understand the Internet, through my own external investigations of race, power and the Internet, as a series of entities that produce racial violence both online and offline, I am aware of the use of the Internet to frame, discuss, and share anti-racist activism. Here we can turn to the work of philosopher Michel de Certeau who conceived the idea of a “tactic” as a way to construct a space of agency in opposition to institutional power. This becomes a way that marginalised groups, such as racialised peoples, can utilise the Internet as a tactical material to assert themselves and their non-compliance with the state. Particularly, shitposting, a tactic often associated with the alt-right, has also been co-opted by those who fight for social justice and rally against oppression both online and offline. As Roderick Graham explores, the Internet, and for this exploration, shitposting, can be used to proliferate deviant and racist material but also as a “deviant” byway of oppositional and anti-racist material. Despite this, a lot can be said about the invisible yet present claims and support of whiteness through Internet and digital technologies, as well as the activity of users channelled through these screens, such as the alt-right and their digital tactics. As Vikki Fraser remarks, “the internet assumes whiteness as the norm – whiteness is made visible through what is left unsaid, through the assumption that white need not be said” (120). It is through the lens of white privilege and claims to white supremacy that online irony, by way of shitposting, is co-opted and understood as an inherently alt-right tool, through the deviance it entails. Their sinister co-option of shitposting bolsters audacious claims as to who has the right to exist, in their support of white identity, but also hides behind a veil of mischief that can hide their more insidious intention and political ideologies. The alt-right have used “shitposting”, an online style of posting and interacting with other users, to create a form of online communication for a translocal identity of white nationalist members. Sean McEwan defines shitposting as “a form of Internet interaction predicated upon thwarting established norms of discourse in favour of seemingly anarchic, poor quality contributions” (19). Far from being random, however, I argue that shitposting functions as a discourse that is employed by online communities to discuss, proliferate, and introduce white supremacist ideals among their communities as well as into the mainstream. In the course of this article, I will introduce racist Zoom-bombing as a tactic situated in shitposting which can be used as a means of white supremacist discourse and an attempt to block anti-racist efforts. By this line, the function of discourse as one “to preserve or to reproduce discourse (within) a closed community” is calculatingly met through shitposting, Zoom-bombing, and more overt forms of white supremacy online (Foucault 225-226). Using memes, dehumanisation, and sarcasm, online white supremacists have created a means of both organising and mainstreaming white supremacy through humour that allows insidious themes to be mocked and then spread online. Foucault writes that “in every society the production of discourse is at once controlled, selected, organised and redistributed according to a certain number of procedures, whose role is to avert its powers and danger, to cope with chance events, to evade ponderous, awesome materiality” (216). As Philippe-Joseph Salazar recontextualises to online white supremacists, “the first procedure of control is to define what is prohibited, in essence, to set aside that which cannot be spoken about, and thus to produce strategies to counter it” (137). By this line, the alt-right reorganises these procedures and allocates a checked speech that will allow their ideas to proliferate in like-minded and growing communities. As a result, online white supremacists becoming a “community of discourse” advantages them in two ways: first, ironic language permits the mainstreaming of hate that allows sinister content to enter the public as the severity of their intentions is doubted due to the sarcastic language employed. Second, shitposting is employed as an entry gate to more serious and dangerous participation with white supremacist action, engagement, and ideologies. It is important to note that white privilege is embodied in these discursive practices as despite this exploitation of emerging technologies to further white supremacy, there are approaches that theorise the alt-right as “crazed product(s) of an isolated, extremist milieu with no links to the mainstream” (Moses 201). In this way, it is useful to consider shitposting as an informal approach that mirrors legitimised white sovereignties and authorised white supremacy. The result is that white supremacist online users succeed in “not only in assembling a community of actors and a collective of authors, on the dual territory of digital communication and grass-roots activism”, but also shape an effective fellowship of discourse that audiences react well to online, encouraging its reception and mainstreaming (Salazar 142). Continuing, as McBain writes, “someone who would not dream of donning a white cap and attending a Ku Klux Klan meeting might find themselves laughing along to a video by the alt-right satirist RamZPaul”. This idea is echoed in a leaked stylistic guide by white supremacist website and message board the Daily Stormer that highlights irony as a cultivated mechanism used to draw new audiences to the far right, step by step (Wilson). As showcased in the screen capture below of the stylistic guide, “the reader is at first drawn in by curiosity or the naughty humor and is slowly awakened to reality by repeatedly reading the same points” (Feinburg). The result of this style of writing is used “to immerse recruits in an online movement culture built on memes, racial panic and the worst of Internet culture” (Wilson).  Figure 1: A screenshot of the Daily Stormer’s playbook, expanding on the stylistic decisions of alt-right writers.  Racist Zoom-Bombing   In the timely text “Racist Zoombombing”, Lisa Nakamura et al. write the following:  Zoombombing is more than just trolling; though it belongs to a broad category of online behavior meant to produce a negative reaction, it has an intimate connection with online conspiracy theorists and white supremacy … . Zoombombing should not be lumped into the larger category of trolling, both because the word “trolling” has become so broad it is nearly meaningless at times, and because zoombombing is designed to cause intimate harm and terrorize its target in distinct ways. (30)  Notwithstanding the seriousness of Zoom-bombing, and to not minimise its insidiousness by understanding it as a form of shitposting, my article seeks to reiterate the seriousness of shitposting, which, in the age of COVID-19, Zoom-bombing has become an example of. I seek to purport the insidiousness of the tactical strategies of the alt-right online in a larger context of white violence online. Therefore, I am proposing a more critical look at the tactical use of the Internet by the alt-right, in theorising shitposting and Zoom-bombing as means of hate crimes wherein they impose upon anti-racist activism and organising. Newlands et al., receiving only limited exposure pre-pandemic, write that “Zoom has become a household name and an essential component for parties (Matyszczyk, 2020), weddings (Pajer, 2020), school and work” (1). However, through this came the strategic use of co-opting the application by the alt-right to digitise terror and ensure a “growing framework of memetic warfare” (Nakamura et al. 31). Kruglanski et al. label this co-opting of online tools to champion white supremacy operations via Zoom-bombing an example of shitposting:  Not yet protesting the lockdown orders in front of statehouses, far-right extremists infiltrated Zoom calls and shared their screens, projecting violent and graphic imagery such as swastikas and pornography into the homes of unsuspecting attendees and making it impossible for schools to rely on Zoom for home-based lessons. Such actions, known as “Zoombombing,” were eventually curtailed by Zoom features requiring hosts to admit people into Zoom meetings as a default setting with an option to opt-out. (128)  By this, we can draw on existing literature that has theorised white supremacists as innovation opportunists regarding their co-option of the Internet, as supported through Jessie Daniels’s work, “during the shift of the white supremacist movement from print to digital online users exploited emerging technologies to further their ideological goals” (“Algorithmic Rise” 63). Selfe and Selfe write in their description of the computer interface as a “political and ideological boundary land” that may serve larger cultural systems of domination in much the same way that geopolitical borders do (418). Considering these theorisations of white supremacists utilising tools that appear neutral for racialised aims and the political possibilities of whiteness online, we can consider racist Zoom-bombing as an assertion of a battle that seeks to disrupt racial justice online but also assert white supremacy as its own legitimate cause. My first encounter of local Zoom-bombing was during the Institute for Culture and Society (ICS) Seminar titled “Intersecting Crises” by Western Sydney University. The event sought to explore the concatenation of deeply inextricable ecological, political, economic, racial, and social crises. An academic involved in the facilitation of the event, Alana Lentin, live tweeted during the Zoom-bombing of the event:  Figure 2: Academic Alana Lentin on Twitter live tweeting the Zoom-bombing of the Intersecting Crises event. Upon reflecting on this instance, I wondered, could efforts have been organised to prevent white supremacy? In considering who may or may not be responsible for halting racist shit-posting, we can problematise the work of R David Lankes, who writes that “Zoom-bombing is when inadequate security on the part of the person organizing a video conference allows uninvited users to join and disrupt a meeting. It can be anything from a prankster logging on, yelling, and logging off to uninvited users” (217). However, this beckons two areas to consider in theorising racist Zoom-bombing as a means of isolated trolling. First, this approach to Zoom-bombing minimises the sinister intentions of Zoom-bombing when referring to people as pranksters. Albeit withholding the “mimic trickery and mischief that were already present in spaces such as real-life classrooms and town halls” it may be more useful to consider theorising Zoom-bombing as often racialised harassment and a counter aggression to anti-racist initiatives (Nakamura et al. 30). Due to the live nature of most Zoom meetings, it is increasingly difficult to halt the threat of the alt-right from Zoom-bombing meetings. In “A First Look at Zoom-bombings” a range of preventative strategies are encouraged for Zoom organisers including “unique meeting links for each participant, although we acknowledge that this has usability implications and might not always be feasible” (Ling et al. 1). The alt-right exploit gaps, akin to co-opting the mainstreaming of trolling and shitposting, to put forward their agenda on white supremacy and assert their presence when not welcome. Therefore, utilising the pandemic to instil new forms of terror, it can be said that Zoom-bombing becomes a new means to shitpost, where the alt-right “exploits Zoom’s uniquely liminal space, a space of intimacy generated by users via the relationship between the digital screen and what it can depict, the device’s audio tools and how they can transmit and receive sound, the software that we can see, and the software that we can’t” (Nakamura et al. 29). Second, this definition of Zoom-bombing begs the question, is this a fair assessment to write that reiterates the blame of organisers? Rather, we can consider other gaps that have resulted in the misuse of Zoom co-opted by the alt-right: “two conditions have paved the way for Zoom-bombing: a resurgent fascist movement that has found its legs and best megaphone on the Internet and an often-unwitting public who have been suddenly required to spend many hours a day on this platform” (Nakamura et al. 29). In this way, it is interesting to note that recommendations to halt Zoom-bombing revolve around the energy, resources, and attention of the organisers to practically address possible threats, rather than the onus being placed on those who maintain these systems and those who Zoom-bomb. As Jessie Daniels states, “we should hold the platform accountable for this type of damage that it's facilitated. It's the platform's fault and it shouldn't be left to individual users who are making Zoom millions, if not billions, of dollars right now” (Ruf 8). Brian Friedberg, Gabrielle Lim, and Joan Donovan explore the organised efforts by the alt-right to impose on Zoom events and disturb schedules: “coordinated raids of Zoom meetings have become a social activity traversing the networked terrain of multiple platforms and web spaces. Raiders coordinate by sharing links to Zoom meetings targets and other operational and logistical details regarding the execution of an attack” (14). By encouraging a mass coordination of racist Zoom-bombing, in turn, social justice organisers are made to feel overwhelmed and that their efforts will be counteracted inevitably by a large and organised group, albeit appearing prankster-like. Aligning with the idea that “Zoombombing conceals and contains the terror and psychological harm that targets of active harassment face because it doesn’t leave a trace unless an alert user records the meeting”, it is useful to consider to what extent racist Zoom-bombing becomes a new weapon of the alt-right to entertain and affirm current members, and engage and influence new members  (Nakamura et al. 34). I propose that we consider Zoom-bombing through shitposting, which is within “the location of matrix of domination (white supremacy, heteropatriarchy, ableism, capitalism, and settler colonialism)” to challenge the role of interface design and Internet infrastructure in enabling racial violence online (Costanza-Chock). Conclusion  As Nakamura et al. have argued, Zoom-bombing is indeed “part of the lineage or ecosystem of trollish behavior”, yet these new forms of alt-right shitposting “[need] to be critiqued and understood as more than simply trolling because this term emerged during an earlier, less media-rich and interpersonally live Internet” (32). I recommend theorising the alt-right in a way that highlights the larger structures of white power, privilege, and supremacy that maintain their online and offline legacies beyond Zoom, “to view white supremacy not as a static ideology or condition, but to instead focus on its geographic and temporal contingency” that allows acts of hate crime by individuals on politicised bodies (Inwood and Bonds 722). This corresponds with Claire Renzetti’s argument that “criminologists theorise that committing a hate crime is a means of accomplishing a particular type of power, hegemonic masculinity, which is described as white, Christian, able-bodied and heterosexual” – an approach that can be applied to theorisations of the alt-right and online violence (136). This violent white masculinity occupies a hegemonic hold in the formation, reproduction, and extension of white supremacy that is then shared, affirmed, and idolised through a racialised Internet (Donaldson et al.). Therefore, I recommend that we situate Zoom-bombing as a means of shitposting, by reiterating the severity of shitposting with the same intentions and sinister goals of hate crimes and racial violence. References  Back, Les, et al. “Racism on the Internet: Mapping Neo-Fascist Subcultures in Cyber-Space.” Nation and Race: The Developing Euro-American Racist Subculture. Eds. Jeffrey Kaplan and Tore Bjørgo. Northeastern UP, 1993. 73-101. Bonds, Anne, and Joshua Inwood. “Beyond White Privilege: Geographies of White Supremacy and Settler Colonialism.” Progress in Human Geography 40 (2015): 715-733. Conway, Maura, et al. “Right-Wing Extremists’ Persistent Online Presence: History and Contemporary Trends.” The International Centre for Counter-Terrorism – The Hague. Policy Brief, 2019. Costanza-Chock, Sasha. “Design Justice and User Interface Design, 2020.” Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology. Association for Computing Machinery, 2020. Daniels, Jessie. “The Algorithmic Rise of the ‘Alt-Right.’” Contexts 17 (2018): 60-65. ———. “Race and Racism in Internet Studies: A Review and Critique.” New Media &amp; Society 15 (2013): 695-719. ———. Cyber Racism: White Supremacy Online and the New Attack on Civil Rights. Rowman and Littlefield, 2009. De Certeau, Michel. The Practice of Everyday Life. First ed. U of California P, 1980. Donaldson, Mike. “What Is Hegemonic Masculinity?” Theory and Society 22 (1993): 643-657. Feinburg, Ashley. “This Is The Daily Stormer’s Playbook.” Huffington Post 13 Dec. 2017. &lt;http://www.huffpost.com/entry/daily-stormer-nazi-style-guide_n_5a2ece19e4b0ce3b344492f2&gt;. Foucault, Michel. “The Discourse on Language.” The Archaeology of Knowledge and the Discourse on Language. Ed. A.M. Sheridan Smith. Pantheon, 1971. 215-237. Fraser, Vicki. “Online Bodies and Sexual Subjectivities: In Whose Image?” The Racial Politics of Bodies, Nations and Knowledges. Eds. Barbara Baird and Damien W. Riggs. Newcastle: Cambridge Scholars Publishing, 2015. 116-132. Friedberg, Brian, Gabrielle Lim, and Joan Donovan. “Space Invaders: The Networked Terrain of Zoom Bombing.” Harvard Shorenstein Center, 2020. Graham, Roderick. “Race, Social Media and Deviance.” The Palgrave Handbook of International Cybercrime and Cyberdeviance. Eds. Thomas J. Holt and Adam M. Bossler, 2019. 67-90. Hawley, George. Making Sense of the Alt-Right. Columbia UP, 2017. Henry, Matthew G., and Lawrence D. Berg. “Geographers Performing Nationalism and Hetero-Masculinity.” Gender, Place &amp; Culture 13 (2006): 629-645. Kruglanski, Arie W., et al. “Terrorism in Time of the Pandemic: Exploiting Mayhem.” Global Security: Health, Science and Policy 5 (2020): 121-132. Lankes, R. David. Forged in War: How a Century of War Created Today's Information Society. Rowman &amp; Littlefield, 2021. Ling, Chen, et al. “A First Look at Zoombombing, 2021.” Proceedings of the 42nd IEEE Symposium on Security and Privacy. Oakland, 2021. McBain, Sophie. “The Alt-Right, and How the Paranoia of White Identity Politics Fuelled Trump’s Rise.” New Statesman 27 Nov. 2017. &lt;http://www.newstatesman.com/culture/books/2017/11/alt-right-and-how-paranoia-white-identity-politics-fuelled-trump-s-rise&gt;. McEwan, Sean. “Nation of Shitposters: Ironic Engagement with the Facebook Posts of Shannon Noll as Reconfiguration of an Australian National Identity.” Journal of Media and Communication 8 (2017): 19-39. Morgensen, Scott Lauria. “Theorising Gender, Sexuality and Settler Colonialism: An Introduction.” Settler Colonial Studies 2 (2012): 2-22. Moses, A Dirk. “‘White Genocide’ and the Ethics of Public Analysis.” Journal of Genocide Research 21 (2019): 1-13. Munn, Luke. “Algorithmic Hate: Brenton Tarrant and the Dark Social Web.” VoxPol, 3 Apr. 2019. &lt;http://www.voxpol.eu/algorithmic-hate-brenton-tarrant-and-the-dark-social-web&gt;. Nagle, Angela. Kill All Normies: Online Culture Wars from 4chan and Tumblr to Trump and the Alt-Right. Zero Books, 2017. Nakamura, Lisa, et al. Racist Zoom-Bombing. Routledge, 2021. Newlands, Gemma, et al. “Innovation under Pressure: Implications for Data Privacy during the COVID-19 Pandemic.” Big Data &amp; Society July-December (2020): 1-14. Perry, Barbara, and Ryan Scrivens. “White Pride Worldwide: Constructing Global Identities Online.” The Globalisation of Hate: Internationalising Hate Crime. Eds. Jennifer Schweppe and Mark Austin Walters. Oxford UP, 2016. 65-78. Renzetti, Claire. Feminist Criminology. Routledge, 2013. Ruf, Jessica. “‘Spirit-Murdering' Comes to Zoom: Racist Attacks Plague Online Learning.” Issues in Higher Education 37 (2020): 8. Salazar, Philippe-Joseph. “The Alt-Right as a Community of Discourse.” Javnost – The Public 25 (2018): 135-143. Selfe, Cyntia L., and Richard J. Selfe, Jr. “The Politics of the Interface: Power and Its Exercise in Electronic Contact Zones.” College Composition and Communication 45 (1994): 480-504. Southern Poverty Law Center. “Alt-Right.” &lt;http://www.splcenter.org/fighting-hate/extremist-files/ideology/alt-right&gt;. Wilson, Jason. “Do the Christchurch Shootings Expose the Murderous Nature of ‘Ironic’ Online Fascism?” The Guardian, 16 Mar. 2019. &lt;http://www.theguardian.com/world/commentisfree/2019/mar/15/do-the-christchurch-shootings-expose-the-murderous-nature-of-ironic-online-fascism&gt;.","",""
"2021","Too dark to see? Explaining adolescents’ contact with online extremism and their ability to recognize it","ABSTRACT Adolescents are considered especially vulnerable to extremists’ online activities because they are ‘always online’ and because they are still in the process of identity formation. However, so far, we know little about (a) how often adolescents encounter extremist content in different online media and (b) how well they are able to recognize extremist messages. In addition, we do not know (c) how individual-level factors derived from radicalization research and (d) media and civic literacy affect extremist encounters and recognition abilities. We address these questions based on a representative face-to-face survey among German adolescents (n = 1,061) and qualitative interviews using a think-aloud method (n = 68). Results show that a large proportion of adolescents encounter extremist messages frequently, but that many others have trouble even identifying extremist content. In addition, factors known from radicalization research (e.g., deprivation, discrimination, specific attitudes) as well as extremism-related media and civic literacy influence the frequency of extremist encounters and recognition abilities.","",""
"2021","MAPPING DISCORD’S DARKSIDE: DISTRIBUTED HATE NETWORKS ON        DISBOARD","Scholars and journalists have noted that Discord, a social application oriented around voice/video chat communities and popular amongst gamers, has a history of harboring white supremacist and toxic groups. Discord has recently undertaken a public rebranding to distance itself from white supremacist, alt-right, and hateful content through a commitment to proactive moderation (Brown, 2020). However, Discord relies extensively on third-party services (like bots and server bulletins), and current scholarship has not adequately accounted for the role of such third-party actors in facilitating hateful and white supremacist networks on private platforms like Discord. This study notes how Discord’s model for curating only popular servers offloads the ethical burden of searchability to server bulletin sites like Disboard, to deleterious effect. This study involves two parts: 1) we use critical technoculture discourse analysis to examine Discord’s blogs, moderation policies, and API (Brock, 2018) and 2) we present data scraped from publicly-available descriptions and tags of 3,600 Discord servers listed on Disboard. Our study finds that thousands of servers on Disboard use overtly white supremacist and hateful tags, often advertising their ‘edgy’ communities as racist, raiding-oriented, and deliberately toxic. These servers exploit Discord’s moderation tools and Disboard’s networked affordances to proliferate within Discord’s distributed ecology. Ultimately, we argue that Discord’s response to hate, as a platform, does not address its reliance on unmoderated third-party services or the networked practices of its toxic communities.","",""
"2021","TAKING THE REDPILL: TALKING ABOUT EXTREMISM","There is great public concern about far-right radicalization online,         the process by which individuals are exposed to internet content and then adopt extremist or         hateful ideas. However, this concept has two major problems. First, the idea of         “radicalization” and “extremism” assumes that to study the radical is to study the other,         yet white supremacy and racism are hardly new phenomenon in America. Second, while the         internet clearly contributes to spreading fringe and far-right beliefs, """"online         radicalization"""" furthers simplistic narratives of media effects that ignore political,         economic, and emotional complexities. Thus, to understand the adoption of fringe and         far-right beliefs outside the problematic frame of “radicalization”, this paper takes up         narratives of “redpilling,” slang for coming to believe something counterfactual to         mainstream consensus. Drawing from qualitative and ethnographic research on far-right online         subcultures and critical terrorism studies, we ask how members of far-right and fringe         communiteis understand and perform their own community enrollment. We conduct critical         discourse theory and qualitative data analysis on a broad corpus of in situ discussions of         redpilling drawn from internet spaces devoted to far-right and fringe discussions, including         12 different subreddits, Gab.ai, Discord, Parler, Telegram, and 4chan. Our preliminary         findings suggest that “online radicalization” is an ongoing process in which people come to         believe extremist viewpoints by consuming far-right content, participating in far-right         internet spaces, viewing interpersonal interactions through an ideological lens, and         interacting with friends and family with similar views.","",""
"2021","Ghosts of white methods? The challenges of Big Data research in exploring racism in digital context"," The paper explores the potential and limitations of big data for researching racism on social media. Informed by critical data studies and critical race studies, the paper discusses challenges of doing big data research and the problems of the so called ‘white method’. The paper introduces the following three types of approach, each with a different epistemological basis for researching racism in digital context: 1) using big data analytics to point out the dominant power relations and the dynamics of racist discourse, 2) complementing big data with qualitative research and 3) revealing new logics of racism in datafied context. The paper contributes to critical data and critical race studies by enhancing the understanding of the possibilities and limitations of big data research. This study also highlights the importance of contextualisation and mixed methods for achieving a more nuanced comprehension of racism and discrimination on social media and in large datasets. ","",""
"2021","Gavin McInnes’s hate machine","At first, the Proud Boys were a seemingly innocuous white boys club that sprouted from the banter and riffs of online talk show host, Gavin McInnes. But the far right group grew into a nation-wide white supremacist organization. The group came about, thanks to McInnes and his The Gavin McInnes Show (TGMS). The Proud Boys and Gavin McInnes are a prime case study of the problem of free speech and the Internet. Here we see hate speech hiding behind the protective cloak of free speech. The conundrum becomes: How do we deal with fascist politics in the democratic space of the internet? The study conducts a frame analysis of over 32 hours of TGMS, utilizing Stanley’s (2018) rubric of fascist politics. By analyzing McInnes’s online discourse — his hate machine — we obtain a deeper understanding of how fascist politics gently slides into the mainstream and becomes a threat to peaceful political action.","",""
"2021","Upvoting extremism: Collective identity formation and the extreme right on Reddit"," Since the advent of the Internet, right-wing extremists and those who subscribe to extreme right views have exploited online platforms to build a collective identity among the like-minded. Research in this area has largely focused on extremists’ use of websites, forums, and mainstream social media sites, but overlooked in this research has been an exploration of the popular social news aggregation site Reddit. The current study explores the role of Reddit’s unique voting algorithm in facilitating “othering” discourse and, by extension, collective identity formation among members of a notoriously hateful subreddit community, r/The_Donald. The results of the thematic analysis indicate that those who post extreme-right content on r/The_Donald use Reddit’s voting algorithm as a tool to mobilize like-minded members by promoting extreme discourses against two prominent out-groups: Muslims and the Left. Overall, r/The_Donald’s “sense of community” facilitates identity work among its members by creating an environment wherein extreme right views are continuously validated. ","",""
"2021","Influence Without Metrics: Analyzing the Impact of Far-Right Users in an Online Discussion Forum"," The study presented in this article explores the processes through which influence takes shape in eclectic online forums with few vanity metrics. Using a dataset of 7.5 million posts in the large Swedish online discussion forum Flashback, it explores who becomes influential, their strategies for appealing to the community, and others’ support of them. While it has been known that Flashback hosts far-right users and content, the current study shows that these sentiments are not fringe or obscure, but instead seemingly widely supported and influential in the forum. It illustrates that the influential users—those who are supported and acknowledged by others as important—exclusively and continuously expressed far-right ideas and displayed an embeddedness within the far-right, as well as in the forum’s culture. The study finds that despite few visible markers, many users learned to recognize influential users and their far-right content as worthy of support. In the absence of built-in functions, some users engaged in manual “liking” and “sharing” of influential users’ content via their replies, acknowledging it as a way to legitimize them. At the same time, the analysis showcased how a lack of vanity metrics countered potential echo chamber effects in the forum as disliked users—advocating progressive gender and immigration ideas—were unintentionally amplified by those who attempted to silence them. The article also discusses the role of Flashback as a platform in the proliferation of hate. ","",""
"2021","Racism, Hate Speech, and Social Media: A Systematic Review and Critique"," Departing from Jessie Daniels’s 2013 review of scholarship on race and racism online, this article maps and discusses recent developments in the study of racism and hate speech in the subfield of social media research. Systematically examining 104 articles, we address three research questions: Which geographical contexts, platforms, and methods do researchers engage with in studies of racism and hate speech on social media? To what extent does scholarship draw on critical race perspectives to interrogate how systemic racism is (re)produced on social media? What are the primary methodological and ethical challenges of the field? The article finds a lack of geographical and platform diversity, an absence of researchers’ reflexive dialogue with their object of study, and little engagement with critical race perspectives to unpack racism on social media. There is a need for more thorough interrogations of how user practices and platform politics co-shape contemporary racisms. ","",""
"2021","Governing Hate: Facebook and Digital Racism"," This article is concerned with identifying the ideological and techno-material parameters that inform Facebook’s approach to racism and racist contents. The analysis aims to contribute to studies of digital racism by showing Facebook’s ideological position on racism and identifying its implications. To understand Facebook’s approach to racism, the article deconstructs its governance structures, locating racism as a sub-category of hate speech. The key findings show that Facebook adopts a post-racial, race-blind approach that does not consider history and material differences, while its main focus is on enforcement, data, and efficiency. In making sense of these findings, we argue that Facebook’s content governance turns hate speech from a question of ethics, politics, and justice into a technical and logistical problem. Secondly, it socializes users into developing behaviors/contents that adapt to race-blindness, leading to the circulation of a kind of flexible racism. Finally, it spreads this approach from Silicon Valley to the rest of the world. ","",""
"2021","When is the “Racist” Designation Truly Applicable? News Media’s Contribution to the Debatability of Racism"," Since the U.S. 2016 presidential election, journalists and news organizations have been forced to confront shifting racial, social and political climates, and re-evaluate practices and norms. However, news coverage of racism is complex, especially because the conceptualization of racism in society is discordant, and the parameters of racism are heavily debated. News coverage can contribute to this debatability, specifically when it presents issues of racism with certain linguistic and topical features. In a content analysis of social media posts from six of the Facebook pages maintained by national broadcast and newspaper organizations, the present study explores contextual and linguistic representations of racism, and how social media users on Facebook engage with news posted by these organizations. Results suggest representations in news coverage signal a public debate about what is and is not racism. Coverage heavily emphasized prominent figures, while social media audiences amplified Trump’s presence in social networks. ","",""
"2022","Politicization and Radicalization of Discourses in the Alt-Tech Ecosystem: A Case Study on Gab Social"," With the increasing popularity of some alternative social media platforms, the flow of information has to some extent shifted from the periphery to the core, where problematic discourses are produced, reproduced, and amplified in the alternative ecosystem, to later find their way into mainstream platforms. The non- or less-moderated nature of some alternative platforms provides a suitable space for politicization and radicalization of discourses. In this article, we use a case study of conversations about vaccination on Gab Social—an alternative platform often conceptualized as a far-right platform—to examine this radicalization process through a mixed-methods analysis of over 68,000 vaccination-related posts from before the COVID-19 pandemic until August 2021. The article shows that while antagonistic and conspiratorial thinking was an element of vaccination discourses on Gab even before the pandemic, such conversations became gradually politicized, and expanded far beyond the medical discourse and entered the domain of organizational politics. ","",""
"2022","Gayservatives on Gab: LGBTQ+ Communities and Far Right Social Media"," In the United States, LGBTQ+ individuals are often imagined as inherently politically progressive, but this assumption overlooks the experiences of self-identified LGBTQ+ conservatives. Likewise, although social media platforms are recognized as spaces of identity and community production for LGBTQ+ people generally, less work has considered how they provide a similar forum for “gayservatives.” In response, this article engages in a critical discourse analysis of LGBTQ+-oriented groups on the far right social media platform Gab. Results indicate that far right social media is utilized to connect with other politically similar LGBTQ+ individuals perceived to be absent in one’s offline community. Participants do so via discourses that both regulate and celebrate LGBTQ+ identities, particularly as it relates to hegemonic masculinity. These strategies generally reinforce, but at times reframe, stereotypical narratives about LGBTQ+ individuals. This study provides groundwork for more nuanced understandings of both LGBTQ+ conservatives and the ways power is socialized and embodied through discourses about sexual and gender identities. ","",""
"2022","It’s Not How You Say It, It’s What You Say: Ambient Digital Racism and Racial Narratives on Twitter"," Social media has been used to disseminate hate speech and racism. Racist opinions can be disguised through a language that may appear to be harmless; however, it can be part of a racist rhetoric toward communities of color. This type of racist communication is called Ambient Digital Racism (ADR). Through a thematic analysis, this project sought to identify and analyze social media racist discourses on Twitter in the context of George Floyd’s death. This research examined original tweets posted during the time of the protests using three known counter Black Lives Matter (BLM) hashtags, namely, #WhiteLivesMatter, #BlueLivesMatter, and #AllLivesMatter. After the analysis, two themes emerged, namely, the discourse of oppressor’s reverse racism and the social criminalization of BLM. These themes described the narratives used by these groups to develop a racist digital discourse that goes unnoticed by social media regulations and policies and that leaves an open space to negotiate what constitutes acceptable race talk and what constitutes a racist discourse. It was found that both themes were grounded on White victimization, color-blind racism, and the dehumanization of BLM as a social and racial justice movement. ","",""
"2023","How dark corners collude: a study on an online Chinese alt-right community","ABSTRACT The rise of the ‘alt-right’ (alternative right) and their communications on the Internet are not unique to the West. This study follows a mixed-methods approach combining topic modeling, social network analysis, and discourse analysis to analyze the discursive and network structure of an online Chinese alt-right community on Weibo. We summarize the topics Chinese alt-right influencers discuss and examine how these topics are interrelated. We find that the Chinese alt-right discourse can be deemed as both an extension and localization of the global alt-right: they frequently discuss global alt-right issues and also hold alt-right ideologies on domestic issues. Meanwhile, influencers in the community are densely connected, suggesting a high level of coordination and cooperation. We particularly identify two discursive strategies that alt-right influencers employ to reproduce the transnational alt-right discourse, namely invented common crisis of majority culture and transnational metaphor usage. These findings provide insights into the transnational aspect of the rise of global alt-right.","",""
"2023","Exploring discourses of whiteness in the Mary Beard Oxfam-Haiti Twitterstorm","ABSTRACT Social media may have amplified the Black Lives Matter movement, but companies like Facebook are often accused of not doing enough to address online hate speech. These platforms nevertheless have the potential to facilitate informal learning about the color blind racism through which whites rationalize the inequalities and injustices experienced by People of Color (PoC). This paper adds to the emergent literature in this area by exploring a high-profile Twitterstorm in February 2018 following a tweet from Cambridge University Professor Mary Beard about the sexual misconduct of Oxfam aid workers in Haiti. Academics like Dr Priya Gopal faced much criticism for suggesting the tweet was evidence of the white fragility and privilege to which they were frequently subjected. A qualitative content analysis of 1718 unique tweets containing ‘Mary Beard’, posted between 16 and 20 February 2018, was conducted to assess whether there was much evidence of agonistic debate between critics and supporters of Beard about whiteness. Results indicate that there were twice as many tweets criticizing Beard for her performative white privilege and frailty than those defending her. While the framing of the Twitterstorm was generally agonistic, there was little evidence of informal learning, with PoC conspicuously under-represented. Indeed, the burden of talking about racism and whiteness fell on the few PoC in the corpus, in much the same way as the ‘pre-social media’ era.","",""
"2023","PATHWAYS TO RADICALISM: EXAMINING ONLINE COMMUNITY AND POLITICAL PATHWAYS TO MORE RADICAL BELIEFS ON REDDIT","Recent research shows that “algorithmic radicalization” and “echo chambers”—the idea that recommendation algorithms on social media have a strong independent effect on radicalization and silo people into ideologically homogeneous communities—are not as prevalent nor influential as once feared. Yet, online political discourse is as toxic as ever while political misinformation continues to plague social media platforms. This begs the question: If algorithms aren’t encouraging radicalization, then what is producing it? Drawing from church-sect theory, this study interviews politically active Reddit users to better understand _how_ they arrived at their current media use, online engagement, and political beliefs. Results show that participants have a deep mistrust of mainstream media, leading them to seek alternative sources of political content. Reddit participation is also driven by a desire for “earnest” political discussions with like-minded individuals and cross-partisans, in part to “reject” partisan polarization. Despite engaging on more extreme subreddits, participants said their beliefs were unchanged, but that other Redditors had moved to more extreme beliefs over time. And, participants perceived their Reddit participation as necessary to _prevent_ radicalization and partisan polarization. Collectively, these results provide preliminary insight into the media and social/psychological pathways that could lead to online radicalization, providing an alternative explanation to algorithmic radicalization. This study also underscores the importance of interrogating the ecological pathways to radicalization for researchers and policy-makers; future interventions should account for attribution bias and the individual-level factors related to radicalization. ","",""
"2023","ONE HUNDRED NAZI SCREENS: INTERFACES AND THE STRUCTURE OF U.S. WHITE NATIONALIST DIGITAL NETWORKS ON TELEGRAM","The “Alt-Right,” a white nationalist online coalition, has collapsed amidst a revolution in digital governance termed the “regulatory turn.” Nevertheless, the regulatory turn remains incomplete because white nationalists utilize graphical user interface (GUI) design to subvert public stewardship. Why have some former Alt-Right platforms collapsed while others have grown despite increased scrutiny? The field’s account is currently limited to social media networks and rooted in positivist methods, lending a static conception of white nationalist networks that is slow to recognize cultural shifts. This paper fills the gap by comparatively critiquing the interfacing affordances of Telegram, an instant messaging app that functions as an """"ideological safe harbor"""" for U.S. white nationalists with content aggregation, blogging, and activist use-cases. I apply interface critique to index how the manipulation of graphical user interfaces allows white nationalists to frame their browsing as a technology of mastery over and against the regulatory turn. I argue that Telegram networks coopt the enclave public, exploiting an ideology of decentralization to mystify the leverage held by white nationalist developers over their users. This occlusion redirects white masculine anxieties against publicity to justify an intensified racist fanaticism and the exportation of violence against racial, religious, and gendered outsiders. White interfacing frames GUI design as a capitalist technology that weaponizes the racist and sexist logic of the “average user” to secure the reproduction of reactionary platforms. This project furthers Internet research by developing a theory of the interface as an ideological mirror of production.","",""
"2023","Surface and Sublevel Hate"," On the face of it, contemporary “alt-tech” platforms appear more moderate than legacy hate havens. Yet it's also clear that virulent hate in the form of misogyny, white supremacy, and xenophobia has not disappeared. Probing this tension, this article conceptualizes two forms of hate: Surface “Hate” (moderate content that is highly visible and easily accessible) and Sublevel Hate (explicit content that is more marginal and less discernible). These terms are illustrated by examining several viral videos on Rumble. This twinned mechanism explains how alt-tech platforms can be both accessible and extreme at the same time. Stratified hate is strategic, heightening the appeal and durability of online communities. Recognizing this dangerous dynamic is key for interventions seeking to counter it. ","",""
"2023","The affordances of extreme speech"," New media studies invested in online political conflict, radical and antagonistic subcultures have taken an interest in the affordances that shape memes, vernaculars and online political communication. One often overlooked affordance is the ensemble of social, communication, platform and legal frameworks stipulating what users can and cannot say, which I call “speech affordances.” To explore this concept, I look at the strategic communication of 4chan, Twitter and YouTube subcultures tied to a historical meme, “Kekistan,” often perceived as a key example of the ideological cacophony of the 2015–2017 online “culture wars.” I focus on how 4chan's policy of user anonymity, YouTube's unmoderated comment sections and Twitter's more proactive moderation practices brought some influencers to alter the original connotations of the meme into “overt” messages tolerable to Twitter and YouTube out-groups and platform moderation policies. Speech affordances bear methodological implications for historical studies of speech moderation and the overall mechanisms in which problematic language adapts to spaces with distinct speech norms. ","",""
"2023","‘Welcome to #GabFam’: Far-right virtual community on Gab"," With large social media platforms coming under increasing pressure to deplatform far-right users, the Alternative Technology movement (Alt-Tech) emerged as a new digital support infrastructure for the far right. We conduct a qualitative analysis of the prominent Alt-Tech platform Gab, a social networking service primarily modelled on Twitter, to assess the far-right virtual community on the platform. We find Gab’s technological affordances – including its lack of content moderation, culture of anonymity, microblogging architecture and funding model – have fostered an ideologically eclectic far-right community united by fears of persecution at the hands of ‘Big Tech’. We argue that this points to the emergence of a novel techno-social victimology as an axis of far-right virtual community, wherein shared experiences or fears of being deplatformed facilitate a coalescing of assorted far-right tendencies online. ","",""
"2023","Making the impossible possible? Framing confrontations of racism on social media as norm-setting"," Although confronting racist speech online can reduce future discriminatory behavior, people may be reluctant to confront because they perceive that it will be ineffective in changing racist attitudes. We test one theoretically grounded strategy for increasing white social media users’ willingness to confront online racism: reframing the confrontation goal from attitude change to norm-setting. We examine whether re-framing the confrontation goal in this way is effective under conditions where confrontation is least likely: (a) when the potential confronter is highly cynical about the efficacy of political discussions and (b) when the confrontation target is relationally distant. In a two-wave panel survey experiment collected during the 2020 US presidential election, participants reported greater likelihood of confronting a target when the goal was to set norms regarding racist speech, when they were less cynical of discussions, and when the target was relationally closer to them (e.g. family members as opposed to strangers). ","",""
"2023","Digital Rage: Testing “the Obama Effect” on Internet-Based Expressions of Racism"," The concept, “White Rage,” has previously been used to describe the way Whites have historically responded to Black advancement with policies and practices designed to quietly disrupt the progress Blacks had been making. White rage is typically subtle, masking its true intent. In contrast, recent research has found that the covert, subtle expressions of racism that are so normal in most mainstream spaces may be less common in internet-based communication. The extent to which online racism is connected to real-world racist attitudes, behaviors, and events, however, is unclear. In this article, we test the effects of real-world racialized events on explicit expressions of racism in online spaces using days that Obama gave speeches as our treatment effect and explicit usage of the “n-word” on the social media platform X (formerly Twitter) as our measurable outcome. Does usage of the n-word, a racial slur, increase in the days following speeches made by President Obama? Our results of over 9 years and more than 2.9 million tweets demonstrate a statistically significant increase of racist speech in response to those speech cycles, which are further placed in contrast to the speeches of other political actors, including President Trump. ","",""
"2023","Hate Influencers’ Mediation of Hate on Telegram: “We Declare War Against the Anti-White System”"," Hate influencers play a critical role in platforming hate. In this article, we illustrate how visible (forward-facing) and invisible (faceless) hate influencers mobilize far-right hate groups in the mobile socio-sphere. Based on our digital multimodal walkthrough method and multimodal discourse analysis, we analyze 16 Telegram channels for two designated hate groups. We focus our analysis on Proud Boys content related to the 6 January attack on Capitol Hill and the White Lives Matter rallies across North America in 2021. To illustrate how hate influencers mobilize these groups, we introduce a three-part model that entails the process (mobile mobilization), means (discourses), and ends (actualizing the objective of the hate group). ","",""
"2023","The Effects of Observer Expectations on Judgments of Anti-Asian Hate Tweets and Online Activism Response"," The rise of racial hate speech on social media has raised critical questions for scholars to explore. It is necessary to understand how outside observers passively evaluate (a) online racial hate speech posts on social media and (b) whether those evaluations are related to observers’ subsequent behavior. This study explored how observers evaluate acts of majority-on-minority and minority-on-minority anti-Asian hate tweets on Twitter. In an experiment ( n = 196) informed by expectancy violations theory, we tested how White observers evaluated anti-Asian tweets ostensibly posted by either a White or Black source. Analysis revealed a moderated-mediation pathway in which observers’ political partisanship (Democrat/Republican) affected how they judged the ethnic prototypicality of White and Black sources of racial hate speech; these source prototypicality judgments were in turn associated with observers’ judgments of tweet offensiveness and self-reported intentions to engage in online activism (i.e., signing an online petition). These results contribute to our understanding of outside observers’ differential expectancies regarding online hate speech, and how those expectancies can affect perceptions of and reactions to acts of racism. ","",""
"2023","(Hash)tagging intersection(ality): Black and Palestinian experiences on Twitter","AbstractIn this article, we examine how Twitter users discuss intersections of the Black American and Palestinian experience in 2021 through the lens of intersectionality. We explore two questions; how is intersectionality discussed and performed by Twitter users in relation to the Palestinian and Black experience against the backdrop of this particular crisis in Gaza? And how do users engage with the language of intersectionality to either reify, contradict, or complicate the intersection of the Palestinian and Black experiences on the platform? We find that intersectionality is mediated by elite users via branded communication, as well as invoked to highlight or deny the intersections of the Black and Palestinian experience by the most peripheral users on the platform.","",""
"2024","Online hate speech and instant messaging apps: An emerging research agenda","This study explores academic literature on hate speech and discriminatory practices in digital chat environments based on instant messaging apps, specifically Telegram and WhatsApp. The sample includes 40 articles in English published in scientific journals between January 2009 and April 2022, available in four databases: Web of Science, Directory of Open Access Journals, Scopus, and Google Scholar. As a result, we discussed five dimensions that characterize the research agenda on hate speech on instant messaging platforms: the plurality of the phenomena observed; the absence of a theoretical-methodological articulation that relates the affordances of the platforms, and the discursive strategies of hate in instant messaging applications; the ambiguity of the principle of privacy in the operating logic of instant messengers; the multiplicity of communicative practices and communicational ambiances on instant messaging platforms; and the diversity of methods used in research on hate speech on instant messaging platforms.","",""
"2024","“I’m not this Person”: Racism, content moderators, and protecting and denying voice online"," Much scholarship across the humanities and social sciences seek to shed light on the intersection of far-right politics and social media platforms. Yet, scholars tend to focus on racist actors and the ideological underpinnings of platform policies while the contingencies that shape the experiences of content reviewers who make decisions about racist content remain underexamined. This article fills this gap by exploring such contingencies from a linguistic anthropological perspective. Drawing on Facebook moderators’ stories, I illustrate the factors adjacent to, and beyond, ideology that animate the adjudication of racist hate speech. ","",""
"2024","Unmasking coordinated hate: Analysing hate speech on Spanish digital news media"," This study examines the characteristics and behaviours of accounts that propagate hate speech through their responses to articles posted on five leading digital news media in Spain on Platform X (previously Twitter). Using non-experimental quantitative research, we analysed 1345 hate-expressing messages from 173,449 user comments on content shared in five leading digital news media during January 2021. Network analysis, the Homophilic Exposure Index (HEI), regression analysis and the k-means algorithm were used to identify features that characterize accounts that disseminate low-intensity hate expressions in a coordinated manner, undermining the moderation efforts of digital news media. As a result, digital news media must develop strategies to reduce the presence of this type of expression and confront accounts that operate covertly in a coordinated manner, using Astroturfing to manipulate debates around the content published on X. ","",""
"2024","An attack on free speech? Examining content moderation, (de-), and (re-) platforming on American right-wing alternative social media"," Contemporary research on social media looks different than it did in the late 2010s, with users facing a high-choice social media environment as new platforms emerge. Subsequently, alt-right sites have experienced a rise in users—sometimes those who have experienced content moderation by traditional social media sites. As such, scholars have investigated the impact of this content moderation (e.g. de-platforming) on users and the content posted on new alt-right platforms. This work seeks to expand extant research through analyzing a survey of Gab, Parler (now defunct), Truth Social, and Rumble users ( N = 427) who have experienced content moderation on other social media sites. While we find that those temporarily or permanently banned from traditional sites are unlikely to leave the platform altogether for a right-wing alternative social media (RWASM) site, there are active users on these sites worth studying. ","",""
"2024","Engagement in subversive online activity predicts susceptibility to persuasion by far-right extremist propaganda"," Despite the widespread assumption that online misbehavior affects outcomes related to political extremism, few studies have provided empirical evidence to this effect. To redress this gap, we performed two studies in which we explored the relationship between subversive online activities and susceptibility to persuasion by far-right extremist propaganda. Study 1 ( N = 404) demonstrates that when individuals are exposed to far-right “scientific racism” propaganda, subversive online activity is significantly associated with feelings of gratification, attribution of credibility to and intention to support the propaganda’s source, as well as decreased resistance (in the form of reactance) to the propaganda. To verify these findings across thematic domains, Study 2 ( N = 396) focused on far-right extremist propaganda that advocates “male supremacy.” Results in Study 2 replicated those from Study 1. These findings have implications for understanding subversive online activity, vis-à-vis its association with one’s susceptibility to persuasion by far-right extremist propaganda. ","",""
"2024","Hate-sharing: A case study of its prevalence and impact on Gab"," This article brings frameworks from literary and cultural studies and methods from network science to bear on a central topic in political communication research: polarization. Recent studies have called into question the argument that digital “echo chambers” exacerbate polarization by preventing members from encountering a diversity of information and opinions. Using Gab, a far-right social media platform, as a case study, we offer further evidence that even members of highly polarized publics do engage in “cross-cutting.” However, we develop a distinct concept of hate-sharing, or sharing content for the purpose of disagreeing with or denigrating it. We show that hate-sharing is common on Gab. Moreover, it is associated with stronger community structure than other kinds of sharing and appears to confer substantially greater influence on those who engage in it. We interpret these findings as evidence that social networks incentivize the production of networked outrage—where “hating on” linked content merges with hate. ","",""
"2024","E-extremism: A conceptual framework for studying the online far right"," Despite a recent surge in the literature on the far right, there has been a theoretical gap in studying the relationship between the dynamics of change in the far right and the changing digital landscape. Drawing on a set of interrelated concepts developed in far-right studies, social movement studies, and media and communication studies, this theoretical paper adopts a framework based on the concepts of digital network repertoires and the mediation opportunity structure to discuss the ways in which various actors on the far right – reactionary conservatives, online antagonistic communities and right-wing extremists and terrorists – exploit the affordances of mainstream and alt-tech platforms for their own purposes. Through this discussion, this article seeks to shed light on the interplay between e-extremism and the online far right. ","",""
"2024","More Than Meets the Reply: Examining Emotional Belonging in Far-Right Social Media Space"," This article challenges prevailing assumptions that fringe social media platforms predominantly serve as unmoderated hate-filled spaces for far-right communication by examining the userbase’s emotional connection to these environments. Focusing on Gab Social, a popular alternative technology website with affordances akin to Twitter, Facebook, and Reddit, and its subgroup, “Introduce Yourself,” the research investigates how participants discuss their attachment and sense of membership within a far-right online community. Employing a constructivist grounded theory approach and a thick data mixed-methods technique encompassing netnography and sentiment analysis, I uncover the complex and impassioned narratives underlying users’ sense of emotional belonging on the platform. The resulting findings demonstrate how counter-mainstream media act as a unifying force by catering to the social needs of participants seeking an in-group of like-minded individuals. Moreover, I argue that fringe social media platforms offer participants far more than mainstream platforms, providing a positive interactive environment and a new virtual home for those feeling rejected and antagonized by other communities, institutions, and organizations, both online and offline. Therefore, the work offers valuable empirical insights into the emotional emphasis participants place on fringe social media and its implications for fostering attachment, community formation, and identity construction within far-right online counterpublics. ","",""

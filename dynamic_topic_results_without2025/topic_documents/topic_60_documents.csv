"year","title","abstract","journal","doi"
"2002","Technologization of Security: Management of Uncertainty and Risk in the Age of Biometrics","Contemporary security policies are characterized by a dramatic focus on high technology like biometrics as a security enabler. The process of the technologization of security, i.e. the making of technology the centerpiece of security systems and its perception as an absolute security provider, started in the US in the Eighties and has since been expanded to the European Union (EU) and to almost all developed countries. In this process, biometrics is accepted as the ultimate technology to identify people with certainty. This article examines this emphasis on biometrics in France and in the US in the context of the transformations of late modernity and analyses the philosophical and ethical issues that the emphasis on the body as the core element of identification systems raises.","",""
"2002","Picturing Algorithmic Surveillance: The Politics of Facial Recognition Systems","This paper opens up for scrutiny the politics of algorithmic surveillance through an examination of Facial Recognition Systems (FRSs) in video surveillance, showing that seemingly mundane design decisions may have important political consequences that ought to be subject to scrutiny. It first focuses on the politics of technology and algorithmic surveillance systems in particular: considering the broad politics of technology; the nature of algorithmic surveillance and biometrics, claiming that software algorithms are a particularly important domain of techno-politics; and finally considering both the growth of algorithmic biometric surveillance and the potential problems with such systems. Secondly, it gives an account of FRS's, the algorithms upon which they are based, and the biases embedded therein. In the third part, the ways in which these biases may manifest itself in real world implementation of FRS's are outlined. Finally, some policy suggestions for the future development of FRS's are made; it is noted that the most common critiques of such systems are based on notions of privacy which seem increasingly at odds with the world of automated systems.","",""
"2002","Wanted Dead or Digitized","A 16 September 2001 New York Times article describes a vision of the future “in the Brave New World of Fortress New York” (Barstow 2001). A diagram of an expensive but secure Times Square displays camera icons at each major intersection. These cameras would be connected to facial recognition technology that can digitally identify people on the street. According to the article, biometrics are a favorite of each of nine security and terrorism experts interviewed. On 17 September, the stock market reopened, and Visionics Corporation, the leading company in facial recognition technology, finished at number six in CNBC’s top ten stocks for the day. This was quite an achievement for a company whose premier product offering, aptly named FaceIt, is easily fooled by lighting changes, camera angle, beards, weight gain, and other minor changes in facial appearance. Much of the press coverage, however, has failed to adequately address the potential fallibility of the technology, instead structuring the debate over new surveillance systems as a trade-off between security and privacy. If the anti-terrorism legislation is any indication, it seems that “privacy” is losing both the discursive and the legislative battle, even if it is not clear how the new surveillance will provide us with increased “security.” Still, to say that privacy rights are suffering post-9/11 is an oversimplification. Consider a Visionics white paper issued on 24 September titled “Protecting Civilization from the Faces of Terror: A Primer on the Role Facial Recognition Technologies Can Plan in Enhancing Airport Security” (http://www.visionics.com/newsroom/downloads/whitepapers/ counterterrorism.pdf). The document offers an outline of how the company’s FaceIt system can be used to nab terrorist suspects without infringing on privacy rights.","",""
"2003","Biometrics and Privacy A note on the politics of theorizing technology","Society's increased surveillance needs are accelerating the spread of biometric security solutions (new authentication and identification technologies based on individual physical characteristics). There are two opposing lines of argument regarding the question of whether biometrics are a threat to privacy or not. This paper analyses the two views on their tacit assumptions regarding the nature of biometric technology. It argues that the different assessments of biometric technology involve different conceptualizations and constructions of the technology in terms of its demarcation as a stabilized object. On a second level, the analysis deals with the philosophical issue of technological determinism. The opposition between deterministic and voluntarist views of technology is shaped by an underlying opposition between reification of technology on the one hand, and a conception of technology as a multifactor contingent human practice on the other . In this paper deterministic and voluntarist constructions of technology are considered as rhetorical devices , and as discursive strategies. This allows me to show how distinctions between inherent features and contingent aspects of biometric technologies, as well as demarcations between human and non-human agency are made, that imply particular distributions of responsibility and negotiating space for human choices and values. Examples are presented showing how each construction of biometric technology serves its own purpose in the political process of shaping biometrics.","",""
"2003","Privacy, Surveillance, Trust and Regulation Identifying people: biometric discourse","This article does not have an abstract","",""
"2009","Culture &amp; biometrics: regional differences in the perception of biometric authentication technologies","","",""
"2011","Review of Pugliese’s Biometrics: Bodies, Technologies, Biopolitics","","",""
"2011","Building Biometrics: Knowledge Construction in the Democratic Control of Surveillance Technology"," If surveillance technologies are to be democratically controlled, then knowledge of these technologies is required. What do they do? How do they work? What are the costs? Yet gaining this knowledge in the context of a new surveillance technology such as biometrics can be problematic, because no settled definition exists. Competing versions of biometrics appear in both public and governmental discourse on the technology: different ideas about how often it fails, where it can be used and even what it does.  This paper is an exploration of how these different versions compete with each other, and how knowledge about a new surveillance technology such as biometrics is thus constructed. Through reference to original research in the context of the use of biometrics in the UK, points of stability and instability in the definition of biometrics are identified, and some of the processes through which instable definitions become stable are tracked.  From this empirical story, conclusions are drawn both for the process of construction of the meaning of technologies, and the general practice of surveillance in modern society. In particular, this paper aims to show how notions such as democratic control (central to the legitimation of state surveillance) become problematic when the very meaning of a technology is negotiable.  ","",""
"2012","Review of Gates's Our Biometric Future","","",""
"2012","Review of Nelson’s America Identified","Nelson, Lisa S. 2011. America Identified: Biometric Technology and Society. Cambridge: MIT Press.  258 pp. Hardcover, $32.00 (US), ISBN 978-0-262-01477-9","",""
"2012","Insecure Identities. The Approval of a Biometric ID Card in Mexico","Former Mexican President, Felipe Calderón, signed a decree to implement a compulsory ID with biometric traits for all the Mexican population. At first sight it seems like a good policy because Mexico has suffered a lot of problems with identification, but if we analyze the policy we  find it to be full of problems and contradictions, particularly because of the corrupt nature of Mexican politics.","",""
"2013","Shoshana Amielle Magnet, When Biometrics Fail: Gender, Race, and the Technology of Identity","When the twin towers fell in 2001, the American government began a “war on terror” on a number of fronts. Some we could see, and others we could not. The Bush administration focused its primary military efforts in various parts of the world to combat and disrupt groups identified as “terrorists.” A second front focused on protecting the home soil by identifying and investigating potential threats. The American government deployed probably the most visible part of this strategy in airports where the practice of taking a domestic flight changed almost overnight. Prior to 9/11, one could walk their family member right up to the departure gate to say good-bye. That practiced completely changed and the state set up security screening in every airport to check people and their carry-on items, just like in international departures. Many everyday items could be potential threats and the TSA agents screened everything with scrutiny, even our shoes. The Bush government also implemented another front in the war on terror in digital space that is much more difficult to see. Aided by the Patriot Act, the intelligence agencies developed and deployed many new information technologies for identifying risks and individuals and monitoring groups. These surveillance technologies included linking criminal databases at every level (local, state, national, international), monitoring cell phone and e-mail traffic, checking individual’s library records, full pipe Internet surveillance, digital fingerprinting, retina recognition, and other biometric technologies for capturing human bodies and turning them into digital data.","",""
"2013","Our Biometric Future: Facial Recognition Technology and the Culture of Surveillance","The need for security, emerged after 9/11, increased the request for a more intrusive government even at the cost of sacrificing civil liberties. The Kelly A. Gates’s book focuses on the visual technologies, specifically on the automated facial recognition technologies on which the research is investing a lot. Through a presentation of the framework for the development of these technologies – which is fruitful since the 1960s – the author carefully examines the constellation of social forces that are shaping technologies for the identification of human faces and facial expressions. The need for security has unified public and private interests earning a place of high priority in the political agenda and the augmenting of the technological research and development in this sector. In the name of the need for greater national security has been given the go ahead for an indiscriminate control of individuals and an accumulation of personal data, as if the acquisition of greater quantities of information could provide higher levels of security. The freedom of citizens is bartered, in this manner, for the (uncritical) acceptance of control and pervasive tracking measures to identify the enemy. The face, with its expressions collected in the database, is considered an index of identity. As these technologies measure the intensity of expression, determining what people are thinking or feeling, human behaviour may be calculated. But the faces are difficult to stabilize, the faces and their representation are not standard objects, they are a set of identities and meanings that can not be classified in a strict manner. Careful to consider the pros and cons of this type of instrument, the author also addresses the philosophical issues that differentiate the ‘see’ from the ‘look’, understood as social and psychological process. The computers only see in a metaphorical sense, and only thanks to a significant human investment. They do not really look at the people. However if we, humans, with our experience go wrong in assessing a person, why a technology could not be wrong? So why do we expect such a perfection from a technology on an issue that is not mechanical?","",""
"2013","Identity and the new eugenics in the Newborn Screening Saves Lives Act","Upon first glance, it is difficult to imagine who would be against protecting newborns. And yet, I argue through a case study of one genetic disease in particular, Pompe Disease, that DNA screening technology has unprecedented implications for the lives of babies born in the United States in ways that do not simply keep them safer. As noted in van Zoonen’s introduction to this section, we continually hear that, in our postmodern world, identities are multiple and fragmented. Yet, as she argues, new identification technologies – from radio-frequency identification (RFID) tags to biometric technologies to ICTs – ‘actively work against multiplicity and towards the fixation of singular identities’, as corporations and government institutions are invested in producing stable identities that can be reliably read off the body, thus generating new forms of corporate profit as well as new constructions of citizenship. I therefore ask how newborn screening might function as a set of knowledges about identity that we purchase in order to make us feel safer, whether or not this is the actual outcome. Furthermore, in understanding DNA screening","",""
"2013","Fixing identity? Biometrics and the tensions of material                     practices","Contemporary surveillance practices by both corporate and state actors increasingly rely on information and communication technologies to be able to categorize, process and analyse vast amounts of data. This is especially true of complex government projects created to manage entire populations. One set of technologies, generically referred to as ‘biometrics’, is being developed and implemented in different contexts in order to read characteristics of people’s bodies and physical behaviours with the aim of fixing identities, or authenticating or sorting them, based on pre-determined categories and logics. Some of the more recognizable examples of biometric technologies include digital fingerprinting, facial recognition, iris scanning and DNA profiling, but the list of technologies is long and constantly growing. Part of the allure of using biometrics to organize and segregate people is that our bodies are thought to provide an objective and verifiable source of truth about our identities, motivations and intentions and these technologies give access to these ‘truths’. Biometrics are also believed to be capable of ‘securing’ or ‘fixing’ identity in a way that makes fraudulent or multiple identities much more difficult, if not impossible, to maintain. Through the use of biometrics, organizations aim to individuate entire populations and then fix identities to administrative markers such as unique identification numbers.","",""
"2013","Review of Magnet's When Biometrics Fail: Gender, Race, and the Technology of Identity","","",""
"2013","‘My fingerprint on Osama’s cup.’ On objectivity and the role of the fictive regarding the acceptance of a biometric technology"," In inquiring about the social acceptance of the digital fingerprint during our research, we discovered the crucial role the fictive plays in our interviewees’ experiencing and assessment of control and security technology. Social acceptance is thus a heterogeneous phenomenon, not only because it depends on the situational features of dealing with the technology, but also, notably, because facts and fiction intermingle, sometimes indistinctly, within the discourses on surveillance and security. Mistrust in the technology tends to feed on fictive imageries, while at the same time resting on an unwavering belief in the objectivity of fingerprint data, presumably a clearly decipherable and reliable form of forensic proof. Against this backdrop, the article seeks to investigate the fictive’s critical role in countering security technologies. ","",""
"2013","From surveillant text to surveilling device: The face in urban transit spaces","This paper considers the power, the significance and the operation of the human face as it transitions from being a mere object for defining dispositions to an agency for transmitting control in contemporary transit spaces. Seen as the most idiosyncratic surface of the body, the face has become a critical object for the measurement of truthfulness in recent years. CCTV and facial recognition technologies have increasingly been deployed in various transit spaces such as airports and railway stations to scan, detect and recognise particular facial characteristics of the target face so as to identify risky situations and intervene before they become hazardous events. However, this paper shows that in addition to these techniques of control-at-a-distance, the face is becoming an important site for a different technique of control as well. Specifically, the face is becoming a key site for circulating a particular range of affects to modulate individuals and to maintain security in transit space. A growing number of transport operators are now supplementing ‘detached’ infrastructures such as CCTV and security personnel, with more ‘affective’ infrastructures of social control such as friendly smiling platform staff and welcoming station concierges at information desks. By taking a closer look at one Japanese railway operator, Keikyu Corporation, and its implementation of the ‘Smile Scan technology’, this paper investigates how the face of the passenger is modulated, made receptive and thus turned into an agent of social control in urban transit spaces.","",""
"2014","Governing through biometrics: the biopolitics of identity","There has been a growing interest in questions of neoliberalism and, associated with this, questions of biopolitics over recent years. These questions just seem to fit with the types of social and ...","",""
"2014","Recognizing Friend and Foe: Biometrics, Veridiction, and the Iraq War","In analyzing the deployment of biomertics in Iraq, argue that whereas the body was seen as a site of verification in 20th century surveillance and identification practices, in the ongoing War on Terror, and the Iraq War more specifically, it became a site of veridiction - a site in which the truth about the security of the state can be analyzed (Foucault 2008:32). The body thus became the basis for determining not so much one’s unique identity but one’s friendliness to the normative state order. Enemies could thus be identified and confined as a group, and in this process the state could be secured. In the ongoing of the War on Terror, the visual regime of veridiction has been further articulated to the logic of digital technologies in order to categorize an unfamiliar diverse population into a binary simplistic schema consistent of true and false, therefore friend or foe, and thus “go” - allowed to move through the country or “no go” - destined to be detained. In other words, the digitization of veridiction as the primary goal of biometrics is evident in the automation of the recognition method, the conversion of the archive into database, the transition away from the anthropological station onto mobile dispersed data-gathering enterprise, and replacement of scientific expertise with easy-to-use automated intelligence.","",""
"2014","Review of Ajana’s Governing through Biometrics","","",""
"2017","Managing Internal and External Communication in a Competitive Climate via EDI Concept","A biometric sensor device is the potential product of the forthcoming biotechnology for real-time tracking of physiological signals to support various healthcare and medical services such as homecare medical service, prevention, diagnosis, and follow-up services. A key agreement scheme between biometric sensor devices is a fundamental requirement to support the security of the healthcare and medical services. Existing key agreement schemes employ high computational cryptography mechanisms or share a pre-deployed key among biometric sensor devices. Due to stringent constraints of hardware capability, it is inadequate to use public cryptography mechanisms for biometric sensor devices. Moreover, it is also inappropriate to install a fixed secret key in implanted devices because, if the key is revealed, a person will have inevitable transplantation surgery for the secret redistribution. In this paper, we propose new lightweight key agreement scheme which requires only symmetric cryptosystem without a pre-deployed secret information for biometric sensor devices.","",""
"2017","Digital Citizenship and Surveillance| To Pre-Empt A Thief","This article explores the implications of recent developments in predictive policing, defined as the use of data-mining tools to predict and preempt criminal activity, for the relationship between citizenship and surveillance. It uses the example of predictive policing to consider the difference between panoptic modes of surveillance and emerging practices of environmental surveillance. The former rely on public awareness of surveillance and the internalization of the monitoring gaze, whereas the latter rely on actuarial modes of prediction. The growing emphasis on strategies for preemption rather than on policies for prevention displace political deliberation with technological expertise and work in the direction of automated decision making about resource allocation and armed response.","",""
"2017","Public faces? A critical exploration of the diffusion of face recognition technologies in online social networks"," In recent years, we have witnessed a rapid spread of biometric technologies from the security domain to commercial and social media applications. In this article, we critically explore the repercussions of this diffusion of face recognition to everyday contexts with an in-depth analysis of Facebook’s “tag suggestions” tool which first introduced the technology to on-line social networks. We use Nissenbaum’s framework of contextual integrity to show how the informational norms associated with biometrics in security and policing - their contexts of emergence - are grafted on-line social networks onto their context of iteration. Our analysis reveals a process that has inadvertently influenced the way users understand face recognition, precluding critical questioning of its wider use. It provides an important deepening of contextually-driven approaches to privacy by showing the process through which contexts are co-constitutive of informational norms. Citizens are also offered a critical tool for understanding the trajectory of biometrics and reflect on the data practices associated with the use of face recognition in social media and society at large. ","",""
"2017","Uncertainties of facial emotion recognition technologies and the automation of emotional labour","ABSTRACT Automated detection and recognition of faces have been implemented in a broad range of media environments. Following that development, what concerns us in this article is the analysis of emotions from facial expressions using computer-based systems, in relation to which we critically investigate the use of theories of basic emotions. We explore in depth the company Affectiva’s attempts to translate, represent and schematize human emotions, as they raise a variety of problems and issues of uncertainty. We analyse the uncertainties concerning the processing of the human face ‘as image’ due to issues concerning temporality and static images as well as polyphony and modulations of the spectrum of expressions. One of our key arguments concerns the temporal character of human emotions, and we address how algorithmically regulated protocols of discretization may be said to prompt specific patterns of emotional responses and expressions based on an ideal of eliminating uncertainty. Through discussions via art pieces by Lauren McCarthy and Kyle McDonald, we question what happens when the protocols of computer systems start to perform aspects of emotional labour for us, making judgments by predicting adequate emotional responses based on the use of the strict metrics criticized in the article.","",""
"2018","A Biometric Security Model for Mobile Applications","A biometric security model for mobile applications is defined. It is a low-complexity design with a security architecture including 2 biometric traits (fingerprint and iris). The fingerprint processing for feature generation is optimized on the mobile device, but the iris template optimization is performed on server. In both cases, the feature space is transformed to provide a suitable trade-off performance vs. complexity for a properly reduced dimensionality. The matching is based on a target-vs.-non-target classification in order to meet the requirements of an identification process in which only a target identity must be recognized. The target identity belongs to the mobile device owner.","",""
"2018","Biometric technology for voter identification: The experience in Ghana","ABSTRACT Our study examines how and why Ghana's first attempt to use biometric technology for voter identification and verification in its 2012 general elections failed. We employ activity theory as the analytical lens and interpretive case study as the methodology. Our findings show that the effectiveness of biometric technology to provide reliable identification does not depend solely on its technical qualities but also on real-time connectivity between registration centres and an electronic national register. Furthermore, the electoral officials need to be trained intensively to operate the machines and given guidance on how to handle situations when breakdowns occur. While biometric technology does introduce powerful capabilities, it is just one piece of a complex human activity system.","",""
"2019","Big Data Biopolitics","Abstract                 This article considers the medial logics of American terrorist watchlist screening in order to study the ways in which digital inequities result from specific computational parameters. Central in its analysis is Secure Flight, an automated prescreening program run by the Transportation Security Administration (TSA) that identifies lowand high-risk airline passengers through name-matching algorithms. Considering Secure Flight through the framework of biopolitics, this article examines how passenger information is aggregated, assessed and scored in order to construct racialised assemblages of passengers that reify discourses of American exceptionalism. Racialisation here is neither a consequence of big data nor a motivating force behind the production of risk-assessment programs. Both positions would maintain that discrimination is simply an effect of an information management system that considers privacy as its ultimate goal, which is easily mitigated with more accurate algorithms. Not simply emerging as an effect of discriminatory practices at airport security, racialisation formats the specific techniques embedded in terrorist watchlist matching, in particular the strategies used to transliterate names across different script systems. I argue thus that the biopolitical production of racialised assemblages forms the ground zero of Secure Flight’s computational parameters, as well as its claims to accuracy. This article concludes by proposing a move away from the call to solve digital inequities with more precise algorithms in order to carefully interrogate the forms of power complicit in the production and use of big data analytics.","",""
"2019","Incoherent Assemblages: Transgender Conflicts in US Security","Several identity-verifying procedures implemented in the wake of September 11, 2001, created conflicts for transgender people in the US who had different sex designations marked on various forms of identification. Trans studies scholars note that these conflicts highlight the assumption that sex is a stable marker of identity and expose that assumption as a fiction. The use of body scanners in airport security illuminates a similar reliance on binary sex categories. However, identity documentation policies and biometrics in airport security operate through different logics about how to solve the problem of affixing individual identities to changing bodies. The experiences of trans people with both identity documentation and airport security body scanners demonstrate that the requirements for passing as a proper citizen differ depending on the context: identity document policies prioritize medical alteration of the body while biometrics register medical alteration of the body as a potential threat to security.","",""
"2019","Platform Biometrics","This article identifies and analyses the emergence of platform biometrics. Biometrics are measurements of behavioral and physical characteristics, such as facial expressions, gait, galvanic skin response, and palm or iris patterns. Platform biometrics not only promise to connect geographically distant actors but also to curate new forms of value. In this piece I describe Microsoft Face, one of the major facial biometric systems currently on the market; this software promises to analyze which of seven “universal” emotions a subject is experiencing. I then offer a critique of the assumptions behind the system. First, theories of emotion are divided on whether emotions can be reliably and measurably expressed by the face. Second, emotions may not be universal, nor are there likely only seven basic emotions. Third, I draw on the work of Rouvroy and Berns (2013) to identify emotion-recognition technologies as a classic example of algorithmic governance. To outcome algorithmic governance is to enable the subject to creation and govern surveillance.  Platform biometrics will therefore provide a key component of surveillance capitalism’s appropriation of human experience (neuro-liberalism).","",""
"2019","The Biometric Assemblage: Surveillance, Experimentation, Profit, and the Measuring of Refugee Bodies"," Biometric technologies are routinely used in the response to refugee crises with the United Nations High Commissioner for Refugees (UNHCR) aiming to have all refugee data from across the world in a central population registry by the end of 2019. The article analyzes biometrics, artificial intelligence (AI), and blockchain as part of a technological assemblage, which I term the biometric assemblage. The article identifies five intersecting logics that explain wider transformations within the humanitarian sector and in turn shape the biometric assemblage. The acceleration of the rate of biometric registrations in the humanitarian sector between 2002 and 2019 reveals serious concerns regarding bias, data safeguards, data-sharing practices with states and commercial companies, experimentation with untested technologies among vulnerable people, and, finally, ethics. Technological convergence amplifies risks associated with each constituent technology of the biometric assemblage. The article finally argues that the biometric assemblage accentuates asymmetries between refugees and humanitarian agencies and ultimately entrenches inequalities in a global context. ","",""
"2020","A Biometric Logic of Revelation: Zach Blas’s &lt;em&gt;SANCTUM&lt;/em&gt; (2018)","Ubiquitous in airports, border checkpoints, and other securitised spaces throughout the world, full-body imaging scanners claim to read bodies in order to identify if they pose security threats. Millimetre-wave body imaging machines—the most common type of body scanner—display to the operating security agent a screen with a generic body outline. If an anomaly is found or if an individual does not align with the machine’s understanding of an “average” body, a small box is highlighted and placed around the “problem” area, prompting further inspection in the form of pat-downs or questioning. In this complex security regime governed by such biometric, body-based technologies, it could be argued that nonalignment with bodily normativity as well as an attendant failure to reveal oneself—to become “transparent” (Hall 295)—marks a body as dangerous. As these algorithmic technologies become more pervasive, so too does the imperative to critically examine their purported neutrality and operative logic of revelation and readability.Biometric technologies are marketed as excavators of truth, with their optic potency claiming to demask masquerading bodies. Failure and bias are, however, an inescapable aspect of such technologies that work with narrow parameters of human morphology. Indeed, surveillance technologies have been taken to task for their inherent racial and gender biases (Browne; Pugliese). Facial recognition has, for example, been critiqued for its inability to read darker skin tones (Buolamwini and Gebru), while body scanners have been shown to target transgender bodies (Keyes; Magnet and Rodgers; Quinan). Critical security studies scholar Shoshana Magnet argues that error is endemic to the technological functioning of biometrics, particularly since they operate according to the faulty notion that bodies are “stable” and unchanging repositories of information that can be reified into code (Magnet 2).Although body scanners are presented as being able to reliably expose concealed weapons, they are riddled with incompetencies that misidentify and over-select certain demographics as suspect. Full-body scanners have, for example, caused considerable difficulties for transgender travellers, breast cancer patients, and people who use prosthetics, such as artificial limbs, colonoscopy bags, binders, or prosthetic genitalia (Clarkson; Quinan; Spalding). While it is not in the scope of this article to detail the workings of body imaging technologies and their inconsistencies, a growing body of scholarship has substantiated the claim that these machines unfairly impact those identifying as transgender and non-binary (see, e.g., Beauchamp; Currah and Mulqueen; Magnet and Rogers; Sjoberg). Moreover, they are constructed according to a logic of binary gender: before each person enters the scanner, transportation security officers must make a quick assessment of their gender/sex by pressing either a blue (corresponding to “male”) or pink (corresponding to “female”) button. In this sense, biometric, computerised security systems control and monitor the boundaries between male and female.The ability to “reveal” oneself is henceforth predicated on having a body free of “abnormalities” and fitting neatly into one of the two sex categorisations that the machine demands. Transgender and gender-nonconforming individuals, particularly those who do not have a binary gender presentation or whose presentation does not correspond to the sex marker in their documentation, also face difficulties if the machine flags anomalies (Quinan and Bresser). Drawing on a Foucauldian analysis of power as productive, Toby Beauchamp similarly illustrates how surveillance technologies not only identify but also create and reshape the figure of the dangerous subject in relation to normative configurations of gender, race, and able-bodiedness. By mobilizing narratives of concealment and disguise, heightened security measures frame gender nonconformity as dangerous (Beauchamp, Going Stealth). Although national and supranational authorities market biometric scanning technologies as scientifically neutral and exact methods of identification and verification and as an infallible solution to security risks, such tools of surveillance are clearly shaped by preconceptions and prejudgements about race, gender, and bodily normativity. Not only are they encoded with “prototypical whiteness” (Browne) but they are also built on “grossly stereotypical” configurations of gender (Clarkson).Amongst this increasingly securitised landscape, creative forms of artistic resistance can offer up a means of subverting discriminatory policing and surveillance practices by posing alternate visualisations that reveal and challenge their supposed objectivity. In his 2018 audio-video artwork installation entitled SANCTUM, UK-based American artist Zach Blas delves into how biometric technologies, like those described above, both reveal and (re)shape ontology by utilising the affectual resonance of sexual submission. Evoking the contradictory notions of oppression and pleasure, Blas describes SANCTUM as “a mystical environment that perverts sex dungeons with the apparatuses and procedures of airport body scans, biometric analysis, and predictive policing” (see full description at https://zachblas.info/works/sanctum/).Depicting generic mannequins that stand in for the digitalised rendering of the human forms that pass through body scanners, the installation transports the scanners out of the airport and into a queer environment that collapses sex, security, and weaponry; an environment that is “at once a prison-house of algorithmic capture, a sex dungeon with no genitals, a weapons factory, and a temple to security.” This artistic reframing gestures towards full-body scanning technology’s germination in the military, prisons, and other disciplinary systems, highlighting how its development and use has originated from punitive—rather than protective—contexts.In what follows, we adopt a methodological approach that applies visual analysis and close reading to scrutinise a selection of scenes from SANCTUM that underscore the sadomasochistic power inherent in surveillance technologies. Analysing visual and aural elements of the artistic intervention allows us to complicate the relationship between transparency and recognition and to problematise the dynamic of mandatory complicity and revelation that body scanners warrant. In contrast to a discourse of visibility that characterises algorithmically driven surveillance technology, Blas suggests opacity as a resistance strategy to biometrics' standardisation of identity. Taking an approach informed by critical security studies and queer theory, we also argue that SANCTUM highlights the violence inherent to the practice of reducing the body to a flat, inert surface that purports to align with some sort of “core” identity, a notion that contradicts feminist and queer approaches to identity and corporeality as fluid and changing. In close reading this artistic installation alongside emerging scholarship on the discriminatory effects of biometric technology, this article aims to highlight the potential of art to queer the supposed objectivity and neutrality of biometric surveillance and to critically challenge normative logics of revelation and readability.Corporeal Fetishism and Body HorrorThroughout both his artistic practice and scholarly work, Blas has been critical of the above narrative of biometrics as objective extractors of information. Rather than looking to dominant forms of representation as a means for recognition and social change, Blas’s work asks that we strive for creative techniques that precisely queer biometric and legal systems in order to make oneself unaccounted for. For him, “transparency, visibility, and representation to the state should be used tactically, they are never the end goal for a transformative politics but are, ultimately, a trap” (Blas and Gaboury 158). While we would simultaneously argue that invisibility is itself a privilege that is unevenly distributed, his creative work attempts to refuse a politics of visibility and to embrace an “informatic opacity” that is attuned to differences in bodies and identities (Blas).In particular, Blas’s artistic interventions titled Facial Weaponization Suite (2011-14) and Face Cages (2013-16) protest against biometric recognition and the inequalities that these technologies propagate by making masks and wearable metal objects that cannot be detected as human faces. This artistic-activist project contests biometric facial recognition and their attendant inequalities by, as detailed on the artist’s website,making ‘collective masks’ in workshops that are modelled from the aggregated facial data of participants, resulting in amorphous masks that cannot be detected as human faces by biometric facial recognition technologies. The masks are used for public interventions and performances.One mask explores blackness and the racist implications that undergird biometric technologies’ inability to detect dark skin. Meanwhile another mask, which he calls the “Fag Face Mask”, points to the heteronormative underpinnings of facial recognition. Created from the aggregated facial data of queer men, this amorphous pink mask implicitly references—and contests—scientific studies that have attempted to link the identification of sexual orientation through rapid facial recognition techniques.Building on this body of creative work that has advocated for opacity as a tool of social and political transformation, SANCTUM resists the revelatory impulses of biometric technology by turning to the use and abuse of full-body imaging. The installation opens with a shot of a large, dark industrial space. At the far end of a red, spotlighted corridor, a black mask flickers on a screen. A shimmering, oscillating sound reverberates—the opening bars of a techno track—that breaks down in rhythm while the mask evaporates into a cloud of smoke. The camera swivels, and a white figure—the generic mannequin of the body scanner screen—is pummelled by invisible forces as if in a wind tunnel. These ghostly silhouettes appear and reappear in different positions, with some being whipped and others stretched and penetrated by a steel anal hook. Rather than conjuring a traditional horror trope of the body’s terrifying, bloody interior, SANCTUM evokes a new kind of feared and fetishized trope that is endemic to the current era of surveillance capitalism: the abstracted body, standardised and datafied, created through the supposedly objective and efficient gaze of AI-driven machinery.Resting on the floor in front of the ominous animated mask are neon fragments arranged in an occultist formation—hands or half a face. By breaking the body down into component parts— “from retina to fingerprints”—biometric technologies “purport to make individual bodies endlessly replicable, segmentable and transmissible in the transnational spaces of global capital” (Magnet 8). The notion that bodies can be seamlessly turned into blueprints extracted from biological and cultural contexts has been described by Donna Haraway as “corporeal fetishism” (Haraway, Modest). In the context of SANCTUM, Blas illustrates the dangers of mistaking a model for a “concrete entity” (Haraway, “Situated” 147). Indeed, the digital cartography of the generic mannequin becomes no longer a mode of representation but instead a technoscientific truth.Several scenes in SANCTUM also illustrate a process whereby substances are extracted from the mannequins and used as tools to enact violence. In one such instance, a silver webbing is generated over a kneeling figure. Upon closer inspection, this geometric structure, which is reminiscent of Blas’s earlier Face Cages project, is a replication of the triangulated patterns produced by facial recognition software in its mapping of distance between eyes, nose, and mouth. In the next scene, this “map” breaks apart into singular shapes that float and transform into a metallic whip, before eventually reconstituting themselves as a penetrative douche hose that causes the mannequin to spasm and vomit a pixelated liquid. Its secretions levitate and become the webbing, and then the sequence begins anew.In another scene, a mannequin is held upside-down and force-fed a bubbling liquid that is being pumped through tubes from its arms, legs, and stomach. These depictions visualise Magnet’s argument that biometric renderings of bodies are understood not to be “tropic” or “historically specific” but are instead presented as “plumbing individual depths in order to extract core identity” (5). In this sense, this visual representation calls to mind biometrics’ reification of body and identity, obfuscating what Haraway would describe as the “situatedness of knowledge”. Blas’s work, however, forces a critique of these very systems, as the materials extracted from the bodies of the mannequins in SANCTUM allude to how biometric cartographies drawn from travellers are utilised to justify detainment. These security technologies employ what Magnet has referred to as “surveillant scopophilia,” that is, new ways and forms of looking at the human body “disassembled into component parts while simultaneously working to assuage individual anxieties about safety and security through the promise of surveillance” (17). The transparent body—the body that can submit and reveal itself—is ironically represented by the distinctly genderless translucent mannequins. Although the generic mannequins are seemingly blank slates, the installation simultaneously forces a conversation about the ways in which biometrics draw upon and perpetuate assumptions about gender, race, and sexuality.Biometric SubjugationOn her 2016 critically acclaimed album HOPELESSNESS, openly transgender singer, composer, and visual artist Anohni performs a deviant subjectivity that highlights the above dynamics that mark the contemporary surveillance discourse. To an imagined “daddy” technocrat, she sings:Watch me…  I know you love me'Cause you're always watching me'Case I'm involved in evil'Case I'm involved in terrorism'Case I'm involved in child molestersEvoking a queer sexual frisson, Anohni describes how, as a trans woman, she is hyper-visible to state institutions. She narrates a voyeuristic relation where trans bodies are policed as threats to public safety rather than protected from systemic discrimination. Through the seemingly benevolent “daddy” character and the play on ‘cause (i.e., because) and ‘case (i.e., in case), she highlights how gender-nonconforming individuals are predictively surveilled and assumed to already be guilty. Reflecting on daddy-boy sexual paradigms, Jack Halberstam reads the “sideways” relations of queer practices as an enactment of “rupture as substitution” to create a new project that “holds on to vestiges of the old but distorts” (226). Upending power and control, queer art has the capacity to both reveal and undermine hegemonic structures while simultaneously allowing for the distortion of the old to create something new.Employing the sublimatory relations of bondage, discipline, sadism, and masochism (BDSM), Blas’s queer installation similarly creates a sideways representation that re-orientates the logic of the biometric scanners, thereby unveiling the always already sexualised relations of scrutiny and interrogation as well as the submissive complicity they demand. Replacing the airport environment with a dark and foreboding mise-en-scène allows Blas to focus on capture rather than mobility, highlighting the ways in which border checkpoints (including those instantiated by the airport) encourage free travel for some while foreclosing movement for others. Building on Sara Ahmed’s “phenomenology of being stopped”, Magnet considers what happens when we turn our gaze to those “who fail to pass the checkpoint” (107). In SANCTUM, the same actions are played out again and again on spectral beings who are trapped in various states: they shudder in cages, are chained to the floor, or are projected against the parameters of mounted screens. One ghostly figure, for instance, lies pinned down by metallic grappling hooks, arms raised above the head in a recognisable stance of surrender, conjuring up the now-familiar image of a traveller standing in the cylindrical scanner machine, waiting to be screened. In portraying this extended moment of immobility, Blas lays bare the deep contradictions in the rhetoric of “freedom of movement” that underlies such spaces.On a global level, media reporting, scientific studies, and policy documents proclaim that biometrics are essential to ensuring personal safety and national security. Within the public imagination, these technologies become seductive because of their marked ability to identify terrorist attackers—to reveal threatening bodies—thereby appealing to the anxious citizen’s fear of the disguised suicide bomber. Yet for marginalised identities prefigured as criminal or deceptive—including transgender and black and brown bodies—the inability to perform such acts of revelation via submission to screening can result in humiliation and further discrimination, public shaming, and even tortuous inquiry – acts that are played out in SANCTUM.Masked GenitalsFeminist surveillance studies scholar Rachel Hall has referred to the impetus for revelation in the post-9/11 era as a desire for a universal “aesthetics of transparency” in which the world and the body is turned inside-out so that there are no longer “secrets or interiors … in which terrorists or terrorist threats might find refuge” (127). Hall takes up the case study of Umar Farouk Abdulmutallab (infamously known as “the Underwear Bomber”) who attempted to detonate plastic explosives hidden in his underwear while onboard a flight from Amsterdam to Detroit on 25 December 2009. Hall argues that this event signified a coalescence of fears surrounding bodies of colour, genitalia, and terrorism. News reports following the incident stated that Abdulmutallab tucked his penis to make room for the explosive, thereby “queer[ing] the aspiring terrorist by indirectly referencing his willingness … to make room for a substitute phallus” (Hall 289).  Overtly manifested in the Underwear Bomber incident is also a desire to voyeuristically expose a hidden, threatening interiority, which is inherently implicated with anxieties surrounding gender deviance. Beauchamp elaborates on how gender deviance and transgression have coalesced with terrorism, which was exemplified in the wake of the 9/11 attacks when the United States Department of Homeland Security issued a memo that male terrorists “may dress as females in order to discourage scrutiny” (“Artful” 359). Although this advisory did not explicitly reference transgender populations, it linked “deviant” gender presentation—to which we could also add Abdulmutallab’s tucking of his penis—with threats to national security (Beauchamp, Going Stealth). This also calls to mind a broader discussion of the ways in which genitalia feature in the screening process. Prior to the introduction of millimetre-wave body scanning technology, the most common form of scanner used was the backscatter imaging machine, which displayed “naked” body images of each passenger to the security agent. Due to privacy concerns, these machines were replaced by the scanners currently in place which use a generic outline of a passenger (exemplified in SANCTUM) to detect possible threats.It is here worth returning to Blas’s installation, as it also implicitly critiques the security protocols that attempt to reveal genitalia as both threatening and as evidence of an inner truth about a body. At one moment in the installation a bayonet-like object pierces the blank crotch of the mannequin, shattering it into holographic fragments. The apparent genderlessness of the mannequins is contrasted with these graphic sexual acts. The penetrating metallic instrument that breaks into the loin of the mannequin, combined with the camera shot that slowly zooms in on this action, draws attention to a surveillant fascination with genitalia and revelation. As Nicholas L. Clarkson documents in his analysis of airport security protocols governing prostheses, including limbs and packies (silicone penis prostheses), genitals are a central component of the screening process. While it is stipulated that physical searches should not require travellers to remove items of clothing, such as underwear, or to expose their genitals to staff for inspection, prosthetics are routinely screened and examined. This practice can create tensions for trans or disabled passengers with prosthetics in so-called “sensitive” areas, particularly as guidelines for security measures are often implemented by airport staff who are not properly trained in transgender-sensitive protocols.ConclusionAccording to media technologies scholar Jeremy Packer, “rather than being treated as one to be protected from an exterior force and one’s self, the citizen is now treated as an always potential threat, a becoming bomb” (382). Although this technological policing impacts all who are subjected to security regimes (which is to say, everyone), this amalgamation of body and bomb has exacerbated the ways in which bodies socially coded as threatening or deceptive are targeted by security and surveillance regimes. Nonetheless, others have argued that the use of invasive forms of surveillance can be justified by the state as an exchange: that citizens should willingly give up their right to privacy in exchange for safety (Monahan 1). Rather than subscribing to this paradigm, Blas’ SANCTUM critiques the violence of mandatory complicity in this “trade-off” narrative. Because their operationalisation rests on normative notions of embodiment that are governed by preconceptions around gender, race, sexuality and ability, surveillance systems demand that bodies become transparent. This disproportionally affects those whose bodies do not match norms, with trans and queer bodies often becoming unreadable (Kafer and Grinberg). The shadowy realm of SANCTUM illustrates this tension between biometric revelation and resistance, but also suggests that opacity may be a tool of transformation in the face of such discriminatory violations that are built into surveillance.ReferencesAhmed, Sara. “A Phenomenology of Whiteness.” Feminist Theory 8.2 (2007): 149–68.Beauchamp, Toby. “Artful Concealment and Strategic Visibility: Transgender Bodies and U.S. State Surveillance after 9/11.” Surveillance &amp; Society 6.4 (2009): 356–66.———. Going Stealth: Transgender Politics and U.S. Surveillance Practices. Durham, NC: Duke UP, 2019.Blas, Zach. “Informatic Opacity.” The Journal of Aesthetics and Protest 9 (2014). &lt;http://www.joaap.org/issue9/zachblas.htm&gt;.Blas, Zach, and Jacob Gaboury. 2016. “Biometrics and Opacity: A Conversation.” Camera Obscura: Feminism, Culture, and Media Studies 31.2 (2016): 154-65.Buolamwini, Joy, and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” Proceedings of Machine Learning Research 81 (2018): 1-15.Browne, Simone. Dark Matters: On the Surveillance of Blackness. Durham, NC: Duke UP, 2015.Clarkson, Nicholas L. “Incoherent Assemblages: Transgender Conflicts in US Security.” Surveillance &amp; Society 17.5 (2019): 618-30.Currah, Paisley, and Tara Mulqueen. “Securitizing Gender: Identity, Biometrics, and Transgender Bodies at the Airport.” Social Research 78.2 (2011): 556-82.Halberstam, Jack. The Queer Art of Failure. Durham: Duke UP, 2011.Hall, Rachel. “Terror and the Female Grotesque: Introducing Full-Body Scanners to U.S. Airports.” Feminist Surveillance Studies. Eds. Rachel E. Dubrofsky and Shoshana Amielle Magnet. Durham, NC: Duke UP, 2015. 127-49.Haraway, Donna. “Situated Knowledges: The Science Question in Feminism and the Privilege of Partial Perspective.” Feminist Studies 14.3 (1988): 575-99.———. Modest_Witness@Second_Millennium. FemaleMan_Meets_OncoMouse: Feminism and Technoscience. New York: Routledge, 1997.Kafer, Gary, and Daniel Grinberg. “Queer Surveillance.” Surveillance &amp; Society 17.5 (2019): 592-601.Keyes, O.S. “The Misgendering Machines: Trans/HCI Implications of Automatic Gender Recognition.” Proceedings of the ACM on Human-Computer Interaction 2. CSCW, Article 88 (2018): 1-22.Magnet, Shoshana Amielle. When Biometrics Fail: Gender, Race, and the Technology of Identity. Durham: Duke UP, 2011.Magnet, Shoshana, and Tara Rodgers. “Stripping for the State: Whole Body Imaging Technologies and the Surveillance of Othered Bodies.” Feminist Media Studies 12.1 (2012): 101–18.Monahan, Torin. Surveillance and Security: Technological Politics and Power in Everyday Life. New York: Routledge, 2006.Packer, Jeremy.  “Becoming Bombs: Mobilizing Mobility in the War of Terror.” Cultural Studies 10.5 (2006): 378-99.Pugliese, Joseph. “In Silico Race and the Heteronomy of Biometric Proxies: Biometrics in the Context of Civilian Life, Border Security and Counter-Terrorism Laws.” Australian Feminist Law Journal 23 (2005): 1-32.Pugliese, Joseph. Biometrics: Bodies, Technologies, Biopolitics New York: Routledge, 2010.Quinan, C.L. “Gender (In)securities: Surveillance and Transgender Bodies in a Post-9/11 Era of Neoliberalism.” Eds. Stef Wittendorp and Matthias Leese. Security/Mobility: Politics of Movement. Manchester: Manchester UP, 2017. 153-69.Quinan, C.L., and Nina Bresser. “Gender at the Border: Global Responses to Gender Diverse Subjectivities and Non-Binary Registration Practices.” Global Perspectives 1.1 (2020). &lt;https://doi.org/10.1525/gp.2020.12553&gt;.Sjoberg, Laura. “(S)he Shall Not Be Moved: Gender, Bodies and Travel Rights in the Post-9/11 Era.” Security Journal 28.2 (2015): 198-215.Spalding, Sally J. “Airport Outings: The Coalitional Possibilities of Affective Rupture.” Women’s Studies in Communication 39.4 (2016): 460-80.","",""
"2020","THE BIOMETRIC LIVES OF MIGRANTS: BORDERS, DISCRIMINATION AND        (IN)JUSTICE","Biometrics, the technology for measuring, analysing and processing a person’s physiological characteristics, such as their fingerprints, iris or facial patterns, is increasingly used in the management of migrant and refugee flows. This panel interrogates the uses of biometric technologies and the consequences for the lives of migrants and refugees. It asks how biometric data are constituted, what their limitations and biases are, how biometric technologies challenge traditional notions of the physical border, in whose interest and with what implications for migrants and refugees. In particular, in bringing together a multidisciplinary group of international experts to develop a critical, comparative and empirically grounded dialogue, the panel explores the consequences of this ‘machinic life’ for the lives of actual people, migrants and refugees who navigate actual and digital borders in the quest of a better life. As such, the panel engages with crucial themes of processes of bordering, extractive logics and commercial dimensions of biometric flows and algorithmic sorting, discrimination and exclusion, and human agency and autonomy. Ultimately, all papers are concerned with the broader intersection of data, computation and justice.","",""
"2021","‘ETHICAL BIOMETRICS’ AND THE FACE OF THE CHILD: THE SURVEILLANCE OF         CHILDREN WITHIN FACIAL RECOGNITION INDUSTRY DISCOURSE","In this paper we analyse data gathered through facial recognition         tradeshow ethnographies and interviews with members of the biometrics industry, as we         consider recent shifts in industry discourse towards promoting the ‘ethical’ use of         biometric technology. As the biometrics industry increasingly moves towards a ‘Video         Surveillance as a Service’ (VSaaS) model, the study of facial recognition infrastructures is         becoming a crucial aspect of the interrogation of the Internet of Things. We demonstrate         that the facial recognition industry is acutely aware of critiques of facial recognition         cameras and biometric technologies as enabling social harms related to intrusiveness and         bias (see Stark, 2019), and that members of the industry are keen to promote a more         prosocial public image of the technology. Towards this end we find that biometric monitoring         of children has gained a prominent place in the promotion of facial recognition technologies         as a mode of ‘careful’ surveillance. We identify three key ‘use cases’ in which the face of         the child takes on a prominent role as justifying and legitimating the use of facial         recognition technologies – in the auditing of humanitarian food supply programs, in the         detection of so-called ‘staging’ of family units at the US border, and in the detection of         underage gambling in Australia. We argue that the immanent ‘ethical’ framing of the child’s         face in this context serves to obscure the political ramifications of the extension of         facial recognition and of biometric surveillance tools more broadly.","",""
"2021","Biometric identity systems in law enforcement and the politics of (voice) recognition: The case of SiiP"," Biometric identity systems are now a prominent feature of contemporary law enforcement, including in Europe. Often advanced on the premise of efficiency and accuracy, they have also been the subject of significant controversy. Much attention has focussed on longer-standing biometric data collection, such as finger-printing and facial recognition, foregrounding concerns with the impact such technologies can have on the nature of policing and fundamental human rights. Less researched is the growing use of voice recognition in law enforcement. This paper examines the case of the recent Speaker Identification Integrated Project, a European wide initiative to create the first international and interoperable database of voice biometrics, now the third largest biometric database at Interpol. Drawing on Freedom of Information requests, interviews and public documentation, we outline the emergence and features of SiiP and explore how voice is recognised and attributed meaning. We understand Speaker Identification Integrated Project as constituting a particular ‘regime of recognition’ premised on the use of soft biometrics (age, language, accent and gender) to disembed voice in order to optimise for difference. This, in turn, has implications for the nature and scope of law enforcement, people's position in society, and justice concerns more broadly. ","",""
"2021","Experimental Indices: Situational Assemblages of Facial Recognition","Facial recognition technologies are increasingly used outside of constricted, laboratory-like settings. While supporters of the technologies contend that they help in identifying threats by linking specific bodies to hard evidence, we argue that the indexical relations they exhibit are best described as experimental, pointing to specific situational constellations within which they were initially created. By revisiting key moments in the development of (semi-)automated facial recognition technologies from the late 1960s to the present, we identify varying situational assemblages of facial recognition that depend on different understandings of indexicality. These experimental indices rely on historical dynamics, including significant government interest in the development of facial recognition technology, expansion in the scale of experimental settings, and dissolution of the formerly strict boundaries between the social spheres of private image-sharing, commercial image distribution, and institutional image forensics for identification. In coupling experimental indices with the development of facial recognition technologies, we hope to show a way forward to comparing the histories of other evidential technical images too.","",""
"2021","The Case for a Ban on Facial Recognition Surveillance in Canada","","",""
"2021","“Smart” Cameras and the Operational Enclosure"," Concerns about the impending implementation of facial recognition technology in public and shared spaces go beyond privacy to include the changing relationship between space and power. This article explores the relationship between automated identification and social sorting, decision-making, and response. To develop a theoretical framework for considering the ways in which facial recognition technology reconfigures power relations, the article considers the effects of treating the face as what Harun Farocki calls an “operative image”: not a representation, but part of a sequence of operations. These operations deprive the face of its distinctive character to facilitate the automated governance of space. For Foucault, environmentality focused on the governance of populations, but digital technology individualizes and particularizes this process. Facial recognition technology raises issues of public concern not simply because it changes the conditions of privacy and recognition in shared spaces, but because it enables new modes of automated control. ","",""
"2021","Peoplemeter Technologies and the Biometric Turn in Audience Measurement"," Between 1980 and 1995, audience measurement companies, including Audits of Great Britain (AGB), Nielsen, Percy, and Arbitron, competed to develop peoplemeter technologies, ranging from remote controls to ultrasonic motion detectors, infrared body heat sensors, face scanners, and wearable devices. In this article, I argue that the evolution of peoplemeter technologies during this era was shaped by the inconsistent cooperation of viewers in the task of being measured, resulting in a turn away from active peoplemeters and toward passive peoplemeter methods. In particular, it marked a preoccupation in audience measurement with biometrics, and a constant onslaught of technological experiments in search of a seamless body/machine integration. In the process, the body became itself a technology: one that, if properly disciplined and utilized in the process of commodification, could make viewers more reliable consumers. ","",""
"2022","The politics of deceptive borders: ‘biomarkers of deceit’ and the case of iBorderCtrl","ABSTRACT This paper critically examines a recently developed proposal for a border control system called iBorderCtrl, designed to detect deception based on facial recognition technology and the measurement of micro-expressions, termed ‘biomarkers of deceit’. Funded under the European Commission’s Horizon 2020 programme, the system is analysed in relation to the wider political economy of ‘emotional AI’ and the history of deception detection technologies. We then move on to interrogate the design of iBorderCtrl using publicly available documents and assess the assumptions and scientific validation underpinning the project design. Finally, drawing on a Bayesian analysis we outline statistical fallacies in the foundational premise of mass screening and argue that it is very unlikely that the model that iBorderCtrl provides for deception detection would work in practice. By interrogating actual systems in this way, we argue that we can begin to question the very premise of the development of data-driven systems, and emotional AI and deception detection in particular, pushing back on the assumption that these systems are fulfilling the tasks they claim to be attending to and instead ask what function such projects carry out in the creation of subjects and management of populations. This function is not merely technical but, rather, we argue, distinctly political and forms part of a mode of governance increasingly shaping life opportunities and fundamental rights.","",""
"2022","The two faces of the child in facial recognition industry discourse: biometric capture between innocence and recalcitrance","ABSTRACT This article explores how the child is evoked in the discursive construction of facial recognition technology. Facial recognition technology is one of the most socially contentious emerging technologies of recent years, heavily criticised for enabling racialized and other forms of social harms. Drawing on data gathered through facial recognition tradeshow ethnographies, and interviews with members of the biometrics industry, we explore how the biometric monitoring of children has gained a prominent place in the industry’s promotion of facial recognition technology as a mode of ‘careful’ surveillance. At the same time, however, the fast-changing face of the growing child is acknowledged as a difficult technical challenge to the efficient development and use of this technology. We argue that in these industry discourses the child is figured as both innocent and recalcitrant, and that the facial recognition industry has productively exploited the tension between these two figurations to legitimate and expand its own enterprise.","",""
"2022","Sensing the human: biometric surveillance and the Japanese technology industry"," This article examines the Japanese biometrics industry and its discourse, with a focus on the language of biometric ‘sensing’ that has shaped its development over the past two decades. Rooted in the ubiquitous computing boom of the early 2000s, the language of sensing reimagines biometric technology as a mediator between the digital and the human, laying the foundation for biometric surveillance’s expansion into everyday settings such as retail ones. In these newer settings, biometric surveillance is promoted as a means for collecting data on human affect and behavior to be used for marketing and other applications. I argue that this growing ambiguity of biometric surveillance re-articulates a convergence between production and consumption, while it also informs safe society discourses and the shifting role of embodiment within digital culture. ","",""
"2022","The ethical application of biometric facial recognition technology","AbstractBiometric facial recognition is an artificial intelligence technology involving the automated comparison of facial features, used by law enforcement to identify unknown suspects from photographs and closed circuit television. Its capability is expanding rapidly in association with artificial intelligence and has great potential to solve crime. However, it also carries significant privacy and other ethical implications that require law and regulation. This article examines the rise of biometric facial recognition, current applications and legal developments, and conducts an ethical analysis of the issues that arise. Ethical principles are applied to mediate the potential conflicts in relation to this information technology that arise between security, on the one hand, and individual privacy and autonomy, and democratic accountability, on the other. These can be used to support appropriate law and regulation for the technology as it continues to develop.","",""
"2022","The unbearable (technical) unreliability of automated facial emotion recognition"," Emotion recognition, and in particular acial emotion recognition (FER), is among the most controversial applications of machine learning, not least because of its ethical implications for human subjects. In this article, we address the controversial conjecture that machines can read emotions from our facial expressions by asking whether this task can be performed reliably. This means, rather than considering the potential harms or scientific soundness of facial emotion recognition systems, focusing on the reliability of the ground truths used to develop emotion recognition systems, assessing how well different human observers agree on the emotions they detect in subjects’ faces. Additionally, we discuss the extent to which sharing context can help observers agree on the emotions they perceive on subjects’ faces. Briefly, we demonstrate that when large and heterogeneous samples of observers are involved, the task of emotion detection from static images crumbles into inconsistency. We thus reveal that any endeavour to understand human behaviour from large sets of labelled patterns is over-ambitious, even if it were technically feasible. We conclude that we cannot speak of actual accuracy for facial emotion recognition systems for any practical purposes. ","",""
"2022","Invasive Yet Inevitable? Privacy Normalization Trends in Biometric Technology"," As biometric technology relies on bodily, physical information, it is among the more intrusive technologies in the contemporary consumer market. Consumer products containing biometric technology are becoming more popular and normalized, yet little is known about public perceptions concerning its privacy implications, especially from the perspective of human agency. This study examines how people perceive biometric technologies in different societal contexts and via different agents in control. Our study revealed that, in large part, people’s perceptions of biometric technology are context-dependent, based on who retrieves and who benefits from the information and the situation where the data are collected. Participants were much more comfortable with more intrusive biometric technology in airport security than in a grocery store, and if it was employed to improve their health. We conclude by considering the implications of the survey for new threats to personal privacy that arise out of emerging technologies. ","",""
"2022","Biometric Bordering and Automatic Gender Recognition: Challenging Binary Gender Norms in Everyday Biometric Technologies","AbstractWith the rise of advanced biometric technologies, the surveilling of populations who do not match racial and gender norms has increased. Modern-day biometrics make assumptions about gender and race based on skin color, facial structure, body type, and body parts, which are encoded in predictive algorithms and other AI-driven systems. Growing empirical evidence points to the obstacles this poses for trans and non-binary individuals in several spheres, including border security, healthcare, and social media. Drawing on autoethnographic vignettes, semi-structured interviews, and survey responses, we look to the increased use of binary-based biometric technologies and automatic gender recognition (AGR), which rely on outmoded understandings of gender as static, measurable, and physiological. Our ethnographic data demonstrate how trans and non-binary bodies are forced to bend to these systems; meanwhile these technologies and algorithms increasingly extract data on trans and non-binary users, which may then be used as challenge sets to refine their accuracy.","",""
"2023","Your face is not new to me – Regulating the surveillance power of facial recognition technologies","","",""
"2023","Facial analysis: automated surveillance and the attempt to quantify emotion","ABSTRACT Even as attempts are being made to curtail the mass deployment of automated facial recognition systems in many countries, there are efforts to harvest new kinds of personal information from faces. Facial emotion recognition is a particularly ambitious form of facial analysis – an automated process in which faces are sorted amongst sets of categories – which has recently drawn criticism. The ways in which basic attributes of facial expression and emotion contradict the foundational assumptions of automated emotion recognition highlight larger conceptual problems with attempts to subject complex human phenomena to automated analysis.","",""
"2023","We have to talk about emotional AI and crime","AbstractEmotional AI is an emerging technology used to make probabilistic predictions about the emotional states of people using data sources, such as facial (micro)-movements, body language, vocal tone or the choice of words. The performance of such systems is heavily debated and so are the underlying scientific methods that serve as the basis for many such technologies. In this article I will engage with this new technology, and with the debates and literature that surround it. Working at the intersection of criminology, policing, surveillance and the study of emotional AI this paper explores and offers a framework of understanding the various issues that these technologies present particularly to liberal democracies. I argue that these technologies should not be deployed within public spaces because there is only a very weak evidence-base as to their effectiveness in a policing and security context, and even more importantly represent a major intrusion to people’s private lives and also represent a worrying extension of policing power because of the possibility that intentions and attitudes may be inferred. Further to this, the danger in the use of such invasive surveillance for the purpose of policing and crime prevention in urban spaces is that it potentially leads to a highly regulated and control-oriented society. I argue that emotion recognition has severe impacts on the right to the city by not only undertaking surveillance of existing situations but also making inferences and probabilistic predictions about future events as well as emotions and intentions.","",""
"2024","Navigating data governance risks: Facial recognition in law enforcement under EU legislation","Facial recognition technologies (FRTs) are used by law enforcement agencies (LEAs) for various purposes, including public security, as part of their legally mandated duty to serve the public interest. While these technologies can aid LEAs in fulfilling their public security responsibilities, they pose significant risks to data protection rights. This article identifies four specific risks associated with the use of FRT by LEAs for public security within the frameworks of the General Data Protection Regulation and Artificial Intelligence Act. These risks particularly concern compliance with fundamental data protection principles, namely data minimisation, purpose limitation, data and system accuracy, and administrative challenges. These challenges arise due to legal, technical, and practical factors in developing algorithms for law enforcement. Addressing these risks and exploring practical mitigations, such as broadening the scope of data protection impact assessments, may enhance transparency and ensure that FRT is used for public security in a manner that serves the public interest.","",""
"2024","Facial recognition","","",""
"2024","Public perceptions about the police’s use of facial recognition technologies","","",""
"2024","Correction to: The ethical application of biometric facial recognition technology","","",""
"2024","Now you see me, now you don’t: why the UK must ban police facial recognition","","",""
"2024","Deconstructing public participation in the governance of facial recognition technologies in Canada","","",""
"2024","Correction: Now you see me, now you don’t: why the UK must ban police facial recognition","","",""
"2024","When facial recognition does not ‘recognise’: erroneous identifications and resulting liabilities","AbstractFacial recognition is an artificial intelligence-based technology that, like many other forms of artificial intelligence, suffers from an accuracy deficit. This paper focuses on one particular use of facial recognition, namely identification, both as authentication and as recognition. Despite technological advances, facial recognition technology can still produce erroneous identifications. This paper addresses algorithmic identification failures from an upstream perspective by identifying the main causes of misidentifications (in particular, the probabilistic character of this technology, its ‘black box’ nature and its algorithmic bias) and from a downstream perspective, highlighting the possible legal consequences of such failures in various scenarios (namely liability lawsuits). In addition to presenting the causes and effects of such errors, the paper also presents measures that can be deployed to reduce errors and avoid liabilities.","",""
"2024","Interoperable and standardized algorithmic images: The domestic war on drugs and mugshots within facial recognition technologies"," Beginning in the 1990s, the National Institute of Standards and Technology (NIST) leveraged the 1980s’ American War on Drugs to improve and expand facial recognition technology (FRT) infrastructure, including the domestic building of FRTs reliant on mugshots. When examining mugshot databases gathered by the NIST, such as the Multiple Encounters Dataset (MEDS) I and II (2010) and Special Database 18 Mugshot Identification Database (SD-18) (2016), it is clear that the same gendered and racialized dynamics present in policing practices related to the War on Drugs is reflected in the mugshot databases that continue to use for FRT research and evaluation into the contemporary moment. This paper details the SD-18 and MEDS databases, as well as the MORPH database, showcasing how their representational, technical and political protocols operate. The desires for frictionless interoperability built into the images’ technical protocols supersede concerns for eugenic political and representational protocols, resulting in a current moment where the deployment of mugshot datasets cannot be contained to their original intended use with FRTs, but leak into other forms of algorithmic governance as well as into algorithmic image-making and visual culture, including generative artificial intelligence systems such as DALL-E. ","",""
"2024","Sounding out voice biometrics: Comparing and contrasting how the state and the private sector determine identity through voice"," The voice biometrics industry is promised today as a new center of digital innovation. Tech companies and state agencies are massively investing in speech recognition and analysis systems, pushed by the belief that the acoustics of voice contain unique individual characteristics to convert into information and value through artificial intelligence. This article responds to this current development by exploring the under-researched datafication of the auditory realm to reveal how the sound of voice is emerging as a site for identity construction by both states and corporations. To do so, we look at two different case studies. First, we examine a patent granted to the streaming service Spotify, which aims to improve the platform's music recommendation system by analyzing users’ speech. Second, we discuss the use of voice biometrics in German asylum procedures, where the country of origin of undocumented asylum seekers is determined through accent analysis. Through these seemingly distinct case studies, we identify not only the common assumptions behind the rationale for adopting voice biometrics, but also important differences in the way the private sector and the State determine identity through the analysis of the sounding voice. These two entities are rarely examined together and are often conflated when addressing practices of auditory surveillance. Thus, our comparative and contrastive approach contributes to existing scholarship that questions the claimed efficiency and ethics of voice biometrics’ extractive practices, further defining the operations and assumptions of the private sector and the State. ","",""
"2024","The Canadian Clearview AI Investigation as a Call for Digital Policy Literacy","In 2020, the Office of the Privacy Commissioner of Canada (OPCC) led a joint federal-provincial investigation into privacy violations stemming from the use of facial recognition technologies. The investigation was prompted specifically by the mobilization of Clearview AI’s facial recognition software in law enforcement, including by regional police services as well as the Royal Canadian Mounted Police. Clearview AI’s technology is based on scraping social media images, which, as the investigation found, constitutes a privacy law violation according to provincial and federal private sector legislation. In response to the investigation, Clearview AI claimed that consent for scraping social media images was not required from users because the information is already public. This common fallacy of social media privacy serves as a pivot point for the integration of digital policy literacy into the OPCC’s digital literacy materials in order to consider the regulatory environment around digital media, alongside their political-economic and infrastructural components. Digital policy literacy is a model that expands what is typically an individual- or organization-level responsibility for privacy protection by considering the wider socio-technical context in which a company like Clearview can emerge.","",""
"2024","Quantifiable Bodies: The Influence of Biometric Technologies in Patient Consent","While research has been done to identify the potential implications of biometric technology on marginalized populations’ privacy and autonomy, this paper contributes to existing research by examining these technologies in healthcare settings. Drawing from insights across surveillance studies, rhetoric of health and medicine, and technical communication, we identify how one leading healthcare institution in New York City has employed rhetorics of efficiency, effectiveness, safety, and security regarding its biometric technology system. This employment of biometric technologies often contributes to patients’ marginalization and dismissal. As we explore, interrogating the language used by the healthcare institution to describe biometrics opens opportunities for us—surveillance studies scholars, patients, allies, students, and more—to ensure that innovations within the healthcare system promote equity, agency, and improved outcomes for all.","",""
"2024","Disaster, facial recognition technology, and the problem of the corpse"," The overlapping disasters of the Australian 2019–2020 bushfire season and the COVID-19 pandemic, figured alongside the imaginary of projected future disasters, have provided a space of legitimation to experiment with controversial facial recognition technologies (FRTs). Drawing upon interviews conducted with senior Australian government administrators and researchers, I argue that FRTs are being used to respond to the trauma of disaster through its novel mediation and refiguration, tied to discourses of resilience which have been used to justify the expansion of FRT as a means for relief and the provision of aid. This legitimation, however, is challenged by the difficulty FRT encounters in capturing the face in its vital and its mortal malleability. What I term ‘the problem of the corpse’ serves to bring to light the ‘paranoid’ gaze of the biometric apparatus, disrupting the aim of using biometric infrastructure to produce a ‘new normal’ in the ongoing aftermath of disaster. ","",""

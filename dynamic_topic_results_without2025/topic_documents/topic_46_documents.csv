"year","title","abstract","journal","doi"
"2014","Understanding the Yelp review filter: An exploratory study","Reviews on Yelp.com can be an important factor in driving customers to a business. However, many business owners have expressed concern with Yelp’s review filtering system, which was created to flag low–quality or fake reviews. This study performs a content analysis of a subset of Yelp restaurant and religious organization reviews, visible and filtered, exploring signals from the reviews or the reviewers that might explain the filtering process. The study finds that factors intrinsic to the review itself are not related to filtering, but factors related to the reviewer are strong predictors. The Yelp system is much more likely to filter reviews from occasional, isolated reviewers than from prolific, socially connected reviewers.  ","",""
"2016","It’s About Ethics in Games Journalism? Gamergaters and Geek Masculinity"," #Gamergate is an online movement ostensibly dedicated to reforming ethics in video games journalism. In practice, it is characterized by viciously sexual and sexist attacks on women in and around gaming communities. #Gamergate is also a site for articulating “Gamergater” as a form of geek masculinity. #Gamergate discussions across social media platforms illustrate how Gamergaters produce and reproduce this gendered identity. Gamergaters perceive themselves as crusaders, under siege from critics they pejoratively refer to as SJWs (social justice warriors). By leveraging social media for concern-trolling about gaming as an innocuous masculine pastime, Gamergaters situate the heterosexual White male as both the typical gamer and the real victim of #Gamergate. #Gamergate is a specific and virulent online node in broader discussions of privilege, difference, and identity politics. Gamergaters are an instructive example of how social media operate as vectors for public discourses about gender, sexual identity, and equality, as well as safe spaces for aggressive and violent misogyny. ","",""
"2017","#Gamergate and The Fappening: How Reddit’s algorithm, governance, and culture support toxic technocultures"," This article considers how the social-news and community site Reddit.com has become a hub for anti-feminist activism. Examining two recent cases of what are defined as “toxic technocultures” (#Gamergate and The Fappening), this work describes how Reddit’s design, algorithm, and platform politics implicitly support these kinds of cultures. In particular, this piece focuses on the ways in which Reddit’s karma point system, aggregation of material across subreddits, ease of subreddit and user account creation, governance structure, and policies around offensive content serve to provide fertile ground for anti-feminist and misogynistic activism. The ways in which these events and communities reflect certain problematic aspects of geek masculinity are also considered. This research is informed by the results of a long-term participant-observation and ethnographic study into Reddit’s culture and community and is grounded in actor-network theory. ","",""
"2018","Digital detritus: 'Error' and the logic of opacity in social media content moderation","The late 2016 case of the Facebook content moderation controversy over the infamous Vietnam-era photo, “The Terror of War,” is examined in this paper for both its specifics, as well as a mechanism to engage in a larger discussion of the politics and economics of the content moderation of user-generated content. In the context of mainstream commercial social media platforms, obfuscation and secrecy work together to form an operating logic of opacity, a term and concept introduced in this paper. The lack of clarity around platform policies, procedures and the values that inform them lead users to wildly different interpretations of the user experience on the same site, resulting in confusion in no small part by the platforms’ own design. Platforms operationalize their content moderation practices under a complex web of nebulous rules and procedural opacity, while governments and other actors clamor for tighter controls on some material, and other members of civil society demand greater freedoms for online expression. Few parties acknowledge the fact that mainstream social media platforms are already highly regulated, albeit rarely in such a way that can be satisfactory to all. The final turn in the paper connects the functions of the commercial content moderation process on social media platforms like Facebook to their output, being either the content that appears on a site, or content that is rescinded: digital detritus. While meaning and intent of user-generated content may often be imagined to be the most important factors by which content is evaluated for a site, this paper argues that its value to the platform as a potentially revenue-generating commodity is actually the key criterion and the one to which all moderation decisions are ultimately reduced. The result is commercialized online spaces that have far less to offer in terms of political and democratic challenge to the status quo and which, in fact, may serve to reify and consolidate power rather than confront it.","",""
"2018","The view from the other side: The border between controversial speech and harassment on Kotaku in Action","In this paper, we use mixed methods to study a controversial Internet site: The Kotaku in Action (KiA) subreddit. Members of KiA are part of GamerGate, a distributed social movement. We present an emic account of what takes place on KiA: who are they, what are their goals and beliefs, and what rules do they follow. Members of GamerGate in general and KiA in particular have often been accused of harassment. However, KiA site policies explicitly prohibit such behavior, and members insist that they have been falsely accused. Underlying the controversy over whether KiA supports harassment is a complex disagreement about what “harassment” is, and where to draw the line between freedom of expression and censorship. We propose a model that characterizes perceptions of controversial speech, dividing it into four categories: criticism, insult, public shaming, and harassment. We also discuss design solutions that address the challenges of moderating harassment without impinging on free speech, and communicating across different ideologies.","",""
"2018","Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms"," Social media platforms play an increasingly important civic role as platforms for discourse, where we discuss, debate, and share information. This article explores how users make sense of the content moderation systems social media platforms use to curate this discourse. Through a survey of users ( n = 519) who have experienced content moderation, I explore users’ folk theories of how content moderation systems work, how they shape the affective relationship between users and platforms, and the steps users take to assert their agency by seeking redress. I find significant impacts of content moderation that go far beyond the questions of freedom of expression that have thus far dominated the debate. Raising questions about what content moderation systems are designed to accomplish, I conclude by conceptualizing an educational, rather than punitive, model for content moderation systems. ","",""
"2018","Beyond the hashtag: Circumventing content moderation on social media"," Social media companies make important decisions about what counts as “problematic” content and how they will remove it. Some choose to moderate hashtags, blocking the results for certain tag searches and issuing public service announcements (PSAs) when users search for troubling terms. The hashtag has thus become an indicator of where problematic content can be found, but this has produced limited understandings of how such content actually circulates. Using pro-eating disorder (pro-ED) communities as a case study, this article explores the practices of circumventing hashtag moderation in online pro-ED communities. It shows how (1) untagged pro-ED content can be found without using the hashtag as a search mechanism; (2) users are evading hashtag and other forms of platform policing, devising signals to identify themselves as “pro-ED”; and (3) platforms’ recommendation systems recirculate pro-ED content, revealing the limitations of hashtag logics in social media content moderation. ","",""
"2019","“Men Are Scum”: Self-Regulation, Hate Speech, and Gender-Based Censorship on Facebook","Because social media sites are self-regulating, each site has developed its own community standards, which serve as regulatory tools. However, the processes of content moderation are often unclear, subjective, and discriminatory. Drawing from a series of interviews with individuals in the “Men Are Scum” movement, this article describes the experiences of women who have been censored on Facebook and explores whether self-regulatory processes on this platform are distinctly gendered. It asserts that both explicit censorship (e.g., limited displays of the body) and implicit censorship (e.g., rampant and unchecked hate speech silencing women’s voices) are operative on Facebook, limiting women’s expressive potentiality. Thus, this article proposes the term “gender-based censorship” as a lens through which to understand women’s experiences on Facebook. These findings help reveal the pitfalls of industry self-regulation in which profit motives are prioritized over protection of users (especially those who may be marginalized offline).","",""
"2019","Picketing the Virtual Storefront: Content Moderation and Political Criticism of Businesses on Yelp","This article examines incidents in which business owners incur criticism on the consumer review platform Yelp based on political ideology. I analyze two case studies from the summer of 2018 by considering the sentiments expressed in the review texts, the application of Yelp’s relevant policies, and the tactical adaptations of reviewers. The case studies evince a normative conflict over how the platform should treat viral criticism of this sort. While Yelp clearly cannot truly function as a laissez-faire public forum, its moderation criteria can be gamed, and its efforts evidently exclude a range of sentiments that some users find meaningful. The article provides an in-depth exploration of a platform that has received somewhat less attention in the growing literature on the role of private intermediaries in shaping what kinds of speech attain visibility in the digital public sphere.","",""
"2019","REDDIT QUARANTINED: CONSEQUENCES OF DEALING WITH DISTRUST IN SOCIAL MEDIA PLATFORMS THROUGH RESTRICTING ENGAGEMENT","Online abuse has become a matter of trust for social media platforms, whose role as a facilitator of public debate has been called into question. In response social media companies have become more active in regulating and banning particular users and channels.&#x0D; Through the use of affordances theory, this paper examines one example of the regulation of content on a social media site, the revamp of the quarantining function on Reddit in late 2018. Quarantines are designed to halt participation within and growth of subreddits without banning them outright.&#x0D; The paper uses quantitative and qualitative data to examine the consequences of this revamp on two subreddits, r/Braincels and r/TheRedPill. Through studying activity levels on these subreddits the paper argues that quarantines did limit discussion within these subreddits. However, it also argues that the revamp had unintended consequences, in particular a growth in distrust between subreddit users and Reddit as a site, and a shift of users away from Reddit to less regulated sites.&#x0D; The paper argues that quarantining shifted the affordances of Reddit, in this instance resulting in greater discouragement of activity on particular subreddits. Using the mechanisms and conditions framework (Davis and Chouinard, 2016) the paper however argues that users adapted to and circumvented this discouragement to continue engaging in particular behavior.&#x0D; While quarantining had short term benefits, using an affordances framework this paper argues it had unintended consequences, ones which can result in a continued radicalization of actions and beliefs, furthering distrust in the online sphere.","",""
"2019","Online content moderation and the Dark Web: Policy responses to radicalizing hate speech and malicious content on the Darknet","De-listing, de-platforming, and account bans are just some of the increasingly common steps taken by major Internet companies to moderate their online content environments. Yet these steps are not without their unintended effects. This paper proposes a surface-to-Dark Web content cycle. In this process, malicious content is initially posted on the surface Web. It is then moderated by platforms. Moderated content does not necessarily disappear when major Internet platforms crackdown, but simply shifts to the Dark Web. From the Dark Web, malicious informational content can then percolate back to the surface Web through a series of three pathways. The implication of this cycle is that managing the online information environment requires careful attention to the whole system, not just content hosted on surface Web platforms per se. Both government and private sector actors can more effectively manage the surface-to-Dark Web content cycle through a series of discrete practices and policies implemented at each stage of the wider process.","",""
"2019","Moderator engagement and community development in the age of algorithms"," Online communities provide a forum for rich social interaction and identity development for billions of Internet users worldwide. In order to manage these communities, platform owners have increasingly turned to commercial content moderation, which includes both the use of moderation algorithms and the employment of professional moderators, rather than user-driven moderation, to detect and respond to anti-normative behaviors such as harassment and spread of offensive content. We present findings from semi-structured interviews with 56 volunteer moderators of online communities across three platforms (Twitch, Reddit, and Facebook), from which we derived a generalized model categorizing the ways moderators engage with their communities and explaining how these communities develop as a result. This model contains three processes: being and becoming a moderator; moderation tasks, actions, and responses; and rules and community development. In this work, we describe how moderators contribute to the development of meaningful communities, both with and without algorithmic support. ","",""
"2019","Platform dialectics: The relationships between volunteer moderators and end users on reddit"," Existing literature on the affordances of Internet platforms rarely examines the complex and recursive relationships between the actions of volunteer moderators and the behaviour of end users. Building on existing studies of Internet communities, affordances and ‘public goods’, this article uses an ethnographic approach to analyse two subreddits, r/paleo and r/nootropics, on the social news site reddit. It elaborates on work by Massanari to illustrate how the moderators of forums with an epistemic focus utilise the affordances at their disposal with the aim of mediating trust and establishing a paradigm for constructive discourse. End users respond to these attempts in unpredictable and unforeseen ways, indicating the interpretive flexibility of affordances. The concept of ‘platform dialectics’ is invoked as an overarching description of this phenomenon. ","",""
"2019","Free Speech and Safe Spaces: How Moderation Policies Shape Online Discussion Spaces"," How do moderation policies affect online discussion? This article analyzes nearly a quarter of a million anonymous comments over a 14-month period from two online Reddit forums matched in topic and size, but with differing moderation policies of “safe space” and “free speech.” I found that in the safe space, moderators removed significantly more comments, and authors deleted their own comments significantly more often as well, suggesting higher rates of self-censorship. Looking only at relatively low frequency posters, I found that language in the safe space is more positive and discussions are more about leisure activities, whereas language in the free speech space is relatively negative and angry, and material personal concerns of work, money, and death are more frequently discussed. Importantly, I found that many of these linguistic differences persisted even in comments by users who were concurrently posting in both subreddits. Altogether, these results suggest that differences in moderation policies may affect self-censorship and language use in online space, implicating moderation policies as key sites of inquiry for scholars of democratic discussion. ","",""
"2019","The Civic Labor of Volunteer Moderators Online"," Volunteer moderators create, support, and control public discourse for millions of people online, even as moderators’ uncompensated labor upholds platform funding models. What is the meaning of this work and who is it for? In this article, I examine the meanings of volunteer moderation on the social news platform reddit. Scholarship on volunteer moderation has viewed this work separately as digital labor for platforms, civic participation in communities, or oligarchy among other moderators. In mixed-methods research sampled from over 52,000 subreddit communities and in over a dozen interviews, I show how moderators adopt all of these frames as they develop and re-develop everyday meanings of moderation—facing the platform, their communities, and other moderators alike. I also show how this civic notion of digital labor brings clarity to a strike by moderators in July 2015. Volunteer governance remains a common approach to managing social relations, conflict, and civil liberties online. Our ability to see how communities negotiate the meaning of moderation will shape our capacity to address digital governance as a society. ","",""
"2020","Reddit quarantined: can changing platform affordances reduce hateful material online?",": This paper studies the efficacy of the Reddit’s quarantine, increasingly implemented in the platform as a means of restricting and reducing misogynistic and other hateful material. Using the case studies of r/TheRedPill and r/Braincels, the paper argues the quarantine successfully cordoned off affected subreddits and associated hateful material from the rest of the platform. It did not, however, reduce the levels of hateful material within the affected spaces. Instead many users reacted by leaving Reddit for less regulated spaces, with Reddit making this hateful material someone else’s problem. The paper argues therefore that the success of the quarantine as a policy response is mixed.","",""
"2020","Expanding the debate about content moderation: Scholarly research agendas for the coming policy debates","Content moderation has exploded as a policy, advocacy, and public concern. But these debates still tend to be driven by high-profile incidents and to focus on the largest, US based platforms. In order to contribute to informed policymaking, scholarship in this area needs to recognise that moderation is an expansive socio-technical phenomenon, which functions in many contexts and takes many forms. Expanding the discussion also changes how we assess the array of proposed policy solutions meant to improve content moderation. Here, nine content moderation scholars working in critical internet studies propose how to expand research on content moderation, with implications for policy.","",""
"2020","TUNING OUT HATE SPEECH ON REDDIT: AUTOMATING MODERATION AND DETECTING   TOXICITY IN THE MANOSPHERE","Over the past two years social media platforms have been struggling to moderate at scale. At the same time, they have come under fire for failing to mitigate the risks of perceived ‘toxic’ content or behaviour on their platforms. In effort to better cope with content moderation, to combat hate speech, ‘dangerous organisations’ and other bad actors present on platforms, discussion has turned to the role that automated machine-learning (ML) tools might play. This paper contributes to thinking about the role and suitability of ML for content moderation on community platforms such as Reddit and Facebook. In particular, it looks at how ML tools operate (or fail to operate) effectively at the intersection between online sentiment within communities and social and platform expectations of acceptable discourse. Through an examination of the r/MGTOW subreddit we problematise current understandings of the notion of ‘tox¬icity’ as applied to cultural or social sub-communities online and explain how this interacts with Google’s Perspective tool.","",""
"2020","HATRED OF/AND DEMOCRACY: THE POLITICAL CONTRADICTIONS OF REDDIT’S MODERATION STRUCTURE","This paper seeks to interpret Reddit moderation as a problem of political theory, rather than as a debate between the merits of human moderation and algorithmic moderation. Analyzing Reddit’s moderation structure shows that both the human moderation and the algorithmic moderation reinforce a form of anti-politics which leaves users feeling like they have no input and thus no interest in the well-being of the subreddits in which they participate. Online governance structures are largely top down and authoritarian in nature, despite often being couched in democratic rhetoric, reflecting what Jacques Rancière describes as a hatred of democracy. By looking at the example of how r/Canada came to be widely disparaged on Reddit as a bastion of hate, I make the argument that the key to rooting out online hate is not through more human moderation or by giving algorithms more control, but by creating a democratic culture of buy-in through which users are empowered with responsibility for the quality of content in a discussion space.","",""
"2020","REDDIT'S COMMUNITIES &amp; CONSEQUENCES","As a platform, Reddit provides a bit of a conundrum. Despite being visited by more people than Netflix and remaining one of the most visited spaces on the web, it remains extraordinarily resistant to generalization. Some of the worst of Internet culture can be found on the site. It has served to amplify the voices of misogynists, supported vigilantism, and hosted child pornography. At the same time, some of the more civil conversations and learning communities appear on the site, with subreddits like Change My View fostering respectful deliberation. Even more than many other platforms, the lack of centralized moderation means that Reddit contains a very wide range of practices, some of them quite extreme. But because these exist on a single platform, users bring these practices with them, both to the “front page” of the platform, and to other areas within. The three papers that make up this panel seek to better understand localized behaviors and how they may relate to global flows of participants and practices. Of course, many of the discursive patterns that were fostered in subreddits make their way into other online and offline contexts. But before they do that, they have often been produced as part of a culture local to one subreddit, or to a “neighborhood” of subreddits. How these practices emerge, evolve, and relate to the actions of their users runs as a thread through the three presentations.","",""
"2020","DISPLACED DISCUSSION: THE IMPLICATIONS OF REDDIT QUARANTINE AND THE   MOVEMENT OF THEREDPILL TO SELF-HOSTING","Reddit.com is a social media site with huge volumes and varieties of content, both hosted on their platform, and imported from other providers. It is also a place of counterpublic community action. Sub-Reddits like r/the_donald, and r/theredpill expound, in turn, alt-right and anti-feminist views. Along with various posts, links, and outside content, the communities support millions of words of user discussion. These communities are policed, at least in part, by their exposure to the community at large, and push-back from other people and groups to their content. When a community sees that its action on a site like Reddit is limited, and perceives that its ideas and ideologies are being silenced, what happens? This paper uses data from a digital ethnography of r/TheRedPill, a sub-community of Reddit dedicated to “discussion of sexual strategy in a culture increasingly lacking a positive identity for men.” The community was quarantined by Reddit in September of 2018, and since that time has engaged in regular discussion about the movement of community discussion and forums away from Reddit to their own hosting site www.trp.red. Beyond simply allowing versus limiting speech, the case of r/TheRedPill provides an opportunity to engage in a discussion about whether progressive sanction is the right way to manage the intersection of counterpublic views with the needs of tech firms. This paper hopes to further that discussion using a community that is less generally associated with hate speech, and can therefore exist on the margins of acceptability.","",""
"2020","Contesting algorithms: Restoring the public interest in content filtering by artificial intelligence"," In recent years, artificial intelligence has been deployed by online platforms to prevent the upload of allegedly illegal content or to remove unwarranted expressions. These systems are trained to spot objectionable content and to remove it, block it, or filter it out before it is even uploaded. Artificial intelligence filters offer a robust approach to content moderation which is shaping the public sphere. This dramatic shift in norm setting and law enforcement is potentially game-changing for democracy. Artificial intelligence filters carry censorial power, which could bypass traditional checks and balances secured by law. Their opaque and dynamic nature creates barriers to oversight, and conceals critical value choices and tradeoffs. Currently, we lack adequate tools to hold them accountable. This paper seeks to address this gap by introducing an adversarial procedure— – Contesting Algorithms. It proposes to deliberately introduce friction into the dominant removal systems governed by artificial intelligence. Algorithmic content moderation often seeks to optimize a single goal, such as removing copyright-infringing materials or blocking hate speech, while other values in the public interest, such as fair use or free speech, are often neglected. Contesting algorithms introduce an adversarial design which reflects conflicting values, and thereby may offer a check on dominant removal systems. Facilitating an adversarial intervention may promote democratic principles by keeping society in the loop. An adversarial public artificial intelligence system could enhance dynamic transparency, facilitate an alternative public articulation of social values using machine learning systems, and restore societal power to deliberate and determine social tradeoffs. ","",""
"2020","No amount of “AI” in content moderation will solve filtering’s prior-restraint problem"," Contemporary policy debates about managing the enormous volume of online content have taken a renewed focus on upload filtering, automated detection of potentially illegal content, and other “proactive measures”. Often, policymakers and tech industry players invoke artificial intelligence as the solution to complex challenges around online content, promising that AI is a scant few years away from resolving everything from hate speech to harassment to the spread of terrorist propaganda. Missing from these promises, however, is an acknowledgement that proactive identification and automated removal of user-generated content raises problems beyond issues of “accuracy” and overbreadth--problems that will not be solved with more sophisticated AI. In this commentary, I discuss how the technical realities of content filtering stack up against the protections for freedom of expression in international human rights law. As policymakers and companies around the world turn to AI for communications governance, it is crucial that we recall why legal protections for speech have included presumptions against prior censorship, and consider carefully how proactive content moderation will fundamentally re-shape the relationship between rules, people, and their speech. ","",""
"2020","Algorithmic content moderation: Technical and political challenges in the automation of platform governance","As government pressure on major technology companies builds, both firms and legislators are searching for technical solutions to difficult platform governance puzzles such as hate speech and misinformation. Automated hash-matching and predictive machine learning tools – what we define here as algorithmic moderation systems – are increasingly being deployed to conduct content moderation at scale by major platforms for user-generated content such as Facebook, YouTube and Twitter. This article provides an accessible technical primer on how algorithmic moderation works; examines some of the existing automated tools used by major platforms to handle copyright infringement, terrorism and toxic speech; and identifies key political and ethical issues for these systems as the reliance on them grows. Recent events suggest that algorithmic moderation has become necessary to manage growing public expectations for increased platform responsibility, safety and security on the global stage; however, as we demonstrate, these systems remain opaque, unaccountable and poorly understood. Despite the potential promise of algorithms or ‘AI’, we show that even ‘well optimized’ moderation systems could exacerbate, rather than relieve, many existing problems with content policy as enacted by platforms for three main reasons: automated moderation threatens to (a) further increase opacity, making a famously non-transparent set of practices even more difficult to understand or audit, (b) further complicate outstanding issues of fairness and justice in large-scale sociotechnical systems and (c) re-obscure the fundamentally political nature of speech decisions being executed at scale.","",""
"2020","Content moderation, AI, and the question of scale"," AI seems like the perfect response to the growing challenges of content moderation on social media platforms: the immense scale of the data, the relentlessness of the violations, and the need for human judgments without wanting humans to have to make them. The push toward automated content moderation is often justified as a necessary response to the scale: the enormity of social media platforms like Facebook and YouTube stands as the reason why AI approaches are desirable, even inevitable. But even if we could effectively automate content moderation, it is not clear that we should. ","",""
"2020","Content moderation: Social media’s sexist assemblages"," This article proposes ‘sexist assemblages’ as a way of understanding how the human and mechanical elements that make up social media content moderation assemble to perpetuate normative gender roles, particularly white femininities, and to police content related to women and their bodies. It investigates sexist assemblages through three of many potential elements: (1) the normatively gendered content presented to users through in-platform keyword and hashtag searches; (2) social media platforms’ community guidelines, which lay out platforms’ codes of conduct and reveal biases and subjectivities and (3) the over-simplification of gender identities that is necessary to algorithmically recommend content to users as they move through platforms. By the time the reader finds this article, the elements of the assemblages we identify might have shifted, but we hope the framework remains useful for those aiming to understand the relationship between content moderation and long-standing forms of inequality. ","",""
"2020","Re-humanizing the platform: Content moderators and the logic of care"," With the goal of re-humanizing discussion platform operations, this study explores the knowledge and aims of commercial content moderators by reframing their work-related ideals through the notion of the “logic of care.” In seeking to expand their professional realm by realigning users, moderators, and technical tools, moderators of discussion forums have turned to machines, ideally freeing up resources for real-time interaction between moderators and those who post. By focusing on care, the study calls for technical innovation that integrates moderators’ aims with artificial intelligence systems. Rather than acknowledging human skills and resources in terms of moderation tools and discussion culture, the current platform logic forces moderators to operate like machines. Their discontent becomes understandable within a logic that diminishes their skills and vision. The moderator is left with assessing separate posts, rather than offering a meta-perspective to the discussion, overseeing and nurturing it. ","",""
"2020","Media in the Post #GamerGate Era: Coverage of Reactionary Fan Anger and the Terrorism of the Privileged"," History has recorded the aggressive fandom campaigns of the 2010s, which drove vulnerable minority actors from their careers and homes. #GamerGate was the most iconic of these movements with its claim to focus on the ethics in games journalism. This article examines select news coverage of the #GamerGate movement from its initial outset in 2014 to today to identify how the movement changed news reporting’s approach to fandom harassment stories. #GamerGate’s development and deployment media of frames were examined to see how the repertoires of contention shaped during this period played into social and political events that followed. ","",""
"2021","LOCALIZING CONTENT MODERATION: APPROACHING THE ORIENTATIONAL SPACES OF         FACEBOOK GROUP ADMINS AND MODS","This paper contributes to the burgeoning literature on content         moderation by focusing on its practice in relation to localized social media contexts, an         area which remains under-researched. It makes two key contributions. Firstly, it presents         the results of a study on moderation practices in relation to place-named Facebook groups         across Greater London. Drawing on in-depth interviews with administrators and moderators         from 16 Facebook groups, we focus on exploring how such administrators and moderators         negotiate an apparent ‘orientational’ tension between ‘translocality’ and ‘locality’. On the         one hand, we explore how administrators and moderators oriented partly to what might be         understood as the 'translocal' space of Facebook as a platform. On the other hand, we also         sought to understand how such administrators and moderators orient to the localised         situation surrounding the place-named Facebook group. Our second key contribution aligns         with the conference theme on co-dependence and social media, outlining a conceptual approach         for researching the geographical contexts or ‘place’ of content moderation more broadly. We         emphasize the inherent, practical locality of content moderation. Drawing on a long         tradition of relational approaches in human geography, cultural anthropology and philosophy,         we conceptualize ‘locality’ as something produced through practical action, rather being         pre-given, specific geographical locations. Approaching the place or context of content         moderation relationally, rather than via geographical scales such as local or global, might         not only provide a more context sensitive approach, but also, underline the limits of         large-scale moderation, whether by platforms or governments, or through human or algorithmic         interventions.","",""
"2021","AUTHENTIFICATION AT THE EDGE: #”GAMERGATE”-ING THE ASCENT OF THE         VERIFIED INTERNET","While platforms may be hard to know, they are increasingly invested         in knowing—and policing— users. From “verified accounts” to “two-factor authentication,”         platform affordances have proffered metrics of “authenticity” as an antidote to the         uncertainties of attention economies, which are ostensibly saturated by fake news,         misinformation, and algorithmic radicalization (Haimson &amp; Hoffman, 2016; Caplan, 2020).         Yet the platformization of realness claims are often weaponized against the most         marginalized, as evidenced by recent events like PornHub’s demonetisation of unverified         accounts. Early hashtag harassment events like #GamerGate foreshadowed the gendered         consequences of digital realness regimes. In the ascent of “verified account” castes and         other digital authenticators, the traditional “black box” conceptualization of platforms         rings increasingly untrue. Rather, we argue, the algorithmic reality for marginalized users         better resembles Wile E. Coyote’s painted tunnels on the side of mountains: vortexes of         selective porosity that invite some roadrunners and flatten others. Through a Critical         Discourse Analysis of #GamerGate coverage from 2014 and 2015, we attend to how ideologies of         “realness” reproduce along gendered and racialized lines. Our paper builds on recent work on         how the ideation of “realness” embeds forms of communicative and audience-managing labour         among networked creators (Abidin 2016; Banet-Weiser, 2012; Duffy, 2017). The ascent of         “authentification-as-safety is historicized within the hashtag harassment event “GamerGate,”         our case study and pivotal moment in the platform veracity ecosystem when influencers and         journalists were exhorted to authenticate their lives or lose their livelihoods. Everyone on         the internet knows you’re a dog. Now what?","",""
"2021","NOT YOUR ROBIN HOOD: GAMESTOP AND PLATFORM ECONOMICS AT PLAY","In January 2021, GameStop and AMC, stocks with no discernable reason         for an increase in value, rose abruptly in price. This was soon understood as the result of         manipulation by members of the r/WallStreetBets subreddit. While individualized stock         trading is often framed as equalizing the playing field, everyday people remain second-class         participants in the stock market, which quickly became clear as trading was suspended on         stocks the Redditors targeted. Backlash against this decision played out across platforms.         We argue that this incident is the inevitable result of treating platforms (and economics)         through the lens of gaming and trolling. The first key issue in the r/WallStreetBets         incident was content moderation, as the Robinhood trading app stopped the trading, the         subreddit the traders used to organize went private, and the corresponding Discord server         was banned. The second important factor in this incident was coordinated inauthentic action,         as thwarted traders immediately turned to the app store and targeted Robinhood with a review         bombing campaign. Third, while the popular response to the r/WallStreetBets saga was         frequently celebratory, seeing their actions as a challenge to capitalism and evidence of         progressive politics, longstanding tropes associating Jewish people with capital, meant that         Redditors’ anti-capitalism slid easily into anti-Semitism. Ultimately, many of the patterns         of the Reddit stock incident locate it in a long history of coordinated internet action         steeped in toxic technocultures. However, the expansion of these practices into taking         direct action on economic systems worth billions of dollars is new and calls for rigorous         attention","",""
"2021","INCELS ON REDDIT: A STUDY IN SOCIAL NORMS AND DECENTRALISED        MODERATION","The social news website Reddit has a long history of hosting communities (‘subreddits’) that advocate or encourage white supremacy (Gillespie 2018), disparagement of minority groups (Topinka 2017), and violence against women (Massanari 2017). As a platform that relies heavily on volunteer moderators to self-govern the subreddits (Matias 2016), Reddit has been criticised for failing to adequately enforce its site-wide rules (Gillespie 2018). Incels—an internet subculture that ascribes to deeply misogynistic beliefs—grew in visibility when they developed subreddits on Reddit. After ongoing criticism and media attention about harmful behaviour of incels both on and off the platform, Reddit imposed escalating sanctions and ultimately banned the most visible of these subreddits over a period of several years. In this paper, we focus on the interaction between formal rules and social norms in incel and related subreddits. This paper aims to improve understanding about how problematic norms are contested in (partially-) decentralised systems of content moderation. We examine discourse about moderation to better understand the role of moderation teams in maintaining and changing social norms in their communities and to examine the interaction between these norms and both sitewide and subreddit-specific rules. Our analysis suggests that the threat of prohibition alone is unlikely to be sufficient to drive cultural change in problematic subreddits. We argue that content moderation is an insufficient frame to understand the regulation of harmful communities; real change requires addressing the underlying cultural norms rather than focusing on individual pieces of content.","",""
"2021","The fabrics of machine moderation: Studying the technical, normative,  and organizational structure  of Perspective API"," Over recent years, the stakes and complexity of online content moderation have been steadily raised, swelling from concerns about personal conflict in smaller communities to worries about effects on public life and democracy. Because of the massive growth in online expressions, automated tools based on machine learning are increasingly used to moderate speech. While ‘design-based governance’ through complex algorithmic techniques has come under intense scrutiny, critical research covering algorithmic content moderation is still rare. To add to our understanding of concrete instances of machine moderation, this article examines Perspective API, a system for the automated detection of ‘toxicity’ developed and run by the Google unit Jigsaw that can be used by websites to help moderate their forums and comment sections. The article proceeds in four steps. First, we present our methodological strategy and the empirical materials we were able to draw on, including interviews, documentation, and GitHub repositories. We then summarize our findings along five axes to identify the various threads Perspective API brings together to deliver a working product. The third section discusses two conflicting organizational logics within the project, paying attention to both critique and what can be learned from the specific case at hand. We conclude by arguing that the opposition between ‘human’ and ‘machine’ in speech moderation obscures the many ways these two come together in concrete systems, and suggest that the way forward requires proactive engagement with the design of technologies as well as the institutions they are embedded in. ","",""
"2021","Disappearing acts: Content moderation and emergent practices to preserve at-risk human rights–related content"," Human rights groups, journalists, and “open source investigators” increasingly depend on social media platforms to collect eyewitness media documenting possible human rights violations and conflicts. And yet, this content—often graphic, controversial, even uploaded by perpetrators—is often removed by the platforms, for various reasons. This article draws on in-depth interviews to examine how practitioners reliant on human rights–related content understand, experience, and deal with platform content moderation and removals in their day-to-day work. Interviews highlighted that both the actual and anticipated removal of social media content complicated and added to practitioners’ work. In addition, practitioners unevenly possess the technical, financial, and organizational resources to mitigate the risks and ramifications of removal by preserving content and appealing content moderation decisions. This article sheds light on the impacts of content moderation for stakeholders other than the primary account holders, and highlights platforms’ affordances and shortcomings as archives of war. ","",""
"2021","Morally Motivated Networked Harassment as Normative Reinforcement"," While online harassment is recognized as a significant problem, most scholarship focuses on descriptions of harassment and its effects. We lack explanations of why people engage in online harassment beyond simple bias or dislike. This article puts forth an explanatory model where networked harassment on social media functions as a mechanism to enforce social order. Drawing from examples of networked harassment taken from qualitative interviews with people who have experienced harassment ( n = 28) and Trust &amp; Safety workers at social platforms ( n = 9), the article builds on Brady, Crockett, and Bavel’s model of moral contagion to explore how moral outrage is used to justify networked harassment on social media. In morally motivated networked harassment, a member of a social network or online community accuses a target of violating their network’s norms, triggering moral outrage. Network members send harassing messages to the target, reinforcing their adherence to the norm and signaling network membership. Frequently, harassment results in the accused self-censoring and thus regulates speech on social media. Neither platforms nor legal regulations protect against this form of harassment. This model explains why people participate in networked harassment and suggests possible interventions to decrease its prevalence. ","",""
"2021","The Role of Suspended Accounts in Political Discussion on Social Media: Analysis of the 2017 French, UK and German Elections"," Content moderation on social media is at the center of public and academic debate. In this study, we advance our understanding on which type of election-related content gets suspended by social media platforms. For this, we assess the behavior and content shared by suspended accounts during the most important elections in Europe in 2017 (in France, the United Kingdom, and Germany). We identify significant differences when we compare the behavior and content shared by Twitter suspended accounts with all other active accounts, including a focus on amplifying divisive issues like immigration and religion and systematic activities increasing the visibility of specific political figures (often but not always on the right). Our analysis suggests that suspended accounts were overwhelmingly human operated and no more likely than other accounts to share “fake news.” This study sheds light on the moderation policies of social media platforms, which have increasingly raised contentious debates, and equally importantly on the integrity and dynamics of political discussion on social media during major political events. ","",""
"2021","Reddit Gaming Communities During Times of Transition"," We studied the Reddit communities for three game franchises at the time of a new release in each franchise to see what the members of the Reddit community for the now-older game would do in terms of their community involvement and membership. Specifically, we sought to determine if community members transitioned to the community for a new game, leaving behind their established community devoted to the now-older game. Our main finding was that most people did not move to the new community, but instead remained with their established community. We argue that nostalgia played an important role for community members, whom we frame as fans. Data came from Reddit communities for The Elder Scrolls, Fallout, and the Civilization franchises, comprising data on over 10 million posts from five different year-long time periods, each centered on the release of a new game in each of those franchises. ","",""
"2022","Moderation and authority-building process: the dynamics of knowledge creation on history subreddits","Abstract For the last 30 years, the web has been used as a space of debate and knowledge creation, including historical knowledge. The digital space has the potential to provide a more democratic history that relies on the inclusion of different voices. However, it also raises questions about editing and authority. When attempting to understand authority relations on the web, moderation gains special prominence as it involves actions of exclusion, organisation, and establishment of norms; moderators heavily influence the content created by web users. Here, we investigate knowledge creation considering moderation bias. We address the effects of different moderation practices in history subreddits by analysing how moderators establish authority relations with other users. For that, we use a mixed-methods approach by interpreting the subreddits’ rules and performing network analysis based on the subreddits’ dialogues (2011–2020). The study indicates that the rules have become progressively extensive and stricter over the years, creating appropriate ways for posting submissions and commenting but also affecting broad participation. As central authority figures, moderators engage in processes of sharing authority, rather than shared authority, tending to dominate knowledge creation.","",""
"2022","r/WatchRedditDie and the politics of reddit’s bans and quarantines","Abstract The subreddit r/WatchRedditDie was founded in 2015 after reddit started implementing anti-harassment policies, and positions itself as a “fire alarm for reddit” meant to voyeuristically watch reddit’s impending (symbolic) death. As conversations around platform governance, moderation, and the role of platforms in controlling hate speech become more complex, r/WatchRedditDie and its affiliated subreddits are dedicated in maintaining a version of reddit tolerant of any and all speech, excluding other more vulnerable users from fully participating on the platform. r/WatchReditDie users advocate for no interference in their activities on the platform—meaning that although they rely on the reddit infrastructure to sustain their community, they aim to self-govern to uphold a libertarian and often manipulated interpretation of free expression. Responding to reddit’s evolving policies, they find community with one another by positioning the platform itself as their main antagonist. Through the social worlds framework, I examine the r/WatchRedditDie community’s responses to platform change, bringing up new questions about the possibility of shared governance between platform and user, as well as participatory culture’s promises and perils.","",""
"2022","Antecedents of support for social media content moderation and platform regulation: the role of presumed effects on self and others","ABSTRACT This study examines support for regulation of and by platforms and provides insights into public perceptions of platform governance. While much of the public discourse surrounding platforms evolves at a policy level between think tanks, journalists, academics and political actors, little attention is paid to how people think about regulation of and by platforms. Through a representative survey study of US internet users (N = 1,022), we explore antecedents of support for social media content moderation by platforms, as well as for regulation of social media platforms by the government. We connect these findings to presumed effects on self (PME1) and others (PME3), concepts that lie at the core of third-person effect (TPE) and influence of presumed influence (IPI) scholarship. We identify third-person perceptions for social media content: Perceived negative effects are stronger for others than for oneself. A first-person perception operates on the platform level: The beneficial effects of social media platforms are perceived to be stronger for the self than for society. At the behavioral level, we identify age, education, opposition to censorship, and perceived negative effects of social media content on others (PME3) as significant predictors of support for content moderation. Concerning support for regulation of platforms by the government, we find significant effects of opposition to censorship, perceived intentional censorship, frequency of social media use, and trust in platforms. We argue that stakeholders involved in platform governance must take more seriously the attitudes of their constituents.","",""
"2022","Cognitive assemblages: The entangled nature of algorithmic content moderation"," This article examines algorithmic content moderation, using the moderation of violent extremist content as a specific case. In recent years, algorithms have increasingly been mobilized to perform essential moderation functions for online social media platforms such as Facebook, YouTube, and Twitter, including limiting the proliferation of extremist speech. Drawing on Katherine Hayles’ concept of “cognitive assemblages” and the Critical Security Studies literature, we show how algorithmic regulation operates within larger assemblages of humans and non-humans to influence the surveillance and regulation of information flows. We argue that the dynamics of algorithmic regulation are more liquid, cobbled together and distributed than it appears. It is characterized by a set of shifting human and machine entities, which mix traditional surveillance methods with more sophisticated tools, and whose linkages and interactions are transient. The processes that enable the consolidation of knowledge about risky profiles and contents are, therefore, collective and distributed among humans and machines. This allows us to argue that the cognitive assemblages involved in content moderation become a cobbled space of preemptive calculation. ","",""
"2022","Shall AI moderators be made visible? Perception of accountability and trust in moderation systems on social media platforms"," This study examines how visibility of a content moderator and ambiguity of moderated content influence perception of the moderation system in a social media environment. In the course of a two-day pre-registered experiment conducted in a realistic social media simulation, participants encountered moderated comments that were either unequivocally harsh or ambiguously worded, and the source of moderation was either unidentified, or attributed to other users or an automated system (AI). The results show that when comments were moderated by an AI versus other users, users perceived less accountability in the moderation system and had less trust in the moderation decision, especially for ambiguously worded harassments, as opposed to clear harassment cases. However, no differences emerged in the perceived moderation fairness, objectivity, and participants confidence in their understanding of the moderation process. Overall, our study demonstrates that users tend to question the moderation decision and system more when an AI moderator is visible, which highlights the complexity of effectively managing the visibility of automatic content moderation in the social media environment. ","",""
"2022","Operationalising ‘toxicity’ in the manosphere: Automation, platform governance and community health","Social media platforms have been struggling to moderate at scale. In an effort to better cope with content moderation discussion has turned to the role that automated machine-learning (ML) tools might play. The development of automated systems by social media platforms is a notoriously opaque process and public values that pertain to the common good are at stake within these often-obscured processes. One site in which social values are being negotiated is in the framing of what is considered ‘toxic’ by platforms in the development of automated moderation processes. This study takes into consideration differing notions of toxicity – community, platform and societal by examining three measures of toxicity and community health (the ML tool Perspective API; Reddit’s 2020 Content Policy; and the Sense of Community Index-2) and how they are operationalised in the context of r/MGTOW – an antifeminist group known for its misogyny. Several stages of content analysis were conducted on the top posts and comments in r/MGTOW to examine how these different measures of toxicity operate. This paper provides insight into the logics and technicalities of automated moderation tools, platform governance structures, and frameworks for understanding community metrics to interrogate existing uses of ‘toxicity’ as applied to cultural or social subcommunities online. We make a distinction between two used terms: civility and toxicity. Our analysis points to a tension between current social framings and operationalised notions of ‘toxicity’. We argue that there is a clear distinction between civility and toxicity – incivility is a measure of internal perceptions of harm within a community, whereas toxicity is a measure of the capacity for social harms outside of the bounds of the community. This nuanced understanding will enable more targeted interventions to be developed to destabilise the internal conditions that make groups like r/MGTOW internally ‘healthy’ yet externally toxic.","",""
"2022","Reddit’s cops and cop-watchers: Resisting and insisting on change in online interpretive communities"," This study explores how two subreddits—r/Bad_Cop_No_Donut (Donut) and r/ProtectAndServe (PnS)—function as online interpretive communities discussing the same topic: police conduct. Members of Donut construct a genre from videos depicting a history of police violence in order to advocate for policing reform, arguing that cop-watching practices that produce this genre are essential to driving changes in policing. Members of PnS construct a genre from similar videos in order to advocate for resisting systemic reform, reading these videos as professional development opportunities for police to reestablish legitimacy with the public. Donut insists on change, while PnS resists change. Donut produces a discourse which engages with historical instances of police misconduct; PnS produces a discourse which rarely engages with this history. Studying these processes of interpretation reveals how dissonant meanings can arise from the same material, how meaning is made in communities consuming and repurposing texts, and how historical narratives are essential to challenging structural inequity. ","",""
"2022","Incels on Reddit:  A study in social norms and decentralised moderation","This paper examines the challenges of combating misogyny and harmful social norms on the social networking and news site Reddit. Reddit has a long history of hosting racist, misogynistic, homophobic, and otherwise harmful communities. Incels — an Internet subculture that ascribes to deeply misogynistic beliefs — flourished on Reddit before several major subreddits were progressively banned. In this paper, we examine four incel subreddits to understand how the dynamic tensions between Reddit’s site-wide rules and the rules of particular incel subreddits play out in practice. We draw on archived Reddit data to provide a detailed analysis of how incel subreddits adapt to and resist pressure from Reddit and other users to comply with the site-wide rules and develop and maintain their rules and norms. Our study suggests that punitive measures, such as the threat of stricter rules and increased external moderation for explicitly prohibited content and behaviour is insufficient to foster prosocial subreddits. We show how stricter enforcement of the formal site-wide rules did not challenge or displace the underlying ideologies that foster toxic communities. Instead, we find that incels justified their toxic behaviour in the face of heavy external criticism. To cultivate safe and inclusive digital environments, we argue that Reddit must focus on improving the culture of subreddits. We conclude that driving real change when moderation is decentralized requires the committed and supported participation of moderators who can undertake the extensive work of tackling the underlying identity and ideology that brings hateful communities together.","",""
"2022","The GameStop saga: Reddit communities and the emerging conflict between new and old media","This paper focuses on the (continuing) GameStop (GME) saga, where users of the subreddit Wallstreetbets (and later r/GME) subreddit challenged the flow of financial information from legacy media outlets in understanding financial manipulations and possibilities involved in the GME stock price. An extraordinary surge in value was attributed to activity on Wallstreetbets subreddit where users had been touting the stock for almost a year. While initially portrayed by traditional media as an almost humorous attempt to strike back at Wall Street, legacy reporting soon turned negative. Financial outlet news and information used top down, expert driven communication strategies to claim the surge was ephemeral and the product of crowd sourcing manipulation techniques. They soon stopped reporting altogether. The subreddit communities evolved, using bottom-up communication strategies to challenge “old” media narratives while using open source analysis and platform functionalities to maintain and expand an organized information and support system that functioned outside of traditional media boundaries. Different approaches to gatekeeping (expert driven vs. moderator/user driven) and analysis and dissemination (legacy/historically valid vs. open source) led to very different, often time conflicting, development of information, perspectives and calls to action. These differences, and the way they are treated in the larger society highlight how the Internet has reanimated the Lippman-Dewey debate over information and decision making from the early part of the twentieth century. While Lippman might claim Reddit sites are an illusion, Dewey might recognize possibilities for different type(s) of distributed activities crucial to a vibrant participatory democracy.  ","",""
"2022","Toxicity detection sensitive to conversational context","User posts whose perceived toxicity depends on conversational context are rare in current toxicity detection datasets. Hence, toxicity detectors trained on existing datasets will also tend to disregard context, making the detection of context-sensitive toxicity harder when it does occur. We construct and publicly release a dataset of 10,000 posts with two kinds of toxicity labels: (i) annotators considered each post with the previous one as context; and (ii) annotators had no additional context. Based on this, we introduce a new task, context sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. We then evaluate machine learning systems on this task, showing that classifiers of practical quality can be developed, and we show that data augmentation with knowledge distillation can improve performance further. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider parent posts, which often may be unnecessary and may otherwise introduce significant additional costs.","",""
"2022","Metaphors in moderation"," Volunteer content moderators are essential to the social media ecosystem through the roles they play in managing and supporting online social spaces. Recent work has described moderation primarily as a functional process of actions that moderators take, such as making rules, removing content, and banning users. However, the nuanced ways in which volunteer moderators envision their roles within their communities remain understudied. Informed by insights gained from 79 interviews with volunteer moderators from three platforms, we present a conceptual map of the territory of social roles in volunteer moderation, which identifies five categories with 22 metaphorical variants that reveal moderators’ implicit values and the heuristics that help them make decisions. These metaphors more clearly enunciate the roles volunteer moderators play in the broader social media content moderation apparatus and can drive purposeful engagement with volunteer moderators to better support the ways they guide and shape their communities. ","",""
"2022","Not all who are bots are evil: A cross-platform analysis of automated agent governance"," The growth of online platforms is accompanied by the increasing use of automated agents. Despite being discussed primarily in the context of opinion manipulation, agents play diverse roles within platform ecosystems that raises the need for governance approaches that go beyond policing agents’ unwanted behaviour. To provide a more nuanced assessment of agent governance, we introduce an analytical framework that distinguishes between different aspects and forms of governance. We then apply it to explore how agents are governed across nine platforms. Our observations show that despite acknowledging diverse roles of agents, platforms tend to focus on governing selected forms of their misuse. We also observe differences in governance approaches used by platforms, in particular when it comes to the agent rights/obligations and transparency of policing mechanisms. These observations highlight the necessity of advancing the algorithmic governance research agenda and developing a generalizable normative framework for agent governance. ","",""
"2022","Signaling the Intent to Change Online Communities: A Case From a Reddit Gaming Community"," This study builds on existing research about churn and community movement, examining if language use on Reddit can be used to determine if people signal their intent to relocate to a new community before they actually do so. Using a computational and semantic approach, we studied the subreddits for the game series Fallout at the time Fallout 76 ( FO76) was released to see if the users of the Fallout 4 ( FO4) subreddit signaled how they would react to the new subreddit. The main difference we found was that those who stay in the FO4 subreddit or use both subreddits on average post more often and create longer posts than those who move to the FO76 subreddit or leave. This adds further evidence to support theories about community as communication, and we suggest this finding can help online community managers identify which users may be about to leave the community, aiding retention and the overall health of the community. ","",""
"2022","From Scalability to Subsidiarity in Addressing Online Harm"," Large social media platforms are generally designed for scalability—the ambition to increase in size without a fundamental change in form. This means that to address harm among users, they favor automated moderation wherever possible and typically apply a uniform set of rules. This article contrasts scalability with restorative and transformative justice approaches to harm, which are usually context-sensitive, relational, and individualized. We argue that subsidiarity—the principle that local social units should have meaningful autonomy within larger systems—might foster the balance between context and scale that is needed for improving responses to harm. ","",""
"2022","Do Not Recommend? Reduction as a Form of Content Moderation"," Public debate about content moderation has overwhelmingly focused on removal: social media platforms deleting content and suspending users, or opting not to do so. However, removal is not the only available remedy. Reducing the visibility of problematic content is becoming a commonplace element of platform governance. Platforms use machine learning classifiers to identify content they judge misleading enough, risky enough, or offensive enough that, while it does not warrant removal according to the site guidelines, warrants demoting them in algorithmic rankings and recommendations. In this essay, I document this shift and explain how reduction works. I then raise questions about what it means to use recommendation as a means of content moderation. ","",""
"2022","Automated Platform Governance Through Visibility and Scale: On the Transformational Power of AutoModerator"," When platforms use algorithms to moderate content, how should researchers understand the impact on moderators and users? Much of the existing literature on this question views moderation as a series of decision-making tasks and evaluates moderation algorithms based on their accuracy. Drawing on literature from the field of platform governance, I argue that content moderation is more than a series of discrete decisions but rather a complex system of rules, mechanism, and procedures. Research must therefore articulate how automated moderation alters the broader regime of governance on a platform. To demonstrate this, I report on the findings of a qualitative study on the Reddit bot AutoModerator, using interviews and trace ethnography. I find that the scale of the bot allows moderators to carefully manage the visibility of content and content moderation on Reddit, fundamentally transforming the basic rules of governance on the platform. ","",""
"2023","From healthy communities to toxic debates: Disqus’ changing ideas about comment moderation","Abstract This article examines how the commenting platform Disqus changed the way it speaks about commenting and moderation over time. To understand this evolving self-presentation, we used the Internet Archive Wayback Machine to analyse the company’s website and blog between 2007 and 2021. By combining interpretative close-reading approaches with computerised distant-reading procedures, we examined how Disqus tried to advance online discussion and dealt with moderation over time. Our findings show that in the mid-2000s, commenting systems were supposed to help filter and surface valuable contributions to public discourse, while ten years later their focus had shifted to the proclaimed goal of protecting public discourse from contamination with potentially harmful (“toxic”) communication. To achieve this, the company developed new tools and features to keep communities “healthy” and to facilitate and semi-automate active and interventive forms of moderation. This rise of platform interventionism was fostered by a turn towards semantics of urgency in the company’s language to legitimise its actions.","",""
"2023","An autoethnography of automated powerlessness: lacking platform affordances in Instagram and TikTok account deletions","             Situated within the field of platform governance studies, this paper shares findings from an ‘autoethnography of automated powerlessness’, drawing from the researcher’s disempowering experience of being a heavily moderated social media user. Using theoretical frameworks blending affordances and World Risk Society theories, this paper contextualises my experiences of moderation of my pole dance instructor, activist and blogger account             @bloggeronpole             from February to October 2021 within social media’s broader de-platforming of nudity and sexuality, finding fallacies within platforms’ own affordances, which lack mechanisms to aid or rehabilitate de-platformed accounts. With little to no information from platforms about the details of their moderation, qualitative, ethnographic and autoethnographic explorations of their governance are all users currently have to fight and understand their puritan, patriarchal censorship of nudity and sexuality, which are often conflated with risk. This study concludes with recommendations for different options for better, more equal and community focused moderation.           ","",""
"2023","Social media’s canaries: content moderators between digital labor and mediated trauma"," This paper takes recent PTSD claims by content moderators working for Microsoft and Google as a starting point to discuss the changing nature of trauma in the context of social media and algorithmic culture. Placing these claims in the longer history of how media came to be regarded by clinicians as potentially traumatic, it considers content moderation as a form of immaterial labor, which brings the possibility to be traumatized into the cycle of digital labor. Therefore, to the extent that content moderators’ trauma exists as a clinical condition, it cannot be taken as an incidental side-effect but as a built-in potentiality. It is about the commodification of traumatic vulnerability itself. The discussion then proceeds to speculate about the possibility of using algorithms to identify potentially traumatic content and what would that mean for the understanding of trauma, especially as a mediated experience. ","",""
"2023","ANTICIPATING SOCIO-TECHNICAL CHANGE: USERS BELIEFS AROUND REDDIT’S IPO","Platform changes are a phenomenon endlessly confronting social media users, impacting how they create and consume content. The social media platform “Reddit” is, as of writing, in the midst of a significant planned shift in their socio-economic foundation. Reddit’s owners are moving the company from being a privately held business to a publicly-traded one. While scholarly attention in domains such as human-computer interaction (HCI) has focused on tracing how users understand and respond to changes in the interface features of social media, less consideration has been given to how users make sense of the socio-economic side of the social media platform equation. Through a thematic analysis of Reddit posts and comments mentioning the Reddit IPO made to “r/announcements,” “r/theoryofReddit,” “r/askreddit,” “r/stocks,” “r/wallstreetbets,” and “r/technology,” this study examines how users discuss and make sense of socio-economic change in this social media environment. Findings from this study focus on three themes: 1) What aspects of the techno-cultural facets of Reddit (defaults, protocols, algorithms, and interfaces) users believe might change as a result of the IPO. 2) What policy changes users believe might occur on Reddit, particularly in relation to moderation practices and the kinds of content allowed on the site. And, 3) User beliefs about the particular interests that these changes may serve.","",""
"2023","THE TOXIC TURN? CONCEPTUAL AND METHODOLOGICAL ADVANCES ON PROBLEMATIC CONTENTS ON SOCIAL MEDIA","The ‘toxic turn’ in social media platforms continues unabated. Hate speech, mis- and disinformation, misogynistic and racist speech, images, memes and videos are all far too common on social media platforms and more broadly on the internet. While the diminishing popularity of populist politicians led to hopes for less social toxicity, the Covid-19 pandemic introduced new and more complex dimensions. Tensions have emerged around what constitutes problematic content and who gets to define it. Co-regulation models, such as for example the EC Code of Conduct against Illegal Hate Speech, focus on the legality of certain types of contents, while leaving other categories of problematic contents to be defined by platforms. In parallel, the social media ecosystem became more diverse, as new platforms with hands off moderation policies attracted users who felt too constrained by the policies of mainstream platforms. The proposed panel examines this complex and dynamic landscape by problematizing what is understood as toxic, deplatformed, removable and in general problematic content on platforms with the aim to probe the boundaries of what is constituted as acceptable discourse on platforms and to map its implications.  In particular, this panel discusses the broad definition of ‘problematic content’ employed by social media platforms, a catch-all term that cuts across hate speech and propaganda, including more politically topical content such as mal-, mis-, and disinformation, hyperpartisan and polarising content, but also abusive, misogynistic, racist, and homophobic discourse. The term is also employed to refer to spam and content that infringes upon the Terms of Service or the Community Standards of social media platforms. As such, it is a broad category that resists a narrower classification given the operational scope of its use. Defining what constitutes problematic content is a key operation of platform content moderation policies but is also the subject of intense debates (de Gregorio, 2020; Gillespie, 2018; Gillespie et al., 2020; Gorwa et al., 2020).  The panel interrogates the many definitions and applications of problematic content on social media platforms and applications through an empirically informed lens and focusing on deleted contents, complex mixed narratives, and grey areas, including hidden misinformation on voice applications. Problematic Content according to Twitter Compliance API presents ongoing work on the Twitter Compliance API and the Compliance Firehose, which allow researchers to identify content that has been deleted, deactivated, protected, or suspended from Twitter, a proxy for problematic content. In Multi-Part Narratives on Telegram Siapera presents ongoing research that probes the intersection between Covid-19 scepticism, far right and other political narratives in vaccine hesitant groups on Telegram. The third contribution, What if Bill Gates really is evil, people? Investigating the infodemic’s grey areas discusses the conceptual and methodological definitions of problematic content in relation to work on anti-vax and other conspiratorial narratives on Instagram and on Twitter. The fourth contribution, Misinformation and other Harmful Content in Third-Party Voice Applications focuses on problematic content that is yet to be identified on voice applications such as personal assistants. The article addresses the methodological challenges of identifying and defining such contents on applications that have currently no content moderation policies. All contributions foreground the difficulties and costs of identifying and dealing with problematic contents on social media.  The panel fits with theme of decolonization in two ways: firstly, because it is concerned with the tensions around how toxic/problematic contents are defined and who gets to define them; and secondly, because of its focus on neo-colonial discourses or justifications for colonialism in both narratives hosted by platforms and in platforms’ attempts to regulate content. As some narratives are flagged for removal by social platforms, they also raise the question of who is deciding and what does problematic content mean, with far right discourses exploiting this tension and ironically denouncing any attempt to regulate the public discourse as ideological enforcement and justification for (neo)colonial practices performed by social media platforms. From this perspective, platforms' own claims about what constitutes acceptable content is uncomfortably close to colonial narratives of civilised discourse and brings to the fore the potential for neo-colonial narratives and practices in digital spaces.  References  De Gregorio, G. (2020). Democratising online content moderation: A constitutional framework. Computer Law &amp; Security Review, 36, 105374.  Gillespie, T. (2018). Custodians of the Internet. Yale University Press.  Gillespie, T., Aufderheide, P., Carmi, E., Gerrard, Y., Gorwa, R., Matamoros-Fernández, A., ... &amp; West, S. M. (2020). Expanding the debate about content moderation: Scholarly research agendas for the coming policy debates. Internet Policy Review, 9(4), Article-number.  Gorwa, R., Binns, R., &amp; Katzenbach, C. (2020). Algorithmic content moderation: Technical and political challenges in the automation of platform governance. Big Data &amp; Society, 7(1), 2053951719897945.","",""
"2023","EXPLOITATION IN ONLINE CONTENT MODERATION","This paper presents a normative framework for evaluating the moral and epistemic exploitation that online content moderation workers experience while working for social media companies (often as subcontractors or as Mechanical Turk workers). I argue that the current labor model of commercial content moderation (CCM) is exploitative in ways that inflict profound moral harm and epistemic injustices on the workers. This detailed account of exploitation enables us to see more clearly the contours and causes of the moral and epistemic injustice involved in CCM, and helps us understand precisely why these forms of exploitation are unjust. It also suggests some practical solutions for a more just labor model for the moderation work that shapes our online ecosystem.","",""
"2023","THE IMPERIAL HAIKU COMMISSION APPROVES THIS MESSAGE’: AN EXAMINATION OF AUTOMATED PLAY AND CULTURE AS (RE)DESIGNED BY BOTS.   ","This paper examines a community called Subreddit Simulator on the social media platform Reddit. It is a digital space where human and social bot co-exist on an ontologically equal footing to co-create culture, community and a sense of 'play'. We recognize this community as a pioneer community against a larger backdrop of deep mediatization. With the recent attention given to bots such as Chat GPT it is imperative that we do not overlook communities in which progressive and revolutionary practice has been happening for years. This extended abstract proposes an ethnographic approach to viewing the Subreddit Simulator community while the longer form work will bring the ethnographic results to bear to discuss philosophically the opportunities and implications of reimagining networked spaces in a less human-centric manor.","",""
"2023","ASSESSING THE IMPACT OF GLOBAL ATTENTION ON SUBREDDIT COMMUNITY PRACTICES: THE CASE OF /R/HONGKONG","What happens to internet communities after a global media event puts them in the public spotlight? Does the flood of new members change a given community's practices, structures, and discussions? Do things go back to normal? These questions lie at the heart of our research project, which examines how a local subcommunity on the popular website Reddit changed as its matter of focus became a global discussion subject.  Specifically, this study analyses changing posting practices on /r/hongkong, a local subreddit whose popularity skyrocketed in 2019, with the Anti-Extradition Law Amendment Bill Movement garnering worldwide media coverage. Now, with global attention shifting away from the protests and the 2020 Hong Kong National Security law, the subreddit no longer exhibits the same level of activity. But what can be learnt beyond simply looking at the numbers? Can a qualitative change be observed on /r/hongkong?  In this extended abstract, we examine the existing research on subreddits as a community, consider the potential significance of media events and subsequent influxes of new users for community practice, outline our methodological approach, and highlight some preliminary findings.","",""
"2023","AFTER DEPLATFORMING: RETRACING CONTENT MODERATION EFFECTS ACROSS PLATFORMS AND A POST-AMERICAN WEB","Half a decade ago, social media platforms were widely perceived as revolutionary devices for maximizing political expression around the world. By opening the floodgates to expression, however, the same platforms were also accused of opening the floodgates of hate – allowing, for example, the self-claimed “revolutionary” return of ideas, speech and actors long thought to be relegated to the dustbins of history. This panel examines a three-fold revolution, namely: populist revolutions (on the right) facilitated by agnostic content moderation philosophies; the internal revolutions that platform content moderation underwent to address the political violence of the former; and the adjustments that digital methods research needs to adopt to facilitate content moderation research in a “post-API” environment. The first paper of this panel examines how Twitter’s content moderation has undergone several arbitrary changes before reaching a form of “normative plasticity”, with reinforcement techniques such as demotion and other forms of conditional content obfuscation. The second paper looks at how, despite making profound changes to prevent furthering political violence during elections, Twitter, Facebook, YouTube and Instagram have tended to moderate the Brazilian elections in a dislocated fashion, turning a blind eye to Brazilian militaristic content and focusing instead on what it primarily moderates in a US context. Finally, the third paper offers a set of methods for empirical researchers to capture and study content moderation metadata over time. All three papers aim to contribute to attempts at archiving and studying speech moderation as a public good, in an international context.","",""
"2023","Ethical scaling for content moderation: Extreme speech and the (in)significance  of artificial intelligence"," In this article, we present new empirical evidence to demonstrate the severe limitations of existing machine learning content moderation methods to keep pace with, let alone stay ahead of, hateful language online. Building on the collaborative coding project “AI4Dignity” we outline the ambiguities and complexities of annotating problematic text in AI-assisted moderation systems. We diagnose the shortcomings of the content moderation and natural language processing approach as emerging from a broader epistemological trapping wrapped in the liberal-modern idea of “the human”. Presenting a decolonial critique of the “human vs machine” conundrum and drawing attention to the structuring effects of coloniality on extreme speech, we propose “ethical scaling” to highlight moderation process as political praxis. As a normative framework for platform governance, ethical scaling calls for a transparent, reflexive, and replicable process of iteration for content moderation with community participation and global parity, which should evolve in conjunction with addressing algorithmic amplification of divisive content and resource allocation for content moderation. ","",""
"2023","Diamond hands to the moon: Idiocultural mobilization and politicization of personal finance on r/wallstreetbets"," In January 2021, the shares of the brick-and-mortar video games retail chain GameStop exploded in value. At the same time, billboards on highways and ads in Times Square in New York City used cryptic visuals and seemingly meaningless emojis with diamonds and rockets. Mainstream media soon had an explanation: This was the story of how the subreddit r/wallstreetbets had mobilized thousands of retail investors in a fight against evil hedge funds. Based on a case study of r/wallstreetbets and the GameStop incident, we analyze how idiosyncratic internet culture was incorporated into a broadly resonant and emotionally inflected narrative that lionized the ‘little man’, focusing on both individual profits and collective grievances. Through a theoretical framework combining sociological theories of internet culture and framing analysis, we identify an overall communication structure that drew on three interconnected discursive layers: idiocultural memes, investment-specific information, and a moralized, collectivized injustice frame with heroes and villains. We further argue that the GameStop (GME) incident instantiates a case of the politicization of personal finance, where the investment practices and strategies of ordinary people were transformed into a political issue. As such, the article makes two contributions to the existing literature. First, we contribute to the nascent literature on internet cultures related to personal finance by looking at a specific subreddit devoted to stock trading and investing. Second, we show how idiocultural elements, such as emojis and memes, can function both as contested and exclusionary material aimed at insiders and as flexible components of communications framed for broad mobilization through emotionally resonant notions of grievance and injustice. ","",""
"2023","Common sense or censorship: How algorithmic moderators and message type influence perceptions of online content deletion"," Hateful content online is a concern for social media platforms, policymakers, and the public. This has led high-profile content platforms, such as Facebook, to adopt algorithmic content-moderation systems; however, the impact of algorithmic moderation on user perceptions is unclear. We experimentally test the extent to which the type of content being removed (profanity vs hate speech) and the explanation given for its removal (no explanation vs link to community guidelines vs specific explanation) influence user perceptions of human and algorithmic moderators. Our preregistered study encompasses representative samples ( N = 2870) from the United States, the Netherlands, and Portugal. Contrary to expectations, our findings suggest that algorithmic moderation is perceived as more transparent than human, especially when no explanation is given for content removal. In addition, sending users to community guidelines for further information on content deletion has negative effects on outcome fairness and trust. ","",""
"2023","Deplatformization and the governance of the platform ecosystem"," This article analyzes deplatformization as an implied governance strategy by major tech companies to detoxify the platform ecosystem of radical content while consolidating their power as designers, operators, and governors of that same ecosystem. Deplatformization is different from deplatforming: it entails a systemic effort to push back encroaching radical right-wing platforms to the fringes of the ecosystem by denying them the infrastructural services needed to function online. We identify several deplatformization strategies, using Gab as an example of a platform that survived its relegation and which subsequently tried to build an alternative at the edge of the mainstream ecosystem. Evaluating deplatformization in terms of governance, the question that arises is who is responsible for cleansing the ecosystem: corporations, states, civil society actors, or all three combined? Understanding the implied governance of deplatformization is imperative to assess the higher stakes in future debates concerning Internet governability. ","",""
"2023","Do users want platform moderation or individual control? Examining the role of third-person effects and free speech support in shaping moderation preferences"," This study examines social media users’ preferences for the use of platform-wide moderation in comparison to user-controlled, personalized moderation tools to regulate three categories of norm-violating content—hate speech, sexually explicit content, and violent content. Via a nationally representative survey of 984 US adults, we explore the influence of third-person effects and support for freedom of expression on this choice. We find that perceived negative effects on others negatively predict while free speech support positively predicts a preference for having personal moderation settings over platform-directed moderation for regulating each speech category. Our findings show that platform governance initiatives need to account for both actual and perceived media effects of norm-violating speech categories to increase user satisfaction. Our analysis also suggests that users do not view personal moderation tools as an infringement on others’ free speech but as a means to assert greater agency over their social media feeds. ","",""
"2023","What Teams Do: Exploring Volunteer Content Moderation Team Labor on Facebook"," Social media sites such as Facebook depend on tens of millions of volunteer moderators across the globe to facilitate platform-based discussion forums. While research has revealed much about the work that these moderators do, some fundamental questions remain. For example, why do volunteer moderators commonly work as teams rather than individuals? In this article, I use data gathered through digital ethnography with Facebook Group moderators to explore the benefits and challenges of moderation team work. I develop a three-part framework to articulate how teams facilitate logistical, discursive, and emotional labor. Finally, I argue that this empirical analysis reveals otherwise hidden and unacknowledged dimensions of volunteer moderation work that make platform-hosted discussion groups possible. ","",""
"2023","YOLO Publics: The Potential for Creative Subversion of an Online Trading Community"," Digitally mediated publics are often discussed in terms of extremism and radicalization, but it remains possible that digital communication technologies can engender new connections and conversations through “creative subversion.” This article explores the potentials of one specific instance of such creative subversion: the “GameStop rescue” as let by members of the subreddit forum “WallStreetBets” (WSB) in the early months of 2021. From a communicative perspective, what is interesting about this series of events is not only the digital platforms and affordances that enabled it, but also the reckless behavior of WSB members and the ways in which this behavior was communicated—and continues to be celebrated and facilitated by the online trading community. Members share “loss porn,” praise each other for having “diamond hands” when holding on to investments that are losing value, and celebrate the principle of YOLO (you only live once). We conceptualize WSB as a YOLO public, an online community that is loosely and temporarily formed through the common action of seizing the opportunity to wreak havoc around power. Furthermore, we understand the events of the GameStop rescue as a controversial encounter that, we argue, offers hope for digitally mediated publics to develop in dynamic relations of difference rather than as stabilized oppositions. ","",""
"2023","Moderation, Networks, and Anti-Social Behavior Online"," Major open platforms, such as Facebook, Twitter, Instagram, and Tik Tok, are bombarded with postings that violate platform community standards, offend societal norms, and cause harm to individuals and groups. To manage such sites requires identification of content and behavior that is anti-social and action to remove content and sanction posters. This process is not as straightforward as it seems: what is offensive and to whom varies by individual, group, and community; what action to take depends on stated standards, community expectations, and the extent of the offense; conversations can create and sustain anti-social behavior (ASB); networks of individuals can launch coordinated attacks; and fake accounts can side-step sanctioning behavior. In meeting the challenges of moderating extreme content, two guiding questions stand out: how do we define and identify ASB online? And, given the quantity and nuances of offensive content: how do we make the best use of automation and humans in the management of offending content and ASB? To address these questions, existing studies on ASB online were reviewed, and a detailed examination was made of social media moderation practices on major media. Pros and cons of automated and human review are discussed in a framework of three layers: environment, community, and crowd. Throughout, the article adds attention to the network impact of ASB, emphasizing the way ASB builds a relation between perpetrator(s) and victim(s), and can make ASB more or less offensive. ","",""
"2024","Decentralised content moderation","","",""
"2024","Repairing the harm: Toward an algorithmic reparations approach to hate speech content moderation"," Content moderation algorithms influence how users understand and engage with social media platforms. However, when identifying hate speech, these automated systems often contain biases that can silence or further harm marginalized users. Recently, scholars have offered both restorative and transformative justice frameworks as alternative approaches to platform governance to mitigate harms caused to marginalized users. As a complement to these recent calls, in this essay, I take up the concept of reparation as one substantive approach social media platforms can use alongside and within these justice frameworks to take actionable steps toward addressing, undoing and proactively preventing the harm caused by algorithmic content moderation. Specifically, I draw on established legal and legislative reparations frameworks to suggest how social media platforms can reconceptualize algorithmic content moderation in ways that decrease harm to marginalized users when identifying hate speech. I argue that the concept of reparations can reorient how researchers and corporate social media platforms approach content moderation, away from capitalist impulses and efficiency and toward a framework that prioritizes creating an environment where individuals from marginalized communities feel safe, protected and empowered. ","",""
"2024","Review bombing the platformed city: Contested political speech in online  local reviews"," Local review platforms like Yelp and Google Maps use systems combining automated and human judgment to delineate the limits of acceptable speech, allowing some reviews to remain public and removing or obscuring others. This article examines the phenomenon of “review bombing,” in which controversial businesses receive an influx of reviews, using spatiotemporal analysis of review activity to analyze their shifting catchment areas, measuring what sociologist Richard Ocejo calls the “extraterritoriality” of their “taste communities”. Specifically, this article examines businesses in the United States that are caught up in political controversies using the locations of their consumer-reviewers on Yelp. The author compiles a test dataset of affected businesses encompassing national and local politics, including the 2016 and 2020 U.S. elections, the #BlackLivesMatter and #MeToo movements, and the COVID-19 pandemic, and selects two for in-depth case studies and spatial analysis: Washington, D.C.-based pizzeria Comet Ping Pong (subject of the #Pizzagate conspiracy theory) and St. Louis-based Pi Pizzeria (caught up in debates about policing and the Black Lives Matter movement). In Comet Ping Pong's case, review bombing resulted in a wider spatial distribution of primarily negative reviewers, while Pi has a much more local pattern, with a fairly even split of supporters and detractors, showing how different political controversies resonate across different scales. The article contrasts Yelp's interventionist approach to content moderation to the relatively laissez-faire attitude of competitors like Google, and considers the consequences of this form of """"algorithmic censorship"""" for small businesses, communities, and online speech. ","",""
"2024","‘Definitely not in the business of wanting to be associated’: Examining public relations in a deplatformization controversy"," In August 2019, a mass shooter in the United States posted a violent manifesto to the anonymous forum 8chan prior to his attack. This was the third such incident that year and afterwards hosting and security services conceded to calls to drop 8chan as a client, pushing 8chan to the margins of the accessible internet. This article examines the deplatforming of 8chan as a public relations crisis, contributing to understanding ‘governance by shock’ (Ananny and Gillespie 2016) by examining who is shocked and their power to turn shock into online regulation. Online platforms and media attention created opportunities to study how the deplatforming was justified, drawing on the theoretical framework of economies of worth (Boltanski and Thevenot 2006) and controversy mapping methods. The examination finds: (1) that this case of deplatforming indicates the openness of infrastructure-as-a-service companies to external challenges over content, rather than hegemonic control. (2) That regulatory gaps, including the broadness of U.S. free speech laws, made these companies, rather than legal processes, the relevant authority. (3) That framing responsibility as following the law – as Cloudflare attempted to do – misunderstands the importance of normative principles, voluntary measures, and contestation in governing online content, underselling the value of policy-making at other levels. The success of the campaign to deplatform 8chan affirms the significance of PR crises in the regulation of online content, rewarding deplatforming as a political tactic for civil society groups and online networks pushing for governance in regulatory gaps. However, the significance of normative enforcement in this case underlines the difficulties of this semi-voluntary style of governance. While normative opposition to violence contributed to 8chan’s deplatforming, other normative oppositions contribute to deplatforming vulnerable users, as in the moral panics that drive the deplatforming of sexual content ( Tiidenberg 2021 ) and feed suspicion over the ideological application of deplatforming. The ambivalence of PR crises as a strategy for influencing platform governance underlines the need for clarity in policy-making at multiple levels. ","",""
"2024","Norm enforcement on and of Reddit: Rules of engagement and participation","The social platform Reddit hosts a set of communities that denounce offensive behavior, invoking scrutiny and shame on (categories of) individuals. Despite varying in their targets, they all promote actionable content to an audience who can view, share and comment on it. These groups allow a global public to air grievances, enabling both accountability and abuse. Following high profile scandals, Reddit routinely sanctions and purges problematic ‘subreddits’. As a matter of self-preservation, subreddits that watch over the public also maintain heightened scrutiny of their own members. Group rules and other prescriptive texts are a means to instill this scrutiny among a broader audience. In analyzing rules and other content management practices in 68 shaming based subreddits, this paper considers how these groups temper platform-based denunciation.","",""
"2024","Opaque algorithms, transparent biases: Automated content moderation during the Sheikh Jarrah Crisis","Social media platforms, while influential tools for human rights activism, free speech, and mobilization, also bear the influence of corporate ownership and commercial interests. This dual character can lead to clashing interests in the operations of these platforms. This study centers on the May 2021 Sheikh Jarrah events in East Jerusalem, a focal point in the Israeli-Palestinian conflict that garnered global attention. During this period, Palestinian activists and their allies observed and encountered a notable increase in automated content moderation actions, like shadow banning and content removal. We surveyed 201 users who faced content moderation and conducted 12 interviews with political influencers to assess the impact of these practices on activism. Our analysis centers on automated content moderation and transparency, investigating how users and activists perceive the content moderation systems employed by social media platforms, and their opacity. Findings reveal perceived censorship by pro-Palestinian activists due to opaque and obfuscated technological mechanisms of content demotion, complicating harm substantiation and lack of redress mechanisms. We view this difficulty as part of algorithmic harms, in the realm of automated content moderation. This dynamic has far-reaching implications for activism’s future and it raises questions about power centralization in digital spaces.","",""
"2024","Unpacking platform governance through meaningful human agency: How Chinese moderators make discretionary decisions in a dynamic network"," How platforms moderate online content remains “black-boxed” due to diverging and sometimes conflicting logics in the governance network of different forces and stakeholders. The key to unpacking the “black-box” lies with meaningful human agency in balancing these logics in contextualized practice. This article examines human moderators’ decision-making based on qualitative data drawn from Chinese leading platforms. Grounded theory analysis theorizes various contributing (f)actors and introduces a model of “bounded discretion and tacit interaction in an elastic dynamic” to address moderation decision-making. It argues that it is though bounded human agency that platform governance is enacted, negotiated, and further “black-boxed” as inevitable human inconsistencies and contingencies are molded into governance. This study provides useful categories for analyzing platform moderation, encodes meaningful human agency as bounded discretion in the governance network, and unpacks the “black-box” of platform governance as an unfolding process of interactions among institutions, forces, and most importantly, human participants. ","",""
"2024","Verified play, precarious work: GamerGate and platformed authenticity in the cultural industries"," This article argues that GamerGate, a critical hashtag event in the history of digital harassment, is key to understanding contemporary identity verification systems and digital labour. We build our argument from a comparative analysis of two case studies: (1) digital journalistic responses to GamerGate and (2) Twitter’s account verification ‘checkmark’ system from 2021 to 2022. These phenomena showcase the linkages between the gendered and raced policing of journalists and users during GamerGate and the rise of ‘authenticity’ as a key resource for journalists and other platformed creators in the present. We draw on digital games, journalism and critical media studies to analyse the work of ‘authenticity’. We argue that platform affordances such as identity verification badges are fundamentally implicated in the work of users to appear ‘real’, even as the visibility requisite for realness brings uneven risks for marginalised cultural workers. ","",""
"2024","Social media regulation, third-person effect, and public views: A comparative study of the United States, the United Kingdom, South Korea, and Mexico"," Given the prevalence of misinformation on social media and accompanying negative externalities, platform regulation has become a highly contested public issue globally. This study investigated (a) what global publics think about platform regulation and (b) the psychological mechanisms underlying such opinions through the lens of the third-person effect. Four national surveys, conducted in the United States, the United Kingdom, South Korea, and Mexico in April–September 2021, revealed that both presumed media influence on self and others play important but different roles in predicting support for two distinctive forms of platform regulation (i.e. government regulation of social media platforms versus content moderation by social media platforms). Self-efficacy (self-perceived ability to spot misinformation) and other-efficacy (perception of others’ ability to spot misinformation) were identified as two crucial antecedents of third-person perception. There were also nuanced but noteworthy differences in public attitudes toward platform regulations across the four countries studied. ","",""
"2024","Why do volunteer content moderators quit? Burnout, conflict, and harmful behaviors"," Moderating content on social media can lead to severe psychological distress. However, little is known about the type, severity, and consequences of distress experienced by volunteer content moderators (VCMs), who do this work voluntarily. We present results from a survey that investigated why Facebook Group and subreddit VCMs quit, and whether reasons for quitting are correlated with psychological distress, demographics, and/or community characteristics. We found that VCMs are likely to experience psychological distress that stems from struggles with other moderators, moderation team leads’ harmful behaviors, and having too little available time, and these experiences of distress relate to their reasons for quitting. While substantial research has focused on making the task of detecting and assessing toxic content easier or less distressing for moderation workers, our study shows that social interventions for VCM workers, for example, to support them in navigating interpersonal conflict with other moderators, may be necessary. ","",""
"2024","‘Just a little hack’: Investigating cultures of content moderation circumvention by Facebook users"," As social media platforms adapt their rules to limit the presence, spread, and amplification of harmful content on their services, users develop strategies to circumvent content moderation policies. To better understand cultures of content moderation circumvention, including the types of rules that Facebook users seek to circumvent, we analysed a sample of YouTube videos and Reddit threads in which users discuss content moderation circumvention. We show how Facebook users turn to others across platforms to obtain information about circumvention methods. We observe that these users often discuss overcoming Facebook’s content moderation policies in terms that downplay the significance of their intended actions. We suggest that where Facebook’s policies and enforcement measures fail to deter rule violations that may facilitate harm, Facebook should consider new culture-driven approaches to platform governance that foster prosocial environments and engender compliance with platform rules. ","",""
"2024","The psychology of volunteer moderators: Tradeoffs between participation, belonging, and norms in online community governance"," Online communities rely on effective governance for success, and volunteer moderators are crucial for ensuring such governance. Despite their significance, much remains to be explored in understanding the relationship between community governance processes and moderators’ psychological experiences. To bridge this gap, we conducted an online survey with over 600 moderators from Reddit communities, exploring the link between different governance strategies and moderators’ needs and motivations. Our investigation reveals a contrast to conventional views on democratic governance within online communities. While participatory processes are associated with higher levels of perceived fairness, they are also linked with reduced feelings of community belonging and lower levels of institutional acceptance among moderators. Our findings challenge the assumption that greater democratic involvement unequivocally leads to positive community outcomes, suggesting instead that more centralized governance approaches can also positively affect moderators’ psychological well-being and, by extension, community cohesion and effectiveness. ","",""
"2024","The owners of information: Content curation practices of middle-level gatekeepers in political Facebook groups"," Volunteer moderators play a key role when making judgements about which online content should be accepted and which should be removed. As such, their work fundamentally shapes the digital social and political spheres. Using the data obtained from 15 Facebook group moderator interviews as research data, this study focused on the content curation work by the middle-level gatekeepers of Finnish political discussion groups on Facebook. The findings show that the moderators feel strong ownership of the groups they moderate and of the information such groups provide, and as a result, they strongly shape the groups’ discussion and governing policy. Facebook’s governing policy for groups is vague, which gives space for group norms and identities to develop. The stakeholder groups (i.e. the platform administration, moderators and users) do not attend to the governance process all together, so negotiations among them are almost non-existent. ","",""
"2024","When content moderation is not about content: How Chinese social media platforms moderate content and why it matters"," Content moderation has become an essential part of the business of social media platforms, yet how it works remains largely a mystery in some important cases, particularly with regard to platforms run by Chinese companies. This research examines the latest automated moderation approaches adopted by Chinese short video platforms. Drawing on expert interviews and documentary research, we argue that Chinese platforms are moving away from a semantic approach, one that aims to grasp the meaning of content, and toward regulating the ambient element, which we define as the pervasive information that immediately surrounds content and enacts its overall character and impact. Applying a consequentialist ethics lens to investigate this turn, we argue that the ambient shift represents a more proactive approach to moderation, one intended to create a generally beneficial informational environment for platform users. This contrasts with reactive, individualistic moderation regimes grounded in the principle of informational neutrality. ","",""
"2024","Who Can Say What? Testing the Impact of Interpersonal Mechanisms and Gender on Fairness Evaluations of Content Moderation"," Content moderation is commonly used by social media platforms to curb the spread of hateful content. Yet, little is known about how users perceive this practice and which factors may influence their perceptions. Publicly denouncing content moderation—for example, portraying it as a limitation to free speech or as a form of political targeting—may play an important role in this context. Evaluations of moderation may also depend on interpersonal mechanisms triggered by perceived user characteristics. In this study, we disentangle these different factors by examining how the gender, perceived similarity, and social influence of a user publicly complaining about a content-removal decision influence evaluations of moderation. In an experiment ( n = 1,586) conducted in the United States, the Netherlands, and Portugal, participants witnessed the moderation of a hateful post, followed by a publicly posted complaint about moderation by the affected user. Evaluations of the fairness, legitimacy, and bias of the moderation decision were measured, as well as perceived similarity and social influence as mediators. The results indicate that arguments about freedom of speech significantly lower the perceived fairness of content moderation. Factors such as social influence of the moderated user impacted outcomes differently depending on the moderated user’s gender. We discuss implications of these findings for content-moderation practices. ","",""

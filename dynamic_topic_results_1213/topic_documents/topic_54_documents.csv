"year","title","abstract","journal","doi"
"2000","Cyberhate and Performative Speech in Accelerated Time(s)","       In foregrounding the performative character of hate speech, legal scholars and activists have sought to demonstrate why hate speech should be prohibited not only on the basis of what it says but on the basis of what it does. In this article I examine the conditions upon which hate speech has been posited as performative speech in order to consider how virtual environments trouble existing understandings of hate speech. In particular, this article seeks to show how cyberspace may in fact create the conditions for a more immediate and radical recontextualisation and recirculation of hate speech where speed operates as a potential source of resistance.        In foregrounding the performative character of hate speech, legal scholars and activists have sought to demonstrate why hate speech should be prohibited not only on the basis of what it says but on the basis of what it does. In this article I examine the conditions upon which hate speech has been posited as performative speech in order to consider how virtual environments trouble existing understandings of hate speech. In particular, this article seeks to show how cyberspace may in fact create the conditions for a more immediate and radical recontextualisation and recirculation of hate speech where speed operates as a potential source of resistance.        Judith Butler maintains that """"speech is always in some ways out of our control"""" (15). This is not to suggest that our speech is bound to misfire. Instead, Butler's claim draws attention to the range of possibilities located precisely within the failed speech act, revealing how such misfires offer the possibility that certain words which carry the potential to injure may eventually become """"disjointed from their power to injure and recontextualised in more affirmative modes"""" (15). Whereas Langston suggests that such speech events inevitably work to silence intended victims, Butler recognises how the subject may also be inaugurated through such linguistic injuries. She maintains that to be injured through language is to """"suffer a loss of context, that is, not to know where you are"""" (4). The """"loss of context"""" or disorientation she claims might result from hate speech not only suggests that hate speech is context-specific but also that hate speech, understood as a performance, might transform or even produce a speaker's social location. In this view, the very words used to """"put someone in their place"""" may also enable people to speak up from their position on the margins of power. Thus, the repetition of hate speech does not necessarily reinforce certain words' power to injure but may result in a pattern of """"slippages"""" which enables the meaning and force of such speech to eventually come undone.        In order to understand the impact of hate speech in virtual environments, it is useful to consider how the spatial and temporal character of cyberspace affects the repetitions to which Butler refers. While we typically speak of cyber-space, the emergence of cyberspace has arguably resulted in an increased preoccupation with time. For Virilio, the shift from space to time appears to be most visible in cyberspace, which he maintains does not represent a space so much as a particular temporal dimension where speed, not territory, holds the most strategic value. However, he further maintains that we have also reached a moment when humans, who have already surpassed the sound and heat barriers, are left with nothing to race against but speed of light -- something which can never be surpassed. And, as he warns, like other technological revolutions, this one is bound to result in an accident, this time not a physical one but instead one which will throw history itself into a disarray. """"Cyberspace looms up like a transfer accident in substantial reality"""", he warns, """"what gets damaged is no longer the substance, the materiality of the tangible world, it is the whole of its constitution"""" (Open Sky 131). Thus, cyberspace not only appears to give rise to an accelerated sort of time but to create the conditions under which we run the risk of suffering a """"fundamental loss of orientation"""" (Speed and Information). I would like to consider how the speed and subsequent loss of orientation Virilio associates with virtual environments may in fact be the very condition which opens up the possibility for a more immediate and radical recontextualisation of hate speech in such spaces.        In positing cyberspace as a radically discontinuous space, Virilio implies that cyberspace may represent a break with history itself. One need only consider how quickly existing forms of inequity have become entrenched in cyberspace to understand the extent to which it is not an entirely new or autonomous space. It follows then that the pattern of perlocutionary effects which apparently enables certain words to injure is not disrupted simply because such words are circulating in a virtual space. However, this is not to suggest that subordinate speech necessarily acts identically on line and off. If to be injured through language is to """"suffer a loss of context, that is, not to know where you are"""" (Butler 4), what might it mean to suffer a loss of context in cyberspace? What might it mean to become disoriented in a space where one's context is always and already destabilised? And, what sorts of possibilities are created in cyberspace when one is thrust into an ill-fitting or undesirable social location? I maintain that in cyberspace the potential to suffer a loss of context as a result of a linguistic injury is always partially foreclosed by the fact that one never knows precisely where one is to begin with. It follows that in cyberspace the intended victim of a verbal assault is also at least less likely to become disarmed, debilitated, and silenced. Without overemphasising the agency one gains in virtual environments, is it not possible that one's ability to """"talk back"""", while not guaranteed, may be made significantly more likely, if only due to the fact that one's rhetorical skills are unlikely to be rendered worthless in the face of physical threats?        To illustrate the extent to which the illocutionary force of hate speech may be undermined by the spatial and temporal character of virtual environments, I draw attention to Reverend Phelps's """"godhatesfags.com"""" site. Once you move past the yellow construction sign reading """"Warning -- Gospel Preaching Ahead"""", you discover a list of Bible Passages which apparently confirm the site's claim that """"god hates fags"""". The site also contains a list of so-called """"fag churches"""" and a variety of news items that draw attention to the supposed dangers of the growing global gay agenda. However, while the Website is clearly offensive, it also seems to produce the possibility for politically promising misinterpretations. The Website's editorials on subjects as diverse as Canada's """"gay Mafia"""" and Finland's allegedly lesbian Prime Minister and news about Phelps's intention to carry out """"missionary work"""" in both of these demonic nations seems more likely to amuse than offend many of the site's visitors. The site's tendency to misinterpellate everyone from lesbians to P-FLAG mothers as """"fags"""" may be read as another amusing misfire, another failed attempt on the part of Phelps to demonise gays, lesbians, and their supporters.        While Phelps's claims are bound to misfire in any context, the ability to find Phelps's claims immediately humourous appears to be at least partially linked to their context. People who seek to prohibit hate speech not only on the basis of what it says but also on the basis of what it does typically maintain that the effects of hate speech are immediate, final, and ultimately, debilitating (Langston; MacKinnon; Matsuda et al.). In cyberspace, such claims appear to be even less easily established than in the material world. As previously argued, the speed associated with virtual environments seems to produce a disorienting effect, making the potential to suffer a """"loss of context"""" in the face of a linguistic """"attack"""" at least less likely. In addition, in contrast to the material world, where an encounter with hate speech is likely to lead to a feeling of entrapment if not complete debilitation, cyberspace invests people with an unprecedented degree of mobility. As a result, in sharp contrast to the experience one might have encountering Phelps's message in a public space near their home, the person who encounters Phelps's message online, where neither proximity nor distance hold their traditional values, can escape both quickly and with little effort.        However, it is also important to consider the extent to which the speed associated with virtual spaces may also affect the pace at which potentially injurious words, images, and ideas are recirculated and recontextualised. Building on Butler, I have emphasised that hate speech is always repeatable speech. If we take for granted the fact that cyberspace not only increases the amount of speech generated but also the rate at which such speech is recontextualised and redeployed, it would appear as if the potential exists both for the amount of hate speech in circulation to increase and for such speech to be repeated more often and more rapidly. If we further accept the claim that the repetition of hate speech is always an imperfect one, bound to result in some loss of meaning or minor transgression, it becomes possible to see how this highly indeterminable context may also enable hate speech to be reclaimed more quickly. Once again, """"godhatesfags.com"""" serves as a useful example.       Shortly after Phelps's Website appeared, online monitoring organisations, including Hate Watch, established direct links to the site. The link between Hate Watch and """"godhatesfags.com"""" not only serves to place Phelps's online activities under surveillance but also to recontextualise the site. Viewed through their link, Phelps's site is quite literally framed by the Hate Watch site, and subsequently, recontextualised by their anti-hate discourse in a surprisingly direct manner. In addition, various features of Phelps's site have also been parodied and appropriated. """"Godhatesfigs.com"""", which among other features includes a list of bible passages which allegedly confirm the Website's claim that """"god hates figs"""", is one such example. The creators of """"Godlovesfags.com"""" gained notoriety when they managed to steal Phelps's domain name and redirect all """"godhatesfags"""" visitors to their counter site for seventy-two hours. """"Godhatesphelps"""" is another site that seeks to parody and repeat aspects of Phelps's message in an effort to reveal the absurd nature of Phelps's claims.               Shortly after Phelps's Website appeared, online monitoring organisations, including Hate Watch, established direct links to the site. The link between Hate Watch and """"godhatesfags.com"""" not only serves to place Phelps's online activities under surveillance but also to recontextualise the site. Viewed through their link, Phelps's site is quite literally framed by the Hate Watch site, and subsequently, recontextualised by their anti-hate discourse in a surprisingly direct manner. In addition, various features of Phelps's site have also been parodied and appropriated. """"Godhatesfigs.com"""", which among other features includes a list of bible passages which allegedly confirm the Website's claim that """"god hates figs"""", is one such example. The creators of """"Godlovesfags.com"""" gained notoriety when they managed to steal Phelps's domain name and redirect all """"godhatesfags"""" visitors to their counter site for seventy-two hours. """"Godhatesphelps"""" is another site that seeks to parody and repeat aspects of Phelps's message in an effort to reveal the absurd nature of Phelps's claims.                   References          Austin, J.L. How to Do Things with Words. 15th ed. Cambridge: Harvard UP, 1997. (Original work published in 1962.)               Butler, Judith. Excited Speech: A Politics of the Performative. New York: Routledge, 1997.         Langston, Rae. Speech Acts and Unspeakable Acts. Philosophy and Public Affairs 22 (1993): 293-330.                 Matsuda, M.J., et al. Words That Wound: Critical Race Theory, Assaultive Speech, and the First Amendment. San Francisco: Westview Press, 1993.                   MacKinnon, C. Only Words. Cambridge: Harvard UP, 1993.                   Virilio, Paul. """"Global Algorithm 1.7: The Silence of the Lambs: Paul Virilio in Conversation (with C.Oliveira)."""" CTHEORY 1995. 18 June 1999 &lt;http://www.ctheory.com/ga1.7-silence.php&gt;.                   Virilio, Paul. Open Sky. Trans. J. Rose. New York: Verso, 1997.                   Virilio, Paul. """"Speed and Information: Cyberspace Alarm!"""" CTHEORY 18.3 (1995). 18 June 1999 &lt;http://www.dds.nl/~n5m/texts/virilio.htm&gt;.                   Virilio, Paul. Speed and Politics: An Essay on Dromololgy. Trans. M. Polizzotti. New York: Semiotext(e), 1986.         Citation reference for this article         MLA style:         Kate Eichhorn. """"Cyberhate and Performative Speech in Accelerated Time(s)."""" M/C: A Journal of Media and Culture 3.3 (2000). [your date of access] &lt;http://www.api-network.com/mc/0006/cyberhate.php&gt;.         Chicago style:         Kate Eichhorn, """"Cyberhate and Performative Speech in Accelerated Time(s),"""" M/C: A Journal of Media and Culture 3, no. 3 (2000), &lt;http://www.api-network.com/mc/0006/cyberhate.php&gt; ([your date of access]).          APA style:         Kate Eichhorn. (2000) Cyberhate and performative speech in accelerated time(s). M/C: A Journal of Media and Culture 3(3). &lt;http://www.api-network.com/mc/0006/cyberhate.php&gt; ([your date of access]).           ","",""
"2002","Digital Representation: Racism on the World Wide Web"," This paper argues that the various rhetorical modes in which hate is expressed on the Web are tailored to the types of messages offered. The unique technologies of the Web, that differentiate it from the earlier media of communication, facilitate the various rhetorical modes. The Web, as an unregulated medium, fosters the worldwide dissemination of both 'actionable' and 'non-actionable' hate messages. The actionable hate messages, regardless of their intensity and potential to excite violent actions, are not legally restricted through any international censorship regulations; the power of restricting such messages is national, if such messages counter national laws and conventions. The questions explored here are: Does the Internet and the Web facilitate the spreading of hate messages? Should Internet hate materials be regulated? If so, how might that be done? What criteria should be used to differentiate between hate and non-hate materials? Is it possible to draw and enforce a line between hate and non-hate messages? What input would measures against hate messages have on the Internet culture itself? ","",""
"2013","Racist comments at online news sites: a methodological dilemma for discourse analysis"," In 2004, awash with the hope for a public sphere reinvigorated by the popular internet, the online arms of many U.S. newspapers opened their websites for comments. Now, nine years into this experiment, many newspapers have abandoned the practice of allowing comments. Online news sites have adopted a variety of strategies to deal with offensive comments, including turning “comments off,” not archiving comments, and adopting aggressive comment moderation policies. These strategies present researchers who wish to understand how racism operates in the new public sphere of mainstream news sites with a set of methodological dilemmas. In this article we (1) lay out the methodological pitfalls for the systematic investigation of the prevalent pattern of racism in online comments in the public sphere and (2) suggest steps by which scholars may deal with these methodological intricacies. We conclude by pointing to the broader implications of online content moderation. ","",""
"2015","#Hashtagging hate: Using Twitter to track racism online","This paper considers three different projects that have used Twitter to track racist language: 1) Racist Tweets in Canada (the author’s original work); 2) Anti-social media (a 2014 study by U.K. think tank DEMOS); and, 3) The Geography of Hate Map (created by researchers at Humboldt University) in order to showcase the ability to track racism online using Twitter. As each of these projects collected racist language on Twitter using very different methods, a discussion of each data collection method used as well as the strengths and challenges of each method is provided. More importantly, however, this paper highlights why Twitter is an important data collection tool for researchers interested in studying race and racism.","",""
"2015","Histories of Hating"," This roundtable discussion presents a dialogue between digital culture scholars on the seemingly increased presence of hating and hate speech online. Revolving primarily around the recent #GamerGate campaign of intensely misogynistic discourse aimed at women in video games, the discussion suggests that the current moment for hate online needs to be situated historically. From the perspective of intersecting cultural histories of hate speech, discrimination, and networked communication, we interrogate the ontological specificity of online hating before going on to explore potential responses to the harmful consequences of hateful speech. Finally, a research agenda for furthering the historical understandings of contemporary online hating is suggested in order to address the urgent need for scholarly interventions into the exclusionary cultures of networked media. ","",""
"2016","Hate Speech and Covert Discrimination on Social Media: Monitoring the Facebook Pages of Extreme-Right Political Parties in Spain","This study considers the ways that overt hate speech and covert discriminatory practices circulate on Facebook despite its official policy that prohibits hate speech. We argue that hate speech and discriminatory practices are not only explained by users’ motivations and actions, but are also formed by a network of ties between the platform’s policy, its technological affordances, and the communicative acts of its users. Our argument is supported with longitudinal multimodal content and network analyses of data extracted from official Facebook pages of seven extreme-right political parties in Spain between 2009 and 2013. We found that the Spanish extreme-right political parties primarily implicate discrimination, which is then taken up by their followers who use overt hate speech in the comment space.","",""
"2018","Inciting anger through Facebook reactions in Belgium: The use of emoji and related vernacular expressions in racist discourse","This article uses the concept of ‘platformed racism‘ to explore the tension between how platforms afford and govern emoji, and how users appropriate them to engage in racist discourse. The paper takes the example of Belgian far-right political party Vlaams Belang’s use of Facebook Reactions to spread anger, and how audiences responded to this call by posting more emoji to express rage and engage in long-running racist tropes. Emoji are central elements of social media and its practices, and represent an opportunity to investigate the material politics of platforms and to explore their role in racist discourse.","",""
"2018","“I see your garbage”: Participatory practices and literacy privilege on “Grammar Nazi” Facebook pages in different sociolinguistic contexts"," In contemporary online culture, Grammar Nazi (GN) is a derogatory term used to label individuals who practice excessive language policing but has also been ironically appropriated by groups of users who engage in evaluation of other people’s grammar for entertainment purposes. In this article, we combine approaches from media studies and sociolinguistics to analyze the adoption of the phenomenon by two GN Facebook pages in two languages: English and Czech. Our mixed-method analysis shows that while both pages can be read as examples of media participation, they also exemplify their users’ “literacy privilege” associated with standard language ideology. However, there are differences in the practices associated with the label, reflecting the specific sociolinguistic contexts. While Czech GNs act as “guardians” of the public space, collecting and displaying localized orthographic errors for collective dissection, the English page is more dedicated to sharing jokes and puns typical of international online culture. ","",""
"2018","The Communicative Constitution of Hate Organizations Online: A Semantic Network Analysis of “Make America Great Again”"," In the context of the 2016 U.S. Presidential Election, President Donald Trump’s use of Twitter to connect with followers and supporters created unprecedented access to Trump’s online political campaign. In using the campaign slogan, “Make America Great Again” (or its acronym “MAGA”), Trump communicatively organized and controlled media systems by offering his followers an opportunity to connect with his campaign through the discursive hashtag. In effect, the strategic use of these networks over time communicatively constituted an effective and winning political organization; however, Trump’s political organization was not without connections to far-right and hate groups that coalesced in and around the hashtag. Semantic network analyses uncovered how the textual nature of #MAGA organized connections between hashtags, and, in doing so, exposed connections to overtly White supremacist groups within the United States and the United Kingdom throughout late November 2016. Cluster analyses further uncovered semantic connections to White supremacist and White nationalist groups throughout the hashtag networks connected to the central slogan of Trump’s presidential campaign. Theoretically, these findings contribute to the ways in which hashtag networks show how Trump’s support developed and united around particular organizing processes and White nationalist language, and provide insights into how these networks discursively create and connect White supremacists’ organizations to Trump’s campaign. ","",""
"2019","Extreme Speech and Global Digital Cultures — Introduction","In this article, we introduce the Special Section on Extreme Speech and Global Digital Cultures by developing the concept of “extreme speech.” In addressing the growing cultures of online vitriol and extremism, this concept advances a critical ethnographic sensibility to situated online speech cultures and a comparative global conversation that moves beyond the legal-normative debates that have been dominant in North America and Europe. We demonstrate this intervention by highlighting three interlinked arguments: Extreme speech inhabits a spectrum of practices rather than a binary opposition between acceptable and unacceptable speech; the sociotechnological aspects of new media embody a context in itself; and the violence of extreme speech acts is productive of identity in historically specific ways. This approach entails a methodological move that takes account of the meanings online users attach to vitriol as historical actors. It thus allows for critical frameworks to emerge from emic terms of action rather than moral concepts superimposed from the outside. Ethnographic explorations of extreme speech, we suggest, open up a new avenue to critique the contemporary global conjuncture of exclusionary politics.","",""
"2019","Extreme Speech| Defining Online Hate and Its “Public Lives”: What is the Place for “Extreme Speech”?","Following Sahana Udupa and Matti Pohjonen's invitation to move the debate beyond a normative understanding of hate speech, this article seeks to build a foundation for conceptual and empirical inquiry of speech commonly considered deviant and disturbing. It develops in three stages. It first maps the public lives of terms that refer to online vitriol and how they have been used by different communities of researchers, politicians, advocacy groups, and national organizations. Second, it shows how different types of “haters” have been interpreted as parts of “swarms” or “armies,” depending on whether their violent potential emerges around critical incidents or whether they respond to longer-term strategies through which communities and their leaders tie their speech acts to explicit narratives. The article concludes by locating “extreme speech” within this broader conceptual tapestry, arguing that the paternalistic gaze that characterizes a lot of research on online hate speech is tied to what Chantal Mouffe has referred to as the “moralization of politics,” a phenomenon that cannot be matched by responses that are themselves moral.","",""
"2019","Extreme Speech| The Digital Traces of #whitegenocide and Alt-Right Affective Economies of Transgression","This article explores how the notion of “extreme speech” can advocate a context-specific, practice-oriented approach to alt-right digital culture while also foregrounding its imbrication in larger histories of racial formation. Designating the popular White-nationalist hashtag #whitegenocide as an alt-right structure of feeling, it uses a data-critical discourse on “digital traces” to support a form of social media ethnography that traces affective communication practices online. Bringing this framework to the analysis of top #whitegenocide retweets, it elaborates the functioning of alt-right affective economies of transgression, which, driven by reactionary irony and a sense of race-based threat, contribute to shaping civil discourse and defining Whiteness in digital spaces. Finally, it investigates how the locative and corporal traces left by individual #whitegenocide retweeters both shape and are shaped by larger affective economies of transgression.","",""
"2019","Extreme Speech| Extreme Speech and Global Digital Cultures — Introduction","In this article, we introduce the Special Section on Extreme Speech and Global Digital Cultures by developing the concept of “extreme speech.” In addressing the growing cultures of online vitriol and extremism, this concept advances a critical ethnographic sensibility to situated online speech cultures and a comparative global conversation that moves beyond the legal-normative debates that have been dominant in North America and Europe. We demonstrate this intervention by highlighting three interlinked arguments: Extreme speech inhabits a spectrum of practices rather than a binary opposition between acceptable and unacceptable speech; the sociotechnological aspects of new media embody a context in itself; and the violence of extreme speech acts is productive of identity in historically specific ways. This approach entails a methodological move that takes account of the meanings online users attach to vitriol as historical actors. It thus allows for critical frameworks to emerge from emic terms of action rather than moral concepts superimposed from the outside. Ethnographic explorations of extreme speech, we suggest, open up a new avenue to critique the contemporary global conjuncture of exclusionary politics.","",""
"2019","Extreme Speech| A Comparative Approach to Social Media Extreme Speech: Online Hate Speech as Media Commentary","By exploring lessons learned from Ethiopia and Finland, this article challenges two assumptions about online hate speech research. First, it challenges the assumption that the best way to understand controversial concepts such as online hate speech is to determine how closely they represent or mirror some underlying set of facts or state of affairs online or in social media. Second, it challenges the assumption that academic research should be seen as separate from the many controversies that surround online hate speech debates globally. In its place, the article proposes the theory of “commentary” as a comparative research framework aimed at explaining how the messy and complex world of online and social media practices is articulated as hate speech over other ways of imagining this growing problem in global digital media environments.","",""
"2019","Extreme Speech| Ritualized Opposition in Danish Practices of Extremist Language and Thought","This article looks at extreme speech practices in Danish weblogs and Facebook comment threads that treat issues of refugees, migration, Islam, and opponents as a cultural war of values and conflict. The article highlights the ritualized ways in which anti-immigrant sentiments are being communicated, received, and responded to. Such recurrent ritualistic communicative patterns include the use of a distinct indignant tone, sarcasm, racialized reasoning, and the use of “high-fives,” as well as a general indifference to facts. The article argues that these online speech patterns can best be understood as a form of “ritualized opposition” that relies on extremist, divisive use of language and a naturalization of racialized difference in its attempt to recruit and consolidate communities of support.","",""
"2019","Visualizing YouTube’s comment space: online hostility as a networked phenomena"," This study examines YouTube’s comment space. By focusing on responses to the provocative musical group, Das Racist, we offer an innovative analysis of online racialized expression as a networked phenomenon. A blend of social network analysis, qualitative coding, and thick data descriptive methods are used to interpret comments posted on the five most viewed Das Racist videos. Given the dearth of literature exploring YouTube’s comment space, this study serves as a critical means to further understand race and the production and consumption of YouTube comments in everyday online encounters. We visualized networked antagonisms, which were found to be significantly racialized, and entangled with other expressions of hostility. YouTube comments are often perceived as individual, random insults or only generalized expressions of “hate.” Our study probes deeper and discovers that racialized expressions also involved networked interactions, where hostile ideas, passed through multiple parts of the comment network, both intra-/inter-video. ","",""
"2020","Trust Me, I’m Trolling: Irony and the Alt-Right’s Political Aesthetic","In August 2017, a white supremacist rally marketed as “Unite the Right” was held in Charlottesville, Virginia. In participation were members of the alt-right, including neo-nazis, white nationalists, neo-confederates, and other hate groups (Atkinson). The rally swiftly erupted in violence between white supremacists and counter protestors, culminating in the death of a counter-protester named Heather Heyer, who was struck by a car driven by white supremacist James Alex Fields, and leaving dozens injured. Terry McQuliffe, the Governor of Virginia, declared a state of emergency on August 12, and the world watched while white supremacists boldly marched in clothing emblazoned with symbols ranging from swastikas to a cartoon frog (Pepe), with flags featuring the nation of “Kekistan”, and carrying tiki torches chanting, “You Will Not Replace Us... Jews Will Not Replace Us”.The purpose of this essay is not, however, to examine the Internet symbols that circulated during the Unite the Right rally but rather to hone in on a specific moment that illustrates a key part of Internet culture that was often overlooked during analysis of the events that occurred during the riots: a documentary filmmaker, C. J. Hunt, was at the rally to record footage for a project on the removal of Confederate monuments. While there, he saw a rally-goer dressed in the white polo t-shirt and khaki pants uniform of the white nationalist group Vanguard America. The rally-goer, a young white man, was being chased by a counter-protester. He began to scream and beg for mercy, and even went as far as stripping off his clothing and denying that he really believed in any of the group’s ideology. In the recording by Hunt, who asks why he was there and why he was undressing, the young white man responded that shouting white power is “fun”, and that he was participating in the event because he, quote, “likes to be offensive” (Hunt).As Hunt notes in a piece for GQ reflecting on his experience at the rally, as soon as the man was cut off from his group and confronted, the runaway racist’s demeanor immediately changed when he had to face the consequences of his actions. Trolls often rely on the safety and anonymity of online forums and digital spaces where they are often free from having to face the consequences of their actions, and for the runaway racist, things became real very quickly when he was forced to own up to his hateful actions. In a way, many members of these movements seem to want politics without consequence for themselves, but with significant repercussions for others. Milo Yiannopoulos, a self-professed “master troll”, built an entire empire worth millions of dollars off of what the far-right defends as ironic hate speech and a form of politics without consequences reserved only for the privileged white men that gleefully engage in it. The runaway racist and Yiannopoulos are borne out of an Internet culture that is built on being offensive, on trolling, and “troll” itself being an aspirational label and identity, but also more importantly, a political aesthetic.In this essay, I argue that trolling itself has become a kind of political aesthetic and identity, and provide evidence via examples like hoaxes, harassment campaigns, and the use of memes to signal to certain online populations and extremist groups in violent attacks. First coined by Walter Benjamin in order to explain a fundamental component of using art to foster consent and compliance in fascist regimes, the term since then has evolved to encompass far more than just works of art. Benjamin’s original conception of the term is in regard to a creation of a spectacle that prevents the masses from recognizing their rights – in short, the aestheticization of politics is not just about the strategies of the fascist regimes themselves but says more about the subjects within them. In the time of Benjamin’s writing, the specific medium was mass propaganda through the newly emerging film industry and other forms of art (W. Benjamin). To Benjamin, these aesthetics served as tools of distracting to make fascism more palatable to the masses. Aesthetic tools of distraction serve an affective purpose, revealing the unhappy consciousness of neoreactionaries (Hui), and provide an outlet for their resentment.Since political aesthetics are concerned with how cultural products like art, film, and even clothing reflect political ideologies and beliefs (Sartwell; McManus; Miller-Idriss), the objects of analysis in this essay are part of the larger visual culture of the alt-right (Bogerts and Fielitz; Stanovsky). Indeed, aesthetic aspects of political systems shift their meaning over time, or are changed and redeployed with transformed effect (Sartwell). In this essay, I am applying the concept of the aestheticization of politics by analyzing how alt-right visual cultures deploy distraction and dissimulation to advance their political agenda through things like trolling campaigns and hoaxes. By analyzing these events, their use of memes, trolling techniques, and their influence on mainstream culture, what is revealed is the influence of trolling on political culture for the alt-right and how the alt-right then distracts the rest of the public (McManus).Who’s Afraid of the Big Bad Troll?Large scale analyses of disinformation and extremist content online tends to examine how certain actors are connected, what topics emerge and how these are connected across platforms, and the ways that disinformation campaigns operate in digital environments (Marwick and Lewis; Starbird; Benkler et al.). Masculine and white-coded technology gave rise to male-dominated digital spaces (R. Benjamin), with trolling often being an issue faced by non-normative users of the Internet and their communities (Benjamin; Lumsden and Morgan; Nakamura; Phillips, Oxygen). Creating a kind of unreality where it is difficult to parse out truth from lies, fiction from non-fiction, the troll creates cultural products, and by hiding behind irony and humor confuses onlookers and is removed from any kind of reasonable blame for their actions. Irony has long been a rhetorical strategy used in politics, and the alt right has been no exception (Weatherby), but for our current sociopolitical landscape, trolling is a political strategy that infuses irony into politics and identity.In the digital era, political memes and internet culture are pervasive components of the spread of hate speech and extremist ideology on digital platforms. Trolling is not an issue that exists in a vacuum – rather, trolls are a product of greater mainstream culture that encourages and allows their behaviors (Phillips, This Is Why; Fichman and Sanfilippo; Marwick and Lewis). Trolls, and meme culture in general, have often been pointed to as being part of the reason for the rise of Trump and fascist politics across the world in recent years (Greene; Lamerichs et al.; Hodge and Hallgrimsdottir; Glitsos and Hall). Although criticism has been expressed about how impactful memes were in the election of Donald Trump, political memes have had an impact on the ways that trolling went from anonymous jerks on forums to figures like Yiannapoulos who built entire careers off of trolling, creating empires of hate (Lang). These memes that are often absurd and incomprehensible to those who are not a part of the community that they come from aim to cheapen, trivialize, and mock social justice movements like Black Lives Matter, feminism, LGBTQ+ rights, and others.But the history of trolling online goes as far back as the Internet itself. “Trolling” is just a catch all term to describe online behaviors meant to antagonize, to disrupt online conversations, and to silence other users (Cole; Fichman and Sanfilippo). As more and more people started moving online and engaging in participatory culture, trolling continued to evolve from seemingly harmless jokes like the “Rick Roll” to targeted campaigns meant to harass women off of social media platforms (Lumsden and Morgan; Graham). Trolling behaviors are more than just an ugly part of the online experience, but are also a way for users to maintain the borders of their online community - it’s meant to drive away those who are perceived to be outsiders not just from the specific forum, but the Internet itself (Graham). With the rise of modern social media platforms, trolling itself is also a part of the political landscape, creating a “toxic counterpublic” that combines irony with a kind of earnestness to spread and inject their beliefs into mainstream political discourse (Greene). As a mode of information warfare, these subversive rhetorical strategies meant to contradict or reverse existing political and value systems have been used throughout history as a political tactic (Blackstock).The goal of trolling is not just to disrupt conversations, but to lead to chaos via confusion about the sincerity and meaning of messages and visuals, and rather than functioning as a politics of outrage (on the part of the adherents), it is a politics of being as outrageous as possible. As a part of larger meme culture, the aesthetics of trolls and their outrageous content manage to operate under the radar by being able to excuse their behaviors and rhetoric as just “trolling” or “joking”. This ambiguity points to trolling on the far right as a political strategy and identity to absolve them of blame or accusations of what their real intentions are. Calling them “trolls” hides the level of sophistication and vast levels of influence that they had on public opinion and discourse in the United States (Geltzer; Starks et al.; Marwick and Lewis). We no longer live in a world apart from the troll’s influence and immune from their toxic discourse – rather, we have long been under the bridge with them.Co-Opted SymbolsOne of the most well-known examples of trolling as a political aesthetic and tactic may be the OK hand sign used by the Christchurch shooter. The idea that the OK hand sign was a secretly white supremacist symbol started as a hoax on 4chan. The initial 2017 hoax purported that the hand sign was meant to stand for “White Power”, with the three fingers representing the W and the circle made with the index finger and thumb as the P (Anti-Defamation League, “Okay Hand Gesture”). The purpose of perpetuating the hoax was to demonstrate that (a) they were being watched and (b) that the mainstream media is stupid and gullible enough to believe this hoax. Meant to incite confusion and to act as a subversive strategy, the OK hand sign was then actually adopted by the alt-right as a sort of meme to not just perpetuate the hoax, but to signal belonging to the larger group (Allyn). Even though the Anti-Defamation League initially listed it as not being a hate symbol and pointed out the origins of the hoax (Anti-Defamation League, “No, the ‘OK’ Gesture Is Not a Hate Symbol”), they then switched their opinion when the OK hand sign was being flashed by white supremacists, showing up in photographs at political events, and other social media content. In fact, the OK hand sign is also a common element in pictures of Pepe the Frog, who is a sort of “alt right mascot” (Tait; Glitsos and Hall), but like the OK hand sign, Pepe the Frog did not start as an alt-right mascot and was co-opted by the alt-right as a mode of representation.The confusion around the actual meaning behind the hand symbol points to how the alt-right uses these modes of representation in ways that are simultaneously an inside joke and a real expression of their beliefs. For instance, the Christchurch shooter referenced a number of memes and other rhetoric typical of 4chan and 8chan communities in his video and manifesto (Quek). In the shooter’s manifesto and video, the vast amounts of content that point to the trolling and visual culture of the alt-right are striking – demonstrating how alt-right memes not only make this violent ideology accessible, but are cultural products meant to be disseminated and ultimately, result in some kind of action (DeCook).The creation and co-optation of symbols by the alt-right like the OK hand sign are not just memes, but a form of language created by extremists for extremists (Greene; Hodge and Hallgrimsdottir). The shooter’s choice of including this type of content in his manifesto as well as certain phrases in his live-streamed video indicate his level of knowledge of what needed to be done for his attack to get as much attention as possible – the 4chan troll is the modern-day bogeyman, and parts of the manifesto have been identified as intentional traps for the mainstream media (Lorenz).Thus, the Christchurch shooter and trolling culture are linked, but referring to the symbols in the manifesto as being a part of “trolling” culture misses the deeper purpose – chaos, through the outrage spectacle, is the intended goal, particularly by creating arguments about the nature and utility of online trolling behavior. The shooter encouraged other 8chan users to disseminate his posted manifesto as well as to share the video of the attack – and users responded by immortalizing the event in meme format. The memes created celebrated the shooter as a hero, and although Facebook did remove the initial livestream video, it was reuploaded to the platform 1.2 million times in the first 24 hours, attempting to saturate the online platform with so many uploads that it would cause confusion and be difficult to remove (Gramenz). Some users even created gifs or set the video to music from the Doom video game soundtrack – a video game where the player is a demon slayer in an apocalyptic world, further adding another layer of symbolism to the attack.These political aesthetics – spread through memes, gifs, and “fan videos” – are the perfect vehicles for disseminating extremist ideology because of what they allow the alt-right to do with them: hide behind them, covering up their intentions, all the while adopting them as signifiers for their movement. With the number of memes, symbols, and phrases posted in his manifesto and spoken aloud in his mainstream, perhaps the Christchurch shooter wanted the onus of the blame to fall on these message board communities and the video games and celebrities referenced – in effect, it was “designed to troll” (Lorenz). But, there is a kernel of truth in every meme, post, image, and comment – their memes are a part of their political aesthetic, thus implicit and explicit allusions to the inner workings of their ideology are present. Hiding behind hoaxes, irony, edginess, and trolling, members of the alt-right and other extremist Internet cultures then engage in a kind of subversion that allows them to avoid taking any responsibility for real and violent attacks that occur as a result of their discourse. Antagonizing the left, being offensive, and participating in this outrage spectacle to garner a response from news outlets, activists, and outsiders are all a part of the same package.Trolls and the Outrage SpectacleThe confusion and the chaos left behind by these kinds of trolling campaigns and hoaxes leave many to ask: How disingenuous is it? Is it meant for mere shock value or is it really reflective of the person’s beliefs? In terms of the theme of dissimulation for this special issue, what is the real intent, and under what pretenses should these kinds of trolling behaviors be understood? Returning to the protestor who claimed “I just like to be offensive”, the skepticism from onlookers still exists: why go so far as to join an alt-right rally, wearing the uniform of Identity Evropa (now the American Identity Movement), as a “joke”?Extremists hide behind humor and irony to cloud judgments from others, begging the question of can we have practice without belief? But, ultimately, practice and belief are intertwined – the regret of the Runaway Racist is not because he suddenly realized he did not “believe”, but rather was forced to face the consequences of his belief, something that he as a white man perhaps never really had to confront. The cultural reach of dissimulation, in particular hiding true intent behind the claim of “irony”, is vast - YouTuber Pewdiepie claimed his use of racial and anti-Semitic slurs and putting on an entire Ku Klux Klan uniform in the middle of a video were “accidental” only after considerable backlash (Picheta). It has to be noted, however, that Pewdiepie is referenced in the manifesto of the Christchurch shooter – specifically, the shooter yelled during his livestream “subscribe to Pewdiepie”, (Lorenz). Pewdiepie and many other trolls, once called out for their behavior, and regardless of their actual intent, double down on their claims of irony to distract from the reality of their behaviors and actions.The normalization of this kind of content in mainstream platforms like Twitter, YouTube, Facebook, and even Instagram show how 4chan and alt-right Internet culture has seeped out of its borders and exists everywhere online. This “coded irony” is not only enabled rhetorically due to irony’s slippery definition, but also digitally via these online media (Weatherby). The aesthetics of the troll are present in every single platform and are disseminated everywhere – memes are small cultural units meant to be passed on (Shifman), and although one can argue it was not memes alone that resulted in the rise of the alt-right and the election of Donald Trump, memes are a part of the larger puzzle of the political radicalization process. The role of the Internet in radicalization is so powerful and insidious because of the presentation of content – it is funny,  edgy, ironic, offensive, and outrageous. But these behaviors and attitudes are not just appealing to some kind of adolescent-like desire to push boundaries of what is and is not socially acceptable and/or politically incorrect (Marwick and Lewis), and calling it such clouds people’s perceptions of their level of sophistication in shaping political discourse.Memes and the alt-right are a noted phenomenon, and these visual cultures created by trolls on message boards have aided in the rise of the current political situation worldwide (Hodge and Hallgrimsdottir). We are well in the midst of a type of warfare based on not weapons and bodies, but information and data - in which memes and other elements of the far right’s political aesthetic play an important role (Molander et al.; Prier; Bogerts and Fielitz). The rise of the online troll as a political player and the alt-right are merely the logical outcomes of these systems.ConclusionThe alt-right’s spread was possible because of the trolling cultures and aesthetics of dissimulation created in message boards that predate 4chan (Kitada). The memes and inflammatory statements made by them serve multiple purposes, ranging from an intention to incite outrage among non-members of the group to signal group belonging and identity. In some odd way, if people do not understand the content, the content actually speaks louder and, in more volumes, that it would if its intent was more straightforward – in their confusion, people give these trolling techniques more attention and amplification in their attempt to make sense of them. Through creating confusion, distraction, and uncertainty around the legitimacy of messages, hand signs, and even memes, the alt-right has elevated the aestheticization of politics to a degree that Walter Benjamin could perhaps not have predicted in his initial lament about the distracted masses of fascist regimes (McManus). The political dimensions of trolling and the cognitive uncertainty that it creates is a part of its goal. Dismissing trolls is no longer an option, but also regarding them as sinister political operatives may be overblowing their significance. In the end, “ironic hate speech” is still hate speech, and by couching their extremist ideology in meme format they make their extremist beliefs more palatable -- and nobody is completely immune to their strategies.ReferencesAllyn, Bobby. “The ‘OK’ Hand Gesture Is Now Listed as a Symbol of Hate.” NPR 2019. &lt;https://www.npr.org/2019/09/26/764728163/the-ok-hand-gesture-is-now-listed-as-a-symbol-of-hate&gt;.Anti-Defamation League. “No, the ‘OK’ Gesture Is Not a Hate Symbol.” Anti-Defamation League. 10 Dec. 2017 &lt;https://www.adl.org/blog/no-the-ok-gesture-is-not-a-hate-symbol&gt;.———. “Okay Hand Gesture.” Anti-Defamation League. 28 Feb. 2020 &lt;https://www.adl.org/education/references/hate-symbols/okay-hand-gesture&gt;.Atkinson, David C. “Charlottesville and the Alt-Right: A Turning Point?” Politics, Groups, and Identities 6.2 (2018): 309-15.Benjamin, Ruha. Race after Technology: Abolitionist Tools for the New Jim Code. Polity, 2019.Benjamin, Walter. The Work of Art in the Age of Mechanical Reproduction. CreateSpace Independent Publishing Platform, 1936.Benkler, Yochai, et al. Network Propaganda: Manipulation, Disinformation, and Radicalization in American Politics. Oxford: Oxford UP, 2018.Blackstock, Paul W. The Strategy of Subversion: Manipulating the Politics of Other Nations. Chicago: Quadrangle Books, 1964.Bogerts, Lisa, and Maik Fielitz. “Do You Want Meme War?”: Understanding the Visual Memes of the German Far Right. 2019.Cole, Kirsti K. “‘It’s Like She’s Eager to Be Verbally Abused’: Twitter, Trolls, and (En)Gendering Disciplinary Rhetoric.” Feminist Media Studies 15.2 (2015): 356-58.DeCook, Julia R. “Memes and Symbolic Violence: #Proudboys and the Use of Memes for Propaganda and the Construction of Collective Identity.” Learning, Media and Technology 43.4 (2018): 485-504.Douglas, Nick. “It’s Supposed to Look Like Shit: The Internet Ugly Aesthetic.” Journal of Visual Culture 13.3 (2014): 314-39.Fichman, Pnina, and Madelyn R. Sanfilippo. Online Trolling and Its Perpetrators: Under the Cyberbridge. Rowman &amp; Littlefield, 2016.Funke, Daniel. “When and How to Use 4chan to Cover Conspiracy Theories.” Poynter, 24 Sep. 2018. &lt;https://www.poynter.org/fact-checking/2018/when-and-how-to-use-4chan-to-cover-conspiracy-theories/&gt;.Geltzer, Joshua A. “Stop Calling Them ‘Russian Troll Farms’ - CNN.” CNN, 2018. &lt;https://www.cnn.com/2018/08/17/opinions/stop-calling-russian-operatives-troll-farms-geltzer/index.html&gt;.Glitsos, Laura, and James Hall. “The Pepe the Frog Meme: An Examination of Social, Political, and Cultural Implications through the Tradition of the Darwinian Absurd.” Journal for Cultural Research 23.4 (2019): 381-95.Graham, Elyse. “Boundary Maintenance and the Origins of Trolling.” New Media &amp; Society  (2019). doi:10.1177/1461444819837561.Gramenz, Jack. “Christchurch Mosque Attack Livestream: Why Facebook Continues to Fail.” New Zealand Herald 17 Feb. 2020. &lt;https://www.nzherald.co.nz/business/news/article.cfm?c_id=3&amp;objectid=12309116&gt;.Greene, Viveca S. “‘Deplorable’ Satire: Alt-Right Memes, White Genocide Tweets, and Redpilling Normies.” Studies in American Humor 5.1 (2019): 31–69.Hodge, Edwin, and Helga Hallgrimsdottir. “Networks of Hate: The Alt-Right, ‘Troll Culture’, and the Cultural Geography of Social Movement Spaces Online.” Journal of Borderlands Studies (2019): 1–18.Hui, Yuk. “On the Unhappy Consciousness of Neoreactionaries.” E-Flux 81 (2017). &lt;https://www.e-flux.com/journal/81/125815/on-the-unhappy-consciousness-of-neoreactionaries/&gt;.Hunt, C. J. “A Charlottesville White Supremacist Stripped Down to Escape Protesters and We Got It on Video.” GQ 2017. &lt;https://www.gq.com/story/charlottesville-white-supremacist-strips-to-escape-protestors&gt;.Kitada, Akihiro. “Japan’s Cynical Nationalism.” Fandom Unbound: Otaku Culture in a Connected World. Eds. Mizuko Ito et al. Yale UP, 2012: 68–84.Lamerichs, Nicolle, et al. “Elite Male Bodies: The Circulation of Alt-Right Memes and the Framing of Politicians on Social Media.” Participations 15.1 (2018): 180–206.Lang, Nico. “Trolling in the Name of ‘Free Speech’: How Milo Yiannopoulos Built an Empire off Violent Harassment.” Salon, 2016. &lt;http://www.salon.com/2016/12/19/trolling-in-the-name-of-free-speech-how-milo-yiannopoulos-built-an-empire-off-violent-harassment/&gt;.Lorenz, Taylor. “The Shooter’s Manifesto Was Designed to Troll.” The Atlantic, 15 Mar. 2019. &lt;https://www.theatlantic.com/technology/archive/2019/03/the-shooters-manifesto-was-designed-to-troll/585058/&gt;.Lumsden, Karen, and Heather Morgan. “Media Framing of Trolling and Online Abuse: Silencing Strategies, Symbolic Violence, and Victim Blaming.” Feminist Media Studies 17.6 (2017): 926–40.Marwick, Alice E., and Rebecca Lewis. “Media Manipulation and Disinformation Online.” Data &amp; Society, 2017. &lt;http://centerformediajustice.org/wp-content/uploads/2017/07/DataAndSociety_MediaManipulationAndDisinformationOnline.pdf&gt;.McManus, Matt. “Walter Benjamin and the Political Practices of the Alt-Right.” New Politics, 27 Dec. 2017. &lt;https://newpol.org/walter-benjamin-and-political-practices-altright/&gt;.Miller-Idriss, Cynthia. The Extreme Gone Mainstream: Commercialization and Far Right Youth Culture in Germany. Princeton UP, 2018.Molander, Roger C., et al. Strategic Information Warfare: A New Face of War. RAND Corporation, 1996. &lt;https://www.rand.org/pubs/monograph_reports/MR661.html&gt;.Nakamura, Lisa. Cybertypes: Race, Ethnicity, and Identity on the Internet. Routledge, 2002.Nissenbaum, Asaf, and Limor Shifman. “Internet Memes as Contested Cultural Capital: The Case of 4chan’s /b/ Board.” New Media &amp; Society 19.4 (2017): 483–501.Phillips, Whitney. The Oxygen of Amplification. Data &amp; Society, 2018. &lt;https://datasociety.net/output/oxygen-of-amplification&gt;.———. This Is Why We Can’t Have Nice Things: Mapping the Relationship between Online Trolling and Mainstream Culture. Cambridge, Mass.: MIT Press, 2015.Picheta, Rob. “PewDiePie Will Take a Break from YouTube, Saying He’s ‘Very Tired.’” CNN, 2019. &lt;https://www.cnn.com/2019/12/16/tech/pewdiepie-taking-break-youtube-scli-intl/index.html&gt;.Prier, Jarred. “Commanding the Trend: Social Media as Information Warfare.” Strategic Studies Quarterly 11.4 (2017): 50–85.Quek, Natasha. Bloodbath in Christchurch: The Rise of Far-Right Terrorism. 2019.Sartwell, Crispin. Political Aesthetics. Cornell UP, 2010.Shifman, Limor. Memes in Digital Culture. Cambridge, Mass.: MIT Press, 2014.Stanovsky, Derek. “Remix Racism: The Visual Politics of the ‘Alt-Right’.” Journal of Contemporary Rhetoric 7 (2017).Starbird, Kate. “Examining the Alternative Media Ecosystem through the Production of Alternative Narratives of Mass Shooting Events on Twitter.” International AAAI Conference on Web and Social Media (2017): 230–239. &lt;https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15603&gt;.Starks, Tim, Laurens Cerulus, and Mark Scott. “Russia’s Manipulation of Twitter Was Far Vaster than Believed.” Politico, 5 Jun. 2019. &lt;https://politi.co/2HXDVQ2&gt;.Tait, Amelia. “First They Came for Pepe: How ‘Ironic’ Nazism Is Taking Over the Internet.” New Statesman 16 Feb. 2017. &lt;http://www.newstatesman.com/science-tech/internet/2017/02/first-they-came-pepe-how-ironic-nazism-taking-over-internet&gt;.","",""
"2020","TRANSCODING BETWEEN HYPER-ANTAGONISTIC MILIEUS: STUDIES ON THE        CROSS-PLATFORM RELATIONS BETWEEN RADICAL POLITICAL WEB SUBCULTURES","This panel brings together research into the cross-platform relations between radical Web subcultures and how they are constitutive of “hyper-antagonistic” politics in broader Web discourses. The papers share a concern with vernacular practices of “fringe” platforms favoured by an insurgent far-right movement and their relations to more “mainstream” social media. They engage with the concept of “transcoding between milieus” (Deleuze &amp; Guattari 1987, 322) as a means to empirically describe multiple transversal processes across different strata of the Web in which “one milieu serves as the basis for another” (313). All papers ground their conceptual analysis in data-driven empirical approaches using historical datasets ranging from “mainstream” platforms like YouTube, to more “fringe” spaces like 4chan. The papers furthermore all use 4chan’s far-right /pol/ board as a reference point for a vernacular “hyper-antagonistic” style that emerged out of this period – a style that has often been related to the “alt-right”. Together, the four papers in this panel offer insights into the apparent insurgency of far-right subcultures within broader online discourse in the Anglo-American context over the course of the last half decade. Each does so with a particular focus, ranging from subcultural conflict between Tumblr and 4chan, the transcoding of the “Kekistan” meme between 4chan and YouTube, the emergence of far-right vernacular in the comments of Breitbart News, and the robustness of hyper-antagonistic discourse after deplatforming measures.","",""
"2020","Shades of hatred online: 4chan duplicate circulation surge during hybrid media events","The 4chan /pol/ platform is a controversial online space on which a surge in hate speech has been observed. While recent research indicates that events may lead to more hate speech, empirical evidence on the phenomenon remains limited. This study analyzes 4chan /pol/ user activity during the mass shootings in Christchurch and Pittsburgh and compares the frequency and nature of user activity prior to these events. We find not only a surge in the use of hate speech and anti-Semitism but also increased circulation of duplicate messages, links, and images and an overall increase in messages from users who self-identify as “white supremacist” or “fascist” primarily voiced from English-speaking IP-based locations: the U.S., Canada, Australia, and Great Britain. Finally, we show how these hybrid media events share the arena with other prominent events involving different agendas, such as the U.S. midterm elections. The significant increase in duplicates during the hybrid media events in this study is interpreted beyond their memetic logic. This increase can be interpreted through what we refer to as activism of hate. Our findings indicate that there is either a group of dedicated users who are compelled to support the causes for which shooting took place and/or that users use automated means to achieve duplication.","",""
"2020","Report and repeat: Investigating Facebook’s hate speech removal process","Social media is rife with hate speech. Although Facebook prohibits this content on its site, little is known about how much of the hate speech reported by users is actually removed by the company. Given the enormous power Facebook has to shape the universe of discourse, this study sought to determine what proportion of reported hate speech is removed from the platform and whether patterns exist in Facebook’s decision-making process. To understand how the company is interpreting and applying its own Community Standards regarding hate speech, the authors identified and reported hundreds of comments, posts, and images featuring hate speech to the company (n=311) and recorded Facebook’s decision regarding whether or not to remove the reported content. A qualitative content analysis was then performed on the content that was and was not removed to identify trends in Facebook’s content moderation decisions about hate speech. Of particular interest was whether the company’s 2018 policy update resulted in any meaningful change.Our results indicated that only about half of reported content containing hate speech was removed. The 2018 policy change also appeared to have little impact on the company’s decision-making. The results suggest that Facebook also had substantial issues including: removing misogynistic hate speech, establishing consistency in removing attacks and threats, an inability to consider context in removal decisions, and a general lack of transparency within the hate speech removal processes. Facebook’s failure to effectively remove reported hate speech allows misethnic discourses to spread and perpetuates stereotypes. The paper concludes with recommendations for Facebook and other social media organizations to consider to minimize the amount and impact of hate speech on their platforms.","",""
"2020","Social Media Posts About Racism Leads to Evaluative Backlash for Black Job Applicants"," Black Americans post about race and race-related issues on social media more than any other racial group. In this study, we investigated whether Black Americans who post about racism on social networking sites (i.e., Facebook) experience evaluative backlash during the employee selection process. Participants ( N = 154) were given a Black job candidate’s cover letter, resume, and a scanned printout of their social media. Depending on what condition they were randomly assigned to, the applicant’s social media contained posts about racism or posts that were race neutral. Results indicated that Black individuals whose posts were about racism were evaluated less favorably than Black individuals whose posts were race neutral. Specifically, they were perceived as being less likable. In addition, Black individuals whose social media posts were related to racism were less likely to be offered an interview for a job. Implications, limitations, and future directions are discussed. ","",""
"2020","Making a Microaggression: Using Big Data and Qualitative Analysis to Map the Reproduction and Disruption of Microaggressions through Social Media"," Racial microaggressions are defined as subtle racial slights that can be offensive or hurtful. One of the defining characteristics of racial microaggressions is how difficult they can be to respond to, and the literature reports that not responding may be the most common response to microaggressions. This study addresses a vital gap in the existing literature by examining the extent to which the silence that characterizes face-to-face experiences with microaggressions extends into online social media spaces. Drawing on a dataset of 254,964 tweets over an 8-year period, we present and examine trends in the usage of the term “microaggressions” over time. Furthermore, we then generate a purposive sample of 1,038 of the most influential tweets to explore discussions and content themes through an in-depth qualitative analysis of these messages. Here, we find both a drastic increase in the usage of the term microaggression on Twitter over time and an intense contestation over its meaning and repercussions for both individuals and society. Implications of these findings in understanding the role of online social media discourse in challenging or reproducing hegemonic racial structures is discussed. ","",""
"2020","Third-Person Effect and Hate Speech Censorship on Facebook"," By recruiting 368 US university students, this study adopted an online posttest-only between-subjects experiment to analyze the impact of several types of hate speech on their attitudes toward hate speech censorship. Results showed that students tended to think the influence of hate speech on others was greater than on themselves. Their perception of such messages’ effect on themselves was a significant indicator of supportive attitudes toward hate speech censorship and of their willingness to flag hateful messages. ","",""
"2020","Manufacturing Hate 4.0: Can Media Studies Rise to the Challenge?"," This article reflects on the growing scourge of hate speech and its propagation via digital social media networks. It discusses how media studies has drawn attention to salient aspects of online hate speech including technological affordances, communication tactics, representational tropes, and audience response. It argues that insights from media studies are vital for unpacking the societal impact of the media and indeed for tackling a destructive force such as online hate speech. It further encourages media studies scholars to engage vigorously with colleagues in and across other disciplines to forge interdisciplinary research collaborations to address pressing societal issues. It urges media studies scholars to connect with the realms of industry, policy making, and civic society to ensure that the public discourse on the challenges of digitalization and mediatization is academically informed, evidence-based, and finely balanced. ","",""
"2021","Zoom-ing in on White Supremacy","The Alt Right Are Not Alright  Academic explorations complicating both the Internet and whiteness have often focussed on the rise of the “alt-right” to examine the co-option of digital technologies to extend white supremacy (Daniels, “Cyber Racism”; Daniels, “Algorithmic Rise”; Nagle). The term “alt-right” refers to media organisations, personalities, and sarcastic Internet users who promote the “alternative right”, understood as extremely conservative, political views online. The alt-right, in all of their online variations and inter-grouping, are infamous for supporting white supremacy online, “characterized by heavy use of social media and online memes. Alt-righters eschew ‘establishment’ conservatism, skew young, and embrace white ethnonationalism as a fundamental value” (Southern Poverty Law Center). Theoretical studies of the alt-right have largely focussed on its growing presence across social media and websites such as Twitter, Reddit, and notoriously “chan” sites 4chan and 8chan, through the political discussions referred to as “threads” on the site (Nagle; Daniels, “Algorithmic Rise”; Hawley). As well, the ability of online users to surpass national boundaries and spread global white supremacy through the Internet has also been studied (Back et al.). The alt-right have found a home on the Internet, using its features to cunningly recruit members and to establish a growing community that mainstream politically extreme views (Daniels, “Cyber Racism”; Daniels, “Algorithmic Rise; Munn). This body of knowledge shows that academics have been able to produce critically relevant literature regarding the alt-right despite the online anonymity of the majority of its members. For example, Conway et al., in their analysis of the history and social media patterns of the alt-right, follow the unique nature of the Christchurch Massacre, encompassing the use and development of message boards, fringe websites, and social media sites to champion white supremacy online. Positioning my research in this literature, I am interested in contributing further knowledge regarding the alt-right, white supremacy, and the Internet by exploring the sinister conducting of Zoom-bombing anti-racist events. Here, I will investigate how white supremacy through the Internet can lead to violence, abuse, and fear that “transcends the virtual world to damage real, live humans beings” via Zoom-bombing, an act that is situated in a larger co-option of the Internet by the alt-right and white supremacists, but has been under theorised as a hate crime (Daniels; “Cyber Racism” 7). Shitposting  I want to preface this chapter by acknowledging that while I understand the Internet, through my own external investigations of race, power and the Internet, as a series of entities that produce racial violence both online and offline, I am aware of the use of the Internet to frame, discuss, and share anti-racist activism. Here we can turn to the work of philosopher Michel de Certeau who conceived the idea of a “tactic” as a way to construct a space of agency in opposition to institutional power. This becomes a way that marginalised groups, such as racialised peoples, can utilise the Internet as a tactical material to assert themselves and their non-compliance with the state. Particularly, shitposting, a tactic often associated with the alt-right, has also been co-opted by those who fight for social justice and rally against oppression both online and offline. As Roderick Graham explores, the Internet, and for this exploration, shitposting, can be used to proliferate deviant and racist material but also as a “deviant” byway of oppositional and anti-racist material. Despite this, a lot can be said about the invisible yet present claims and support of whiteness through Internet and digital technologies, as well as the activity of users channelled through these screens, such as the alt-right and their digital tactics. As Vikki Fraser remarks, “the internet assumes whiteness as the norm – whiteness is made visible through what is left unsaid, through the assumption that white need not be said” (120). It is through the lens of white privilege and claims to white supremacy that online irony, by way of shitposting, is co-opted and understood as an inherently alt-right tool, through the deviance it entails. Their sinister co-option of shitposting bolsters audacious claims as to who has the right to exist, in their support of white identity, but also hides behind a veil of mischief that can hide their more insidious intention and political ideologies. The alt-right have used “shitposting”, an online style of posting and interacting with other users, to create a form of online communication for a translocal identity of white nationalist members. Sean McEwan defines shitposting as “a form of Internet interaction predicated upon thwarting established norms of discourse in favour of seemingly anarchic, poor quality contributions” (19). Far from being random, however, I argue that shitposting functions as a discourse that is employed by online communities to discuss, proliferate, and introduce white supremacist ideals among their communities as well as into the mainstream. In the course of this article, I will introduce racist Zoom-bombing as a tactic situated in shitposting which can be used as a means of white supremacist discourse and an attempt to block anti-racist efforts. By this line, the function of discourse as one “to preserve or to reproduce discourse (within) a closed community” is calculatingly met through shitposting, Zoom-bombing, and more overt forms of white supremacy online (Foucault 225-226). Using memes, dehumanisation, and sarcasm, online white supremacists have created a means of both organising and mainstreaming white supremacy through humour that allows insidious themes to be mocked and then spread online. Foucault writes that “in every society the production of discourse is at once controlled, selected, organised and redistributed according to a certain number of procedures, whose role is to avert its powers and danger, to cope with chance events, to evade ponderous, awesome materiality” (216). As Philippe-Joseph Salazar recontextualises to online white supremacists, “the first procedure of control is to define what is prohibited, in essence, to set aside that which cannot be spoken about, and thus to produce strategies to counter it” (137). By this line, the alt-right reorganises these procedures and allocates a checked speech that will allow their ideas to proliferate in like-minded and growing communities. As a result, online white supremacists becoming a “community of discourse” advantages them in two ways: first, ironic language permits the mainstreaming of hate that allows sinister content to enter the public as the severity of their intentions is doubted due to the sarcastic language employed. Second, shitposting is employed as an entry gate to more serious and dangerous participation with white supremacist action, engagement, and ideologies. It is important to note that white privilege is embodied in these discursive practices as despite this exploitation of emerging technologies to further white supremacy, there are approaches that theorise the alt-right as “crazed product(s) of an isolated, extremist milieu with no links to the mainstream” (Moses 201). In this way, it is useful to consider shitposting as an informal approach that mirrors legitimised white sovereignties and authorised white supremacy. The result is that white supremacist online users succeed in “not only in assembling a community of actors and a collective of authors, on the dual territory of digital communication and grass-roots activism”, but also shape an effective fellowship of discourse that audiences react well to online, encouraging its reception and mainstreaming (Salazar 142). Continuing, as McBain writes, “someone who would not dream of donning a white cap and attending a Ku Klux Klan meeting might find themselves laughing along to a video by the alt-right satirist RamZPaul”. This idea is echoed in a leaked stylistic guide by white supremacist website and message board the Daily Stormer that highlights irony as a cultivated mechanism used to draw new audiences to the far right, step by step (Wilson). As showcased in the screen capture below of the stylistic guide, “the reader is at first drawn in by curiosity or the naughty humor and is slowly awakened to reality by repeatedly reading the same points” (Feinburg). The result of this style of writing is used “to immerse recruits in an online movement culture built on memes, racial panic and the worst of Internet culture” (Wilson).  Figure 1: A screenshot of the Daily Stormer’s playbook, expanding on the stylistic decisions of alt-right writers.  Racist Zoom-Bombing   In the timely text “Racist Zoombombing”, Lisa Nakamura et al. write the following:  Zoombombing is more than just trolling; though it belongs to a broad category of online behavior meant to produce a negative reaction, it has an intimate connection with online conspiracy theorists and white supremacy … . Zoombombing should not be lumped into the larger category of trolling, both because the word “trolling” has become so broad it is nearly meaningless at times, and because zoombombing is designed to cause intimate harm and terrorize its target in distinct ways. (30)  Notwithstanding the seriousness of Zoom-bombing, and to not minimise its insidiousness by understanding it as a form of shitposting, my article seeks to reiterate the seriousness of shitposting, which, in the age of COVID-19, Zoom-bombing has become an example of. I seek to purport the insidiousness of the tactical strategies of the alt-right online in a larger context of white violence online. Therefore, I am proposing a more critical look at the tactical use of the Internet by the alt-right, in theorising shitposting and Zoom-bombing as means of hate crimes wherein they impose upon anti-racist activism and organising. Newlands et al., receiving only limited exposure pre-pandemic, write that “Zoom has become a household name and an essential component for parties (Matyszczyk, 2020), weddings (Pajer, 2020), school and work” (1). However, through this came the strategic use of co-opting the application by the alt-right to digitise terror and ensure a “growing framework of memetic warfare” (Nakamura et al. 31). Kruglanski et al. label this co-opting of online tools to champion white supremacy operations via Zoom-bombing an example of shitposting:  Not yet protesting the lockdown orders in front of statehouses, far-right extremists infiltrated Zoom calls and shared their screens, projecting violent and graphic imagery such as swastikas and pornography into the homes of unsuspecting attendees and making it impossible for schools to rely on Zoom for home-based lessons. Such actions, known as “Zoombombing,” were eventually curtailed by Zoom features requiring hosts to admit people into Zoom meetings as a default setting with an option to opt-out. (128)  By this, we can draw on existing literature that has theorised white supremacists as innovation opportunists regarding their co-option of the Internet, as supported through Jessie Daniels’s work, “during the shift of the white supremacist movement from print to digital online users exploited emerging technologies to further their ideological goals” (“Algorithmic Rise” 63). Selfe and Selfe write in their description of the computer interface as a “political and ideological boundary land” that may serve larger cultural systems of domination in much the same way that geopolitical borders do (418). Considering these theorisations of white supremacists utilising tools that appear neutral for racialised aims and the political possibilities of whiteness online, we can consider racist Zoom-bombing as an assertion of a battle that seeks to disrupt racial justice online but also assert white supremacy as its own legitimate cause. My first encounter of local Zoom-bombing was during the Institute for Culture and Society (ICS) Seminar titled “Intersecting Crises” by Western Sydney University. The event sought to explore the concatenation of deeply inextricable ecological, political, economic, racial, and social crises. An academic involved in the facilitation of the event, Alana Lentin, live tweeted during the Zoom-bombing of the event:  Figure 2: Academic Alana Lentin on Twitter live tweeting the Zoom-bombing of the Intersecting Crises event. Upon reflecting on this instance, I wondered, could efforts have been organised to prevent white supremacy? In considering who may or may not be responsible for halting racist shit-posting, we can problematise the work of R David Lankes, who writes that “Zoom-bombing is when inadequate security on the part of the person organizing a video conference allows uninvited users to join and disrupt a meeting. It can be anything from a prankster logging on, yelling, and logging off to uninvited users” (217). However, this beckons two areas to consider in theorising racist Zoom-bombing as a means of isolated trolling. First, this approach to Zoom-bombing minimises the sinister intentions of Zoom-bombing when referring to people as pranksters. Albeit withholding the “mimic trickery and mischief that were already present in spaces such as real-life classrooms and town halls” it may be more useful to consider theorising Zoom-bombing as often racialised harassment and a counter aggression to anti-racist initiatives (Nakamura et al. 30). Due to the live nature of most Zoom meetings, it is increasingly difficult to halt the threat of the alt-right from Zoom-bombing meetings. In “A First Look at Zoom-bombings” a range of preventative strategies are encouraged for Zoom organisers including “unique meeting links for each participant, although we acknowledge that this has usability implications and might not always be feasible” (Ling et al. 1). The alt-right exploit gaps, akin to co-opting the mainstreaming of trolling and shitposting, to put forward their agenda on white supremacy and assert their presence when not welcome. Therefore, utilising the pandemic to instil new forms of terror, it can be said that Zoom-bombing becomes a new means to shitpost, where the alt-right “exploits Zoom’s uniquely liminal space, a space of intimacy generated by users via the relationship between the digital screen and what it can depict, the device’s audio tools and how they can transmit and receive sound, the software that we can see, and the software that we can’t” (Nakamura et al. 29). Second, this definition of Zoom-bombing begs the question, is this a fair assessment to write that reiterates the blame of organisers? Rather, we can consider other gaps that have resulted in the misuse of Zoom co-opted by the alt-right: “two conditions have paved the way for Zoom-bombing: a resurgent fascist movement that has found its legs and best megaphone on the Internet and an often-unwitting public who have been suddenly required to spend many hours a day on this platform” (Nakamura et al. 29). In this way, it is interesting to note that recommendations to halt Zoom-bombing revolve around the energy, resources, and attention of the organisers to practically address possible threats, rather than the onus being placed on those who maintain these systems and those who Zoom-bomb. As Jessie Daniels states, “we should hold the platform accountable for this type of damage that it's facilitated. It's the platform's fault and it shouldn't be left to individual users who are making Zoom millions, if not billions, of dollars right now” (Ruf 8). Brian Friedberg, Gabrielle Lim, and Joan Donovan explore the organised efforts by the alt-right to impose on Zoom events and disturb schedules: “coordinated raids of Zoom meetings have become a social activity traversing the networked terrain of multiple platforms and web spaces. Raiders coordinate by sharing links to Zoom meetings targets and other operational and logistical details regarding the execution of an attack” (14). By encouraging a mass coordination of racist Zoom-bombing, in turn, social justice organisers are made to feel overwhelmed and that their efforts will be counteracted inevitably by a large and organised group, albeit appearing prankster-like. Aligning with the idea that “Zoombombing conceals and contains the terror and psychological harm that targets of active harassment face because it doesn’t leave a trace unless an alert user records the meeting”, it is useful to consider to what extent racist Zoom-bombing becomes a new weapon of the alt-right to entertain and affirm current members, and engage and influence new members  (Nakamura et al. 34). I propose that we consider Zoom-bombing through shitposting, which is within “the location of matrix of domination (white supremacy, heteropatriarchy, ableism, capitalism, and settler colonialism)” to challenge the role of interface design and Internet infrastructure in enabling racial violence online (Costanza-Chock). Conclusion  As Nakamura et al. have argued, Zoom-bombing is indeed “part of the lineage or ecosystem of trollish behavior”, yet these new forms of alt-right shitposting “[need] to be critiqued and understood as more than simply trolling because this term emerged during an earlier, less media-rich and interpersonally live Internet” (32). I recommend theorising the alt-right in a way that highlights the larger structures of white power, privilege, and supremacy that maintain their online and offline legacies beyond Zoom, “to view white supremacy not as a static ideology or condition, but to instead focus on its geographic and temporal contingency” that allows acts of hate crime by individuals on politicised bodies (Inwood and Bonds 722). This corresponds with Claire Renzetti’s argument that “criminologists theorise that committing a hate crime is a means of accomplishing a particular type of power, hegemonic masculinity, which is described as white, Christian, able-bodied and heterosexual” – an approach that can be applied to theorisations of the alt-right and online violence (136). This violent white masculinity occupies a hegemonic hold in the formation, reproduction, and extension of white supremacy that is then shared, affirmed, and idolised through a racialised Internet (Donaldson et al.). Therefore, I recommend that we situate Zoom-bombing as a means of shitposting, by reiterating the severity of shitposting with the same intentions and sinister goals of hate crimes and racial violence. References  Back, Les, et al. “Racism on the Internet: Mapping Neo-Fascist Subcultures in Cyber-Space.” Nation and Race: The Developing Euro-American Racist Subculture. Eds. Jeffrey Kaplan and Tore Bjørgo. Northeastern UP, 1993. 73-101. Bonds, Anne, and Joshua Inwood. “Beyond White Privilege: Geographies of White Supremacy and Settler Colonialism.” Progress in Human Geography 40 (2015): 715-733. Conway, Maura, et al. “Right-Wing Extremists’ Persistent Online Presence: History and Contemporary Trends.” The International Centre for Counter-Terrorism – The Hague. Policy Brief, 2019. Costanza-Chock, Sasha. “Design Justice and User Interface Design, 2020.” Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology. Association for Computing Machinery, 2020. Daniels, Jessie. “The Algorithmic Rise of the ‘Alt-Right.’” Contexts 17 (2018): 60-65. ———. “Race and Racism in Internet Studies: A Review and Critique.” New Media &amp; Society 15 (2013): 695-719. ———. Cyber Racism: White Supremacy Online and the New Attack on Civil Rights. Rowman and Littlefield, 2009. De Certeau, Michel. The Practice of Everyday Life. First ed. U of California P, 1980. Donaldson, Mike. “What Is Hegemonic Masculinity?” Theory and Society 22 (1993): 643-657. Feinburg, Ashley. “This Is The Daily Stormer’s Playbook.” Huffington Post 13 Dec. 2017. &lt;http://www.huffpost.com/entry/daily-stormer-nazi-style-guide_n_5a2ece19e4b0ce3b344492f2&gt;. Foucault, Michel. “The Discourse on Language.” The Archaeology of Knowledge and the Discourse on Language. Ed. A.M. Sheridan Smith. Pantheon, 1971. 215-237. Fraser, Vicki. “Online Bodies and Sexual Subjectivities: In Whose Image?” The Racial Politics of Bodies, Nations and Knowledges. Eds. Barbara Baird and Damien W. Riggs. Newcastle: Cambridge Scholars Publishing, 2015. 116-132. Friedberg, Brian, Gabrielle Lim, and Joan Donovan. “Space Invaders: The Networked Terrain of Zoom Bombing.” Harvard Shorenstein Center, 2020. Graham, Roderick. “Race, Social Media and Deviance.” The Palgrave Handbook of International Cybercrime and Cyberdeviance. Eds. Thomas J. Holt and Adam M. Bossler, 2019. 67-90. Hawley, George. Making Sense of the Alt-Right. Columbia UP, 2017. Henry, Matthew G., and Lawrence D. Berg. “Geographers Performing Nationalism and Hetero-Masculinity.” Gender, Place &amp; Culture 13 (2006): 629-645. Kruglanski, Arie W., et al. “Terrorism in Time of the Pandemic: Exploiting Mayhem.” Global Security: Health, Science and Policy 5 (2020): 121-132. Lankes, R. David. Forged in War: How a Century of War Created Today's Information Society. Rowman &amp; Littlefield, 2021. Ling, Chen, et al. “A First Look at Zoombombing, 2021.” Proceedings of the 42nd IEEE Symposium on Security and Privacy. Oakland, 2021. McBain, Sophie. “The Alt-Right, and How the Paranoia of White Identity Politics Fuelled Trump’s Rise.” New Statesman 27 Nov. 2017. &lt;http://www.newstatesman.com/culture/books/2017/11/alt-right-and-how-paranoia-white-identity-politics-fuelled-trump-s-rise&gt;. McEwan, Sean. “Nation of Shitposters: Ironic Engagement with the Facebook Posts of Shannon Noll as Reconfiguration of an Australian National Identity.” Journal of Media and Communication 8 (2017): 19-39. Morgensen, Scott Lauria. “Theorising Gender, Sexuality and Settler Colonialism: An Introduction.” Settler Colonial Studies 2 (2012): 2-22. Moses, A Dirk. “‘White Genocide’ and the Ethics of Public Analysis.” Journal of Genocide Research 21 (2019): 1-13. Munn, Luke. “Algorithmic Hate: Brenton Tarrant and the Dark Social Web.” VoxPol, 3 Apr. 2019. &lt;http://www.voxpol.eu/algorithmic-hate-brenton-tarrant-and-the-dark-social-web&gt;. Nagle, Angela. Kill All Normies: Online Culture Wars from 4chan and Tumblr to Trump and the Alt-Right. Zero Books, 2017. Nakamura, Lisa, et al. Racist Zoom-Bombing. Routledge, 2021. Newlands, Gemma, et al. “Innovation under Pressure: Implications for Data Privacy during the COVID-19 Pandemic.” Big Data &amp; Society July-December (2020): 1-14. Perry, Barbara, and Ryan Scrivens. “White Pride Worldwide: Constructing Global Identities Online.” The Globalisation of Hate: Internationalising Hate Crime. Eds. Jennifer Schweppe and Mark Austin Walters. Oxford UP, 2016. 65-78. Renzetti, Claire. Feminist Criminology. Routledge, 2013. Ruf, Jessica. “‘Spirit-Murdering' Comes to Zoom: Racist Attacks Plague Online Learning.” Issues in Higher Education 37 (2020): 8. Salazar, Philippe-Joseph. “The Alt-Right as a Community of Discourse.” Javnost – The Public 25 (2018): 135-143. Selfe, Cyntia L., and Richard J. Selfe, Jr. “The Politics of the Interface: Power and Its Exercise in Electronic Contact Zones.” College Composition and Communication 45 (1994): 480-504. Southern Poverty Law Center. “Alt-Right.” &lt;http://www.splcenter.org/fighting-hate/extremist-files/ideology/alt-right&gt;. Wilson, Jason. “Do the Christchurch Shootings Expose the Murderous Nature of ‘Ironic’ Online Fascism?” The Guardian, 16 Mar. 2019. &lt;http://www.theguardian.com/world/commentisfree/2019/mar/15/do-the-christchurch-shootings-expose-the-murderous-nature-of-ironic-online-fascism&gt;.","",""
"2021","The virtual stages of hate: Using Goffman’s work to conceptualise the motivations for online hate"," Cyber hate is increasing. Every 30 seconds, a woman, somewhere, receives an abusive comment on Twitter (Amnesty International). And, it is estimated that around 20% of college students in the United States have been cyber-bullied. This article explores the motivational factors encouraging online hate and abuse. It will draw on Goffman’s seminal work, The Presentation of the Self in Everyday Life, to critically understand online communication, interaction and behaviour. It will define virtual frontstages and virtual backstages. By critically understanding the different characteristics of online and offline communication, it will help us comprehend how Goffman’s dramaturgical model is compromised when applied to online communication. Therefore, the work attempts to update this model, illustrating that virtual stages have blurred which affects behaviours, and exacerbates performances of hate online. As a result, many online platforms have become Virtual Stages of Hate. ","",""
"2021","MAPPING DISCORD’S DARKSIDE: DISTRIBUTED HATE NETWORKS ON        DISBOARD","Scholars and journalists have noted that Discord, a social application oriented around voice/video chat communities and popular amongst gamers, has a history of harboring white supremacist and toxic groups. Discord has recently undertaken a public rebranding to distance itself from white supremacist, alt-right, and hateful content through a commitment to proactive moderation (Brown, 2020). However, Discord relies extensively on third-party services (like bots and server bulletins), and current scholarship has not adequately accounted for the role of such third-party actors in facilitating hateful and white supremacist networks on private platforms like Discord. This study notes how Discord’s model for curating only popular servers offloads the ethical burden of searchability to server bulletin sites like Disboard, to deleterious effect. This study involves two parts: 1) we use critical technoculture discourse analysis to examine Discord’s blogs, moderation policies, and API (Brock, 2018) and 2) we present data scraped from publicly-available descriptions and tags of 3,600 Discord servers listed on Disboard. Our study finds that thousands of servers on Disboard use overtly white supremacist and hateful tags, often advertising their ‘edgy’ communities as racist, raiding-oriented, and deliberately toxic. These servers exploit Discord’s moderation tools and Disboard’s networked affordances to proliferate within Discord’s distributed ecology. Ultimately, we argue that Discord’s response to hate, as a platform, does not address its reliance on unmoderated third-party services or the networked practices of its toxic communities.","",""
"2021","Ghosts of white methods? The challenges of Big Data research in exploring racism in digital context"," The paper explores the potential and limitations of big data for researching racism on social media. Informed by critical data studies and critical race studies, the paper discusses challenges of doing big data research and the problems of the so called ‘white method’. The paper introduces the following three types of approach, each with a different epistemological basis for researching racism in digital context: 1) using big data analytics to point out the dominant power relations and the dynamics of racist discourse, 2) complementing big data with qualitative research and 3) revealing new logics of racism in datafied context. The paper contributes to critical data and critical race studies by enhancing the understanding of the possibilities and limitations of big data research. This study also highlights the importance of contextualisation and mixed methods for achieving a more nuanced comprehension of racism and discrimination on social media and in large datasets. ","",""
"2021","Assessing the Extent and Types of Hate Speech in Fringe Communities: A Case Study of Alt-Right Communities on 8chan, 4chan, and Reddit"," Recent right-wing extremist terrorists were active in online fringe communities connected to the alt-right movement. Although these are commonly considered as distinctly hateful, racist, and misogynistic, the prevalence of hate speech in these communities has not been comprehensively investigated yet, particularly regarding more implicit and covert forms of hate. This study exploratively investigates the extent, nature, and clusters of different forms of hate speech in political fringe communities on Reddit, 4chan, and 8chan. To do so, a manual quantitative content analysis of user comments ( N = 6,000) was combined with an automated topic modeling approach. The findings of the study not only show that hate is prevalent in all three communities (24% of comments contained explicit or implicit hate speech), but also provide insights into common types of hate speech expression, targets, and differences between the studied communities. ","",""
"2021","What is the “lite” in “alt-lite?” The discourse of white vulnerability and dominance among YouTube’s reactionaries"," This article examines the discourse of “alt-lite” YouTube personalities in a North American context, with a focus on how whiteness is understood and represented. It argues that, despite their self-presentation as color-blind conservatives, these figures are firmly embedded within white supremacist ideology. A qualitative approach to content analysis is adopted to excavate the logics underlying these videos and to highlight the rhetorical tools at work. By framing themselves as the vulnerable targets of progressive movements, “alt-lite” personalities have helped to revive and legitimize a discourse of white victimhood. Their videos emphasize the historic dominance of “white culture” while bemoaning the current and future vulnerability of white people in a politically correct, social-justice-oriented world. Ultimately, the article argues that “alt-lite” figures are united by a set of mitigating rhetorical strategies, which are used to temper and obfuscate their reactionary views. These strategies include performatively aligning with one minority group to denigrate another; highlighting personal relationships with non-white people and knowledge of non-white cultures; embracing a color-blind worldview purportedly rooted in the civil rights movement; and maintaining ironic distance when espousing more overtly hateful racial stereotypes. The adoption of these strategies by right-wing micro-celebrities should not deter scholars and civil society groups from acknowledging when those same figures traffic in white supremacist rhetoric. ","",""
"2021","Racism, Hate Speech, and Social Media: A Systematic Review and Critique"," Departing from Jessie Daniels’s 2013 review of scholarship on race and racism online, this article maps and discusses recent developments in the study of racism and hate speech in the subfield of social media research. Systematically examining 104 articles, we address three research questions: Which geographical contexts, platforms, and methods do researchers engage with in studies of racism and hate speech on social media? To what extent does scholarship draw on critical race perspectives to interrogate how systemic racism is (re)produced on social media? What are the primary methodological and ethical challenges of the field? The article finds a lack of geographical and platform diversity, an absence of researchers’ reflexive dialogue with their object of study, and little engagement with critical race perspectives to unpack racism on social media. There is a need for more thorough interrogations of how user practices and platform politics co-shape contemporary racisms. ","",""
"2021","Governing Hate: Facebook and Digital Racism"," This article is concerned with identifying the ideological and techno-material parameters that inform Facebook’s approach to racism and racist contents. The analysis aims to contribute to studies of digital racism by showing Facebook’s ideological position on racism and identifying its implications. To understand Facebook’s approach to racism, the article deconstructs its governance structures, locating racism as a sub-category of hate speech. The key findings show that Facebook adopts a post-racial, race-blind approach that does not consider history and material differences, while its main focus is on enforcement, data, and efficiency. In making sense of these findings, we argue that Facebook’s content governance turns hate speech from a question of ethics, politics, and justice into a technical and logistical problem. Secondly, it socializes users into developing behaviors/contents that adapt to race-blindness, leading to the circulation of a kind of flexible racism. Finally, it spreads this approach from Silicon Valley to the rest of the world. ","",""
"2021","When is the “Racist” Designation Truly Applicable? News Media’s Contribution to the Debatability of Racism"," Since the U.S. 2016 presidential election, journalists and news organizations have been forced to confront shifting racial, social and political climates, and re-evaluate practices and norms. However, news coverage of racism is complex, especially because the conceptualization of racism in society is discordant, and the parameters of racism are heavily debated. News coverage can contribute to this debatability, specifically when it presents issues of racism with certain linguistic and topical features. In a content analysis of social media posts from six of the Facebook pages maintained by national broadcast and newspaper organizations, the present study explores contextual and linguistic representations of racism, and how social media users on Facebook engage with news posted by these organizations. Results suggest representations in news coverage signal a public debate about what is and is not racism. Coverage heavily emphasized prominent figures, while social media audiences amplified Trump’s presence in social networks. ","",""
"2022","Bots Amplify and Redirect Hate Speech in Online Discourse About Racism During the COVID-19 Pandemic"," Online talk about racism has been salient throughout the COVID-19 pandemic. Yet while such social media conversations reflect existing tensions in the offline world, the same discourse has also become a target for information operations aiming to heighten social divisions. This article examines Twitter discussions of racism in the first and sixth months since COVID-19 was accorded pandemic status by the World Health Organization and uncovers dynamic associations with bot activity and hate speech. Humans initially constituted the most hateful accounts in online conversations about racism in March, but in August, bots dominated hate speech. Over time, greater bot activity likewise amplified levels of hate speech a week later. Moreover, while discourse about racism in March primarily featured an organic focus on racial identities like Asian and Chinese, we further observed a bot-dominated focus in August toward political identities like president, Democrat, and Republican. Although hate speech targeting Asian groups remained present among racism discussions in August, these findings suggest a bot-fueled redirection from focusing on racial groups at the onset of the pandemic to targeting politics closer to the 2020 US elections. This work enhances understanding of the complexity of racism discussions during the pandemic, its vulnerability to manipulation through information operations, and the large-scale quantitative study of inorganic hate campaigns in online social networks. ","",""
"2022","It’s Not How You Say It, It’s What You Say: Ambient Digital Racism and Racial Narratives on Twitter"," Social media has been used to disseminate hate speech and racism. Racist opinions can be disguised through a language that may appear to be harmless; however, it can be part of a racist rhetoric toward communities of color. This type of racist communication is called Ambient Digital Racism (ADR). Through a thematic analysis, this project sought to identify and analyze social media racist discourses on Twitter in the context of George Floyd’s death. This research examined original tweets posted during the time of the protests using three known counter Black Lives Matter (BLM) hashtags, namely, #WhiteLivesMatter, #BlueLivesMatter, and #AllLivesMatter. After the analysis, two themes emerged, namely, the discourse of oppressor’s reverse racism and the social criminalization of BLM. These themes described the narratives used by these groups to develop a racist digital discourse that goes unnoticed by social media regulations and policies and that leaves an open space to negotiate what constitutes acceptable race talk and what constitutes a racist discourse. It was found that both themes were grounded on White victimization, color-blind racism, and the dehumanization of BLM as a social and racial justice movement. ","",""
"2023","How dark corners collude: a study on an online Chinese alt-right community","ABSTRACT The rise of the ‘alt-right’ (alternative right) and their communications on the Internet are not unique to the West. This study follows a mixed-methods approach combining topic modeling, social network analysis, and discourse analysis to analyze the discursive and network structure of an online Chinese alt-right community on Weibo. We summarize the topics Chinese alt-right influencers discuss and examine how these topics are interrelated. We find that the Chinese alt-right discourse can be deemed as both an extension and localization of the global alt-right: they frequently discuss global alt-right issues and also hold alt-right ideologies on domestic issues. Meanwhile, influencers in the community are densely connected, suggesting a high level of coordination and cooperation. We particularly identify two discursive strategies that alt-right influencers employ to reproduce the transnational alt-right discourse, namely invented common crisis of majority culture and transnational metaphor usage. These findings provide insights into the transnational aspect of the rise of global alt-right.","",""
"2023","Social media hate speech in the walk of Ethiopian political reform: analysis of hate speech prevalence, severity, and natures","ABSTRACT It is evident in this study that the social media sphere which has been highly controlled by the Ethiopian government for a long seems to untie right after the advent of the new political reform commenced in 2018. Following the transition, it is apparent that people are relatively enjoying the freedom of expression. On the contrary, the new digital platform is deemed to be subdued by the emergence of hate speech which is attributed to political, ethnic, and religious underpinnings. The study employs a quantitative approach to analyze users’ comments collected from rivalry ethnic-based Television channels’ Facebook and YouTube. Using a binary analysis, a substantial prevalence of social media hate speech is found. Mainly ethnic, religious, and political-based hate natures are also found. Most of the hate comments are offensive; however, a few comments are laid on incitement to violence, and genocide severity levels. Contesting memory of the past, associating ethnicity with religion, culture, and language issues are the main triggering factors of hate speech in Ethiopia. The online commenters imitate the offline ethnic tension in the country that upsurge during the political reform. Thus, identity-driven hate speech in tandem with the reform incidents has suffocated the Ethiopian social media.","",""
"2023","Weaponizing reproductive rights: a mixed-method analysis of White nationalists’ discussion of abortions online","ABSTRACT According to the Great Replacement conspiracy theory, nonwhites, globalists and elites are plotting to eliminate the white race and its dominance through anti-white policies and increased immigration. In that context, abortion among white women is perceived by white nationalists (WN) as a betrayal of their ‘biological’ and ‘traditional’ gender role – procreation of white babies. While WN condemn abortion among white women as a murderous sin, at times they encourage the practice among nonwhites to solve demographic threats to white dominance. In this study, we use mixed methods, combining unsupervised machine learning with close textual analysis of 30,725 posts including the term ‘abortion’ published on the WN website Stormfront between 2001 and 2017. We identify three broad themes: White genocide, focused on the conspiracy theory and detailing the active actors in its alleged execution; political, focused on political agendas and laws; and WN reproductive reasoning, articulating and justifying the contradiction between supporting abortion for nonwhites but not for whites via politics of difference that emphasize nonwhites’ supposed inferior morality. We discuss WN’s unique and explicitly racist discourse around a medical topic like abortion, a staple of the conservative and religious right for decades, and how it is used to alleviate their cognitive dissonance resulting from their dual-stance on abortion. Such discourse could be harnessed to recruit members into the movement and normalize extreme, racist ideologies.","",""
"2023","ONE HUNDRED NAZI SCREENS: INTERFACES AND THE STRUCTURE OF U.S. WHITE NATIONALIST DIGITAL NETWORKS ON TELEGRAM","The “Alt-Right,” a white nationalist online coalition, has collapsed amidst a revolution in digital governance termed the “regulatory turn.” Nevertheless, the regulatory turn remains incomplete because white nationalists utilize graphical user interface (GUI) design to subvert public stewardship. Why have some former Alt-Right platforms collapsed while others have grown despite increased scrutiny? The field’s account is currently limited to social media networks and rooted in positivist methods, lending a static conception of white nationalist networks that is slow to recognize cultural shifts. This paper fills the gap by comparatively critiquing the interfacing affordances of Telegram, an instant messaging app that functions as an """"ideological safe harbor"""" for U.S. white nationalists with content aggregation, blogging, and activist use-cases. I apply interface critique to index how the manipulation of graphical user interfaces allows white nationalists to frame their browsing as a technology of mastery over and against the regulatory turn. I argue that Telegram networks coopt the enclave public, exploiting an ideology of decentralization to mystify the leverage held by white nationalist developers over their users. This occlusion redirects white masculine anxieties against publicity to justify an intensified racist fanaticism and the exportation of violence against racial, religious, and gendered outsiders. White interfacing frames GUI design as a capitalist technology that weaponizes the racist and sexist logic of the “average user” to secure the reproduction of reactionary platforms. This project furthers Internet research by developing a theory of the interface as an ideological mirror of production.","",""
"2023","Surface and Sublevel Hate"," On the face of it, contemporary “alt-tech” platforms appear more moderate than legacy hate havens. Yet it's also clear that virulent hate in the form of misogyny, white supremacy, and xenophobia has not disappeared. Probing this tension, this article conceptualizes two forms of hate: Surface “Hate” (moderate content that is highly visible and easily accessible) and Sublevel Hate (explicit content that is more marginal and less discernible). These terms are illustrated by examining several viral videos on Rumble. This twinned mechanism explains how alt-tech platforms can be both accessible and extreme at the same time. Stratified hate is strategic, heightening the appeal and durability of online communities. Recognizing this dangerous dynamic is key for interventions seeking to counter it. ","",""
"2023","The affordances of extreme speech"," New media studies invested in online political conflict, radical and antagonistic subcultures have taken an interest in the affordances that shape memes, vernaculars and online political communication. One often overlooked affordance is the ensemble of social, communication, platform and legal frameworks stipulating what users can and cannot say, which I call “speech affordances.” To explore this concept, I look at the strategic communication of 4chan, Twitter and YouTube subcultures tied to a historical meme, “Kekistan,” often perceived as a key example of the ideological cacophony of the 2015–2017 online “culture wars.” I focus on how 4chan's policy of user anonymity, YouTube's unmoderated comment sections and Twitter's more proactive moderation practices brought some influencers to alter the original connotations of the meme into “overt” messages tolerable to Twitter and YouTube out-groups and platform moderation policies. Speech affordances bear methodological implications for historical studies of speech moderation and the overall mechanisms in which problematic language adapts to spaces with distinct speech norms. ","",""
"2023","I’ll be there for you? Effects of Islamophobic online hate speech and counter speech on Muslim in-group bystanders’ intention to intervene"," Online hate speech is very common. This is problematic as degrading social groups can traumatize targets, evoke stress, and depression. Since no reaction of others could suggest the acceptability of hate speech, bystander intervention is essential. However, it is unclear when and how minorities react to hate speech. Drawing from social identity theory and research on in-group intervention, we inquire how Islamophobic online hate speech and counter speech by majority or minority members shape Muslims’ willingness to intervene. Thus, in an online experiment ( N = 362), we varied the presence of Islamophobic online hate speech and counter speech by a (non-) Muslim. Results showed that Islamophobic online hate speech led to a perceived religious identity threat which, in turn, increased the personal responsibility to intervene and resulted in higher intentions to utter factual counter speech. In addition, counter speech by both majority and minority members directly reduced Muslims’ intentions to counterargue hatefully. ","",""
"2023","Making the impossible possible? Framing confrontations of racism on social media as norm-setting"," Although confronting racist speech online can reduce future discriminatory behavior, people may be reluctant to confront because they perceive that it will be ineffective in changing racist attitudes. We test one theoretically grounded strategy for increasing white social media users’ willingness to confront online racism: reframing the confrontation goal from attitude change to norm-setting. We examine whether re-framing the confrontation goal in this way is effective under conditions where confrontation is least likely: (a) when the potential confronter is highly cynical about the efficacy of political discussions and (b) when the confrontation target is relationally distant. In a two-wave panel survey experiment collected during the 2020 US presidential election, participants reported greater likelihood of confronting a target when the goal was to set norms regarding racist speech, when they were less cynical of discussions, and when the target was relationally closer to them (e.g. family members as opposed to strangers). ","",""
"2023","Digital Rage: Testing “the Obama Effect” on Internet-Based Expressions of Racism"," The concept, “White Rage,” has previously been used to describe the way Whites have historically responded to Black advancement with policies and practices designed to quietly disrupt the progress Blacks had been making. White rage is typically subtle, masking its true intent. In contrast, recent research has found that the covert, subtle expressions of racism that are so normal in most mainstream spaces may be less common in internet-based communication. The extent to which online racism is connected to real-world racist attitudes, behaviors, and events, however, is unclear. In this article, we test the effects of real-world racialized events on explicit expressions of racism in online spaces using days that Obama gave speeches as our treatment effect and explicit usage of the “n-word” on the social media platform X (formerly Twitter) as our measurable outcome. Does usage of the n-word, a racial slur, increase in the days following speeches made by President Obama? Our results of over 9 years and more than 2.9 million tweets demonstrate a statistically significant increase of racist speech in response to those speech cycles, which are further placed in contrast to the speeches of other political actors, including President Trump. ","",""
"2023","Hate Influencers’ Mediation of Hate on Telegram: “We Declare War Against the Anti-White System”"," Hate influencers play a critical role in platforming hate. In this article, we illustrate how visible (forward-facing) and invisible (faceless) hate influencers mobilize far-right hate groups in the mobile socio-sphere. Based on our digital multimodal walkthrough method and multimodal discourse analysis, we analyze 16 Telegram channels for two designated hate groups. We focus our analysis on Proud Boys content related to the 6 January attack on Capitol Hill and the White Lives Matter rallies across North America in 2021. To illustrate how hate influencers mobilize these groups, we introduce a three-part model that entails the process (mobile mobilization), means (discourses), and ends (actualizing the objective of the hate group). ","",""
"2023","The Effects of Observer Expectations on Judgments of Anti-Asian Hate Tweets and Online Activism Response"," The rise of racial hate speech on social media has raised critical questions for scholars to explore. It is necessary to understand how outside observers passively evaluate (a) online racial hate speech posts on social media and (b) whether those evaluations are related to observers’ subsequent behavior. This study explored how observers evaluate acts of majority-on-minority and minority-on-minority anti-Asian hate tweets on Twitter. In an experiment ( n = 196) informed by expectancy violations theory, we tested how White observers evaluated anti-Asian tweets ostensibly posted by either a White or Black source. Analysis revealed a moderated-mediation pathway in which observers’ political partisanship (Democrat/Republican) affected how they judged the ethnic prototypicality of White and Black sources of racial hate speech; these source prototypicality judgments were in turn associated with observers’ judgments of tweet offensiveness and self-reported intentions to engage in online activism (i.e., signing an online petition). These results contribute to our understanding of outside observers’ differential expectancies regarding online hate speech, and how those expectancies can affect perceptions of and reactions to acts of racism. ","",""
"2024","Online hate speech and instant messaging apps: An emerging research agenda","This study explores academic literature on hate speech and discriminatory practices in digital chat environments based on instant messaging apps, specifically Telegram and WhatsApp. The sample includes 40 articles in English published in scientific journals between January 2009 and April 2022, available in four databases: Web of Science, Directory of Open Access Journals, Scopus, and Google Scholar. As a result, we discussed five dimensions that characterize the research agenda on hate speech on instant messaging platforms: the plurality of the phenomena observed; the absence of a theoretical-methodological articulation that relates the affordances of the platforms, and the discursive strategies of hate in instant messaging applications; the ambiguity of the principle of privacy in the operating logic of instant messengers; the multiplicity of communicative practices and communicational ambiances on instant messaging platforms; and the diversity of methods used in research on hate speech on instant messaging platforms.","",""
"2024","A relational approach to digital racism: Toward a theoretical model","In recent decades, anti-immigrant, racist and nationalist attitudes have become increasingly mainstream, transforming public debates on immigration and immigrants in Europe and beyond. These attitudes and sentiments have been widely disseminated and amplified through digital communication, including commercial social media platforms. To better understand the relationship between racism and digital communication, it is necessary to move beyond media-centric explanations and simplified discussions of online hate speech and platform regulation. Therefore, this paper proposes a multi-theoretical approach to digital racism, i.e., racist content produced and circulated online. The paper presents an understanding of digital racism in relation to four perspectives: networked affect, nationalism, masculinity and conspiracy thinking. It takes a relational approach to these theoretical perspectives and discusses how they could be utilized to understand and analytically approach digital racism at the macro, meso and micro levels.","",""
"2024","“I’m not this Person”: Racism, content moderators, and protecting and denying voice online"," Much scholarship across the humanities and social sciences seek to shed light on the intersection of far-right politics and social media platforms. Yet, scholars tend to focus on racist actors and the ideological underpinnings of platform policies while the contingencies that shape the experiences of content reviewers who make decisions about racist content remain underexamined. This article fills this gap by exploring such contingencies from a linguistic anthropological perspective. Drawing on Facebook moderators’ stories, I illustrate the factors adjacent to, and beyond, ideology that animate the adjudication of racist hate speech. ","",""
"2024","How social media users perceive different forms of online hate speech: A qualitative multi-method study"," Although many social media users have reported encountering hate speech, differences in the perception between different users remain unclear. Using a qualitative multi-method approach, we investigated how personal characteristics, the presentation form, and content-related characteristics influence social media users’ perceptions of hate speech, which we differentiated as first-level (i.e. recognizing hate speech) and second-level perceptions (i.e. attitude toward it). To that end, we first observed 23 German-speaking social media users as they scrolled through a fictitious social media feed featuring hate speech. Next, we conducted remote self-confrontation interviews to discuss the content and semi-structured interviews involving interactive tasks. Although it became apparent that perceptions are highly individual, some overarching tendencies emerged. The results suggest that the perception of and indignation toward hate speech decreases as social media use increases. Moreover, direct and prosecutable hate speech is perceived as being particularly negative, especially in visual presentation form. ","",""
"2024","Influence of hate speech about refugees in search algorithms on political attitudes: An online experiment"," This article assesses the effects of hate speech compared to positive and neutral content about refugees in search engines on trust and policy preferences through a survey experiment in Germany. The study uncovers that individuals with an extreme-right political ideology become more hostile toward refugees after being exposed to refugee-related hate speech in search queries. Moreover, politically biased search engines erode trust similarly to politicized sources like politicians, and positively and negatively biased content is trusted less than neutral content. However, individuals with a right political ideology trust more hate speech content than individuals with a left-wing ideology. Individuals with right-wing ideology are also almost three times as likely to intend to click on hate speech suggestions compared to left-wing counterparts. ","",""
"2024","Unmasking coordinated hate: Analysing hate speech on Spanish digital news media"," This study examines the characteristics and behaviours of accounts that propagate hate speech through their responses to articles posted on five leading digital news media in Spain on Platform X (previously Twitter). Using non-experimental quantitative research, we analysed 1345 hate-expressing messages from 173,449 user comments on content shared in five leading digital news media during January 2021. Network analysis, the Homophilic Exposure Index (HEI), regression analysis and the k-means algorithm were used to identify features that characterize accounts that disseminate low-intensity hate expressions in a coordinated manner, undermining the moderation efforts of digital news media. As a result, digital news media must develop strategies to reduce the presence of this type of expression and confront accounts that operate covertly in a coordinated manner, using Astroturfing to manipulate debates around the content published on X. ","",""
"2024","Mapping Discord’s darkside: Distributed hate networks on Disboard"," Discord, a popular community chat application, has rhetorically distanced itself from its associations with white supremacist content through a public commitment to proactive moderation. However, Discord relies extensively on third-party services (like bots and server bulletins), which have been overlooked in their role in facilitating hateful networks. This study notes how Discord offloads searchability to server bulletin sites like Disboard, to deleterious effect. This study involves two parts: (1) we use critical technoculture discourse analysis to examine Discord’s blogs, policies, and application programming interface and (2) we present data scraped from 2741 Discord servers listed on Disboard, revealing networks of hateful and white supremacist communities that openly use “edgy,” raiding-oriented, and toxic messaging. These servers exploit Discord’s moderation tools and affordances to proliferate within Discord’s distributed ecology. We argue that Discord’s policies fail to address its reliance on unmoderated third-party services or the networked practices of its toxic communities. ","",""
"2024","Youth on standby? Explaining adolescent and young adult bystanders’ intervention against online hate speech"," Most adolescents and young adults frequently encounter hate speech online. Although online bystander intervention is essential to combating such hate, young bystanders may need support with initiating interventions online. Thus, to illuminate the factors of young bystanders’ intervention, we conducted a nationwide, quota-based, quantitative online survey of 1180 young adults in Germany. Among the results, perceived personal responsibility for combating online hate speech positively predicted online bystanders’ direct and indirect intervention. Moreover, frequent exposure to online hate speech was positively associated with bystander intervention, whereas, a perceived threat or low self-efficacy reduced the likelihood of intervention. Also, a greater acceptance of negative consequences and being educated about online hate speech through peers or campaigns all positively predicted some direct and indirect forms of online bystander intervention. ","",""
"2024","The Power of Images: How Multimodal Hate Speech Shapes Prejudice and Prosocial Behavioral Intentions"," While online hate speech has become a serious problem in multimedia environments, most studies in this area have examined text-based hateful content, with less attention paid to its other visual aspects. From a multimodal perspective, we conducted an online experiment ( N = 799) to investigate how multimodal hate speech (i.e., text and images presented together to convey hateful meanings) on social media affected users’ prejudicial attitudes and prosocial behavioral intentions. The results showed that participants in the text-plus-image (vs. text-only) condition felt more sympathy, which led to less implicit prejudice toward the target group and more prosocial behavioral intentions. In addition, exposure to text-plus-image hate speech had an indirect effect on prosocial behavioral intentions through sympathy and implicit prejudice. The findings contribute to scholarship on online hate speech and provide insights into the affect heuristics that individuals rely on when processing multimodal information. ","",""
"2024","Social Media Users’ Motives for (Not) Engaging With Hate Speech: An Explorative Investigation"," Despite extensive research on what causes social media users to recognize hate speech and what motivates their reactions to it, little is known about a crucial intermediate step that leads to users’ engagement or non-engagement with hate speech online. In our study, drawing on the uses and gratifications approach, we theoretically derived motives representing affective and entertainment, personal identity and social-integrative as well as cognitive dimensions for social media users to engage or not engage with hate speech. To empirically investigate those motives, we conducted a quota-based online survey of adult social media users in Germany ( N = 4,020) and subjected the responses to exploratory factor analysis. We found that a range of personal and social gratifications, far beyond simple expressions of approval or disapproval, encourage social media users to engage with hate speech online, whereas intentions to protect oneself and others from potential harm discourage such behavior. ","",""
"2025","The politics of platform folklore: Emotion, identity, and sense-making in far-right populist Twitter communities"," Platforms are increasingly part of everyday life, but they remain opaque and impenetrable spaces for most users. To manage life on platforms, users thus need to engage with sense-making practices that help them understand and navigate online spaces. This paper studies how far-right populist activists interpret and navigate their presence on Twitter, using the concept of platform folklore, that is, unofficial and collective narratives aimed at relieving feelings of uncertainty associated with the opacity of platforms. The data consists of 20 life-history interviews with Swedish and American Twitter activists from right-wing populist communities, who all participated in populist Twitter debates. The analysis shows how platform folklore is constructed not only based on observations of content moderation, as emphasized in previous research, but also in correspondence with the political ideology and identities of the activists. This led them to interpret Twitter as a leftist organization that disfavored them, which stopped them from developing strategies to reach the goals they held as central to their Twitter activism. The paper concludes by discussing the role of emotions and ideology in platform folklore within political communities and suggesting directions for future research. ","",""
"2025","EXPLORING SURVEY INSTRUMENTS IN ONLINE HATE SPEECH RESEARCH: A COMPREHENSIVE SCOPING REVIEW","Research on online hate speech is burgeoning, highlighting its significant negative consequences on individual and societal levels. Still, there is a lack of attention directed towards understanding the perceptions and experiences of individuals in the general population toward this phenomenon. Such insights could significantly help decision-makers and scholars in formulating initiatives to inform and educate the public, thereby preventing further harm. Therefore, quality insight into the understanding and perceptions of hate speech, as well as related experiences and behaviour is needed. In the absence of established survey instruments and given the fragmented and limited nature of the existing data, the primary objective of this scoping review is to systematically identify online hate speech survey measures, as well as the topics and populations the associated research addressed. Preliminary findings reveal a disproportionate focus on students and young adults and the predominant use of non-representative sampling methods, leaving older population groups under-researched and raising concerns about results’ generalizability. Broad research scopes are predominant; in order to better understand real-world experiences, scholars should aim for more detailed research scopes, including studying OHS in specific online contexts and within specific groups. In terms of topics, existing survey measures mostly cover the topics of direct exposure/perpetration of OHS, neglecting the whole hate speech ecosystem; what comes before and after the dissemination of hate speech online. In terms of question types, ordinal scales are most common, but the lack of standardized scales results in fragmented findings that are difficult to compare.","",""
"2025","AMBIENT AMPLIFICATION: ATTENTION HIJACKING AND SOCIAL MEDIA PROPAGANDA","Over the last decade, the rise of memetic media in combination with multimodal platform environments have radically transformed our experience of digital culture. On TikTok, gestures and sounds go viral. Twitter (X) @tags operate as bonding tools and ignite ‘supercharged’ critical publics. Coordinated link-sharing and attention-hijacking drive cross-platform engagement. Narratives promoted by state institutions become part of the global digital culture war. Networked content made up of heterogeneous elements sparks new forms of presencing, propaganda, and play, producing conflict-ridden communities of practice.  The amalgamation of this all into people’s everyday lives marks a qualitative shift in the ways we use social media. While there has always been an affective component to digital objects such as hashtags (Papacharissi 2015), contemporary audiovisual platforms increasingly target the full sensory experience of human bodies. The focus shifts from fleeting encounters with recommended content to ambient arrangements of linked sounds, networked text, clickable icons, and moving images (Han and Zappavigna 2024; Parry 2023). Within these structures, hybrid cultures of amplification emerge, relying on both intensification and extension. As expressive modalities evolve, amplification not only animates momentary affective impulses but also manifests through repeated attempts at attention hijacking that spread across platforms via everyday acts of sharing (John 2017; Citton 2017).  In view of these transitions, internet scholars turn to the role of affect to describe the rhythms of online exchanges that are not reducible to singular constituents and can both diminish and increase the engaging potential of content- and data-informed connectivity (Hillis, Paasonen and Petit 2015; Boler and Davis 2020; Slaby 2019). Acts of participation that open up spaces of amplification escape any clear-cut demarcation. Platform communities assembled through different digital objects sidestep binary conceptions of authenticity and (coordinated) performance, allowing amplification to emerge from multiple discrepancies (Graham et al. 2021; DiResta 2021). Platform-mediated processes of authentication target misinformation campaigns, aiming to identify ‘trustworthy’ content (Burton, Chun et al. 2023). At the same time, seemingly straightforward contributions we like and share can be anything but (Phillips and Milner 2021).  Understood as a web of affective stimulations (Siapera 2019), ambient amplification refers to social media encounters that bear potential for contestation in different registers of online performance, human and nonhuman. On the one hand, the proliferation of echo chambers and filter bubbles drawing together like-minded communities easily fits into the 'crowd modulation' project through the exhaustion of collective inclinations and correlated metadata (Rogers and Niederer 2020; Apprich et al. 2019). On the other hand, the layeredness of networked embodiment on audiovisual platforms rewires predefined trajectories of amplification through 'dissonant' connections that refuse to be contained in neat taxonomies. Although a great variety of scholarly work is dedicated to polarised engagement, there is still a large gap in studying how different modes of amplification are made to work in more ambiguous contexts (Paasonen 2023).  The main challenge in approaching this research field is that the messiness of platform-mediated communication is difficult to comprehend. The shifts in relations of data-intensive participation and networked attention capture have made up the platforms' appeal since the very beginnings of the social web (Gillespie 2010), yet the actual ways these relations are made to work remain understudied. Moving away from the analysis of symptoms–as in the most visible content and events of peak intensity–this panel focuses on the ambient logic of amplification forged by the various attachments that online engagement in multimodal social media affords.  Starting from the premise of plurality, it brings together five papers, each of which explores a different aspect of ambient amplification: Paper 1 explores the role of ‘thirst trap propaganda’ in military image wars on TikTok. Paper 2 reflects on the role of gestures in targeted war propaganda placements, presenting a visual method of slow circulation for amplified TikTok content. Paper 3 analyzes hashtagging- and @-tagging practices on X that undergird a polyvocal infrastructure exposing journalists to networked critique. Paper 4 looks into the spectrum of coordination in the service of attention hijacking, investigating what makes coordinated link-sharing on Facebook look ‘authentic enough’. Paper 5 interrogates the weaponization of narratives of a global culture war by Russian embassies, uncovering geopolitical strategies of amplification.","",""
"2025","Quantifying the spread of racist content on fringe social media: A case study of Parler","Few studies characterize the diffusion of racist content on fringe social media platforms. We demonstrate how racism spread on Parler, a far right, un(der)-moderated social media platform, and that a single comment to a racist post increases the likelihood a person will generate and propagate new racist content. We found that racism on Parler was a social “contagion.” Using 50,375 posts from 2018 to 2021 that contained racist remarks, we quantified the spread of racism from the posts-to-comments (micro) and user-to-user (macro) levels. Comments on racist posts were 21 times more likely to be racist than comments on non-racist posts. On an average, Parler users posted 166% more racist content after an engagement with a racist post. At the posts-to-comments level, the spread of racist sentiment is alarming within ethnic subgroups (e.g. anti-Jewish-specific comments were 191 times more likely to appear on “anti-Jewish” posts; anti-Black-specific comments were 227 times more likely to appear on “anti-Black” posts). The spread of racist rhetoric between subgroups is also significant, albeit lower than within subgroups, suggesting a spillover effect. Aggregate user posting patterns also suggest that the spread of racist rhetoric between subgroups is lower, but significant. Our findings therefore suggest interventions should target users actively engaging with racist content. Furthermore, our study provides evidence that Parler served as a gateway platform that radicalized users toward racist content production, underscoring the urgency of intervention. Given the drawbacks of traditional moderation strategies, we propose additional interventions, policies, and structural changes for social media platforms to mitigate racism online.","",""
"2025","From prejudice to marginalization: Tracing the forms of online hate speech targeting LGBTQ+ and Muslim communities"," This study investigates online hate speech in Finland, particularly Twitter messages targeting people of Muslim faith and the LGBTQ+ community, using a mixed-methods approach that combines quantitative text classification with a BERT model and qualitative thematic analysis via BERTopic and examination of highly interacted posts from 2018 to 2023. The study shows increasing instances of hate speech occurring online, primarily against Muslims, with topic modeling uncovering 32 topics for Muslims and 41 for the LGBTQ+ community, featuring themes of violence, cultural conflict, and challenges to traditional values. The LGBTQ+ community is depicted as undermining traditional norms, whereas Muslims are presented with hostility. The research underscores the necessity for digital platforms to employ nuanced strategies to counter hate speech, advocating for policies that tackle hate speech while also addressing the underlying factors and enhancing understanding of the social and cultural contexts of the targeted groups to refine detection accuracy. ","",""
"2025","Humorous hate speech on social media: A mixed-methods investigation of users’ perceptions and processing of hateful memes"," Humor that denigrates social groups can be just as harmful as hate speech. Despite research indicating the prevalence of humorous hate speech, how audiences perceive and process the combination of humor (e.g. irony as a humor cue) and hate speech (e.g. dehumanization as a hate cue) remains unclear. Using a sequential mixed-methods approach combining a qualitative think-aloud study (Study 1, N = 41) with an experiment involving implicit measurements of response times (Study 2, N = 65), it was examined how individuals perceive memes that contain both humor and hate cues. While think-aloud interviews indicated that processing humorous hate speech may require multiple steps, the relative time spent by participants in Study 2 to rate humorous and non-humorous hate speech as being hostile or not did not entirely support that conclusion. However, findings imply that hostile views may become more commonplace when hate speech is masked by humor. ","",""
"2025","Understanding the role of participatory-moral abilities, motivation, and behavior in European adolescents’ responses to online hate"," Researchers have repeatedly discussed how to strengthen supportive and pro-social responses to online hate, such as reporting and commenting. Researchers and practitioners commonly call for the promotion of media literacy measures that are believed to be positively associated with countermeasures against online hate. In this study (conducted in 2021), we examined relationships between media literacy proficiencies of (1) moral-participatory motivation and abilities and, consequently, (2) the establishment of moral-participatory behaviors and the correspondence with prosocial responses to online hate. A sample of 1489 adolescents and young adults (16–22 years old) from eight European countries is examined. Results confirmed that higher participatory-moral motivation and behavior were significantly associated with stronger intentions to report online hate. Commenting on hateful online content, on the other hand, was significantly related to participatory-moral abilities and past experiences with online harassment. Implications for the role of social media literacy in the context of online hate are discussed. ","",""
"2025","Online toxic speech as positioning acts: Hate as discursive mechanisms for othering and belonging","While digital platforms foster a sense of community and identity, they also facilitate harmful exclusionary practices. In this context, toxic and hateful speech are key mechanisms not only for harming others but also marking processes of othering and belonging. In this article, we examine the role of hateful and toxic speech in structuring processes of in- and out-group formation and maintenance by focusing on a public Colombian Telegram group. More specifically, we examine how members use toxic speech to position themselves and others in relation to narratives emerging from the group by analyzing 3221 posts with high levels of toxicity. Our analysis yields insights into the complex and paradoxical uses of antisocial behavior on social media platforms. Overall, the findings of this study deepen our understanding of the social gratifications that underlie how hate and toxic speech are used to disenfranchise individuals.","",""
"2025","Contested Meanings of Hate Speech and the Post-Truth Condition on Digital Platforms","In the everyday context, the term “hate speech” has become increasingly politicized and emotionally charged, yet these vernacular constructions of hate speech remain under-explored. Used as both a rhetorical weapon and an object of genuine concern, various understandings of hate speech circulate within interactive everyday cultures of digital media, shaped by the digitalised media environment. With the combination of computational and qualititative research methods, this article explores the struggle over meanings of hate speech. From a large dataset of 289,933 messages, we identified the 10 most relevant themes. We further used articulation theory to analyze different political and social articulations of hate speech. We situate these articulations to the context of post-truth condition, characterized with crisis of trust and truth-telling. Our study identified articulations of confusion, uncertainty, ridicule, trivialization, and censorship in the context of hate speech. The results show that the struggle over the power to define hate speech simultaneously involves a struggle to overturn the definitional power of research institutions and official and knowledge authorities. Overall, the study contributes to the research on hate speech by showing the vernacular, contextual and localized nature of hate speech that emerges in reference to particular political actors, events and debates. Furthermore, the study illustrates the societal importance of the hate speech debate and the ways in which the concept itself, through multiple articulations, is used as tool in the post-truth battle to impede and disturb democratic debate and to serve particular political ends.","",""
"2025","Empowerment Is Key? How Perceived Political and Critical Digital Media Literacy Explain Direct and Indirect Bystander Intervention in Online Hate Speech","             Hate speech is widespread in digital media, and such incidents can harm individuals and fuel hostile discourses. Therefore, understanding the factors that shape bystander intervention is crucial. Despite frequent calls for more research, there is a need for greater understanding of how perceived political and digital media literacy are related to the frequency of various forms of online bystander intervention, such as counter-speech or reporting. Based on a national online survey of German citizens (             N              = 2,691), we investigated how perceived political and digital media literacy of individuals with prior experience in addressing online incivilities (             n              = 672) relates to (private and public) direct and indirect forms of intervention against online hate speech. The results indicate that a sense of empowerment regarding digital media content particularly increases direct, public interventions, such as uttering counter-speech.           ","",""

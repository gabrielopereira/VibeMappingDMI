"year","title","abstract","journal","doi"
"2016","It’s About Ethics in Games Journalism? Gamergaters and Geek Masculinity"," #Gamergate is an online movement ostensibly dedicated to reforming ethics in video games journalism. In practice, it is characterized by viciously sexual and sexist attacks on women in and around gaming communities. #Gamergate is also a site for articulating “Gamergater” as a form of geek masculinity. #Gamergate discussions across social media platforms illustrate how Gamergaters produce and reproduce this gendered identity. Gamergaters perceive themselves as crusaders, under siege from critics they pejoratively refer to as SJWs (social justice warriors). By leveraging social media for concern-trolling about gaming as an innocuous masculine pastime, Gamergaters situate the heterosexual White male as both the typical gamer and the real victim of #Gamergate. #Gamergate is a specific and virulent online node in broader discussions of privilege, difference, and identity politics. Gamergaters are an instructive example of how social media operate as vectors for public discourses about gender, sexual identity, and equality, as well as safe spaces for aggressive and violent misogyny. ","",""
"2018","Digital detritus: 'Error' and the logic of opacity in social media content moderation","The late 2016 case of the Facebook content moderation controversy over the infamous Vietnam-era photo, “The Terror of War,” is examined in this paper for both its specifics, as well as a mechanism to engage in a larger discussion of the politics and economics of the content moderation of user-generated content. In the context of mainstream commercial social media platforms, obfuscation and secrecy work together to form an operating logic of opacity, a term and concept introduced in this paper. The lack of clarity around platform policies, procedures and the values that inform them lead users to wildly different interpretations of the user experience on the same site, resulting in confusion in no small part by the platforms’ own design. Platforms operationalize their content moderation practices under a complex web of nebulous rules and procedural opacity, while governments and other actors clamor for tighter controls on some material, and other members of civil society demand greater freedoms for online expression. Few parties acknowledge the fact that mainstream social media platforms are already highly regulated, albeit rarely in such a way that can be satisfactory to all. The final turn in the paper connects the functions of the commercial content moderation process on social media platforms like Facebook to their output, being either the content that appears on a site, or content that is rescinded: digital detritus. While meaning and intent of user-generated content may often be imagined to be the most important factors by which content is evaluated for a site, this paper argues that its value to the platform as a potentially revenue-generating commodity is actually the key criterion and the one to which all moderation decisions are ultimately reduced. The result is commercialized online spaces that have far less to offer in terms of political and democratic challenge to the status quo and which, in fact, may serve to reify and consolidate power rather than confront it.","",""
"2018","The view from the other side: The border between controversial speech and harassment on Kotaku in Action","In this paper, we use mixed methods to study a controversial Internet site: The Kotaku in Action (KiA) subreddit. Members of KiA are part of GamerGate, a distributed social movement. We present an emic account of what takes place on KiA: who are they, what are their goals and beliefs, and what rules do they follow. Members of GamerGate in general and KiA in particular have often been accused of harassment. However, KiA site policies explicitly prohibit such behavior, and members insist that they have been falsely accused. Underlying the controversy over whether KiA supports harassment is a complex disagreement about what “harassment” is, and where to draw the line between freedom of expression and censorship. We propose a model that characterizes perceptions of controversial speech, dividing it into four categories: criticism, insult, public shaming, and harassment. We also discuss design solutions that address the challenges of moderating harassment without impinging on free speech, and communicating across different ideologies.","",""
"2018","Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms"," Social media platforms play an increasingly important civic role as platforms for discourse, where we discuss, debate, and share information. This article explores how users make sense of the content moderation systems social media platforms use to curate this discourse. Through a survey of users ( n = 519) who have experienced content moderation, I explore users’ folk theories of how content moderation systems work, how they shape the affective relationship between users and platforms, and the steps users take to assert their agency by seeking redress. I find significant impacts of content moderation that go far beyond the questions of freedom of expression that have thus far dominated the debate. Raising questions about what content moderation systems are designed to accomplish, I conclude by conceptualizing an educational, rather than punitive, model for content moderation systems. ","",""
"2018","Beyond the hashtag: Circumventing content moderation on social media"," Social media companies make important decisions about what counts as “problematic” content and how they will remove it. Some choose to moderate hashtags, blocking the results for certain tag searches and issuing public service announcements (PSAs) when users search for troubling terms. The hashtag has thus become an indicator of where problematic content can be found, but this has produced limited understandings of how such content actually circulates. Using pro-eating disorder (pro-ED) communities as a case study, this article explores the practices of circumventing hashtag moderation in online pro-ED communities. It shows how (1) untagged pro-ED content can be found without using the hashtag as a search mechanism; (2) users are evading hashtag and other forms of platform policing, devising signals to identify themselves as “pro-ED”; and (3) platforms’ recommendation systems recirculate pro-ED content, revealing the limitations of hashtag logics in social media content moderation. ","",""
"2019","“Men Are Scum”: Self-Regulation, Hate Speech, and Gender-Based Censorship on Facebook","Because social media sites are self-regulating, each site has developed its own community standards, which serve as regulatory tools. However, the processes of content moderation are often unclear, subjective, and discriminatory. Drawing from a series of interviews with individuals in the “Men Are Scum” movement, this article describes the experiences of women who have been censored on Facebook and explores whether self-regulatory processes on this platform are distinctly gendered. It asserts that both explicit censorship (e.g., limited displays of the body) and implicit censorship (e.g., rampant and unchecked hate speech silencing women’s voices) are operative on Facebook, limiting women’s expressive potentiality. Thus, this article proposes the term “gender-based censorship” as a lens through which to understand women’s experiences on Facebook. These findings help reveal the pitfalls of industry self-regulation in which profit motives are prioritized over protection of users (especially those who may be marginalized offline).","",""
"2019","REDDIT QUARANTINED: CONSEQUENCES OF DEALING WITH DISTRUST IN SOCIAL MEDIA PLATFORMS THROUGH RESTRICTING ENGAGEMENT","Online abuse has become a matter of trust for social media platforms, whose role as a facilitator of public debate has been called into question. In response social media companies have become more active in regulating and banning particular users and channels.&#x0D; Through the use of affordances theory, this paper examines one example of the regulation of content on a social media site, the revamp of the quarantining function on Reddit in late 2018. Quarantines are designed to halt participation within and growth of subreddits without banning them outright.&#x0D; The paper uses quantitative and qualitative data to examine the consequences of this revamp on two subreddits, r/Braincels and r/TheRedPill. Through studying activity levels on these subreddits the paper argues that quarantines did limit discussion within these subreddits. However, it also argues that the revamp had unintended consequences, in particular a growth in distrust between subreddit users and Reddit as a site, and a shift of users away from Reddit to less regulated sites.&#x0D; The paper argues that quarantining shifted the affordances of Reddit, in this instance resulting in greater discouragement of activity on particular subreddits. Using the mechanisms and conditions framework (Davis and Chouinard, 2016) the paper however argues that users adapted to and circumvented this discouragement to continue engaging in particular behavior.&#x0D; While quarantining had short term benefits, using an affordances framework this paper argues it had unintended consequences, ones which can result in a continued radicalization of actions and beliefs, furthering distrust in the online sphere.","",""
"2019","Online content moderation and the Dark Web: Policy responses to radicalizing hate speech and malicious content on the Darknet","De-listing, de-platforming, and account bans are just some of the increasingly common steps taken by major Internet companies to moderate their online content environments. Yet these steps are not without their unintended effects. This paper proposes a surface-to-Dark Web content cycle. In this process, malicious content is initially posted on the surface Web. It is then moderated by platforms. Moderated content does not necessarily disappear when major Internet platforms crackdown, but simply shifts to the Dark Web. From the Dark Web, malicious informational content can then percolate back to the surface Web through a series of three pathways. The implication of this cycle is that managing the online information environment requires careful attention to the whole system, not just content hosted on surface Web platforms per se. Both government and private sector actors can more effectively manage the surface-to-Dark Web content cycle through a series of discrete practices and policies implemented at each stage of the wider process.","",""
"2019","Autism and online recruiting methods: A comparison of Mechanical Turk and discussion forums","In a study by a team at the intersection of information and communication sciences and disorders, researchers worked to design an interactive, online professional development system for academic librarians to better serve students with autism spectrum disorder (ASD). In creating this program, it was imperative to have stakeholder input and support; recruiting members of this population, students with ASD, was critical. Amazon’s Mechanical Turk and online discussion forums, including Reddit, were used for recruitment for an online survey. While there was some overlap in results, there were also marked differences in responses based on online sampling frame. This paper details the online methods used for recruiting members of this community, and compares and contrasts success rates, challenges, and numbers associated with each method.","",""
"2019","Moderator engagement and community development in the age of algorithms"," Online communities provide a forum for rich social interaction and identity development for billions of Internet users worldwide. In order to manage these communities, platform owners have increasingly turned to commercial content moderation, which includes both the use of moderation algorithms and the employment of professional moderators, rather than user-driven moderation, to detect and respond to anti-normative behaviors such as harassment and spread of offensive content. We present findings from semi-structured interviews with 56 volunteer moderators of online communities across three platforms (Twitch, Reddit, and Facebook), from which we derived a generalized model categorizing the ways moderators engage with their communities and explaining how these communities develop as a result. This model contains three processes: being and becoming a moderator; moderation tasks, actions, and responses; and rules and community development. In this work, we describe how moderators contribute to the development of meaningful communities, both with and without algorithmic support. ","",""
"2019","Platform dialectics: The relationships between volunteer moderators and end users on reddit"," Existing literature on the affordances of Internet platforms rarely examines the complex and recursive relationships between the actions of volunteer moderators and the behaviour of end users. Building on existing studies of Internet communities, affordances and ‘public goods’, this article uses an ethnographic approach to analyse two subreddits, r/paleo and r/nootropics, on the social news site reddit. It elaborates on work by Massanari to illustrate how the moderators of forums with an epistemic focus utilise the affordances at their disposal with the aim of mediating trust and establishing a paradigm for constructive discourse. End users respond to these attempts in unpredictable and unforeseen ways, indicating the interpretive flexibility of affordances. The concept of ‘platform dialectics’ is invoked as an overarching description of this phenomenon. ","",""
"2019","Free Speech and Safe Spaces: How Moderation Policies Shape Online Discussion Spaces"," How do moderation policies affect online discussion? This article analyzes nearly a quarter of a million anonymous comments over a 14-month period from two online Reddit forums matched in topic and size, but with differing moderation policies of “safe space” and “free speech.” I found that in the safe space, moderators removed significantly more comments, and authors deleted their own comments significantly more often as well, suggesting higher rates of self-censorship. Looking only at relatively low frequency posters, I found that language in the safe space is more positive and discussions are more about leisure activities, whereas language in the free speech space is relatively negative and angry, and material personal concerns of work, money, and death are more frequently discussed. Importantly, I found that many of these linguistic differences persisted even in comments by users who were concurrently posting in both subreddits. Altogether, these results suggest that differences in moderation policies may affect self-censorship and language use in online space, implicating moderation policies as key sites of inquiry for scholars of democratic discussion. ","",""
"2019","The Civic Labor of Volunteer Moderators Online"," Volunteer moderators create, support, and control public discourse for millions of people online, even as moderators’ uncompensated labor upholds platform funding models. What is the meaning of this work and who is it for? In this article, I examine the meanings of volunteer moderation on the social news platform reddit. Scholarship on volunteer moderation has viewed this work separately as digital labor for platforms, civic participation in communities, or oligarchy among other moderators. In mixed-methods research sampled from over 52,000 subreddit communities and in over a dozen interviews, I show how moderators adopt all of these frames as they develop and re-develop everyday meanings of moderation—facing the platform, their communities, and other moderators alike. I also show how this civic notion of digital labor brings clarity to a strike by moderators in July 2015. Volunteer governance remains a common approach to managing social relations, conflict, and civil liberties online. Our ability to see how communities negotiate the meaning of moderation will shape our capacity to address digital governance as a society. ","",""
"2020","Expanding the debate about content moderation: Scholarly research agendas for the coming policy debates","Content moderation has exploded as a policy, advocacy, and public concern. But these debates still tend to be driven by high-profile incidents and to focus on the largest, US based platforms. In order to contribute to informed policymaking, scholarship in this area needs to recognise that moderation is an expansive socio-technical phenomenon, which functions in many contexts and takes many forms. Expanding the discussion also changes how we assess the array of proposed policy solutions meant to improve content moderation. Here, nine content moderation scholars working in critical internet studies propose how to expand research on content moderation, with implications for policy.","",""
"2020","THE STATE OF GLOBAL HARMFUL CONTENT REGULATION: EMPIRICAL   OBSERVATIONS","Online intermediaries have always been regulated, locked in heated battles around intermediary liability for copyright or privacy reasons (Tusikov, 2016; Gorwa 2019). But a notable trend is the rapidly growing use of policy to try and govern user-generated content with a host of other perceived social or individual harms, such as disinformation, hate speech, and terrorist propaganda (Kaye, 2019; York 2019; Suzor 2019). Even as increasing academic and policy attention is paid to the global ‘techlash’, and leading voices outlining the various ways in which expression online is currently under threat, our understanding of the overall policy landscape remains ad hoc and incomplete. The goal of this paper is thus to present some initial observations on the state of harmful content regulation around the world, drawing upon a new original dataset that seeks to capture the global universe of harmful-content regulatory initiatives for user-generated content online. The first part of the paper presents descriptive results, showing the evolution (and notable increase) in policy development in the past two decades. The second half of the paper provides insight into which specific issue areas have attracted the most formal and informal regulatory arrangements, and assesses the scope (what kind of actors are seen as being a ‘platform,’ and how that is defined), key policy mechanisms (takedown regimes, transparency rules, technical standards), and sanctioning procedures (fines, criminal liability) enacted in these regulations.","",""
"2020","TUNING OUT HATE SPEECH ON REDDIT: AUTOMATING MODERATION AND DETECTING   TOXICITY IN THE MANOSPHERE","Over the past two years social media platforms have been struggling to moderate at scale. At the same time, they have come under fire for failing to mitigate the risks of perceived ‘toxic’ content or behaviour on their platforms. In effort to better cope with content moderation, to combat hate speech, ‘dangerous organisations’ and other bad actors present on platforms, discussion has turned to the role that automated machine-learning (ML) tools might play. This paper contributes to thinking about the role and suitability of ML for content moderation on community platforms such as Reddit and Facebook. In particular, it looks at how ML tools operate (or fail to operate) effectively at the intersection between online sentiment within communities and social and platform expectations of acceptable discourse. Through an examination of the r/MGTOW subreddit we problematise current understandings of the notion of ‘tox¬icity’ as applied to cultural or social sub-communities online and explain how this interacts with Google’s Perspective tool.","",""
"2020","HATRED OF/AND DEMOCRACY: THE POLITICAL CONTRADICTIONS OF REDDIT’S MODERATION STRUCTURE","This paper seeks to interpret Reddit moderation as a problem of political theory, rather than as a debate between the merits of human moderation and algorithmic moderation. Analyzing Reddit’s moderation structure shows that both the human moderation and the algorithmic moderation reinforce a form of anti-politics which leaves users feeling like they have no input and thus no interest in the well-being of the subreddits in which they participate. Online governance structures are largely top down and authoritarian in nature, despite often being couched in democratic rhetoric, reflecting what Jacques Rancière describes as a hatred of democracy. By looking at the example of how r/Canada came to be widely disparaged on Reddit as a bastion of hate, I make the argument that the key to rooting out online hate is not through more human moderation or by giving algorithms more control, but by creating a democratic culture of buy-in through which users are empowered with responsibility for the quality of content in a discussion space.","",""
"2020","Contesting algorithms: Restoring the public interest in content filtering by artificial intelligence"," In recent years, artificial intelligence has been deployed by online platforms to prevent the upload of allegedly illegal content or to remove unwarranted expressions. These systems are trained to spot objectionable content and to remove it, block it, or filter it out before it is even uploaded. Artificial intelligence filters offer a robust approach to content moderation which is shaping the public sphere. This dramatic shift in norm setting and law enforcement is potentially game-changing for democracy. Artificial intelligence filters carry censorial power, which could bypass traditional checks and balances secured by law. Their opaque and dynamic nature creates barriers to oversight, and conceals critical value choices and tradeoffs. Currently, we lack adequate tools to hold them accountable. This paper seeks to address this gap by introducing an adversarial procedure— – Contesting Algorithms. It proposes to deliberately introduce friction into the dominant removal systems governed by artificial intelligence. Algorithmic content moderation often seeks to optimize a single goal, such as removing copyright-infringing materials or blocking hate speech, while other values in the public interest, such as fair use or free speech, are often neglected. Contesting algorithms introduce an adversarial design which reflects conflicting values, and thereby may offer a check on dominant removal systems. Facilitating an adversarial intervention may promote democratic principles by keeping society in the loop. An adversarial public artificial intelligence system could enhance dynamic transparency, facilitate an alternative public articulation of social values using machine learning systems, and restore societal power to deliberate and determine social tradeoffs. ","",""
"2020","No amount of “AI” in content moderation will solve filtering’s prior-restraint problem"," Contemporary policy debates about managing the enormous volume of online content have taken a renewed focus on upload filtering, automated detection of potentially illegal content, and other “proactive measures”. Often, policymakers and tech industry players invoke artificial intelligence as the solution to complex challenges around online content, promising that AI is a scant few years away from resolving everything from hate speech to harassment to the spread of terrorist propaganda. Missing from these promises, however, is an acknowledgement that proactive identification and automated removal of user-generated content raises problems beyond issues of “accuracy” and overbreadth--problems that will not be solved with more sophisticated AI. In this commentary, I discuss how the technical realities of content filtering stack up against the protections for freedom of expression in international human rights law. As policymakers and companies around the world turn to AI for communications governance, it is crucial that we recall why legal protections for speech have included presumptions against prior censorship, and consider carefully how proactive content moderation will fundamentally re-shape the relationship between rules, people, and their speech. ","",""
"2020","Algorithmic content moderation: Technical and political challenges in the automation of platform governance","As government pressure on major technology companies builds, both firms and legislators are searching for technical solutions to difficult platform governance puzzles such as hate speech and misinformation. Automated hash-matching and predictive machine learning tools – what we define here as algorithmic moderation systems – are increasingly being deployed to conduct content moderation at scale by major platforms for user-generated content such as Facebook, YouTube and Twitter. This article provides an accessible technical primer on how algorithmic moderation works; examines some of the existing automated tools used by major platforms to handle copyright infringement, terrorism and toxic speech; and identifies key political and ethical issues for these systems as the reliance on them grows. Recent events suggest that algorithmic moderation has become necessary to manage growing public expectations for increased platform responsibility, safety and security on the global stage; however, as we demonstrate, these systems remain opaque, unaccountable and poorly understood. Despite the potential promise of algorithms or ‘AI’, we show that even ‘well optimized’ moderation systems could exacerbate, rather than relieve, many existing problems with content policy as enacted by platforms for three main reasons: automated moderation threatens to (a) further increase opacity, making a famously non-transparent set of practices even more difficult to understand or audit, (b) further complicate outstanding issues of fairness and justice in large-scale sociotechnical systems and (c) re-obscure the fundamentally political nature of speech decisions being executed at scale.","",""
"2020","Content moderation, AI, and the question of scale"," AI seems like the perfect response to the growing challenges of content moderation on social media platforms: the immense scale of the data, the relentlessness of the violations, and the need for human judgments without wanting humans to have to make them. The push toward automated content moderation is often justified as a necessary response to the scale: the enormity of social media platforms like Facebook and YouTube stands as the reason why AI approaches are desirable, even inevitable. But even if we could effectively automate content moderation, it is not clear that we should. ","",""
"2020","Content moderation: Social media’s sexist assemblages"," This article proposes ‘sexist assemblages’ as a way of understanding how the human and mechanical elements that make up social media content moderation assemble to perpetuate normative gender roles, particularly white femininities, and to police content related to women and their bodies. It investigates sexist assemblages through three of many potential elements: (1) the normatively gendered content presented to users through in-platform keyword and hashtag searches; (2) social media platforms’ community guidelines, which lay out platforms’ codes of conduct and reveal biases and subjectivities and (3) the over-simplification of gender identities that is necessary to algorithmically recommend content to users as they move through platforms. By the time the reader finds this article, the elements of the assemblages we identify might have shifted, but we hope the framework remains useful for those aiming to understand the relationship between content moderation and long-standing forms of inequality. ","",""
"2020","Re-humanizing the platform: Content moderators and the logic of care"," With the goal of re-humanizing discussion platform operations, this study explores the knowledge and aims of commercial content moderators by reframing their work-related ideals through the notion of the “logic of care.” In seeking to expand their professional realm by realigning users, moderators, and technical tools, moderators of discussion forums have turned to machines, ideally freeing up resources for real-time interaction between moderators and those who post. By focusing on care, the study calls for technical innovation that integrates moderators’ aims with artificial intelligence systems. Rather than acknowledging human skills and resources in terms of moderation tools and discussion culture, the current platform logic forces moderators to operate like machines. Their discontent becomes understandable within a logic that diminishes their skills and vision. The moderator is left with assessing separate posts, rather than offering a meta-perspective to the discussion, overseeing and nurturing it. ","",""
"2020","The COVID-19 Mental Health Content Moderation Conundrum"," At the time of writing (mid-May 2020), mental health charities around the world have experienced an unprecedented surge in demand. At the same time, record-high numbers of people are turning to social media to maintain personal connections due to restrictions on physical movement. But organizations like the mental health charity Mind and even the UK Government have expressed concerns about the possible strain on mental health that may come from spending more time online during COVID-19. These concerns are unsurprising, as debates about the link between heavy social media use and mental illness raged long before the pandemic. But our newly heightened reliance on platforms to replace face-to-face communication has created even more pressure for social media companies to heighten their safety measures and protect their most vulnerable users. To develop and enact these changes, social media companies are reliant on their content moderation workforces, but the COVID-19 pandemic has presented them with two related conundrums: (1) recent changes to content moderation workforces means platforms are likely to be less safe than they were before the pandemic and (2) some of the policies designed to make social media platforms safer for people’s mental health are no longer possible to enforce. This Social Media + Society: 2K essay will address these two challenges in depth. ","",""
"2020","Media in the Post #GamerGate Era: Coverage of Reactionary Fan Anger and the Terrorism of the Privileged"," History has recorded the aggressive fandom campaigns of the 2010s, which drove vulnerable minority actors from their careers and homes. #GamerGate was the most iconic of these movements with its claim to focus on the ethics in games journalism. This article examines select news coverage of the #GamerGate movement from its initial outset in 2014 to today to identify how the movement changed news reporting’s approach to fandom harassment stories. #GamerGate’s development and deployment media of frames were examined to see how the repertoires of contention shaped during this period played into social and political events that followed. ","",""
"2021","“Dangerous organizations: Facebook’s content moderation decisions and ethnic visibility in Myanmar”"," On February 5th, 2019 Facebook labeled four Ethnic Armed Organizations (EAOs) in Myanmar as “Dangerous Organizations” thereby formally banning them from using the company’s platform. At the time of the company’s announcement, all four of these groups were in open conflict with the Myanmar military (Tatmadaw) who were themselves in the process of being prosecuted for genocide in the International Court of Justice. As a principle vector for communication in Myanmar, Facebook’s decision directly impacted the ability of these groups to connect with national and international stakeholders during their conflicts with the Tatmadaw. This study looks to examine this decision and other content moderation decisions involving ethnic speech within Myanmar to document Facebook’s evolution from a tool for democratic liberalization to international political authority. While outwardly projecting a stance of neutrality in foreign affairs, this work seeks to demarcate how Facebook’s content moderation practices have transformed the company into a new governmental apparatus freely adjudicating political speech claims around the globe with virtual impunity. Building on scholarly discussions around content moderation and digital governance in media studies, I look to interrogate how Facebook’s positionality affects ethnic visibility in nations beholden to the company for national and worldwide recognition. ","",""
"2021","SAFE FROM “HARM”: THE GOVERNANCE OF VIOLENCE BY PLATFORMS","Platforms have long been under fire for how they create and enforce policies around hate speech, harmful content, and violence. In this study, we examine how three major platforms (Facebook, Twitter, and YouTube) conceptualize and implement policies around how they moderate “harm,” “violence,” and “danger” on their platforms. Through a feminist discourse analysis of public facing policy documents from official blogs and help pages, we found that platforms are often narrowly defining harm and violence in ways that perpetuate ideological hegemony around what violence is, how it manifests, and who it affects. Through this governance, they continue to control normative notions of harm and violence, denying their culpability, and effectively manage perceptions of their actions and directing users’ understanding of what is “harmful” versus what is not. Rather than changing the mechanisms of their design that enable harm, the platforms reconfigure intentionality and causality to try to stop users from being “harmful,” which, ironically, perpetuates harm.","",""
"2021","LOCALIZING CONTENT MODERATION: APPROACHING THE ORIENTATIONAL SPACES OF         FACEBOOK GROUP ADMINS AND MODS","This paper contributes to the burgeoning literature on content         moderation by focusing on its practice in relation to localized social media contexts, an         area which remains under-researched. It makes two key contributions. Firstly, it presents         the results of a study on moderation practices in relation to place-named Facebook groups         across Greater London. Drawing on in-depth interviews with administrators and moderators         from 16 Facebook groups, we focus on exploring how such administrators and moderators         negotiate an apparent ‘orientational’ tension between ‘translocality’ and ‘locality’. On the         one hand, we explore how administrators and moderators oriented partly to what might be         understood as the 'translocal' space of Facebook as a platform. On the other hand, we also         sought to understand how such administrators and moderators orient to the localised         situation surrounding the place-named Facebook group. Our second key contribution aligns         with the conference theme on co-dependence and social media, outlining a conceptual approach         for researching the geographical contexts or ‘place’ of content moderation more broadly. We         emphasize the inherent, practical locality of content moderation. Drawing on a long         tradition of relational approaches in human geography, cultural anthropology and philosophy,         we conceptualize ‘locality’ as something produced through practical action, rather being         pre-given, specific geographical locations. Approaching the place or context of content         moderation relationally, rather than via geographical scales such as local or global, might         not only provide a more context sensitive approach, but also, underline the limits of         large-scale moderation, whether by platforms or governments, or through human or algorithmic         interventions.","",""
"2021","NOT YOUR ROBIN HOOD: GAMESTOP AND PLATFORM ECONOMICS AT PLAY","In January 2021, GameStop and AMC, stocks with no discernable reason         for an increase in value, rose abruptly in price. This was soon understood as the result of         manipulation by members of the r/WallStreetBets subreddit. While individualized stock         trading is often framed as equalizing the playing field, everyday people remain second-class         participants in the stock market, which quickly became clear as trading was suspended on         stocks the Redditors targeted. Backlash against this decision played out across platforms.         We argue that this incident is the inevitable result of treating platforms (and economics)         through the lens of gaming and trolling. The first key issue in the r/WallStreetBets         incident was content moderation, as the Robinhood trading app stopped the trading, the         subreddit the traders used to organize went private, and the corresponding Discord server         was banned. The second important factor in this incident was coordinated inauthentic action,         as thwarted traders immediately turned to the app store and targeted Robinhood with a review         bombing campaign. Third, while the popular response to the r/WallStreetBets saga was         frequently celebratory, seeing their actions as a challenge to capitalism and evidence of         progressive politics, longstanding tropes associating Jewish people with capital, meant that         Redditors’ anti-capitalism slid easily into anti-Semitism. Ultimately, many of the patterns         of the Reddit stock incident locate it in a long history of coordinated internet action         steeped in toxic technocultures. However, the expansion of these practices into taking         direct action on economic systems worth billions of dollars is new and calls for rigorous         attention","",""
"2021","INCELS ON REDDIT: A STUDY IN SOCIAL NORMS AND DECENTRALISED        MODERATION","The social news website Reddit has a long history of hosting communities (‘subreddits’) that advocate or encourage white supremacy (Gillespie 2018), disparagement of minority groups (Topinka 2017), and violence against women (Massanari 2017). As a platform that relies heavily on volunteer moderators to self-govern the subreddits (Matias 2016), Reddit has been criticised for failing to adequately enforce its site-wide rules (Gillespie 2018). Incels—an internet subculture that ascribes to deeply misogynistic beliefs—grew in visibility when they developed subreddits on Reddit. After ongoing criticism and media attention about harmful behaviour of incels both on and off the platform, Reddit imposed escalating sanctions and ultimately banned the most visible of these subreddits over a period of several years. In this paper, we focus on the interaction between formal rules and social norms in incel and related subreddits. This paper aims to improve understanding about how problematic norms are contested in (partially-) decentralised systems of content moderation. We examine discourse about moderation to better understand the role of moderation teams in maintaining and changing social norms in their communities and to examine the interaction between these norms and both sitewide and subreddit-specific rules. Our analysis suggests that the threat of prohibition alone is unlikely to be sufficient to drive cultural change in problematic subreddits. We argue that content moderation is an insufficient frame to understand the regulation of harmful communities; real change requires addressing the underlying cultural norms rather than focusing on individual pieces of content.","",""
"2021","The fabrics of machine moderation: Studying the technical, normative,  and organizational structure  of Perspective API"," Over recent years, the stakes and complexity of online content moderation have been steadily raised, swelling from concerns about personal conflict in smaller communities to worries about effects on public life and democracy. Because of the massive growth in online expressions, automated tools based on machine learning are increasingly used to moderate speech. While ‘design-based governance’ through complex algorithmic techniques has come under intense scrutiny, critical research covering algorithmic content moderation is still rare. To add to our understanding of concrete instances of machine moderation, this article examines Perspective API, a system for the automated detection of ‘toxicity’ developed and run by the Google unit Jigsaw that can be used by websites to help moderate their forums and comment sections. The article proceeds in four steps. First, we present our methodological strategy and the empirical materials we were able to draw on, including interviews, documentation, and GitHub repositories. We then summarize our findings along five axes to identify the various threads Perspective API brings together to deliver a working product. The third section discusses two conflicting organizational logics within the project, paying attention to both critique and what can be learned from the specific case at hand. We conclude by arguing that the opposition between ‘human’ and ‘machine’ in speech moderation obscures the many ways these two come together in concrete systems, and suggest that the way forward requires proactive engagement with the design of technologies as well as the institutions they are embedded in. ","",""
"2021","Disappearing acts: Content moderation and emergent practices to preserve at-risk human rights–related content"," Human rights groups, journalists, and “open source investigators” increasingly depend on social media platforms to collect eyewitness media documenting possible human rights violations and conflicts. And yet, this content—often graphic, controversial, even uploaded by perpetrators—is often removed by the platforms, for various reasons. This article draws on in-depth interviews to examine how practitioners reliant on human rights–related content understand, experience, and deal with platform content moderation and removals in their day-to-day work. Interviews highlighted that both the actual and anticipated removal of social media content complicated and added to practitioners’ work. In addition, practitioners unevenly possess the technical, financial, and organizational resources to mitigate the risks and ramifications of removal by preserving content and appealing content moderation decisions. This article sheds light on the impacts of content moderation for stakeholders other than the primary account holders, and highlights platforms’ affordances and shortcomings as archives of war. ","",""
"2021","The Role of Suspended Accounts in Political Discussion on Social Media: Analysis of the 2017 French, UK and German Elections"," Content moderation on social media is at the center of public and academic debate. In this study, we advance our understanding on which type of election-related content gets suspended by social media platforms. For this, we assess the behavior and content shared by suspended accounts during the most important elections in Europe in 2017 (in France, the United Kingdom, and Germany). We identify significant differences when we compare the behavior and content shared by Twitter suspended accounts with all other active accounts, including a focus on amplifying divisive issues like immigration and religion and systematic activities increasing the visibility of specific political figures (often but not always on the right). Our analysis suggests that suspended accounts were overwhelmingly human operated and no more likely than other accounts to share “fake news.” This study sheds light on the moderation policies of social media platforms, which have increasingly raised contentious debates, and equally importantly on the integrity and dynamics of political discussion on social media during major political events. ","",""
"2021","Reddit Gaming Communities During Times of Transition"," We studied the Reddit communities for three game franchises at the time of a new release in each franchise to see what the members of the Reddit community for the now-older game would do in terms of their community involvement and membership. Specifically, we sought to determine if community members transitioned to the community for a new game, leaving behind their established community devoted to the now-older game. Our main finding was that most people did not move to the new community, but instead remained with their established community. We argue that nostalgia played an important role for community members, whom we frame as fans. Data came from Reddit communities for The Elder Scrolls, Fallout, and the Civilization franchises, comprising data on over 10 million posts from five different year-long time periods, each centered on the release of a new game in each of those franchises. ","",""
"2022","Moderation and authority-building process: the dynamics of knowledge creation on history subreddits","Abstract For the last 30 years, the web has been used as a space of debate and knowledge creation, including historical knowledge. The digital space has the potential to provide a more democratic history that relies on the inclusion of different voices. However, it also raises questions about editing and authority. When attempting to understand authority relations on the web, moderation gains special prominence as it involves actions of exclusion, organisation, and establishment of norms; moderators heavily influence the content created by web users. Here, we investigate knowledge creation considering moderation bias. We address the effects of different moderation practices in history subreddits by analysing how moderators establish authority relations with other users. For that, we use a mixed-methods approach by interpreting the subreddits’ rules and performing network analysis based on the subreddits’ dialogues (2011–2020). The study indicates that the rules have become progressively extensive and stricter over the years, creating appropriate ways for posting submissions and commenting but also affecting broad participation. As central authority figures, moderators engage in processes of sharing authority, rather than shared authority, tending to dominate knowledge creation.","",""
"2022","Antecedents of support for social media content moderation and platform regulation: the role of presumed effects on self and others","ABSTRACT This study examines support for regulation of and by platforms and provides insights into public perceptions of platform governance. While much of the public discourse surrounding platforms evolves at a policy level between think tanks, journalists, academics and political actors, little attention is paid to how people think about regulation of and by platforms. Through a representative survey study of US internet users (N = 1,022), we explore antecedents of support for social media content moderation by platforms, as well as for regulation of social media platforms by the government. We connect these findings to presumed effects on self (PME1) and others (PME3), concepts that lie at the core of third-person effect (TPE) and influence of presumed influence (IPI) scholarship. We identify third-person perceptions for social media content: Perceived negative effects are stronger for others than for oneself. A first-person perception operates on the platform level: The beneficial effects of social media platforms are perceived to be stronger for the self than for society. At the behavioral level, we identify age, education, opposition to censorship, and perceived negative effects of social media content on others (PME3) as significant predictors of support for content moderation. Concerning support for regulation of platforms by the government, we find significant effects of opposition to censorship, perceived intentional censorship, frequency of social media use, and trust in platforms. We argue that stakeholders involved in platform governance must take more seriously the attitudes of their constituents.","",""
"2022","Cognitive assemblages: The entangled nature of algorithmic content moderation"," This article examines algorithmic content moderation, using the moderation of violent extremist content as a specific case. In recent years, algorithms have increasingly been mobilized to perform essential moderation functions for online social media platforms such as Facebook, YouTube, and Twitter, including limiting the proliferation of extremist speech. Drawing on Katherine Hayles’ concept of “cognitive assemblages” and the Critical Security Studies literature, we show how algorithmic regulation operates within larger assemblages of humans and non-humans to influence the surveillance and regulation of information flows. We argue that the dynamics of algorithmic regulation are more liquid, cobbled together and distributed than it appears. It is characterized by a set of shifting human and machine entities, which mix traditional surveillance methods with more sophisticated tools, and whose linkages and interactions are transient. The processes that enable the consolidation of knowledge about risky profiles and contents are, therefore, collective and distributed among humans and machines. This allows us to argue that the cognitive assemblages involved in content moderation become a cobbled space of preemptive calculation. ","",""
"2022","Shall AI moderators be made visible? Perception of accountability and trust in moderation systems on social media platforms"," This study examines how visibility of a content moderator and ambiguity of moderated content influence perception of the moderation system in a social media environment. In the course of a two-day pre-registered experiment conducted in a realistic social media simulation, participants encountered moderated comments that were either unequivocally harsh or ambiguously worded, and the source of moderation was either unidentified, or attributed to other users or an automated system (AI). The results show that when comments were moderated by an AI versus other users, users perceived less accountability in the moderation system and had less trust in the moderation decision, especially for ambiguously worded harassments, as opposed to clear harassment cases. However, no differences emerged in the perceived moderation fairness, objectivity, and participants confidence in their understanding of the moderation process. Overall, our study demonstrates that users tend to question the moderation decision and system more when an AI moderator is visible, which highlights the complexity of effectively managing the visibility of automatic content moderation in the social media environment. ","",""
"2022","Operationalising ‘toxicity’ in the manosphere: Automation, platform governance and community health","Social media platforms have been struggling to moderate at scale. In an effort to better cope with content moderation discussion has turned to the role that automated machine-learning (ML) tools might play. The development of automated systems by social media platforms is a notoriously opaque process and public values that pertain to the common good are at stake within these often-obscured processes. One site in which social values are being negotiated is in the framing of what is considered ‘toxic’ by platforms in the development of automated moderation processes. This study takes into consideration differing notions of toxicity – community, platform and societal by examining three measures of toxicity and community health (the ML tool Perspective API; Reddit’s 2020 Content Policy; and the Sense of Community Index-2) and how they are operationalised in the context of r/MGTOW – an antifeminist group known for its misogyny. Several stages of content analysis were conducted on the top posts and comments in r/MGTOW to examine how these different measures of toxicity operate. This paper provides insight into the logics and technicalities of automated moderation tools, platform governance structures, and frameworks for understanding community metrics to interrogate existing uses of ‘toxicity’ as applied to cultural or social subcommunities online. We make a distinction between two used terms: civility and toxicity. Our analysis points to a tension between current social framings and operationalised notions of ‘toxicity’. We argue that there is a clear distinction between civility and toxicity – incivility is a measure of internal perceptions of harm within a community, whereas toxicity is a measure of the capacity for social harms outside of the bounds of the community. This nuanced understanding will enable more targeted interventions to be developed to destabilise the internal conditions that make groups like r/MGTOW internally ‘healthy’ yet externally toxic.","",""
"2022","The GameStop saga: Reddit communities and the emerging conflict between new and old media","This paper focuses on the (continuing) GameStop (GME) saga, where users of the subreddit Wallstreetbets (and later r/GME) subreddit challenged the flow of financial information from legacy media outlets in understanding financial manipulations and possibilities involved in the GME stock price. An extraordinary surge in value was attributed to activity on Wallstreetbets subreddit where users had been touting the stock for almost a year. While initially portrayed by traditional media as an almost humorous attempt to strike back at Wall Street, legacy reporting soon turned negative. Financial outlet news and information used top down, expert driven communication strategies to claim the surge was ephemeral and the product of crowd sourcing manipulation techniques. They soon stopped reporting altogether. The subreddit communities evolved, using bottom-up communication strategies to challenge “old” media narratives while using open source analysis and platform functionalities to maintain and expand an organized information and support system that functioned outside of traditional media boundaries. Different approaches to gatekeeping (expert driven vs. moderator/user driven) and analysis and dissemination (legacy/historically valid vs. open source) led to very different, often time conflicting, development of information, perspectives and calls to action. These differences, and the way they are treated in the larger society highlight how the Internet has reanimated the Lippman-Dewey debate over information and decision making from the early part of the twentieth century. While Lippman might claim Reddit sites are an illusion, Dewey might recognize possibilities for different type(s) of distributed activities crucial to a vibrant participatory democracy.  ","",""
"2022","Metaphors in moderation"," Volunteer content moderators are essential to the social media ecosystem through the roles they play in managing and supporting online social spaces. Recent work has described moderation primarily as a functional process of actions that moderators take, such as making rules, removing content, and banning users. However, the nuanced ways in which volunteer moderators envision their roles within their communities remain understudied. Informed by insights gained from 79 interviews with volunteer moderators from three platforms, we present a conceptual map of the territory of social roles in volunteer moderation, which identifies five categories with 22 metaphorical variants that reveal moderators’ implicit values and the heuristics that help them make decisions. These metaphors more clearly enunciate the roles volunteer moderators play in the broader social media content moderation apparatus and can drive purposeful engagement with volunteer moderators to better support the ways they guide and shape their communities. ","",""
"2022","Not all who are bots are evil: A cross-platform analysis of automated agent governance"," The growth of online platforms is accompanied by the increasing use of automated agents. Despite being discussed primarily in the context of opinion manipulation, agents play diverse roles within platform ecosystems that raises the need for governance approaches that go beyond policing agents’ unwanted behaviour. To provide a more nuanced assessment of agent governance, we introduce an analytical framework that distinguishes between different aspects and forms of governance. We then apply it to explore how agents are governed across nine platforms. Our observations show that despite acknowledging diverse roles of agents, platforms tend to focus on governing selected forms of their misuse. We also observe differences in governance approaches used by platforms, in particular when it comes to the agent rights/obligations and transparency of policing mechanisms. These observations highlight the necessity of advancing the algorithmic governance research agenda and developing a generalizable normative framework for agent governance. ","",""
"2022","Signaling the Intent to Change Online Communities: A Case From a Reddit Gaming Community"," This study builds on existing research about churn and community movement, examining if language use on Reddit can be used to determine if people signal their intent to relocate to a new community before they actually do so. Using a computational and semantic approach, we studied the subreddits for the game series Fallout at the time Fallout 76 ( FO76) was released to see if the users of the Fallout 4 ( FO4) subreddit signaled how they would react to the new subreddit. The main difference we found was that those who stay in the FO4 subreddit or use both subreddits on average post more often and create longer posts than those who move to the FO76 subreddit or leave. This adds further evidence to support theories about community as communication, and we suggest this finding can help online community managers identify which users may be about to leave the community, aiding retention and the overall health of the community. ","",""
"2022","From Scalability to Subsidiarity in Addressing Online Harm"," Large social media platforms are generally designed for scalability—the ambition to increase in size without a fundamental change in form. This means that to address harm among users, they favor automated moderation wherever possible and typically apply a uniform set of rules. This article contrasts scalability with restorative and transformative justice approaches to harm, which are usually context-sensitive, relational, and individualized. We argue that subsidiarity—the principle that local social units should have meaningful autonomy within larger systems—might foster the balance between context and scale that is needed for improving responses to harm. ","",""
"2022","Do Not Recommend? Reduction as a Form of Content Moderation"," Public debate about content moderation has overwhelmingly focused on removal: social media platforms deleting content and suspending users, or opting not to do so. However, removal is not the only available remedy. Reducing the visibility of problematic content is becoming a commonplace element of platform governance. Platforms use machine learning classifiers to identify content they judge misleading enough, risky enough, or offensive enough that, while it does not warrant removal according to the site guidelines, warrants demoting them in algorithmic rankings and recommendations. In this essay, I document this shift and explain how reduction works. I then raise questions about what it means to use recommendation as a means of content moderation. ","",""
"2022","Safety for Whom? Investigating How Platforms Frame and Perform Safety and Harm Interventions"," This article reports on a thematic content analysis of 486 newsroom posts published between 2016 and 2021 by five prominent digital platforms (Facebook, Tinder, YouTube, TikTok, and Twitter). We aimed to understand how these platforms frame and define the issues of harm and safety, and to identify the interventions they publicly report introducing to address these issues. We found that platforms respond to and draw upon external controversies and media panics to selectively construct matters of concern related to safety and harm. They then reactively propose solutions that serve as justification for further investment in and scaling up of automated, data-intensive surveillance and verification technologies. We examine four key themes in the data: locating harm with bad actors and discrete content objects (Theme 1), framing surveillance and policing as solutions to harm (Theme 2), policing “borderline” content through suppression strategies (Theme 3), and performing diversity and inclusion (Theme 4). ","",""
"2022","Automated Platform Governance Through Visibility and Scale: On the Transformational Power of AutoModerator"," When platforms use algorithms to moderate content, how should researchers understand the impact on moderators and users? Much of the existing literature on this question views moderation as a series of decision-making tasks and evaluates moderation algorithms based on their accuracy. Drawing on literature from the field of platform governance, I argue that content moderation is more than a series of discrete decisions but rather a complex system of rules, mechanism, and procedures. Research must therefore articulate how automated moderation alters the broader regime of governance on a platform. To demonstrate this, I report on the findings of a qualitative study on the Reddit bot AutoModerator, using interviews and trace ethnography. I find that the scale of the bot allows moderators to carefully manage the visibility of content and content moderation on Reddit, fundamentally transforming the basic rules of governance on the platform. ","",""
"2023","From healthy communities to toxic debates: Disqus’ changing ideas about comment moderation","Abstract This article examines how the commenting platform Disqus changed the way it speaks about commenting and moderation over time. To understand this evolving self-presentation, we used the Internet Archive Wayback Machine to analyse the company’s website and blog between 2007 and 2021. By combining interpretative close-reading approaches with computerised distant-reading procedures, we examined how Disqus tried to advance online discussion and dealt with moderation over time. Our findings show that in the mid-2000s, commenting systems were supposed to help filter and surface valuable contributions to public discourse, while ten years later their focus had shifted to the proclaimed goal of protecting public discourse from contamination with potentially harmful (“toxic”) communication. To achieve this, the company developed new tools and features to keep communities “healthy” and to facilitate and semi-automate active and interventive forms of moderation. This rise of platform interventionism was fostered by a turn towards semantics of urgency in the company’s language to legitimise its actions.","",""
"2023","Splintering and centralizing platform governance: how Facebook adapted its content moderation practices to the political and legal contexts in the United States, Germany, and South Korea","ABSTRACT The proliferation of hate speech and disinformation on social media has prompted democratic countries around the world to discuss adequate regulations to limit the power exerted by platforms over national politics. As a result, the once ostensibly uniform content moderation practices of social media companies are becoming increasingly territorialized, and the governance of online political speech is constantly negotiated between global social media platforms and national governments. To comprehend the evolving landscape of online political speech governance, this paper scrutinizes how Facebook has adapted its content moderation practices to the political and legal contexts of three democratic nations: the United States, Germany, and South Korea. We assessed national laws and governmental documents to explain the regulatory landscapes of the three countries, and used VPNs and corporate PR materials to see how Facebook’s platform design and public communication diverge by location. The findings suggest that the seemingly ‘splintering’ regulatory frameworks still have a ‘centralizing’ effect: Facebook formally complies with national laws, but its platform interface and communication activities steer users away from the local systems and towards its centralized operations. We discuss future implications for the regulation of online political speech in democratic nations.","",""
"2023","Social media’s canaries: content moderators between digital labor and mediated trauma"," This paper takes recent PTSD claims by content moderators working for Microsoft and Google as a starting point to discuss the changing nature of trauma in the context of social media and algorithmic culture. Placing these claims in the longer history of how media came to be regarded by clinicians as potentially traumatic, it considers content moderation as a form of immaterial labor, which brings the possibility to be traumatized into the cycle of digital labor. Therefore, to the extent that content moderators’ trauma exists as a clinical condition, it cannot be taken as an incidental side-effect but as a built-in potentiality. It is about the commodification of traumatic vulnerability itself. The discussion then proceeds to speculate about the possibility of using algorithms to identify potentially traumatic content and what would that mean for the understanding of trauma, especially as a mediated experience. ","",""
"2023","ANTICIPATING SOCIO-TECHNICAL CHANGE: USERS BELIEFS AROUND REDDIT’S IPO","Platform changes are a phenomenon endlessly confronting social media users, impacting how they create and consume content. The social media platform “Reddit” is, as of writing, in the midst of a significant planned shift in their socio-economic foundation. Reddit’s owners are moving the company from being a privately held business to a publicly-traded one. While scholarly attention in domains such as human-computer interaction (HCI) has focused on tracing how users understand and respond to changes in the interface features of social media, less consideration has been given to how users make sense of the socio-economic side of the social media platform equation. Through a thematic analysis of Reddit posts and comments mentioning the Reddit IPO made to “r/announcements,” “r/theoryofReddit,” “r/askreddit,” “r/stocks,” “r/wallstreetbets,” and “r/technology,” this study examines how users discuss and make sense of socio-economic change in this social media environment. Findings from this study focus on three themes: 1) What aspects of the techno-cultural facets of Reddit (defaults, protocols, algorithms, and interfaces) users believe might change as a result of the IPO. 2) What policy changes users believe might occur on Reddit, particularly in relation to moderation practices and the kinds of content allowed on the site. And, 3) User beliefs about the particular interests that these changes may serve.","",""
"2023","THE TOXIC TURN? CONCEPTUAL AND METHODOLOGICAL ADVANCES ON PROBLEMATIC CONTENTS ON SOCIAL MEDIA","The ‘toxic turn’ in social media platforms continues unabated. Hate speech, mis- and disinformation, misogynistic and racist speech, images, memes and videos are all far too common on social media platforms and more broadly on the internet. While the diminishing popularity of populist politicians led to hopes for less social toxicity, the Covid-19 pandemic introduced new and more complex dimensions. Tensions have emerged around what constitutes problematic content and who gets to define it. Co-regulation models, such as for example the EC Code of Conduct against Illegal Hate Speech, focus on the legality of certain types of contents, while leaving other categories of problematic contents to be defined by platforms. In parallel, the social media ecosystem became more diverse, as new platforms with hands off moderation policies attracted users who felt too constrained by the policies of mainstream platforms. The proposed panel examines this complex and dynamic landscape by problematizing what is understood as toxic, deplatformed, removable and in general problematic content on platforms with the aim to probe the boundaries of what is constituted as acceptable discourse on platforms and to map its implications.  In particular, this panel discusses the broad definition of ‘problematic content’ employed by social media platforms, a catch-all term that cuts across hate speech and propaganda, including more politically topical content such as mal-, mis-, and disinformation, hyperpartisan and polarising content, but also abusive, misogynistic, racist, and homophobic discourse. The term is also employed to refer to spam and content that infringes upon the Terms of Service or the Community Standards of social media platforms. As such, it is a broad category that resists a narrower classification given the operational scope of its use. Defining what constitutes problematic content is a key operation of platform content moderation policies but is also the subject of intense debates (de Gregorio, 2020; Gillespie, 2018; Gillespie et al., 2020; Gorwa et al., 2020).  The panel interrogates the many definitions and applications of problematic content on social media platforms and applications through an empirically informed lens and focusing on deleted contents, complex mixed narratives, and grey areas, including hidden misinformation on voice applications. Problematic Content according to Twitter Compliance API presents ongoing work on the Twitter Compliance API and the Compliance Firehose, which allow researchers to identify content that has been deleted, deactivated, protected, or suspended from Twitter, a proxy for problematic content. In Multi-Part Narratives on Telegram Siapera presents ongoing research that probes the intersection between Covid-19 scepticism, far right and other political narratives in vaccine hesitant groups on Telegram. The third contribution, What if Bill Gates really is evil, people? Investigating the infodemic’s grey areas discusses the conceptual and methodological definitions of problematic content in relation to work on anti-vax and other conspiratorial narratives on Instagram and on Twitter. The fourth contribution, Misinformation and other Harmful Content in Third-Party Voice Applications focuses on problematic content that is yet to be identified on voice applications such as personal assistants. The article addresses the methodological challenges of identifying and defining such contents on applications that have currently no content moderation policies. All contributions foreground the difficulties and costs of identifying and dealing with problematic contents on social media.  The panel fits with theme of decolonization in two ways: firstly, because it is concerned with the tensions around how toxic/problematic contents are defined and who gets to define them; and secondly, because of its focus on neo-colonial discourses or justifications for colonialism in both narratives hosted by platforms and in platforms’ attempts to regulate content. As some narratives are flagged for removal by social platforms, they also raise the question of who is deciding and what does problematic content mean, with far right discourses exploiting this tension and ironically denouncing any attempt to regulate the public discourse as ideological enforcement and justification for (neo)colonial practices performed by social media platforms. From this perspective, platforms' own claims about what constitutes acceptable content is uncomfortably close to colonial narratives of civilised discourse and brings to the fore the potential for neo-colonial narratives and practices in digital spaces.  References  De Gregorio, G. (2020). Democratising online content moderation: A constitutional framework. Computer Law &amp; Security Review, 36, 105374.  Gillespie, T. (2018). Custodians of the Internet. Yale University Press.  Gillespie, T., Aufderheide, P., Carmi, E., Gerrard, Y., Gorwa, R., Matamoros-Fernández, A., ... &amp; West, S. M. (2020). Expanding the debate about content moderation: Scholarly research agendas for the coming policy debates. Internet Policy Review, 9(4), Article-number.  Gorwa, R., Binns, R., &amp; Katzenbach, C. (2020). Algorithmic content moderation: Technical and political challenges in the automation of platform governance. Big Data &amp; Society, 7(1), 2053951719897945.","",""
"2023","EXPLOITATION IN ONLINE CONTENT MODERATION","This paper presents a normative framework for evaluating the moral and epistemic exploitation that online content moderation workers experience while working for social media companies (often as subcontractors or as Mechanical Turk workers). I argue that the current labor model of commercial content moderation (CCM) is exploitative in ways that inflict profound moral harm and epistemic injustices on the workers. This detailed account of exploitation enables us to see more clearly the contours and causes of the moral and epistemic injustice involved in CCM, and helps us understand precisely why these forms of exploitation are unjust. It also suggests some practical solutions for a more just labor model for the moderation work that shapes our online ecosystem.","",""
"2023","PLATFORM PR – THE PUBLIC MODERATION OF PLATFORM VALUES THROUGH TIKTOK FOR GOOD","TikTok wants to “inspire creativity” and “spark joy,” Meta aims to “bring the world closer together,” and YouTube aspires to “give everyone a voice and show them to the world.” Platforms claim that they want to do $2 . However, they regularly get international attention for being $2 instead. Social media data scandals are a prominent point of research. Yet initiatives to counterbalance these backlashes, such as YouTube’s Black Voices Fund or TikTok for Good are rarely investigated. Although platform initiatives' content is often not on the top of your For You Page (FYP), such social initiatives can tell much about what values a platform aims to promote. Examining the values promoted through social initiatives of platform companies provides a way to understand what these companies try to center as important or worthwhile. This project investigates the promotion of platform values through “TikTok for Good,” based on an inductive and thematic analysis of TikTok for Good videos (n=180). With this study, I aim to explore how platform values can enhance our understanding of the construction of what a platform counts as $2 , what is worth being visible, and in turn, what is not.","",""
"2023","MODERATING MENTAL HEALTH: ARE AUTOMATED SYSTEMS TOO RISK AVERSE?","Across commercial social media platforms and dedicated support forums alike, mental health content raises important questions about what constitutes risk and harm online, and how automated and human moderation practices can be re-configured to accommodate resilient behaviours and social support. In work with three Australian mental health organisations that provide successful discussion and support forums, this paper identifies moderation practices that can help to re-think how mental health content is managed. The work aims to improve safety and resilience in these spaces, drawing insights from successful practices to inform algorithmic and moderator treatment of mental health content more widely across social media. Through an analysis of interviews and workshops with forum managers and moderators, I paper argue that platforms must incorporate strengths-based context (resilience indicators) into their moderation systems and practices, challenging simplistic assessments of mental health content as risk and harm.","",""
"2023","MENTAL HEALTH AND THE DIGITAL CARE ASSEMBLAGE: MODERATION PRACTICES &amp; USER EXPERIENCES","This paper examines the socio-technical ecosystems that shape the moderation of mental health content. To explore how care is formulated across and between different actors and automated systems, I focus on the experiences of moderators and users of three peer-based mental health support platforms. The analysis is framed by the notion of the 'digital care assemblage' to delineate the interactions between goal-oriented moderation policies, automated systems, human content moderators or platform managers, and users seeking or giving help in relation to mental ill-health. Each of these actors contribute to the supportive capacity of the platforms for addressing mental health issues in the community.","",""
"2023","AFTER DEPLATFORMING: RETRACING CONTENT MODERATION EFFECTS ACROSS PLATFORMS AND A POST-AMERICAN WEB","Half a decade ago, social media platforms were widely perceived as revolutionary devices for maximizing political expression around the world. By opening the floodgates to expression, however, the same platforms were also accused of opening the floodgates of hate – allowing, for example, the self-claimed “revolutionary” return of ideas, speech and actors long thought to be relegated to the dustbins of history. This panel examines a three-fold revolution, namely: populist revolutions (on the right) facilitated by agnostic content moderation philosophies; the internal revolutions that platform content moderation underwent to address the political violence of the former; and the adjustments that digital methods research needs to adopt to facilitate content moderation research in a “post-API” environment. The first paper of this panel examines how Twitter’s content moderation has undergone several arbitrary changes before reaching a form of “normative plasticity”, with reinforcement techniques such as demotion and other forms of conditional content obfuscation. The second paper looks at how, despite making profound changes to prevent furthering political violence during elections, Twitter, Facebook, YouTube and Instagram have tended to moderate the Brazilian elections in a dislocated fashion, turning a blind eye to Brazilian militaristic content and focusing instead on what it primarily moderates in a US context. Finally, the third paper offers a set of methods for empirical researchers to capture and study content moderation metadata over time. All three papers aim to contribute to attempts at archiving and studying speech moderation as a public good, in an international context.","",""
"2023","Ethical scaling for content moderation: Extreme speech and the (in)significance  of artificial intelligence"," In this article, we present new empirical evidence to demonstrate the severe limitations of existing machine learning content moderation methods to keep pace with, let alone stay ahead of, hateful language online. Building on the collaborative coding project “AI4Dignity” we outline the ambiguities and complexities of annotating problematic text in AI-assisted moderation systems. We diagnose the shortcomings of the content moderation and natural language processing approach as emerging from a broader epistemological trapping wrapped in the liberal-modern idea of “the human”. Presenting a decolonial critique of the “human vs machine” conundrum and drawing attention to the structuring effects of coloniality on extreme speech, we propose “ethical scaling” to highlight moderation process as political praxis. As a normative framework for platform governance, ethical scaling calls for a transparent, reflexive, and replicable process of iteration for content moderation with community participation and global parity, which should evolve in conjunction with addressing algorithmic amplification of divisive content and resource allocation for content moderation. ","",""
"2023","Common sense or censorship: How algorithmic moderators and message type influence perceptions of online content deletion"," Hateful content online is a concern for social media platforms, policymakers, and the public. This has led high-profile content platforms, such as Facebook, to adopt algorithmic content-moderation systems; however, the impact of algorithmic moderation on user perceptions is unclear. We experimentally test the extent to which the type of content being removed (profanity vs hate speech) and the explanation given for its removal (no explanation vs link to community guidelines vs specific explanation) influence user perceptions of human and algorithmic moderators. Our preregistered study encompasses representative samples ( N = 2870) from the United States, the Netherlands, and Portugal. Contrary to expectations, our findings suggest that algorithmic moderation is perceived as more transparent than human, especially when no explanation is given for content removal. In addition, sending users to community guidelines for further information on content deletion has negative effects on outcome fairness and trust. ","",""
"2023","Do users want platform moderation or individual control? Examining the role of third-person effects and free speech support in shaping moderation preferences"," This study examines social media users’ preferences for the use of platform-wide moderation in comparison to user-controlled, personalized moderation tools to regulate three categories of norm-violating content—hate speech, sexually explicit content, and violent content. Via a nationally representative survey of 984 US adults, we explore the influence of third-person effects and support for freedom of expression on this choice. We find that perceived negative effects on others negatively predict while free speech support positively predicts a preference for having personal moderation settings over platform-directed moderation for regulating each speech category. Our findings show that platform governance initiatives need to account for both actual and perceived media effects of norm-violating speech categories to increase user satisfaction. Our analysis also suggests that users do not view personal moderation tools as an infringement on others’ free speech but as a means to assert greater agency over their social media feeds. ","",""
"2023","What Teams Do: Exploring Volunteer Content Moderation Team Labor on Facebook"," Social media sites such as Facebook depend on tens of millions of volunteer moderators across the globe to facilitate platform-based discussion forums. While research has revealed much about the work that these moderators do, some fundamental questions remain. For example, why do volunteer moderators commonly work as teams rather than individuals? In this article, I use data gathered through digital ethnography with Facebook Group moderators to explore the benefits and challenges of moderation team work. I develop a three-part framework to articulate how teams facilitate logistical, discursive, and emotional labor. Finally, I argue that this empirical analysis reveals otherwise hidden and unacknowledged dimensions of volunteer moderation work that make platform-hosted discussion groups possible. ","",""
"2023","YOLO Publics: The Potential for Creative Subversion of an Online Trading Community"," Digitally mediated publics are often discussed in terms of extremism and radicalization, but it remains possible that digital communication technologies can engender new connections and conversations through “creative subversion.” This article explores the potentials of one specific instance of such creative subversion: the “GameStop rescue” as let by members of the subreddit forum “WallStreetBets” (WSB) in the early months of 2021. From a communicative perspective, what is interesting about this series of events is not only the digital platforms and affordances that enabled it, but also the reckless behavior of WSB members and the ways in which this behavior was communicated—and continues to be celebrated and facilitated by the online trading community. Members share “loss porn,” praise each other for having “diamond hands” when holding on to investments that are losing value, and celebrate the principle of YOLO (you only live once). We conceptualize WSB as a YOLO public, an online community that is loosely and temporarily formed through the common action of seizing the opportunity to wreak havoc around power. Furthermore, we understand the events of the GameStop rescue as a controversial encounter that, we argue, offers hope for digitally mediated publics to develop in dynamic relations of difference rather than as stabilized oppositions. ","",""
"2023","Moderation, Networks, and Anti-Social Behavior Online"," Major open platforms, such as Facebook, Twitter, Instagram, and Tik Tok, are bombarded with postings that violate platform community standards, offend societal norms, and cause harm to individuals and groups. To manage such sites requires identification of content and behavior that is anti-social and action to remove content and sanction posters. This process is not as straightforward as it seems: what is offensive and to whom varies by individual, group, and community; what action to take depends on stated standards, community expectations, and the extent of the offense; conversations can create and sustain anti-social behavior (ASB); networks of individuals can launch coordinated attacks; and fake accounts can side-step sanctioning behavior. In meeting the challenges of moderating extreme content, two guiding questions stand out: how do we define and identify ASB online? And, given the quantity and nuances of offensive content: how do we make the best use of automation and humans in the management of offending content and ASB? To address these questions, existing studies on ASB online were reviewed, and a detailed examination was made of social media moderation practices on major media. Pros and cons of automated and human review are discussed in a framework of three layers: environment, community, and crowd. Throughout, the article adds attention to the network impact of ASB, emphasizing the way ASB builds a relation between perpetrator(s) and victim(s), and can make ASB more or less offensive. ","",""
"2024","Repairing the harm: Toward an algorithmic reparations approach to hate speech content moderation"," Content moderation algorithms influence how users understand and engage with social media platforms. However, when identifying hate speech, these automated systems often contain biases that can silence or further harm marginalized users. Recently, scholars have offered both restorative and transformative justice frameworks as alternative approaches to platform governance to mitigate harms caused to marginalized users. As a complement to these recent calls, in this essay, I take up the concept of reparation as one substantive approach social media platforms can use alongside and within these justice frameworks to take actionable steps toward addressing, undoing and proactively preventing the harm caused by algorithmic content moderation. Specifically, I draw on established legal and legislative reparations frameworks to suggest how social media platforms can reconceptualize algorithmic content moderation in ways that decrease harm to marginalized users when identifying hate speech. I argue that the concept of reparations can reorient how researchers and corporate social media platforms approach content moderation, away from capitalist impulses and efficiency and toward a framework that prioritizes creating an environment where individuals from marginalized communities feel safe, protected and empowered. ","",""
"2024","Sharenting and social media properties: Exploring vicarious data harms and sociotechnical mitigations"," In this paper, we demonstrate how social media technologies can co-produce data-related harms unless preventative measures are instituted. To this end, we draw on a passive ethnography of a public Facebook group in the UK practicing sharenting which occurs when parents and guardians post sensitive and identifying information about children in their care on social media. Theoretically, we draw on the ‘harm translation’ concept from digital criminology and the ‘seductions of crime’ perspective from cultural criminology. Further we analyse documents on the operations of Facebook's content filtering algorithms published by Meta (Facebook's parent company). With insights from these sources, we demonstrate how platform technologies go beyond facilitation to the inadvertent co-production of harm via embedded mediative properties that shape user perception and action. We show that, in the specific context of sharenting, the properties invite rather than simply facilitate the practice and can also invite subsequent misuses of child-centric data. Through our analysis of these dynamics, we set out an empirical basis for challenging reductive depictions of social media technologies as solely facilitative of human action including harmful conduct. We also outline our vision to integrate insights from the analysis into a new sociotechnical harm prevention framework informed by Natural Language Processing approaches. ","",""
"2024","Platform policy and online abuse: Understanding differential protections for public figures"," Public figures are subject to high rates of online abuse than everyday users. This article presents findings from a study on digital platforms’ higher threshold for protecting public figures in contrast to everyday users. Presenting a summary of extant literature on the experience, impact and harms of online abuse of public figures, we analyse 31 platform terms of service and related policies to understand the extent to which platforms openly differentiate between public figures and other users. We focus on platforms’ use of ‘newsworthiness’ and ‘public interest’ to justify the differential threshold. Using a cultural-informed approach, we analyse platforms’ reliance on ‘newsworthiness’ and ‘public interest’ justifications to argue that these justifications are utilised without regard for the histories, risk assessment, ethics and labour-intensive processes in which the concepts of newsworthiness and public interest became familiar among more traditional media forms such as news organisations. ","",""
"2024","Norm enforcement on and of Reddit: Rules of engagement and participation","The social platform Reddit hosts a set of communities that denounce offensive behavior, invoking scrutiny and shame on (categories of) individuals. Despite varying in their targets, they all promote actionable content to an audience who can view, share and comment on it. These groups allow a global public to air grievances, enabling both accountability and abuse. Following high profile scandals, Reddit routinely sanctions and purges problematic ‘subreddits’. As a matter of self-preservation, subreddits that watch over the public also maintain heightened scrutiny of their own members. Group rules and other prescriptive texts are a means to instill this scrutiny among a broader audience. In analyzing rules and other content management practices in 68 shaming based subreddits, this paper considers how these groups temper platform-based denunciation.","",""
"2024","Opaque algorithms, transparent biases: Automated content moderation during the Sheikh Jarrah Crisis","Social media platforms, while influential tools for human rights activism, free speech, and mobilization, also bear the influence of corporate ownership and commercial interests. This dual character can lead to clashing interests in the operations of these platforms. This study centers on the May 2021 Sheikh Jarrah events in East Jerusalem, a focal point in the Israeli-Palestinian conflict that garnered global attention. During this period, Palestinian activists and their allies observed and encountered a notable increase in automated content moderation actions, like shadow banning and content removal. We surveyed 201 users who faced content moderation and conducted 12 interviews with political influencers to assess the impact of these practices on activism. Our analysis centers on automated content moderation and transparency, investigating how users and activists perceive the content moderation systems employed by social media platforms, and their opacity. Findings reveal perceived censorship by pro-Palestinian activists due to opaque and obfuscated technological mechanisms of content demotion, complicating harm substantiation and lack of redress mechanisms. We view this difficulty as part of algorithmic harms, in the realm of automated content moderation. This dynamic has far-reaching implications for activism’s future and it raises questions about power centralization in digital spaces.","",""
"2024","Unpacking platform governance through meaningful human agency: How Chinese moderators make discretionary decisions in a dynamic network"," How platforms moderate online content remains “black-boxed” due to diverging and sometimes conflicting logics in the governance network of different forces and stakeholders. The key to unpacking the “black-box” lies with meaningful human agency in balancing these logics in contextualized practice. This article examines human moderators’ decision-making based on qualitative data drawn from Chinese leading platforms. Grounded theory analysis theorizes various contributing (f)actors and introduces a model of “bounded discretion and tacit interaction in an elastic dynamic” to address moderation decision-making. It argues that it is though bounded human agency that platform governance is enacted, negotiated, and further “black-boxed” as inevitable human inconsistencies and contingencies are molded into governance. This study provides useful categories for analyzing platform moderation, encodes meaningful human agency as bounded discretion in the governance network, and unpacks the “black-box” of platform governance as an unfolding process of interactions among institutions, forces, and most importantly, human participants. ","",""
"2024","Social media regulation, third-person effect, and public views: A comparative study of the United States, the United Kingdom, South Korea, and Mexico"," Given the prevalence of misinformation on social media and accompanying negative externalities, platform regulation has become a highly contested public issue globally. This study investigated (a) what global publics think about platform regulation and (b) the psychological mechanisms underlying such opinions through the lens of the third-person effect. Four national surveys, conducted in the United States, the United Kingdom, South Korea, and Mexico in April–September 2021, revealed that both presumed media influence on self and others play important but different roles in predicting support for two distinctive forms of platform regulation (i.e. government regulation of social media platforms versus content moderation by social media platforms). Self-efficacy (self-perceived ability to spot misinformation) and other-efficacy (perception of others’ ability to spot misinformation) were identified as two crucial antecedents of third-person perception. There were also nuanced but noteworthy differences in public attitudes toward platform regulations across the four countries studied. ","",""
"2024","Why do volunteer content moderators quit? Burnout, conflict, and harmful behaviors"," Moderating content on social media can lead to severe psychological distress. However, little is known about the type, severity, and consequences of distress experienced by volunteer content moderators (VCMs), who do this work voluntarily. We present results from a survey that investigated why Facebook Group and subreddit VCMs quit, and whether reasons for quitting are correlated with psychological distress, demographics, and/or community characteristics. We found that VCMs are likely to experience psychological distress that stems from struggles with other moderators, moderation team leads’ harmful behaviors, and having too little available time, and these experiences of distress relate to their reasons for quitting. While substantial research has focused on making the task of detecting and assessing toxic content easier or less distressing for moderation workers, our study shows that social interventions for VCM workers, for example, to support them in navigating interpersonal conflict with other moderators, may be necessary. ","",""
"2024","‘Just a little hack’: Investigating cultures of content moderation circumvention by Facebook users"," As social media platforms adapt their rules to limit the presence, spread, and amplification of harmful content on their services, users develop strategies to circumvent content moderation policies. To better understand cultures of content moderation circumvention, including the types of rules that Facebook users seek to circumvent, we analysed a sample of YouTube videos and Reddit threads in which users discuss content moderation circumvention. We show how Facebook users turn to others across platforms to obtain information about circumvention methods. We observe that these users often discuss overcoming Facebook’s content moderation policies in terms that downplay the significance of their intended actions. We suggest that where Facebook’s policies and enforcement measures fail to deter rule violations that may facilitate harm, Facebook should consider new culture-driven approaches to platform governance that foster prosocial environments and engender compliance with platform rules. ","",""
"2024","Facebook’s platform coloniality: At the nexus of political economy, nation-state’s internal colonialism, and the political activism of the marginalized"," This article explores Facebook’s censorship of Kurdish political activism at the request of the Turkish government. I argue that Facebook’s censorship of political voices belonging to the marginalized Kurdish community is an articulation of platform coloniality, an outcome constituted by the intersecting of the social media giant’s global political economy imperatives with racialized and hierarchized conceptions of human worth. The effects of platform coloniality are exacerbated due to it being mediated by the Turkish nation-state’s internal colonial politics and militarist regional policies, thus intensifying the marginalization of Kurds inside and outside Turkey. Covered in a typical neoliberal discourse of freedom and human rights, platform coloniality represents a continuation of the age-old patterns of Western power and its flow toward the Global South. ","",""
"2024","The psychology of volunteer moderators: Tradeoffs between participation, belonging, and norms in online community governance"," Online communities rely on effective governance for success, and volunteer moderators are crucial for ensuring such governance. Despite their significance, much remains to be explored in understanding the relationship between community governance processes and moderators’ psychological experiences. To bridge this gap, we conducted an online survey with over 600 moderators from Reddit communities, exploring the link between different governance strategies and moderators’ needs and motivations. Our investigation reveals a contrast to conventional views on democratic governance within online communities. While participatory processes are associated with higher levels of perceived fairness, they are also linked with reduced feelings of community belonging and lower levels of institutional acceptance among moderators. Our findings challenge the assumption that greater democratic involvement unequivocally leads to positive community outcomes, suggesting instead that more centralized governance approaches can also positively affect moderators’ psychological well-being and, by extension, community cohesion and effectiveness. ","",""
"2024","The owners of information: Content curation practices of middle-level gatekeepers in political Facebook groups"," Volunteer moderators play a key role when making judgements about which online content should be accepted and which should be removed. As such, their work fundamentally shapes the digital social and political spheres. Using the data obtained from 15 Facebook group moderator interviews as research data, this study focused on the content curation work by the middle-level gatekeepers of Finnish political discussion groups on Facebook. The findings show that the moderators feel strong ownership of the groups they moderate and of the information such groups provide, and as a result, they strongly shape the groups’ discussion and governing policy. Facebook’s governing policy for groups is vague, which gives space for group norms and identities to develop. The stakeholder groups (i.e. the platform administration, moderators and users) do not attend to the governance process all together, so negotiations among them are almost non-existent. ","",""
"2024","When content moderation is not about content: How Chinese social media platforms moderate content and why it matters"," Content moderation has become an essential part of the business of social media platforms, yet how it works remains largely a mystery in some important cases, particularly with regard to platforms run by Chinese companies. This research examines the latest automated moderation approaches adopted by Chinese short video platforms. Drawing on expert interviews and documentary research, we argue that Chinese platforms are moving away from a semantic approach, one that aims to grasp the meaning of content, and toward regulating the ambient element, which we define as the pervasive information that immediately surrounds content and enacts its overall character and impact. Applying a consequentialist ethics lens to investigate this turn, we argue that the ambient shift represents a more proactive approach to moderation, one intended to create a generally beneficial informational environment for platform users. This contrasts with reactive, individualistic moderation regimes grounded in the principle of informational neutrality. ","",""
"2024","Who Can Say What? Testing the Impact of Interpersonal Mechanisms and Gender on Fairness Evaluations of Content Moderation"," Content moderation is commonly used by social media platforms to curb the spread of hateful content. Yet, little is known about how users perceive this practice and which factors may influence their perceptions. Publicly denouncing content moderation—for example, portraying it as a limitation to free speech or as a form of political targeting—may play an important role in this context. Evaluations of moderation may also depend on interpersonal mechanisms triggered by perceived user characteristics. In this study, we disentangle these different factors by examining how the gender, perceived similarity, and social influence of a user publicly complaining about a content-removal decision influence evaluations of moderation. In an experiment ( n = 1,586) conducted in the United States, the Netherlands, and Portugal, participants witnessed the moderation of a hateful post, followed by a publicly posted complaint about moderation by the affected user. Evaluations of the fairness, legitimacy, and bias of the moderation decision were measured, as well as perceived similarity and social influence as mediators. The results indicate that arguments about freedom of speech significantly lower the perceived fairness of content moderation. Factors such as social influence of the moderated user impacted outcomes differently depending on the moderated user’s gender. We discuss implications of these findings for content-moderation practices. ","",""
"2025","Platform gaslighting: A user-centric insight into social media corporate communications of content moderation"," This paper delves into communications dynamics between social media platforms and their users as they navigate the opacity of governance policies. Using Meta and TikTok as case studies, we reveal that gaslighting – traditionally linked with relationship abuse where one partner undermines the validity of the other's experience – has become a pervasive communication strategy for platforms, manifesting in numerous instances where automated and human platform communications have directly contradicted users’ experiences, evidence and research. We analyse 36 diverse interview datasets and six public platform responses to content moderation issues, highlighting the systemic nature of this phenomenon within digital spaces. By moving beyond shadowbanning and isolated platform-to-user dialogue, we expand the scholarly understanding of platform gaslighting to encompass a broader spectrum of governance-related communications. Our dataset draws from seemingly disparate groups who share moderation experiences: Jewish creators engaged in combating antisemitism, Palestinian creators advocating for human rights and sex-positive creators, whose expertise and stories are dismissed and belittled by platforms. Our participants' experiences demonstrate that conceptualising platform communications as gaslighting can help expose corporate power imbalances in platform-user interactions, particularly in cases where platforms govern through undisclosed practices such as shadowbanning and de-platforming triggered by malicious flagging, along with ambiguous communication from their representatives. We demonstrate how the dismissal or minimisation of participants’ traumatic experiences by platforms’ automated processes and human teams is weaponised to inflict epistemic injustice, consolidate power, safeguard public image and evade accountability. ","",""
"2025","‘We do not marshal your feed’: How Alt Tech platforms (re) conceptualise safety","             This article explores how Alt Tech platforms – Parler, Truth Social, Gab, Rumble, BitChute, Odyssey, Gettr, and Minds – conceptualise a core social media value: safety. These platforms, often associated with the Alt-Right, challenge mainstream social media by reinterpreting the value of safety. Using a mixed-method approach to study values promoted in social media policies, this study reveals that Alt Tech platforms reject traditional safety measures, viewing them as censorship, manifesting in three key strategies: advocating for unrestricted freedom of expression; reframing safety-oriented governance as censorship, and promoting an ideal of the digitally             mature             , user. Our findings illuminate how Alt Tech platforms both perform safety while discursively rejecting its enforcement.           ","",""
"2025","Safer spaces by design? Federated socio-technical architectures in content moderation","","",""
"2025","Introduction to the special issue on content moderation on digital platforms","","",""
"2025","DOES ALGORITHMIC CONTENT MODERATION RPOMOTE DEMOCRATIC DISCOURSE? RADICAL DEMOCRATIC CRITIQUE OF TOXIC LANGUAGE AI","Algorithmic content moderation is becoming a common practice employed by many social media platforms to regulate ‘toxic’ language and to promote democratic public conversations. This paper provides a normative critique of politically liberal assumption of civility embedded in algorithmic moderation, illustrated by Google’s Perspective API. From a radical democratic standpoint, this paper normatively and empirically distinguishes between incivility and intolerance because they have different implications for democratic discourse. The paper recognises the potential political, expressive, and symbolic values of incivility, especially for the socially marginalised. We, therefore, argue against regulation of incivility using AI. There are, however, good reasons to regulate hate speech but it is incumbent upon the users of AI moderation to show that this can be done reliably. The paper emphasises the importance of detecting diverse forms of hate speech that convey intolerant and exclusionary ideas without using explicitly hateful or extremely emotional wording. The paper then empirically evaluates the performance of the current algorithmic moderation to see whether it can discern incivility and intolerance and whether it can detect diverse forms of intolerance. Empirical findings reveal that the current algorithmic moderation does not promote democratic discourse, but rather deters it by silencing the uncivil but pro-democratic voices of the marginalised as well as by failing to detect intolerant messages whose meanings are embedded in nuances and rhetoric. New algorithmic moderation should focus on the reliable and transparent identification of hate speech and be in line with the feminist, anti-racist, and critical theories of democratic discourse.","",""
"2025","LOCALIZED VOLUNTEER MODERATION AND ITS DISCURSIVE CONSTRUCTION","The social media industry has begun more prominently positioning itself as a vehicle for tapping into local community. Facebook offers hundreds of region-specific community groups, proudly touting these in nation-wide commercials. Reddit has hundreds of subreddits focused on specific states, cities, and towns. And Nextdoor encourages users to sign up and “Get the most out of your neighborhood.” In these locally oriented digital spaces, users interact, discuss community issues, and share information about what is happening around them. Volunteer moderators with localized knowledge are important agents in the creation, maintenance, and upkeep of these digital spaces. And, as we show, Facebook, Reddit, and Nextdoor create strategic communication to guide this localized volunteer moderator labor to realize specific goals within these spaces. In this work, we ask: “What are the promises the social media industry make about local community groups, and how do they position volunteer moderators to help realize those promises?” Through a qualitative content analysis of 849 documents produced by Facebook, Reddit, and NextDoor, we trace how platforms position their version of local community as slightly different utopian spaces, and channel volunteer moderator labor both through direct instruction and appeals to civic virtue.","",""
"2025","TRUST IN ALTERNATIVE GOVERNORS: EXPLORING USER CONFIDENCE IN COMPANIES, STATES AND CIVIL SOCIETY IN PLATFORM CONTENT MODERATION","        Social media platforms such as Facebook, X and TikTok are the “new governors” or “custodians” of the Internet (Klonick 2018; Gillespie 2018). How they moderate global speech online affects the communication practices of billions of people and it can make or break social movements and political resistance, and generally be a critical risk factor for human rights violations. These platforms are increasingly joined by states, international organizations, civil society, journalists and others in defining and interpreting the limitations of speech online, be it through legislation, guidelines or by helping platforms to distinguish misinformation from legitimate content. In parallel, researchers ponder questions concerning the legitimacy of various approaches of content moderation (Haggart &amp; Keller 2021; Suzor 2019), which must extend to the question of which actors ought to fulfill which function in the moderation of content. A legitimate content moderation constellation (and potentially division of labor) is arguably one that is perceived to be legitimate by the “governed” themselves (for whatever qualities are appraised by them). As of today, however, we have little empirical knowledge about what users actually think about content moderation. The current paper presents novel empirical evidence on how users perceive platform content moderation and how they perceive content moderation roles of different governors of speech. The quantitative analysis is based on a survey of more than 15,000 Facebook and Instagram users in 33 countries in the Global South and Eastern Europe, which was conducted in six languages in late 2022 and early 2023.        ","",""
"2025","GPT4 V THE OVERSIGHT BOARD: USING LARGE LANGUAGE MODELS FOR CONTENT MODERATION","Large-scale automated content moderation on major social media platforms continues to be highly controversial. Moderation and curation are central to the value propositions that platforms provide, but companies have struggled to convincingly demonstrate that their automated systems are fair and effective. For a long time, the limitations of automated content classifiers in dealing with borderline cases have seemed intractable. With the recent expansion in the capabilities and availability of large language models, however, there is reason to suspect that more nuanced automated assessment of content in context may now be possible. In this paper, we set out to understand how the emergence of generative AI tools might transform industrial content moderation practices. We investigate whether the current generation of pre-trained foundation models may expand the established boundaries of the types of tasks that are considered amenable to automation in content moderation.  This paper presents the results of a pilot study into the potential use of GPT4 for content moderation. We use the hate speech decisions of Meta’s Oversight Board as examples of covert hate speech and counterspeech that have proven difficult for existing automated tools. Our preliminary results suggest that, given a generic prompt and Meta’s hate speech policies, GPT4 can approximate the decisions and accompanying explanations of the Oversight Board in almost all current cases. We interrogate several clear challenges and limitations, including particularly the sensitivity of variations in prompting, options for validating answers, and generalisability to examples with unseen content.","",""
"2025","AUTOMODERATOR AS AN EXAMPLE OF COMMUNITY DRIVEN PRODUCT DESIGN","Rushes to adopt the latest technologies to the field of community moderation are generally inequitable for volunteer communities. The closed-door nature of product development at the majority of tech companies means that the logic underlying the creation of new features is opaque. What does this mean for those who want to equitably employ newer technologies in service of volunteer moderators? We present the development and deployment of the Wikimedia Foundation’s Automoderator product as a contemporary alternative to product development processes.  We focus on the collaborative process undertaken between the Moderator Tools product team at the Wikimedia Foundation, and volunteer moderator communities, to design and build Automoderator. Automoderator is an automated anti-vandalism tool, which uses a language-agnostic ML model that predicts the probability of an edit being reverted. The product team integrated volunteer feedback and direction on a continuous basis. This included the use of existing community-created tools to guide Automoderator's direction, the creation and dissemination of a spreadsheet-based testing tool, soliciting user feedback on a central project page, and integrating an extension to allow communities to control Automoderator's behavior directly. We conclude by discussing the limitations and trade-offs of this approach to product development.","",""
"2025","BORDERLINE CONTENT AND PLATFORMISED SPEECH GOVERNANCE: MAPPING TIKTOK’S MODERATION CONTROVERSIES IN SOUTH AND SOUTHEAST ASIA","Content moderation comes with trade-offs and moral dilemmas, particularly for transnational platforms governing borderline content where the boundaries of acceptability are subject to debate. While extensive research has explored the legality and legitimacy of $2 in democratic contexts, few address the complexities of less-than-democratic developing nations. Through socio-legal analysis and controversy mapping of TikTok’s localised moderation in South and Southeast Asia, the study examines how major actors negotiate the shifting boundaries of online speech. The analysis reveals that neither the platform nor regional states effectively govern borderline content. Primarily, TikTok localises its moderation based on pragmatic necessity rather than moral obligations, intentionally sidestepping contentious political controversies. Governments demonstrate strong will to control online discourse, leveraging legal uncertainty to advance political interests. Local content governance thus always relies on vague rationales around securitisation and morality. The contradictory goals of (de)politicising borderline moderation seemingly counterbalance each other, yet in practice lead to an accountability vacuum without legitimate interests. Given the lack of normative common ground, the study highlights the significance of procedural justice and civic participation to mitigate rhetoric that rationalises imposition of speech norms hinging on imbalanced political power.","",""
"2025","Lexical algorithmic resistance: Tactics of deceiving Arabic content moderation algorithms on Facebook","This study examines the lexical tactics of algorithmic resistance employed by Arab users to deceive Facebook content moderation algorithms in the case of censoring pro-Palestinian voices on Facebook, a phenomenon particularly conspicuous during the Palestine–Israel incidents of evicting East Jerusalem's Sheikh Jarrah neighbourhood in May 2021. It has since escalated, becoming increasingly pronounced with the ongoing war that commenced on 7 October 2023, known as the ‘Operation Al-Aqsa Flood’ and continues to unfold to the present day. To achieve this aim, Facebook data scraping was used to extract comprehensive insights on the most frequent lexical resistance techniques, the most used resistance keywords, the geographical mapping of users and the related socio-political context. This study draws upon critical perspectives from data colonialism, resistance studies, critical platform studies and digital humanities to propose the concept of lexical algorithmic resistance as a conceptual framework for understanding the dynamics of language-based algorithmic resistance. By elucidating how language becomes a site of resistance, this study contributes to perspectives that deepen our understanding of power dynamics and resistance tactics in the face of data colonialism and highlights the dual nature of digital tools, which can be wielded both as instruments of oppression and tools of resistance. The study revealed the diversity of lexical algorithmic resistance techniques employed, the keywords of resistance utilised in the Arabic sphere and the variations thereof at spatial and regional levels.","",""
"2025","Reporting online abuse to platforms: Factors, interfaces and the potential for care","Evidence suggests that the rate of reporting abuse, harassment and problematic content to platforms is substantially low. This article assesses the extent to which platform interfaces may contribute to discouraging the use of reporting as a remedy to online harms. Using a walkthrough method, we analyse reporting interfaces for the extent to which they may contribute to a lack of trust in reporting. The study found that reporting interfaces (1) did not provide appropriate access to platform policy or guidelines, (2) failed to provide options for dialogue, testimonials or mechanisms to report in formats supporting user wellbeing needs, (3) were consistently framed as individualising and transactional rather than brokering care or peer support, and (4) added to the opacity of platform intervention and decision-making processes. We argue the available interfaces do not do enough to protect users from digital harms.","",""
"2025","A history of the advice genre on Reddit: Evolutionary paths and sibling rivalries","Though there is robust literature on the history of the advice genre, Reddit is an unrecognized but significant medium for the genre. This lack of attention, in part, stems from the lack of a coherent timeline and framework for understanding the emergence of dozens of advice-related subreddits. Noting the challenges of Reddit historiography, I trace the development of the advice genre on the platform, using the metaphors of evolutionary and family trees. I make use of data dumps of early Reddit submissions and interviews with subreddit founders and moderators to plot the development of advice subreddits through the periods of subreddit explosion (2009–2010), the emergence of judgment subreddits (2011–2013; 2019–2021), and the rise of meta subreddits (2020–2023). Additionally, I specify a lexicon for understanding the relationships between subreddits using the metaphor of tree branches. For example, new subreddits might spawn, fork, or split relative to existing subreddits, and their content is cultivated by meta subreddits by way of filtration, compilation, and syndication.","",""
"2025","Navigating the gray areas of content moderation: Professional moderators’ perspectives on uncivil user comments and the role of (AI-based) technological tools"," Professional content moderators are responsible for limiting the negative effects of online discussions on news platforms and social media. However, little is known about how they adjust to platform and company moderation strategies while viewing and dealing with uncivil comments. Using qualitative interviews ( N = 18), this study examines which types of comments professional moderators classify as actionable, which (automated) strategies they use to moderate them, and how these perceptions and strategies differ between organizations, platforms, and individuals. Our results show that moderators divide content requiring intervention into clearly problematic and “gray area” comments. They (automatically) delete clear cases but use interactive or motivational moderation techniques for “gray areas.” While moderators crave more advanced technologies, they deem them incapable of addressing context-heavy comments. These findings highlight the need for nuanced regulations, emphasize the crucial role of moderators in shaping public discourse, and offer practical implications for (semi-)automated content moderation strategies. ","",""
"2025","Politics of Deliberate Inaction: The disconnect between platform justifications and user imaginaries on content moderation in a ‘free speech’ online forum"," This article analyses the ‘free speech’ online forum Flashback, which adheres to a strict non-interference policy when it comes to user-generated content, but beyond this also forbids users from deleting their own content or accounts. Through a qualitative content analysis, this article sought to understand the relationship between the platform and its users with respect to this unconventional approach to moderation and content removal. This article discusses both the position(s) taken by Flashback as it pertains to its policy of minimal moderation, and the expectations as expressed by users navigating Flashbacks rules and their practical implementations. The article shows a discrepancy between how Flashback (incoherently) justifies minimal moderation and how users had imagined the platform operating. The article also discusses how Flashback maintains these policies through its community’s active encouragement via supportive posting and silencing of non-conformers, and the consequences that Flashback’s inaction has in terms of residual hate. ","",""
"2025","Moderating mental health: Addressing the human–machine alignment problem through an adaptive logic of care"," Covid-19 deepened the need for digital-based support for people experiencing mental ill-health. Discussion platforms have long filled gaps in health service provision and access, offering peer-based support usually maintained by a mix of professional and volunteer peer moderators. Even on dedicated support platforms, however, mental health content poses difficulties for human and machine moderation. While automated systems are considered essential for maintaining safety, research is lagging in understanding how human and machine moderation interacts when addressing mental health content. Working with three digital mental health services, we examine the interaction between human and automated moderation of discussion platforms, contrasting ‘reactive’ and ‘adaptive’ moderation practices. Presenting ways forward for improving digital mental health services, we argue that an integrated ‘adaptive logic of care’ can help manage the interaction between human and machine moderators as they address a tacit ‘risk matrix’ when dealing with sensitive mental health content. ","",""
"2025","To let content be or not be: Understanding the decision-making process of content moderators on social media platforms","Content moderation plays a pivotal role in structuring online speech, but the human labour and the everyday decision-making process in content moderation remain underexamined. Informed by in-depth interviews with 16 content moderators in India, in this research, we analyse the decision-making process of commercial content moderators through the concept of sensemaking. We argue that moderation decisions are made in the context of the industry’s plural policies and efficiency requirements. An interplay of four cognitive processes of pattern identification, subjective perceptions, shared knowledge, and process optimization influences the final judgement. Once sense is enacted in the decision-making process, the sensibilities are retained by the adept moderator for future moderation decisions. Visibilizing the labour process behind commercial content moderation, we argue that everyday moderation decisions unfold in a socio-technical and economic assemblage wherein decisions are decontextualised and plausibility driven rather than consistency driven.","",""
"2025","The importance of centering harm in data infrastructures for ‘soft moderation’: X’s Community Notes as a case study","This article critically examines the social implications of data infrastructures designed to moderate contested content categories such as disinformation. It does so in the context of new online safety regulation (e.g. the EU Digital Services Act) that pushes digital platforms to improve how they tackle both illegal and ‘legal but harmful’ content. In particular, we investigate and conceptualise X’s Community Notes, a tool that uses ‘human-AI cooperation’ to add context to tweets, as a data infrastructure for ‘soft moderation’. We find that Community Notes is limited when dealing with under-acknowledged online harms, such as those derived from the intersection between disinformation and humour. While research points to the potential of content moderation solutions that combine automation with humans-in-the-loop, we show how this approach can fail when disinformation is poorly defined in policy and practice.","",""
"2025","From Volunteerism to Corporatization: Analyzing Participation in the 2015 and 2023 Reddit Blackouts"," Reddit, one of the largest global social media platforms, has undergone significant transformations since its inception in 2005. From a loosely structured, niche platform to a globally recognized company with a standardized and regulated governance system, Reddit’s evolution has been marked by a shift in the power dynamics between its owners, moderators, and users. 2015 and 2023 were marked by the occurrence of two prominent protests, termed “blackouts.” Moderators of numerous subreddits, though not all, disabled public access to their subreddits, thereby protesting the company’s policies and policy changes and challenging the company’s endeavors to exert further control over the platform. Drawing on Bourdieusian theory and relational methodology, we establish a computational social science approach to investigate the structural causes behind the two blackouts and contextualize the differences between them. We argue that these blackouts signify growing tensions within the socio-technical space of Reddit and an ongoing political, cultural, and economic reconfiguration of its power structure and political economy. ","",""
"2025","The Balancing Acts: Communicating Legitimacy in Global Speech Governance","The governance of online speech is increasingly a battleground shaped by competing social expectations. This study investigates TikTok’s content moderation in Indonesia and Pakistan, two countries with vast market potential and delicate social and moral stances. Through document analysis and in-depth interviews with government officials, industry representatives, and civil society experts, it examines how stakeholders navigate normative and pragmatic considerations in global speech governance. The findings first highlight distinct regulatory approaches: Indonesia’s collaborative yet paternalistic model preferring fines over bans. It emphasizes administrative compliance through jurisdictional control over platform rules. In contrast, Pakistan’s defensive stance prioritizes infrastructure-level monitoring and restrictions, often resorting to platform bans to enforce control over moral and religious content. Unlike its Silicon Valley counterparts, TikTok demonstrates strategic compliance, deliberately sidestepping controversy by delegating sensitive decisions to state authorities and avoiding political roles. While normative consensus on appropriate content remains elusive, civil society organizations mediate crucial accountability relationships through strategic activism, coalition-building, and international networks. The study discusses the tensions and cost-benefit appraisals of each actor group, identifies essential principles for legitimate speech governance, and examines challenges in translating these principles into actionable frameworks.","",""
"2025","The “Unsing Heroes” of the “Infocalypse”: Company Representations of Commercial Content Moderators"," Historically, companies have invisibilized some media workers to maintain underlying infrastructural ideologies. Likewise, contemporary commercial content moderators (CCMs) are hidden to obscure tensions inherent in digital platforms’ neoliberal ideologies and data economies. Despite this, CCMs are increasingly demanding visibility and legal rights. However, companies are responding by crafting their own narratives about CCMs for different audiences. What representations do these companies create about CCMs, and with what effects? Using thematic analysis of three case study examples, I outline discourses of “heroic battles,” resilience and wellness, and humanness. These discourses naturalize underlying infrastructural ideologies and could impact future legal protections. ","",""

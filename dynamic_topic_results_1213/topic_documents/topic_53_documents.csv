"year","title","abstract","journal","doi"
"2018","I do not believe you: how providing a source corrects health misperceptions across social media platforms","ABSTRACT Social media are often criticized as serving as a source of misinformation, but in this study we examine how they may also function to correct misperceptions on an emerging health issue. We use an experimental design to consider social correction that occurs via peers, testing both the type of correction (i.e., whether a source is provided or not) and the platform on which the correction ocratcurs (i.e., Facebook versus Twitter). Our results suggest that a source is necessary to correct misperceptions about the causes of the Zika virus on both Facebook and Twitter, but the mechanism by which such correction occurs differs across platforms. Implications for successful social media campaigns to address health misinformation are addressed.","",""
"2018","The agenda-setting power of fake news: A big data analysis of the online media landscape from 2014 to 2016"," This study examines the agenda-setting power of fake news and fact-checkers who fight them through a computational look at the online mediascape from 2014 to 2016. Although our study confirms that content from fake news websites is increasing, these sites do not exert excessive power. Instead, fake news has an intricately entwined relationship with online partisan media, both responding and setting its issue agenda. In 2016, partisan media appeared to be especially susceptible to the agendas of fake news, perhaps due to the election. Emerging news media are also responsive to the agendas of fake news, but to a lesser degree. Fake news coverage itself is diverging and becoming more autonomous topically. While fact-checkers are autonomous in their selection of issues to cover, they were not influential in determining the agenda of news media overall, and their influence appears to be declining, illustrating the difficulties fact-checkers face in disseminating their corrections. ","",""
"2018","How People Weave Online Information Into Pseudoknowledge"," Misinformation has found a new natural habitat in the digital age. Thousands of forums, blogs, and alternative news sources amplify fake news and inaccurate information to such a degree that it impacts our collective intelligence. Researchers and policy makers are troubled by misinformation because it is presumed to energize or even carry false narratives that can motivate poor decision-making and dangerous behaviors. Yet, while a growing body of research has focused on how viral misinformation spreads, little work has examined how false narratives are in fact constructed. In this study, we move beyond contagion inspired approaches to examine how people construct a false narrative. We apply prior work in cognitive science on narrative understanding to illustrate how the narrative changes over time and in response to social dynamics, and examine how forum participants draw upon a diverse set of online sources to substantiate the narrative. We find that the narrative is based primarily on reinterpretations of conventional and scholarly sources, and then used to provide an alternate account of unfolding events. We conclude that the link between misinformation, conventional knowledge, and false narratives is more complex than is often presumed, and advocate for a more direct study of this relationship. ","",""
"2019","Where ‘fake news’ flourishes: a comparison across four Western democracies","ABSTRACT How does the content of so-called ‘fake news’ differ across Western democracies? While previous research on online disinformation has focused on the individual level, the current study aims to shed light on cross-national differences. It compares online disinformation re-published by fact checkers from four Western democracies (the US, the UK, Germany, and Austria). The findings reveal significant differences between English-speaking and German-speaking countries. In the US and the UK, the largest shares of partisan disinformation are found, while in Germany and Austria sensationalist stories prevail. Moreover, in English-speaking countries, disinformation frequently attacks political actors, whereas in German-speaking countries, immigrants are most frequently targeted. Across all of the countries, topics of false stories strongly mirror national news agendas. Based on these results, the paper argues that online disinformation is not only a technology-driven phenomenon but also shaped by national information environments.","",""
"2019","FAKE NEWS DURING NATURAL DISASTER: INFORMATION FLOW, NEWS PRACTICES AND FACT-CHECKING IN INDONESIA","After an earthquake and tsunami struck Palu city and its surrounding areas in Indonesia on September 28, 2018, fake news were rampantly circulated on online platforms. To address lack of studies on how fake news during natural disaster is handled through working process of news and fact-check professionals in Indonesia, this study aims to examine how fake news during natural disaster were handled by news and fact-check professionals in Indonesia.&#x0D; Primarily built from multilevel analyses of Hierarchy of Influences Model (HOI), this study analyzed four dimensions that shaped news information. Key codes for this study are under individual factors (i.e. personal trait and professional value), routine (i.e. information gathering, information processing, information distribution and fact-checking), organizational factors (i.e. editorial policies and organizational culture) and social institution (government and third party fact-checking organization).&#x0D; Through a mixed-method approach, web-observation examines the information flow of selected Palu fake news cases to provide overview on development of each case, including responses from government, media, fact-check organizations and the public. Next, in-depth interview will examine how news professionals from both traditional news media and web-only news media along with how third party fact-checkers handled Palu fake news.&#x0D; Theoretically, this study expands HOI’s multilevel applications to investigate how news and fact-check professionals in Indonesia handled Palu fake news. Practically, the findings will shed light for news and fact-check professionals to assess and improve their practices in handling fake news. This work-in-progress research will finish data collection in March 2019, followed by data analysis in April.&#x0D;  ","",""
"2019","Big Data and quality data for fake news and misinformation detection"," Fake news has become an important topic of research in a variety of disciplines including linguistics and computer science. In this paper, we explain how the problem is approached from the perspective of natural language processing, with the goal of building a system to automatically detect misinformation in news. The main challenge in this line of research is collecting quality data, i.e., instances of fake and real news articles on a balanced distribution of topics. We review available datasets and introduce the MisInfoText repository as a contribution of our lab to the community. We make available the full text of the news articles, together with veracity labels previously assigned based on manual assessment of the articles’ truth content. We also perform a topic modelling experiment to elaborate on the gaps and sources of imbalance in currently available datasets to guide future efforts. We appeal to the community to collect more data and to make it available for research purposes. ","",""
"2019","Spreading Disinformation on Facebook: Do Trust in Message Source, Risk Propensity, or Personality Affect the Organic Reach of “Fake News”?"," There is considerable concern about the propagation of disinformation through social media, particularly for political purposes. “Organic reach” has been found to be important in the propagation of disinformation on social networks. This is the phenomenon whereby social media users extend the audience for a piece of information: interacting with it, or sharing it with their wider networks, greatly increases the number of people the information reaches. This project evaluated the extent to which characteristics of the message source (how trustworthy they were) and the recipient (risk propensity and personality) influenced the organic reach of a potentially false message. In an online study, 357 Facebook users completed personality and risk propensity scales and rated their likelihood of interacting in various ways with a message posted by either a trustworthy or untrustworthy source. Message source impacted on overall organic reach, with messages from trusted sources being more likely to be propagated. Risk propensity did not influence reach. However, low scores on trait agreeableness predicted greater likelihood of interacting with a message. The findings provide preliminary evidence that both message source and recipient characteristics can potentially influence the spread of disinformation. ","",""
"2020","Counteracting Misleading Protobacco YouTube Videos: The Effects of Text-Based and Narrative Correction Interventions and the Role of Identification","YouTube’s propagation of misleading protobacco content to youth has the potential to increase their protobacco beliefs, attitudes, and smoking behavior. We assessed the effects of potential interventions aimed at ameliorating the effect of misleading protobacco videos. An online experiment randomly exposed past and current young tobacco users ( N  = 716) between the ages of 15 and 19 years to real protobacco, pipe-focused YouTube content that was either shown in its original uncorrected form or edited to include either a propositional voiced and text-based rebuttal that warned about the health effects of smoking or a counternarrative that showed that a person who promoted protobacco messages was diagnosed with and eventually died from esophageal cancer. On average, the two interventions were equally effective at reducing the effects of protobacco messages on beliefs and attitudes. However, the narrative correction was more effective for participants who strongly identified with the character. Practical and theoretical implications are discussed.","",""
"2020","Combating misinformation online: re-imagining social media for policy-making","Social media have created communication channels between citizens and policymakers but are also susceptible to rampant misinformation. This new context demands new social media policies that can aid policymakers in making evidence-based decisions for combating misinformation online. This paper reports on data collected from policymakers in Austria, Greece, and Sweden, using focus groups and in-depth interviews. Analyses provide insights into challenges and identify four important themes for supporting policy-making for combating misinformation: a) creating a trusted network of experts and collaborators, b) facilitating the validation of online information, c) providing access to visualisations of data at different levels of granularity, and d) increasing the transparency and explainability of flagged misinformative content. These recommendations have implications for rethinking how revised social media policies can contribute to evidence-based decision-making.","",""
"2020","Fake news as an informational moral panic: the symbolic deviancy of social media during the 2016 US presidential election","ABSTRACT A persistent story about the 2016 US presidential election was the preponderance of fake news stories on social media, and on Facebook in particular, that had no basis in fact but were wholly concocted to quickly amass clicks that could be converted into advertising revenues. This study steps outside of arguments about the spread or efficacy of fake news to instead interrogate its symbolic dimensions and its meaning for both journalism and the larger system of political communication. To conceptualize the role of fake news as a particular symbol, this paper approaches the journalistic condemnation of fake news as an ‘informational moral panic.’ This concept builds off Cohen’s classic formulation of moral panics as public anxiety that a particular social threat will lead to declining standards. The ability to define a phenomenon as an informational moral panic is an exercise in cultural power that ascribes deviancy to particular actors while validating others. In the case of fake news, the anxiety is not so much directed toward a particular group but aimed at the larger transformation of informational spaces made possible by social media. An examination of journalists’ responses in the US press during November 2016 reveals four domains of focus ‒ production, platform, subsidy, and consumption – each with its own narratives of blame and remedy. Fake news becomes a particular signifier that condenses broader concerns surrounding the eroding boundaries of traditional journalistic channels, click-driven news, the extension of mediated voices, and the growing role of social media in news distribution.","",""
"2020","Fake news and the discursive construction of technology companies’ social power"," In the research and commentary around ‘fake news’, there has been growing attention to the way the phrase evidences a growing field of technology industry critique, operating as a shorthand for understanding the nature of social media companies’ power over the public sphere. This article interrogates elite and popular discourses surrounding ‘fake news’, using the tools of critical discourse analysis to show how public commentary constitutes a discursive field that renders tech industry power intelligible by first defining the issue of fake news as a sociotechnical problem, then debating the infrastructural nature of platform companies’ social power. This article concludes that, as commentary moves beyond a focus on fake news and critiques of technology industries grow more complex, strains of elite discourse reveal productive constraints on tech power, articulating the conditions under which limits on that power are understood as legitimate. ","",""
"2020","Caution: Rumors ahead—A case study on the debunking of false information on Twitter"," As false information may spread rapidly on social media, a profound understanding of how it can be debunked is required. This study offers empirical insights into the development of rumors after they are debunked, the various user groups who are involved in the process, and their network structures. As crisis situations are highly sensitive to the spread of rumors, Twitter posts from during the 2017 G20 summit are examined. Tweets regarding five rumors that were debunked during this event were manually coded into the following categories: rumor, debunking message, uncertainty about rumor, uncertainty about debunking message, and others. Our findings show that rumors which are debunked early and vehemently by official sources are the most likely to be stopped. When individuals participate in the process, they typically do so by sharing uncommented media content, as opposed to contributing user-generated content. Depending on the conditions in which a rumor arises, different network structures can be found. Since some rumors are easier for individuals to verify than others, our results have implications for the priorities of journalists and official sources. ","",""
"2020","Mobilizing Users: Does Exposure to Misinformation and Its Correction Affect Users’ Responses to a Health Misinformation Post?"," Misinformation spreads on social media when users engage with it, but users can also respond to correct it. Using an experimental design, we examine how exposure to misinformation and correction on Twitter about unpasteurized milk affects participants’ likelihood of responding to the misinformation, and we code open-ended responses to see what participants would say if they did respond. Results suggest that participants are overall unlikely to reply to the misinformation tweet. However, content analysis of hypothetical replies suggests they largely do provide correct information, especially after seeing other corrections. These results suggest that user corrections offer untapped potential in responding to misinformation on social media but effort must be made to consider how users can be mobilized to provide corrections given their general unwillingness to reply. ","",""
"2020","Real Talk About Fake News: Identity Language and Disconnected Networks of the US Public’s “Fake News” Discourse on Twitter"," This article studies “fake news” beyond the consumption and dissemination of misinformation and disinformation. We uncover how the term “fake news” serves as a discursive device for ordinary citizens to consolidate group identity in everyday political utterances on Twitter. Using computational linguistic and network analyses, we demonstrate that over the period of 2016–2018, there is an uptrend in the use of identity language in US Twitter users’ discussions about “fake news,” manifested by the increased frequency of group pronouns in combination with issues and sentiments that boost one’s ingroup and derogate the outgroup. Furthermore, as opposed to the conventional wisdom that “fake news” is a right-wing term, we uncover two disconnected retweet networks surrounding liberal and conservative opinion leaders. Like-minded individuals selectively amplify ingroup messages to claim the power to define falsehood and make group-serving blame attributions. We discuss the theoretical implications of our findings and offer new directions for future research on “fake news,” misinformation, and disinformation. ","",""
"2021","The Effects of Message Order and Debiasing Information in Misinformation Correction","Misinformation continues to influence inferences even after being discredited, making it extremely difficult to completely erase its detrimental effects. With a two-wave online experiment, this research tested how the effectiveness of misinformation correction is influenced by (1) whether correction is presented before or after misinformation and (2) whether correction is accompanied by a message that enhances the coherence between misinformation and correction message. The results showed that a correction was most effective when it was delivered after the misinformation and with a debiasing message. These effects persisted at least one week after the initial exposure to the correction. The results were consistent with the Knowledge Revision Components (KReC) framework and the schemata-plus-tag model of negation comprehension. The findings also provided a comprehension-based explanation to previous findings from meta-analysis regarding the order of presentation of misinformation and corrective messages. Practical implications for misinformation correction practices are discussed.","",""
"2021","Comparative Approaches to Mis/Disinformation| Motivations for Sharing Misinformation: A Comparative Study in Six Sub-Saharan African Countries","In most African countries, “fake news,” politically motivated disinformation, and misinformation in the media were common occurrences before these became a preoccupation in the Global North. However, with a fast-growing population of mobile users, and the popularization of apps such as WhatsApp, misinformation has become much  more pervasive across the continent. Researchers have shown that perceived exposure to false information is high in some African countries, and yet citizens often share made-up news intentionally. This article explores the motivations and contributing factors for sharing misinformation in six sub-Saharan African countries. Our analysis of 12 focus groups with university students reveals two common motivations: civic duty and fun. The sharing of political (dis)information was uneven, but common among students with high levels of self-reported political engagement. We also present an array of cues used to determine credibility, which often determines the shareability of information. Cross-national differences are also discussed.","",""
"2021","Special Section on Comparative Approaches to Mis/Disinformation Introduction","From misleading news articles around elections in Brazil and the United States to mob lynchings fueled by false social media messages in India to made-up stories about COVID-19 vaccination, a deluge of disinformation and misinformation is affecting various aspects of citizens' lives around the world. Although there is an increasing number of research papers dealing with disinformation or misinformation, a majority of these have focused on the United States. This Special Section on comparative approaches to mis/disinformation features conceptual and data-informed articles with international and global perspectives on the prevalence, impact, and diffusion of mis/disinformation in different countries. Articles selected for the Special Section provide new theoretical and empirical contributions to existing bodies of knowledge whether focusing on one country or offering comparative perspectives involving multiple countries. The articles, individually and collectively, offer important scholarly and policy implications for studying and combating mis/disinformation around the world.","",""
"2021","“One Big Fake News”: Misinformation at the Intersection of User-Based and Legacy Media","This article explores audiences’ online reactions to public service broadcasting content manipulations. Drawing on a case study of Israeli televised content, we discuss the role user comments play in mediated calls for media literacy and civic awareness, allowing audiences to gather and discuss the impact of misinformation and fake news on culture, civic participation, and trust in public service media and other democratic institutions. We show how online mediated spaces that are considered aggressive and counterproductive should also be understood as facilitators of calls against misuse of public resources and manipulations spread in society. We thus suggest that alongside legacy mainstream media, user comments can become part of the solution for the prevalence of disinformation in our current digital media ecosystem.","",""
"2021","Fake news as fake politics: the digital materialities of YouTube misinformation videos about Brazilian oil spill catastrophe"," This article investigates misinformation chains – fake news and clickbait – related to the 2019 oil spill along the coast of Northeast Brazil. A link between the intensive use of misinformation on YouTube and the environmental impact of digital media and algorithmic performativity has been found by analyzing videos about the 2019 Brazilian oil spill. A total of 591 YouTube videos were extracted based on a search for the hashtags ‘oleononordeste’, ‘vazamentopetroleo’, and ‘greenpixe’. The data thus obtained suggest that most of the corpus (80.37%) consists of misinformation, of which 65.82% (389 videos) is clickbait and 14.55% (86 videos) fake news. YouTube misinformation videos produced around 1.42 MtCO2e, the equivalent of burning 3.30 barrels of oil. We argue that misinformation chains increase pollution and carbon footprint as a result of at least three factors: (a) the extra energy cost of feeding algorithms; (b) increased algorithmic resistance to the visibility of journalistic information; and (c) undermining public debate about environmental catastrophes in favor of private interests (fake politics). ","",""
"2021","‘FAKE NEWS’ AND OTHER PROBLEMATIC INFORMATION: STUDYING DISSEMINATION AND DISCOURSE PATTERNS","Encompassed by the disputed term ‘fake news’, a variety of overtly or covertly biased, skewed, or falsified reports claiming to present factual information are now seen to constitute a critical challenge to the effective dissemination of news and information across established and emerging democratic societies. Such content – variously also classifiable as propaganda, selective reporting, conspiracy theory, inadvertent misinformation, and deliberate disinformation – in itself is not new; however, contemporary digital and social media networks enable its global dissemination and amplification, by human and algorithmic actors (Woolley &amp; Howard 2017), ordinary users and professional agents, outside of, in opposition to, or sometimes also in collusion with, the mainstream media (Shao et al. 2017; Vargo et al. 2017). Various political, commercial, and state actors are suspected to have exploited this ‘fake news’ ecosystem to influence public opinion, in major votes ranging from the Brexit referendum to national elections, and/or to utilise discourse around ‘fake news’ to generally undermine trust in media, political, and state institutions. This panel brings together a number of perspectives that combine systematic, large-scale, mixed-methods analysis of the empirical evidence for the global dissemination of, engagement with, and visibility of problematic information in public debate with the study of the public discourse about ‘fake news’, and the operationalisation of this concept by politicians and other societal actors to downplay inconvenient facts or reject critical questions. In combination, its five papers present a substantive collection of innovative approaches to the ‘fake news’ concept, exploring the dissemination of problematic information itself at larger and smaller scales as well as examining the operationalisation of the idea of ‘fake news’ in pursuit of specific ideological aims. This produces a new and more comprehensive picture of the overall impact of ‘fake news’, in all its forms, on contemporary societies.","",""
"2021","SOCIAL MEDIA USE, TRUST AND TECHNOLOGY ACCEPTANCE: INVESTIGATING THE        EFFECTIVENESS OF A CO-CREATED BROWSER PLUGIN IN MITIGATING THE SPREAD OF MISINFORMATION ON        SOCIAL MEDIA","Social media have become online spaces where misinformation abounds and spreads virally in the absence of professional gatekeeping. This information landscape requires everyday citizens, who rely on these technologies to access information, to cede control of information. This work sought to examine whether the control of information can be regained by humans with the support of a co-created browser plugin, which integrated credibility labels and nudges, and was informed by artificial intelligence models and rule engines. Given the literature on the complexity of information evaluation on social media, we investigated the role of technological, situational and individual characteristics in “liking” or “sharing” misinformation. We adopted a mixed-methods research design with 80 participants from four European sites, who viewed a curated timeline of credible and non-credible posts on Twitter, with (n=40) or without (n=40) the presence of the plugin. The role of the technological intervention was important: the absence of the plugin strongly correlated with misinformation endorsement (via “liking”). Trust in the technology and technology acceptance were correlated and emerged as important situational characteristics, with participants with higher trust profiles being less likely to share misinformation. Findings on individual characteristics indicated that only social media use was a significant predictor for trusting the plugin. This work extends ongoing research on deterring the spread of misinformation by situating the findings in an authentic social media environment using a co-created technological intervention. It holds implications for how to support a misinformation-resilient citizenry with the use of artificial intelligence-driven tools.","",""
"2021","Four years of fake news: A quantitative analysis of the scientific literature","Since 2016, “fake news” has been the main buzzword for online misinformation and disinformation. This term has been widely used and discussed by scholars, leading to hundreds of publications in a few years. This report provides a quantitative analysis of the scientific literature on this topic by using frequency analysis of metadata and automated lexical analysis of 2,368 scientific documents retrieved from Scopus, a large scientific database, mentioning “fake news” in the title or abstract.&#x0D; Findings show that until 2016 the number of documents mentioning the term was less than 10 per year, suddenly rising from 2017 and steadily increasing in the following years. Among the most prolific countries are the U.S. and European countries such as the U.K., but also many non-Western countries such as India and China. Computer science and social sciences are the disciplinary fields with the largest number of documents published. Three main thematic areas emerged: computational methodologies for fake news detection, the social and individual dimension of fake news, and fake news in the public and political sphere. There are 10 documents with more than 200 citations, and two papers with a record number of citations.","",""
"2021","Correction Experiences on Social Media During COVID-19"," Despite a wealth of research examining the effectiveness of correction of misinformation, not enough is known about how people experience such correction when it occurs on social media. Using a study of US adults in late March 2020, we measure how often people witness correction, correct others, or are corrected themselves, using the case of COVID-19 misinformation on social media. Descriptively, our results suggest that all three experiences related to correction on social media are relatively common and occur across partisan divides. Importantly, a majority of those who report seeing misinformation also report seeing it corrected, and a majority of those who report sharing misinformation report being corrected by others. Those with more education are more likely to engage in correction, and younger respondents are more likely to report all three experiences with correction. While experiences with correction are generally unrelated to misperceptions about COVID-19, those who correct others have higher COVID-19 misperceptions. ","",""
"2022","Separating truth from lies: comparing the effects of news media literacy interventions and fact-checkers in response to political misinformation in the US and Netherlands","ABSTRACT Although previous research has offered important insights into the consequences of mis- and disinformation and the effectiveness of corrective information, we know markedly less about how different types of corrective information – news media literacy interventions and fact-checkers – can be combined to counter different forms of misinformation. Against this backdrop, this paper reports on experiments in the US and the Netherlands (N = 1,091) that exposed people to evidence-based or fact-free anti-immigration misinformation, fact-checkers and/or a media literacy intervention. The main findings indicate that evidence-based misinformation is seen as more accurate than fact-free misinformation, and the combination of news media literacy interventions and fact-checkers is most effective in lowering issue agreement and perceived accuracy of misinformation across countries. These findings have important implications for journalism practice and policy makers that aim to combat mis- and disinformation.","",""
"2022","Review essay: fake news, and online misinformation and disinformation","The attempted over-turning of the result of the 2020 US presidential election involved the proliferation of multiple online conspiracy theories and fake stories, and culminated in the assault on the US Congress while it was in the process of validating the electoral college count on 6 January 2021. This represented the apotheosis of the growth of misinformation and disinformation in the USA from around the middle of the previous decade. Social media is commonly assumed to be culpable for this growth, with ‘the news’ and current affairs deemed the epicentre of the battle for information credibility. This review begins by explaining the key definitions and discussions of the subject of fake news, and online misinformation and disinformation with the aid of each book in turn. It then moves on to focus on the following themes common to all three books as a means of attempting to provide a comprehensive analysis of the subject at hand: the use of memes and ironic content; the globalisation of misinformation, disinformation and fake news, and the impact on democratic societies; the limitations of media literacy approaches.","",""
"2022","When a story contradicts: correcting health misinformation on social media through different message formats and mechanisms","ABSTRACT The study examined the effects of message format (narrative vs. nonnarrative) and correction mechanism (social vs. algorithmic correction) in correcting e-cigarette related misinformation on social media. Two experimental studies were conducted. In study 1, correction mechanisms explicitly endorsed the message corrective (n = 235). As an explicit endorsement may reveal persuasive intent and influence narrative persuasion, Study 2 replicated the design and employed a manipulation for correction mechanism with a more implicit endorsement (n = 235). Findings generally suggest that nonnarrative correction is more effective when it is suggested by social media contacts; narrative correction may have merit when it is prompted by algorithms with explicit endorsement. Credibility evaluations and narrative transportation highlight the psychological mechanisms for understanding this interaction effect.","",""
"2022","Assessing the relative merits of news literacy and corrections in responding to misinformation on Twitter"," Extending previous research, we test two solutions for addressing misinformation by pairing news literacy (NL) messages with corrective responses to health misinformation shared on Twitter. Importantly, we consider a range of outcomes, including not just credibility or misperceptions, but also feelings of news literacy and support for its value. Using an experiment, we find that user corrections of a meme containing false information reduced credibility assessments of the misinformation post and misperceptions but seeing misinformation also produced lower perceptions of personal news literacy and its value for society, regardless of whether it is corrected or not. Exposure to an NL message did not enhance the effectiveness of these corrective responses nor boost NL attitudes and may have generated cynicism. We discuss the challenges of designing NL messages for social media that achieve the wide range of goals news literacy interventions aspire to address. ","",""
"2022","Health-related fake news on social media platforms: A systematic literature review"," This review aims to (a) investigate the characteristics of both the research community and the published research on health-related fake news on social media platforms, and (b) identify the challenges and provide recommendations for future research on the subject. We reviewed 69 journal articles found in the main academic databases up to April 2021. The studies extracted data mainly from Twitter, YouTube, and Facebook. Most articles aimed to investigate the public’s reaction to fake health information, concluding that health agencies and professionals should increase their online presence. The articles also suggest that future work should aim to improve the quality of health information on social media platforms, develop new tools and strategies to combat fake news sharing, and study the credibility of health information. Nonetheless, those in control of the platforms are the only ones which can take effective measures to ensure that their users receive reliable information. ","",""
"2022","PM Me the Truth? The Conditional Effectiveness of Fact-Checks Across Social Media Sites"," People use multiple social media daily. Some platforms feature public interactions like Facebook, others emphasize private communications such as Line. Although misinformation is rampant on all platforms, literature on fact-checks (FC) focuses primarily on public ones. This article provides an integrated psychological model and argues that FC is less effective on private platforms. People expect to encounter “unwelcome” FCs (incongruent with their beliefs) on public platforms, but selectively approach the “welcome” FC on private platforms. An experiment ( n = 601) and a national survey ( n = 1060) were implemented to test these hypotheses in the 2020 Taiwan Presidential Election. The experiment shows that respondents prefer FC on Line, which helps their party, but prefer FC on Facebook which disadvantages their party. The survey shows that consuming FC with more private platform usage has lower media literacy, while is the opposite on public platforms. Future work should focus on both FC and how it is consumed. ","",""
"2022","Fact-Checking the Crisis: COVID-19, Infodemics, and the Platformization of Truth"," During the onset of the COVID-19 pandemic, various officials flagged the critical threat of false information. In this study, we explore how three major social media platforms (Facebook, Twitter, and YouTube) responded to this “infodemic” during early stages of the pandemic via emergent fact-checking policies and practices, and consider what this means for ensuring a well-informed public. We accomplish this through a thematic analysis of documents published by the three platforms that address fact-checking, particularly those that focus on COVID-19. In addition to examining what the platforms said they did, we also examined what the platforms actually did in practice via a retrospective case study drawing on secondary data about the viral conspiracy video, Plandemic. We demonstrate that the platforms focused their energies primarily on the visibility of COVID-19 mis/disinformation on their sites via (often vaguely described) policies and practices rife with subjectivity. Moreover, the platforms communicated the expectation that users should ultimately be the ones to hash out what they believe is true. We argue that this approach does not necessarily serve the goal of ensuring a well-informed public, as has been the goal of fact-checking historically, and does little to address the underlying conditions and structures that permit the circulation and amplification of false information online. ","",""
"2022","“My People Already Know That”: The Imagined Audience and COVID-19 Health Information Sharing Practices on Social Media"," This article examines how imagined audiences and impression management strategies shape COVID-19 health information sharing practices on social media and considers the implications of this for combatting the spread of misinformation online. In an interview study with 27 Canadian adults, participants were shown two infographics about masks and vaccines produced by the World Health Organization (WHO) and asked whether or not they would share these on social media. We find that interviewees’ willingness to share the WHO infographics is negotiated against their mental perception of the online audience, which is conceptualized in three distinct ways. First, interviewees who would not share the infographics frequently describe a self-similar audience of peers that are “in the know” about COVID-19; second, those who might share the infographics conjure a specific and contextual audience who “needs” the information; and finally, those who said they would share the infographics most frequently conjure an abstract audience of “the public” or “my community” to explain that decision. Implications of these sharing behaviors for combatting the spread of misinformation are discussed. ","",""
"2022","The Roles of Worry, Social Media Information Overload, and Social Media Fatigue in Hindering Health Fact-Checking"," Health misinformation has become a salient issue on social media. To lower the risk of health misinformation, fact-checking matters. However, most existing studies investigated fact-checking from the journalism angle, while little is known about how information-seekers’ social media use affects their fact-checking behaviors. Also, it remains unclear how individuals’ health worry is associated with health fact-checking. Based on the O-S-O-R model, this study explored the underlying mechanism through which health worry and social media might hinder users’ fact-checking. Specifically, with a two-wave panel survey conducted in China during the COVID-19 pandemic, this study showed that individuals’ worry about COVID-19 increased social media information overload, which resulted in social media fatigue that could reduce health fact-checking. Also, the direct relationship between worry and fact-checking was not significant, but was completely mediated by social media information overload and social media fatigue. The findings demonstrate the negative roles of worry and social media in inhibiting users’ fact-checking behaviors. Important theoretical and practical implications for promoting effective fact-checking are discussed. ","",""
"2023","Misinformation","","",""
"2023","Persuasive strategies in online health misinformation: a systematic review","ABSTRACT A proliferation of a variety of health misinformation is present online, particularly during times of public health crisis. To combat online health misinformation, numerous studies have been conducted to taxonomize health misinformation or examine debunking strategies for various types of health misinformation. However, one of the root causes – strategies in such misinformation that may persuade the readers – is rarely studied. This systematic review aimed to fill this gap. We searched Web of Science, Scopus, PsycINFO, and Communication and Mass Media Complete for studies published between 2011 and 2021 on 29 May 2021. Peer-reviewed studies that discussed persuasive strategies in online misinformation messages were included. Of 1,700 articles identified, 58 were eligible and 258 persuasive strategies were extracted. Following the affinity diagraming process, 225 persuasive strategies in online health misinformation were categorized into 12 thematic groups, including: fabricating narrative with details, using anecdotes and personal experience as evidence, distrusting government or pharmaceutical companies, politicizing health issues, highlighting uncertainty and risk, inappropriate use of scientific evidence, rhetorical tricks, biased reasoning to make a conclusion, emotional appeals, distinctive linguistic features, and establishing legitimacy. Possible antecedents for why and how these persuasive strategies in online health misinformation may influence individuals were discussed. The findings suggest that media literacy education is essential for the public to combat health misinformation.","",""
"2023","Degrees of deception: the effects of different types of COVID-19 misinformation and the effectiveness of corrective information in crisis times","ABSTRACT Responding to widespread concerns about misinformation’s impact on democracy, we conducted an experiment in which we exposed German participants to different degrees of misinformation on COVID-19 connected to politicized (immigration) and apolitical (runners) issues (N = 1,490). Our key findings show that partially false information is more credible and persuasive than completely false information, and also more difficult to correct. People with congruent prior attitudes are more likely to perceive misinformation as credible and agree with its positions than people with incongruent prior attitudes. We further show that although fact-checkers can lower the perceived credibility of misinformation on both runners and migrants, corrective messages do not affect attitudes toward migrants. As a key contribution, we show that different degrees of misinformation can have different impacts: more nuanced deviations from facticity may be more harmful as they are difficult to detect and correct while being more credible.","",""
"2023","Perceived prevalence of misinformation fuels worries about COVID-19: a cross-country, multi-method investigation","ABSTRACT Data suggests that the majority of citizens in various countries came across ‘fake news’ during the COVID-19 pandemic. We test the relationship between perceived prevalence of misinformation and people’s worries about COVID-19. In Study 1, analyses of a survey across 17 countries indicate a positive association: perceptions of high prevalence of misinformation are correlated with high worries about COVID-19. However, the relationship is weaker in countries with higher levels of case-fatality ratios, and independent from the actual amount of misinformation per country. Study 2 replicates the relationship using experimental data. Furthermore, Study 2 demonstrates the underlying mechanism, that is, perceived prevalence of misinformation fosters the belief that COVID-19 is spiralling out of control, which in turn, increases worries. Our findings suggest that perceived prevalence of misinformation can have significant psychological effects, even though audience members reject the information as being false.","",""
"2023","The sharing of disinformation in cross-national comparison: analyzing patterns of resilience","ABSTRACT Although the problem of disinformation is on the rise across the globe, previous research has found that countries differ in the extent of widespread disinformation. In this study, we examine the willingness to disseminate disinformation across six countries (Belgium, France, Germany, Switzerland, the U.K. and the U.S.). We use a model by Humprecht et al. (2020) to study to what degree various systemic-structural factors influence individual behavior and contribute to resilience to disinformation. We draw on uniformly collected primary survey data and use regression analyses to examine which factors may explain citizens’ decisions to not further propagate disinformation. The results of our cross-national study show that resilience factors are country-specific and are highly dependent on the respective political and information environments. While in some countries extreme ideology weakens resilience, in others low education can have such an effect. Cross-national resilience factors include heavy social media use, the use of alternative media, and populist party support. We discuss what kind of tailored measures in combating online disinformation are needed to improve social resilience across different countries.","",""
"2023","UNRAVELING DISINFORMATION: EXAMINING THE HUMAN INFRASTRUCTURE OF MISINFORMATION IN BRAZIL THROUGH THE LENS OF HETEROMATION","In recent years, major technology companies have taken much of the public blame for this reality, given their algorithms facilitate the sharing of—and sometimes even promote—falsehoods. This, however, misses a key reality; social media, search engines, and messaging services are not fully automated technologies. Rather, they are heteromated: they are reliant on participatory humans to serve their economic goals. Focusing on users, and on the sharing, rather than the origination, of disinformation, we connect theories of heteromation with those surrounding the Human Infrastructure of Misinformation (HIM) with the express purpose of contributing to a more holistic understanding of how and why misinformation is so prevalent online.","",""
"2023","Toward an integrated framework for misinformation and correction sharing: A systematic review across domains"," Although misinformation and correction sharing is a topic that spans various domains and disciplines, the ultimate aim of such research is to better understand how to reduce misinformation sharing while motivating correction sharing in an increasingly decentralized and dispersed informational landscape. This review aims to (a) provide a systematic and structured overview of empirical studies on both misinformation sharing and correction sharing, as differentiated phenomenon, by examining article elements such as theoretical lenses, methodologies, topics of research, and (b) collect and organize factors predicting both misinformation sharing and correction sharing into an integrated model, which provides the foundation for an interdisciplinary framework of misinformation sharing and correction sharing. A total of 64 relevant empirical articles published before October 2021 were identified for analysis. Finally, a discussion regarding the academic and practical implications of this study, and gaps in the literature aim to provide direction for future research. ","",""
"2023","Fact-checking, reputation, and political falsehoods in Italy and the United States"," This article develops a reputational theory of political falsehoods. Politicians are motivated by the desire to build a positive reputation, therefore, they will be more likely to deliver false statements (incurring the risk of being fact-checked) when the potential benefit outweighs the cost. This happens as new elections come closer, since the electoral benefit of falsehoods increases along with the probability of being checked too late (after the election day). Politicians are less likely to issue falsehoods in detailed statements and in scripted communication, since the reputational cost is higher because such falsehoods would be considered intentional. Conversely, the stronger trust that voters attribute to politicians on issues they own, allows politicians to lie on such topics. Statistical analysis of almost 8000 statements released by politicians and assessed by fact-checkers, in the United States and Italy (2007–2018), supports the hypotheses. The results hold irrespective of party affiliation. ","",""
"2023","Believing and sharing misinformation, fact-checks, and accurate information on social media: The role of anxiety during COVID-19"," The COVID-19 pandemic went hand in hand with what some have called a “(mis)infodemic” about the virus on social media. Drawing on partisan motivated reasoning and partisan selective sharing, this study examines the influence of political viewpoints, anxiety, and the interactions of the two on believing and willingness to share false, corrective, and accurate claims about COVID-19 on social media. A large-scale 2 (emotion: anxiety vs relaxation) × 2 (slant of news outlet: MSNBC vs Fox News) experimental design with 719 US participants shows that anxiety is a driving factor in belief in and willingness to share claims of any type. Especially for Republicans, a state of heightened anxiety leads them to believe and share more claims. Our findings expand research on partisan motivated reasoning and selective sharing in online settings, and enhance the understanding of how anxiety shapes individuals’ processing of risk-related claims in issue contexts with high uncertainty. ","",""
"2023","Asymmetric adjustment: Partisanship and correcting misinformation on Facebook"," Across two studies, we test two of Facebook’s attempts to fight misinformation: labeling misinformation as disputed or false and including fact checks as related articles. We propose hypotheses based on a two-step model of motivated reasoning, which provides insight into how misinformation is corrected. For study 1 ( n = 1,262) and study 2 ( n = 1,586), we created a mock Facebook News Feed consisting of five different articles—four were actual news stories and the fifth was misinformation. Both studies tested (a) the effect of misinformation without correction, (b) Facebook’s changes to its platform, and (c) an alternative we theorized could be more effective. The findings, in line with the two-step model of motivated reasoning, provide evidence of symmetric party effects for the belief in misinformation. In both studies, we find partisan differences in responses to fact checking. We find modest evidence that our improvements to Facebook’s attempts at correcting misinformation reduce misperceptions across partisan divides. ","",""
"2023","Digital false information at scale in the European Union: Current state of research in various disciplines, and future directions"," Digital false information is a global problem and the European Union (EU) has taken profound actions to counter it. However, from an academic perspective the United States has attracted particular attention. This article aims at mapping the current state of academic inquiry into false information at scale in the EU across fields. Systematic filtering of academic contributions resulted in the identification of 93 papers. We found that Italy is the most frequently studied country, and the country of affiliation for most contributing authors. The fields that are best represented are computer science and information studies, followed by social science, communication, and media studies. Based on the review, we call for (1) a greater focus on cross-platform studies; (2) resampling of similar events, such as elections, to detect reoccurring patterns; and (3) longitudinal studies across events to detect similarities, for instance, in who spreads misinformation. ","",""
"2023","Peer correction of misinformation on social media: (In)civility, success experience and relationship consequences"," Misinformation often involves sensitive topics, and individuals may attempt to correct their peers using uncivil tones. We examined the effect of civil versus uncivil corrections on the perceived success of the correction and the reported relationship consequences. We used three-wave panel data consisting of 1513 participants in the first wave, and followed 686 individuals who participated in all three waves. Our results indicate that demographic variables were important predictors of the frequency and tone of correction. Furthermore, individuals reported an equal number of successful and unsuccessful correction experiences. Importantly, we found that more frequent civil correction was associated with a higher likelihood of success, and a successful correction experience was associated with positive relationship outcomes. In contrast, uncivil correction was associated with negative relationship consequences. In addition, individuals with higher appraisal literacy and those correcting close ties were more likely to report successful correction experiences. ","",""
"2023","Fake News and the Web of Plausibility"," This article explores the presentation of fake news, the most salient kind of disinformation, focusing neither on its text-based content nor its image-based form, but instead on its overall aesthetic composition—and how and why that composition contributes to the proliferation of disinformation. It begins with an analysis of “real news”—the genre that fake news attempts to copy—and its reliance on what Gaye Tuchman calls the “web of facticity” to communicate “good” information. It then turns to examine how fake news uses the logic of graphic design to exploit features of the web of facticity to create a “web of plausibility”—the web of facticity’s evil twin—to generate momentum for circulation through the analysis of several specific aesthetic features of the news genre. The conclusion offers some possible ways that this sort of perspective can better equip us to help stop the spread of disinformation. ","",""
"2023","Why Do People Share Political Information and Misinformation Online? Developing a Bottom-Up Descriptive Framework"," Social media users are key actors in the spreading of misleading or incorrect information. To develop an integrative parsimonious summary of social media users’ own accounts of motives for sharing political information, we conducted: (1) a literature review of motives for personally sharing false information as reported by social media users and (2) qualitative research concerning these motives using an innovative, ecologically valid method. Based on our findings, we developed a pool of items evaluating social media users’ motives for sharing false political information, which we then tested and analyzed the dimensionality of in (3) a pre-registered questionnaire-based study to identify key clusters of users’ own accounts of motives for sharing both true and false political information. The current findings show that there are distinct sets of motives people report for their misinformation sharing behavior: prosocial activism, attack or manipulation of others, entertainment, awareness, political self-expression, and fighting false information. Also, these sets of motives are associated with variables known to predict sharing misinformation, and some of these sets predict social media users’ self-reports of having shared misinformation in the past. Our findings highlight and elaborate on users’ motives that reflect a concern with “making things better” and acting in a manner that is beneficial to society as a whole, and suggest that different interventions may be required to combat misinformation sharing driven by different motives. A potential set of 18 items that could be used in questionnaires measuring motivations for sharing political news online is described. ","",""
"2023","Misinformation on Misinformation: Conceptual and Methodological Challenges"," Alarmist narratives about online misinformation continue to gain traction despite evidence that its prevalence and impact are overstated. Drawing on research examining the use of big data in social science and reception studies, we identify six misconceptions about misinformation and highlight the conceptual and methodological challenges they raise. The first set of misconceptions concerns the prevalence and circulation of misinformation. First, scientists focus on social media because it is methodologically convenient, but misinformation is not just a social media problem. Second, the internet is not rife with misinformation or news, but with memes and entertaining content. Third, falsehoods do not spread faster than the truth; how we define (mis)information influences our results and their practical implications. The second set of misconceptions concerns the impact and the reception of misinformation. Fourth, people do not believe everything they see on the internet: the sheer volume of engagement should not be conflated with belief. Fifth, people are more likely to be uninformed than misinformed; surveys overestimate misperceptions and say little about the causal influence of misinformation. Sixth, the influence of misinformation on people’s behavior is overblown as misinformation often “preaches to the choir.” To appropriately understand and fight misinformation, future research needs to address these challenges. ","",""
"2023","Developing Misinformation Immunity: How to Reason-Check Fallacious News in a Human–Computer Interaction Environment"," To counter the fake news phenomenon, the scholarly community has attempted to debunk and prebunk disinformation. However, misinformation still constitutes a major challenge due to the variety of misleading techniques and their continuous updates which call for the exercise of critical thinking to build resilience. In this study we present two open access chatbots, the Fake News Immunity Chatbot and the Vaccinating News Chatbot, which combine Fallacy Theory and Human–Computer Interaction to inoculate citizens and communication gatekeepers against misinformation. These chatbots differ from existing tools both in function and form. First, they target misinformation and enhance the identification of fallacious arguments; and second, they are multiagent and leverage discourse theories of persuasion in their conversational design. After having described both their backend and their frontend design, we report on the evaluation of the user interface and impact on users’ critical thinking skills through a questionnaire, a crowdsourced survey, and a pilot qualitative experiment. The results shed light on the best practices to design user-friendly active inoculation tools and reveal that the two chatbots are perceived as increasing critical thinking skills in the current misinformation ecosystem. ","",""
"2023","Studying the Downstream Effects of Fact-Checking on Social Media: Experiments on Correction Formats, Belief Accuracy, and Media Trust"," Repeated exposure to misinformation not only reduces the accuracy of people’s beliefs, but it also decreases confidence in institutions such as the news media. Can fact-checking—journalism’s main weapon against misinformation—worsen or ameliorate distrust in journalists and the media? To answer this question, we conducted two pre-registered experiments in Chile (total N = 1,472) manipulating message and receiver factors known to regulate the persuasiveness of fact-checks: transparency elements, arousing images, and political alignment. The results of both studies show that, across message formats, fact-checks are similarly effective at reducing people’s misperceptions. However, these positive effects on belief accuracy come at a cost: Compared to control groups, users exposed to political fact-checks trust news less and perceive the media as more biased, especially after reading corrections debunking pro-attitudinal misinformation. We close with a discussion of the theoretical and practical implications of these findings. ","",""
"2023","One Dose Is Not Enough: The Beneficial Effect of Corrective COVID-19 Information Is Diminished If Followed by Misinformation"," The World Health Organization (WHO) released a series of mythbuster infographics to combat misinformation during the COVID-19 infodemic. While the corrective effects of such debunking interventions have typically been examined in the immediate aftermath of intervention delivery; the durability of these corrective effects and their resilience against subsequent misinformation remains poorly understood. To this end, we asked younger and older adults to rate the truthfulness and credibility of 10 statements containing misinformation about common COVID-19 myths, as well as their willingness to share the statements through social media. They did this three times, before and after experimental interventions within a single study session. In keeping with established findings, exposure to the WHO’s myth-busting infographics—(a) improved participants’ ratings of the misinformation statements as untruthful and uncredible and (b) reduced their reported willingness to share the statements. However, within-subject data revealed these beneficial effects were diminished if corrective information was presented shortly by misinformation, but the effects remained when further corrective information was presented. Throughout the study, younger adults rated the misinformation statements as more truthful and credible and were more willing to share them. Our data reveal that the benefit of COVID-19 debunking interventions may be short-lived if followed shortly by misinformation. Still, the effect can be maintained in the presence of further corrective information. These outcomes provide insights into the effectiveness and durability of corrective information and can influence strategies for tackling health-related misinformation, especially in younger adults. ","",""
"2024","Misinformation’s missing human"," From pandemics to political campaigns, online misinformation has become acute. In response, a plethora of interventions have been offered, from debunking and prebunking to fact-checking and labeling. While the technical efficacy of these “solutions” are debatable, I suggest a more fundamental failure: they rely on a humanlike caricature, a rational and ethical figure who only needs better facts to disavow misguided misinfo practices. Instead I argue that misinformation studies must incorporate a more holistic human. Drawing from the broader humanities, this article conceptualizes the actually-existing human who can be emotional, factional, and bigoted – all qualities instrumentalized and amplified by online media. Reinserting this missing figure reintroduces agency and antipathy into misinformation studies. Misinformation is not something done to innocent subjects who merely need to be educated, but is an active practice shaped by identity and sociality that reflects the contradictions and frictions intrinsic to human nature. ","",""
"2024","Online misinformation and everyday ontological narratives of social distinction"," Most research into online misinformation has investigated its direct effects—the impact it may have on citizens’ beliefs and behavior. Much less attention has been paid to how citizens themselves make sense of misinformation as a broader social problem. We integrate theories of narrative, identity, cultural capital, and social distinction to examine how people construct the problem of misinformation and their orientation to it. We show how people engage in everyday ontological narratives of social distinction. These involve making a variety of discursive moves to position one’s “taste” in information consumption as superior to others constructed as lower in a social hierarchy. This serves to enhance social status by separating oneself from misinformation, which is presented as “other people’s problem.” We argue that these narratives have significant implications not only for citizens’ vigilance toward misinformation but also their receptiveness to interventions by policymakers, fact-checkers, news organizations, and media educators. ","",""
"2024","Sowing “seeds of doubt”: Cottage industries of election and medical misinformation in Brazil and the United States"," We conducted ethnographic research with 31 misinformation creators and consumers in Brazil and the United States before, during, and after a major election to understand consumption and production of election and medical misinformation. This study contributes to research on misinformation ecosystems by focusing on poorly understood “micro-influencers” who create misinformation in peer-to-peer networks. We detail four key tactics that micro-influencers use. First, they disseminate “gray area” content rather than expert-falsified claims, using aesthetic and rhetorical tactics to evade moderation. Second, they post in small, closed groups where members feel predisposed to trust content. Third, they target consumers’ emotional and social needs. Finally, they post high volumes of short, repetitive content to plant “seeds of doubt” and build trust. We discuss the implications these micro-influencers have for misinformation interventions and platforms’ efforts to moderate misinformation. ","",""
"2024","Perceiving AI intervention does not compromise the persuasive effect of fact-checking"," Efforts to scale up fact-checking through technology, such as artificial intelligence (AI), are increasingly being suggested and tested. This study examines whether previously observed effects of reading fact-checks remain constant when readers are aware of AI’s involvement in the fact-checking process. We conducted three online experiments ( N = 3,978), exposing participants to fact-checks identified as either human-generated or AI-assisted, simulating cases where AI fully generates the fact-check or automatically retrieves human fact-checks. Our findings indicate that the persuasive effect of fact-checking, specifically in increasing truth discernment, persists even among participants without a positive prior attitude toward AI. Additionally, in some cases, awareness of AI’s role reduced perceived political bias in fact-checks among Republicans. Finally, neither AI-generated nor human fact-checks significantly affected participants’ feelings toward or their perceptions of the competence of the targeted politicians. ","",""
"2024","<i>Disinforming the unbiased</i>: How online users experience and cope with dissonance after climate change disinformation exposure"," The emergence of disinformation challenges today’s democracies. Selective exposure research assumes that psychological biases cause people to turn to attitude-reinforcing disinformation, though studies indicate that this only holds true for small niches of online audiences. However, when online, unbiased users as well may encounter disinformation, which for them appear to be attitude-challenging. How unbiased online users experience and cope with dissonance triggered by this, and whether this affects their pre-existing attitudes, has hardly been explored. This research gap is addressed using the polarized topic of climate change as an example. An experimental research design is applied combining stimulus exposure, survey research, eye tracking, and interviews ( n = 50). The findings indicate that unbiased users are not entirely resistant to disinformation influence. However, attitude effects could not be fully explained by selection behavior but instead through different feelings and strategies of coping with dissonance and patterns of performing online information searches. ","",""
"2024","People believe misinformation is a threat because they assume others are gullible"," Alarmist narratives about the flow of misinformation and its negative consequences have gained traction in recent years. If these fears are to some extent warranted, the scientific literature suggests that many of them are exaggerated. Why are people so worried about misinformation? In two pre-registered surveys conducted in the United Kingdom ( Nstudy_1 = 300, Nstudy_2 = 300) and replicated in the United States ( Nstudy_1 = 302, Nstudy_2 = 299), we investigated the psychological factors associated with perceived danger of misinformation and how it contributes to the popularity of alarmist narratives on misinformation. We find that the strongest, and most reliable, predictor of perceived danger of misinformation is the third-person effect (i.e. the perception that others are more vulnerable to misinformation than the self) and, in particular, the belief that “distant” others (as opposed to family and friends) are vulnerable to misinformation. The belief that societal problems have simple solutions and clear causes was consistently, but weakly, associated with perceived danger of online misinformation. Other factors, like negative attitudes toward new technologies and higher sensitivity to threats, were inconsistently, and weakly, associated with perceived danger of online misinformation. Finally, we found that participants who report being more worried about misinformation are more willing to like and share alarmist narratives on misinformation. Our findings suggest that fears about misinformation tap into our tendency to view other people as gullible. ","",""
"2024","Fake thumbs in play: A large-scale exploration of false amplification and false diminution in online news comment spaces"," This study explores how disinformation can dampen general users’ expressions of opinion online. In the context of a proven disinformation case in South Korea, this study analyzes externally validated click-logs of 1389 fake accounts and more than a million logs of 45,769 general users in a highly popular web portal. Findings show that the inflated visibility of anti-governmental opinions in the manipulated comment space was incongruent with the overall political tone that general users had spontaneously encountered from the broader media ecosystem beyond the manipulated space. Subsequently, this opinion “climate” incongruence decreased the likelihood of commenting in the manipulated space. The study concludes that false amplification (of the opinions that the manipulators promote) and false diminution (of general users’ political expressions) work in tandem to create a distorted opinion environment. ","",""
"2024","Global misinformation trends: Commonalities and differences in topics, sources of falsehoods, and deception strategies across eight countries"," In a quantitative content analysis of 3,154 debunking articles from 23 fact-checking organizations, this study examines global misinformation trends and regional nuances across eight countries in Europe and Latin America (UK, DE, PT, SP, AR, BR, CL, and VZ). It strives to elucidate commonalities and differences based on political and media system indicators. Notably, countries with a substantial online presence of far-right parties avoid disclosing (fake) ordinary accounts to evade engaging in inauthentic coordinated actions. While entirely fabricated stories are infrequent, they stand out in Brazil and Spain, the two countries with higher political polarization. Despite variations, aggregated forms of fabrication (invented, manipulated, imposter, or decontextualized content) are more prominent in Latin America due to high social media use for news and low reliance on public media. Conversely, in Europe, countries are more impacted by misleading (cherry-picked, exaggerated, and twisted) information. ","",""
"2024","Let’s verify and rectify! Examining the nuanced influence of risk appraisal and norms in combatting misinformation"," Mounting concerns about COVID-19 misinformation and its insidious fallout drive the search for viable solutions. Both scholarly and practical efforts have turned toward raising risk appraisal of misinformation and motivating verification and debunking behaviors. However, individuals remain reluctant to verify and correct misinformation, suggesting a need to develop persuasion strategies to motivate such behaviors. Therefore, with an experiment of 256 participants recruited from Amazon MTurk, this study examines how effectively norm-based messages improve positive behavioral intentions during the COVID-19 pandemic. Findings suggest that among individuals with high perceived severity of misinformation, exposure to both descriptive and injunctive norms about verification reduced their intention to rectify misinformation. However, both descriptive and injunctive norms about debunking misinformation increased intentions to engage in preventive behaviors. By probing the “self–other” discrepancy and the “trade-off effect” of risk appraisal, the study further reveals that the perceived severity of misinformation merits in-depth exploration in future research. ","",""
"2024","User agency–based versus machine agency–based misinformation interventions: The effects of commenting and AI fact-checking labeling on attitudes toward the COVID-19 vaccination"," This study aimed to examine the effects of commenting on a Facebook misinformation post by comparing a user agency–based intervention and machine agency–based intervention in the form of artificial intelligence (AI) fact-checking labeling on attitudes toward the COVID-19 vaccination. We found that both interventions were effective at promoting positive attitudes toward vaccination compared to the misinformation-only condition. However, the intervention effects manifested differently depending on participants’ residential locations, such that the commenting intervention emerged as a promising tool for suburban participants. The effectiveness of the AI fact-checking labeling intervention was pronounced for urban populations. Neither of the fact-checking interventions showed salient effects with the rural population. These findings suggest that although user agency- and machine agency–based interventions might have potential against misinformation, these interventions should be developed in a more sophisticated way to address the unequal effects among populations in different geographic locations. ","",""
"2024","A systematic literature review of the motivations to share fake news on social media platforms and how to fight them"," This review aims (a) to investigate the motivations to share fake news on Social Media Platforms (SMPs) according to the Self-Determination Theory (SDT); (b) to identify the solutions to fight these motivations and the agents in charge of implementing them; and (c) the user’s role in this process. We reviewed 64 journal articles published up to April 2022. Misinformation belief and entertainment stood out as the most cited intrinsic motivations, while self-promotion, conspiracy theory, and political ideology were the most cited extrinsic motivations in the reviewed literature. The main solutions to fight fake news spreading on SMPs are improving users’ digital literacy, refining interventions, rating headlines, and sources, and promoting users’ engagement to consume content sustainably. These interventions should be adopted by four agents: governments, SMPs, civil society, and private health organizations. However, the role of SMP users themselves is critical in this process. ","",""
"2025","Not all skepticism is “healthy” skepticism: Theorizing accuracy- and identity-motivated skepticism toward social media misinformation"," Fostering skepticism has been seen as key to addressing misinformation on social media. This article reveals that not all skepticism is “healthy” skepticism by theorizing, measuring, and testing the effects of two types of skepticism toward social media misinformation: accuracy- and identity-motivated skepticism. A two-wave panel survey experiment shows that when people’s skepticism toward social media misinformation is driven by accuracy motivations, they are less likely to believe in congruent misinformation later encountered. They also consume more mainstream media, which in turn reinforces accuracy-motivated skepticism. In contrast, when skepticism toward social media misinformation is driven by identity motivations, people not only fall for congruent misinformation later encountered, but also disregard platform interventions that flag a post as false. Moreover, they are more likely to see social media misinformation as favoring opponents and intentionally avoid news on social media, both of which form a vicious cycle of fueling more identity-motivated skepticism. ","",""
"2025","Misinformation rules!? Could “group rules” reduce misinformation in online personal messaging?"," Personal messaging platforms are hugely popular and often implicated in the spread of misinformation. We explore an unexamined practice on them: when users create “group rules” to prevent misinformation entering everyday interactions. Our data are a subset of in-depth interviews with 33 participants in a larger program of longitudinal qualitative fieldwork ( N = 102) we conducted over 16 months. Participants could also donate examples of misinformation via our customized smartphone application. We find that some participants created group rules to mitigate what they saw as messaging’s harmful affordances. In the context of personalized trust relationships, these affordances were perceived as making it likely that misinformation would harm social ties. Rules reduce the vulnerability and can stimulate metacommunication that, over time, fosters norms of collective reflection and epistemic vigilance, although the impact differs subtly according to group size and membership. Subject to further exploration, group rulemaking could reduce the spread of online misinformation. ","",""
"2025","Truth be told: How “true” and “false” labels influence user engagement with fact-checks"," When do users share fact-checks on social media? We describe a survey experiment conducted during the 2019 election in Argentina measuring the propensity of voters to share corrections to political misinformation that randomly confirm or challenge their initial beliefs. We find evidence of selective sharing—the notion that individuals prefer to share pro-attitudinal rather than counter-attitudinal fact-checks. This effect, however, is conditioned by the type of adjudication made by fact-checkers. More specifically, in line with motivated reasoning processes, respondents report a higher intent to share confirmations (i.e. messages fact-checked with a “true” rating) compared with refutations (i.e. messages fact-checked with a “false” rating). Experimental results are partially confirmed with a regression discontinuity analysis of observational data of Twitter and replicated with additional experiments. Our findings suggest that fact-checkers could increase exposure to their verifications on social media by framing their corrections as confirmations of factually correct information. ","",""
"2025","Fact-checking misinformation on Chinese Social Media: Impact of corrections, awareness prompts, and legal warnings on endorsement","This study investigates the differential impacts of corrections, awareness prompts, and legal warnings on the endorsement of fact-checking information (through both “likes” and expressed support in associated comments) across three types of misinformation motivation (dread, wedge-driving, wish) on Weibo, a major Chinese social media platform. Through manual labeling and BERT (a pretrained large language model), we analyzed a cleaned dataset of 4,942 original fact-checking Weibo posts from 18 November 2010 to 31 May 2022, created or shared by Weibo Piyao. Results indicate that government posts or those with visual cues received fewer “likes” but garnered more supportive comments, while awareness prompts and legal warnings received more supportive comments across three misinformation types. This research provides valuable insights into the practice of fact-checking on social media, highlighting how different strategies may vary in their impact depending on the nature of the misinformation being addressed.","",""
"2025","Facts or Feelings? Leveraging Emotionality as a Fact-Checking Strategy on Social Media in the United States"," Emotionality is a well-established strategy for boosting audience engagement on social media. While fact-checking is positioned to provide objective information, fact-checking posts on social media often involve heightened emotionality. How much emotionality is present and how emotionality influences audience engagement and public sentiment toward fact-checked targets remain largely understudied. Informed by social psychological frameworks explicating message-level factors influencing public engagement and sentiment, the present study examines emotionality in 49,270 fact-checking posts created by 10 United States fact-checking organizations on Facebook from 2017 to 2022. Results showed that emotionality in fact-checking posts significantly increased by 13.5% over the years. Editorial fact-checkers (e.g., Washington Post) used higher levels of emotionality than independent fact-checkers (e.g., snopes.com). Emotionality positively indicated public engagement as predicted. However, in both fact-checked true and false information, emotionality was negatively associated with the public’s sentiment toward fact-checked targets, suggesting a potential spillover effect on stories verified to be true. This study reveals that emotionality in fact-checking posts boosts social media engagement yet with the potential of compromising fact-checking effectiveness. ","",""
"2025","Corrective Democracy? The Relationship Between Correction of Misinformation on Social Media and Connective Democratic Norms","Of the many solutions to address political misinformation spreading on social media, user correction holds special promise for connective democracy given its emphasis on prioritizing user autonomy and fostering communication and connections across lines of disagreement. But for the connective democratic benefits to be realized, these user corrections should ideally come from those who express strong support for democratic norms. Using a nationally representative survey of Americans immediately after the 2020 U.S. presidential election, we find the opposite is true: self-reported correctors also tended to support political violence to achieve their goals. Rather than treating self-reported correction as a clear positive force for democracy, researchers and practitioners should consider the potential drawbacks and limitations of self-reported correction, particularly when coming from those with less supportive attitudes toward connective democracy.","",""
"2025","Doing What Is Right: Role of Social Media Users in Resilience to Disinformation","Resilience to disinformation on social media relies on the user’s ability to critically assess disinformation and even counter it. Active users, who, with their actions, can curate the information environment of others, can play a crucial role in stopping the dissemination of disinformation. Their activities, such as correcting or reporting, in the decentralized social media environment may prove more effective than institutional responses. Considering this, the study looks specifically at how active users engage with disinformation. Through 60 semi-structured interviews over 3 years, we explore how crises like COVID-19 and the Russia–Ukraine war impact Czech users’ motivations and strategies. Findings indicate that users are driven by a moral obligation to provide accurate information. Both people sharing and correcting disinformation believe in their critical skills, with their desire to help amplified by crises. However, the ones correcting often face frustration and demotivation due to hostile interactions and a lack of visible impact, while the ones sharing remain persistent. Strategies are influenced by the perceptions of the individuals and the type of disinformation. Completely false information is often ignored as not worth debunking, whereas partially false information prompts active correction due to the perceived ease of rebuttal. The study highlights the need for social media platforms to support users in corrective actions and address algorithmic issues that may impede these efforts.","",""

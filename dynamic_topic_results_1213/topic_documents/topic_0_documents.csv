"year","title","abstract","journal","doi"
"2000","Audience, Uglossia, and CONLANG","                Could we also imagine a language in which a person could write down or                 give vocal expression to his inner experiences -- his feelings, moods, and                 the rest -- for his private use? Well, can't we do so in our ordinary                 language? -- But that is not what I mean. The individual words of this                 language are to refer to what can only be known to the person speaking; to                 his immediate private sensations. So another person cannot understand the                 language.                  -- Ludwig Wittgenstein, Philosophical Investigations par. 243                I will be using 'audience' in two ways in the following essay: as a                 phenomenon that produces and is produced by media technologies (readers,                 hearers, viewers, Internet-users), and as something, audiens, that is                 essential to language itself, something without which language cannot be.                  I shall do so in specific references to invented languages. Who, then, are                 the 'consumers' of invented languages?                In referring to invented languages, I am not talking about speakers of                 Esperanto or Occidental; I am not concerned with the invention of                 international auxiliary languages. These projects, already well-debated,                 have roots that go back at least as far as the 17th-century language                 philosophers who were at pains to undo the damage of Babel and restore a                 common language to the world. While Esperanto never became what it                 intended to be, it at least has readers and speakers.                I am also not even talking about speakers of Klingon or Quenya. These                 privately invented languages have had the good fortune to be attached to                 popular invented cultures, and to media with enough money and publicity to                 generate a multitude of fans.                Rather, I am talking about a phenomenon on the Internet and in a well-                 populated listserv whereby a number of people from all over the globe have                 discovered each other on-line. They all have a passion for what Jeffrey                 Schnapp calls uglossia ('no-language', after utopia, 'no-place'). Umberto                 Eco calls it 'technical insanity' or glottomania. Linguist Marina                 Yaguello calls language inventors fous du langage ('language lunatics') in                 her book of the same title. Jeffrey Henning prefers the term 'model                 language' in his on-line newsletter: 'miniaturized versions that provide                 the essence of something'. On CONLANG, people call themselves conlangers                 (from 'constructed language') and what they do conlanging. By forming                 this list, they have created a media audience for themselves, in the first                 sense of the term, and also literally in the second sense, as a number of                 them are setting up soundbytes on their elaborately illustrated and                 explicated Webpages.                Originally devoted to advocates for international auxiliary languages,                 CONLANG started out about eight years ago, and as members joined who were                 less interested in the politics than in the hobby of language invention,                 the list has become almost solely the domain of the latter, whereas the                 'auxlangers', as they are called, have moved to another list. An                 important distinguishing feature of 'conlangers' is that, unlike the                 'auxlangers', there is no sustained hope that their languages will have a                 wide-body of hearers or users. They may wish it, but they do not advocate                 for it, and as a consequence their languages are free to be a lot weirder,                 whereas the auxlangs tend to strive for regularity and useability.                CONLANG is populated by highschool, college, and graduate students;                 linguists; computer programmers; housewives; librarians; professors; and                 other users worldwide. The old debate about whether the Internet has                 become the 'global village' that Marshall McLuhan predicted, or whether it                 threatens to atomise communication 'into ever smaller worlds where                 enthusiasms mutate into obsessions', as Jeff Salamon warns, seems                 especially relevant to a study of CONLANG whose members indulge in an                 invention that by its very nature excludes the casual listener-in.                And yet the audio-visual capacities of the Internet, along with its speed                 and efficiency of communication, have made it the ideal forum for                 conlangers. Prior to the Web, how were fellow inventors to know that                 others were doing -- in secret? J.R.R. Tolkien has been lauded as a rare                 exception in the world of invention, but would his elaborate linguistic                 creations have become so famous had he not published The Lord of the Rings                 and its Appendix? Poignantly, he tells in """"A Secret Vice"""" about                 accidentally overhearing another army recruit say aloud: 'Yes! I think I                 shall express the accusative by a prefix!'. Obviously, silent others                 besides Tolkien were inventing languages, but they did not have the means                 provided by the Internet to discover one another except by chance.                Tolkien speaks of the 'shyness' and 'shame' attached to this pursuit,                 where 'higher developments are locked in secret places'. It can win no                 prizes, he says, nor make birthday presents for aunts. His choice of                 title (""""A Secret Vice"""") echoes a Victorian phrase for the closet, and                 conlangers have frequently compared conlanging to homosexuality, both                 being what conservative opinion expects one to grow out of after puberty.                  The number of gay men on the list has been wondered at as more than                 coincidental. In a survey I conducted in October 1998, many of the                 contributors to CONLANG felt that the list put them in touch with an                 audience that provided them with intellectual and emotional feedback.                  Their interests were misunderstood by parents, spouses, lovers, and                 employers alike, and had to be kept under wraps. Most of those I surveyed                 said that they had been inventing a language well before they had heard of                 the list; that they had conceived of what they were doing as unique or                 peculiar, until discovery of CONLANG; and that other people's Websites                 astounded them with the pervasive fascination of this pursuit.                There are two ways to look at it: conlanging, as Henning writes, may be                 as common and as humanly creative as any kind of model-making, i.e.,                 dollhouses, model trains, role-playing, or even the constructed cultures                 with city plans and maps in fantasy novels such as Terry Pratchett's                 Discworld. The Web is merely a means to bring enthusiasts together. Or it                 may provide a site that, with the impetus of competition and showmanship,                 encourages inutile and obsessive activity. Take your pick. From                 Hildegard von Bingen's Lingua Ignota to Dante's Inferno and the babbling                 Nimrod to John Dee's Enochian and on, invented languages have smacked of                 religious ecstacy, necromancy, pathology, and the demonic. Twin speech,                 or 'pathological idioglossia', was dramatised by Jodie Foster in Nell.                  Hannah Green's 'Language of Yr' was the invention of her schizophrenic                 protagonist in I Never Promised You a Rose Garden. Language itself is the                 centre of furious theoretical debate. Despite the inventive 'deformities'                 it is put to in poetry, punning, jest, singing, and lying, human language,                 our most 'natural' of technologies, is a social machine, used by                 multitudes and expected to get things done. It is expected of language                 that it be understood and that it have not only hearers but also                 answerers. All human production is founded on this assumption. A                 language without an audience of other speakers is no language. 'Why                 aren't you concentrating on real languages?' continues to be the most                 stinging criticism.                Audience is essential to Wittgenstein's remark quoted at the beginning of                 this essay. Wittgenstein posits his 'private languages theory' as a kind                 of impossibility: all natural languages, because they exist by consensus,                 can only refer to private experience externally. Hence, a truly private                 language, devoted to naming 'feelings and moods' which the subject has                 never heard about or shared with others, is impossible among socialised                 speakers who are called upon to define subjective experience in public                 terms. His is a critique of solipsism, a charge often directed at                 language inventors. But very few conlangers that I have encountered are                 making private languages in Wittgenstein's sense, because most of them are                 interested in investing their private words with public meaning, even when                 they are doing it privately. For them, it is audience, deeply desireable,                 that has been impossible until now.                Writing well before the development of CONLANG, Yaguello takes the stance                 that inventing a language is an act of madness. 'Just look at the lunatic                 in love with language', she writes:                sitting in his book-lined study, he collects great piles of information,   he collates and classifies it, he makes lists and fills card indexes. He   is in the clutches of a denominatory delirium, of a taxonomic madness.    He   has to name everything, but before being able to name, he has to   recognize   and classify concepts, to enclose the whole Universe in a system of   notation: produce enumerations, hierarchies, and paradigms.               She is of course describing John Wilkins, whose Real Character and                 Universal Language in 1668 was an attempt to make each syllable of his                 every invented word denote its placement in a logical scheme of                 classification. 'A lunatic ambition', Yaguello pronounces, because it                 missed the essential quality of language: that its signs are arbitrary,                 practical, and changeable, so as to admit neologism and cultural                 difference. But Yaguello denounces auxiliary language makers in general                 as amateurs 'in love with language and with languages, and ignorant of the                 science of language'. Her example of 'feminine' invention comes from                 Helene Smith, the medium who claimed to be channeling Martian (badly                 disguised French). One conlanger noted that Yaguello's chapter entitled                 'In Defence of Natural Languages' reminded him of the US Federal 'Defense                 of Marriage Act', whereby the institution of heterosexual marriage is                 'defended' from homosexual marriage. Let homosexuals marry or lunatics                 invent language, and both marriage and English (or French) will come                 crashing to the ground.                Schnapp praises Yaguello's work for being the most comprehensive                 examination of the phenomenon to date, but neither he nor she addresses                 linguist Suzette Haden Elgin's creative work on Láadan, a language                 designed for women, or even Quenya or Klingon -- languages that have                 acquired at least an audience of readers. Schnapp is less condemnatory                 than Yaguello, and interested in seeing language inventors as the                 'philologists of imaginary worlds', 'nos semblables, nos frères, nos                 soeurs' -- after all. Like Yaguello, he is given to some generalities:                 imaginary languages are 'infantile': 'the result is always [my emphasis]                 an """"impoverishment"""" of the natural languages in question: reduced to a                 limited set of open vowels [he means """"open syllables""""], prone to syllabic                 reduplication and to excessive syntactical parallelisms and symmetries'.                  To be sure, conlangs will never replicate the detail and history of a real                 language, but to call them 'impoverishments of the natural languages'                 seems as strange as calling dollhouses 'impoverishments of actual houses'.                 Why this perception of threat or diminishment? The critical, academic               """"audience"""" for language invention has come largely from non-language                 inventors and it is woefully uninformed. It is this audience that                 conlangers dislike the most: the outsiders who cannot understand what they                 are doing and who belittle it.                The field, then, is open to re-examination, and the recent phenomenon of                 conlanging is evidence that the art of inventing languages is neither                 lunatic nor infantile. But if one is not Tolkien or a linguist supported                 by the fans of Star Trek, how does one justify the worthwhile nature of                 one's art? Is it even art if it has an audience of one ... its artist?                Conlanging remains a highly specialised and technical pursuit that is, in                 the end, deeply subjective. Model builders and map-makers can expect                 their consumers to enjoy their products without having to participate in                 the minutia of their building. Not so the conlanger, whose consumer must                 internalise it, and who must understand and absorb complex linguistic                 concepts. It is different in the world of music. The Cocteau Twins,                 Bobby McFerrin in his Circle Songs, Lisa Gerrard in Duality, and the new                 group Ekova in Heaven's Dust all use 'nonsense' words set to music --                 either to make songs that sound like exotic languages or to convey a kind                 of melodic glossolalia. Knowing the words is not important to their                 hearers, but few conlangers yet have that outlet, and must rely on text                 and graphs to give a sense of their language's structure. To this end,                 then, these are unheard, unaudienced languages, existing mostly on screen.                 A few conlangers have set their languages to music and recorded them. What                 they are doing, however, is decidedly different from the extempore of                 McFerrin. Their words mean something, and are carefully worked out                 lexically and grammatically.                So What Are These Conlangs Like?               On CONLANG and their links to Websites you will find information on                 almost every kind of no-language imaginable. Some sites are text only;                 some are lavishly illustrated, like the pages for Denden, or they feature                 a huge inventory of RealAudio and MP3 files, like The Kolagian Languages,                 or the songs of Teonaht. Some have elaborate scripts that the newest                 developments in fontography have been able to showcase. Some, like Tokana                 and Amman-Iar, are the result of decades of work and are immensely                 sophisticated. Valdyan has a Website with almost as much information about                 the 'conculture' as the conlang. Many are a posteriori languages, that                 is, variations on natural languages, like Brithenig (a mixture of the                 features of Brythonic and Romance languages); others are a priori --                 starting from scratch -- like Elet Anta.                Many conlangers strive to make their languages as different from European                 paradigms as possible. If imaginary languages are bricolages, as Schnapp                 writes, then conlangers are now looking to Tagalog, Basque, Georgian,                 Malagasay, and Aztec for ideas, instead of to Welsh, Finnish, and Hebrew,                 languages Tolkien drew upon for his Elvish. """"Ergative"""" and """"trigger""""                 languages are often preferred to the """"nominative"""" languages of Europe.                  Some people invent for sheer intellectual challenge; others for the beauty                 and sensuality of combining new and privately meaningful sounds.                There are many calls for translation exercises, one of the most popular                 being 'The Tower of Babel' (Genesis 10: 1-9). The most recent innovation,                 and one that not only showcases these languages in all their variety but                 provides an incentive to learn another conlanger's conlang, is the                 Translation Relay Game: someone writes a short poem or composition in his                 or her language and sends it with linguistic information to someone else,                 who sends a translation with directions to the next in line all the way                 around again, like playing 'telephone'. The permutations that the Valdyan                 Starling Song went through give good evidence that these languages are not                 just relexes, or codes, of natural languages, but have their own                 linguistic, cultural, and poetic parameters of expression. They differ                 from real languages in one important respect that has bearing on my                 remarks about audience: very few conlangers have mastered their languages                 in the way one masters a native tongue. These creations are more like                 artefacts (several have compared it to poetry) than they are like                 languages. One does not live in a dollhouse. One does not normally think                 or speak in one's conlang, much less speak to another, except through a                 laborious process of translation.                It remains to a longer cultural and sociolinguistic study (underway) to                 tease out the possibilities and problems of conlanging: why it is done,                 what does it satisfy, why so few women do it, what are its demographics,                 or whether it can be turned to pedagogical use in a 'hands-on', high-                 participation study of language. In this respect, CONLANG is one of the                 'coolest' of on-line media. Only time will show what direction conlanging                 and attitudes towards it will take as the Internet becomes more powerful                 and widely used.                Will the Internet democratise, and eventually make banal, a pursuit that                 has until now been painted with the romantic brush of lunacy and secrecy?                 (You can currently download LangMaker, invented by Jeff Henning, to help                 you construct your own language.) Or will it do the opposite and make                 language and linguistics -- so often avoided by students or reduced in                 university programs -- inventive and cutting edge? (The inventor of Tokana                 has used in-class language invention as a means to study language                 typology.) Now that we have it, the Internet at least provides conlangers                 with a place to hang their logodaedalic tapestries, and the technology for                 some of them to be heard.                                          References                Von Bingen, Hildegard. Lingua Ignota, or Wörterbuch der unbekannten                   Sprache. Eds. Marie-Louise Portmann and Alois Odermatt. Basel: Verlag                   Basler Hildegard-Gesellschaft, 1986.                 Eco, Umberto. The Search for the Perfect Language. Trans. James                   Fentress. Oxford, England, and Cambridge, Mass.: Blackwell, 1995, 1997.                 Elgin, Suzette Haden. A First Dictionary and Grammar of Láadan.                    Madison, WI: Society for the Furtherance and Study of Fantasy and Science-                   Fiction, 1985.                 Henning, Jeffrey. Model Languages: The Newsletter Discussing Newly                   Imagined Words for Newly Imagined Worlds. &lt;http://www.Langmaker.com/ml00.htm&gt;.                 Kennaway, Richard. Some Internet Resources Relating to Constructed                   Languages. &lt;http://www.sys.uea.ac.uk/jrk/conlang.php&gt;. (The most                   comprehensive list (with links) of invented languages on the Internet.)                 Laycock, Donald C. The Complete Enochian Dictionary: A Dictionary of the                   Angelic Language as Revealed to Dr. John Dee and Edward Kelley. York                   Beach, Maine: Samuel Weiser, 1994.                 McLuhan, Marshall. Understanding Media. Reprinted. Cambridge, MA: MIT                   P, 1994.                 Salamon, Jeff. """"Revenge of the Fanboys."""" Village Voice 13 Sep., 1994.                 Schnapp, Jeffrey. """"Virgin Words: Hildegard of Bingen's Lingua Ignota                   and the Development of Imaginary Languages Ancient and Modern."""" Exemplaria                   3.2 (1991): 267-98.                 Tolkien, J.R.R. """"A Secret Vice."""" The Monsters and the Critics and Other                   Essays. Ed. Christopher Tolkien. Boston: Houghton Mifflin, 1984. 198-223.                 Wilkins, John. An Essay Towards a Real Character and a Philosophical                   Language. Presented to the Royal Society of England in 1668.                 Wittgenstein, Ludwig. Philosophical Investigations. 3rd ed. Trans.                   G.E.M. Anscombe. Englewood Cliffs, NJ: Prentice Hall, 1958.                 Yaguello, Marina. Lunatic Lovers of Language: Imaginary Languages and                   Their Inventors. Trans. Catherine Slater. (Les fous du langage. 1985.)                   London: The Athlone Press, 1991.                                     Citation reference for this article                 MLA style:                    Sarah L. Higley. """"Audience, Uglossia, and CONLANG: Inventing Languages on                   the Internet."""" M/C: A Journal of Media and Culture 3.1 (2000). [your date                   of access] &lt;http://www.uq.edu.au/mc/0003/languages.php&gt;. Chicago style:                    Sarah L. Higley, """"Audience, Uglossia, and CONLANG: Inventing Languages on                   the Internet,"""" M/C: A Journal of Media and Culture 3, no. 1 (2000), &lt;http://www.uq.edu.au/mc/0003/languages.php&gt; ([your date of access]).  APA style:                     Sarah L. Higley. (2000) Audience, Uglossia, and CONLANG: Inventing                   Languages on the Internet. M/C: A Journal of Media and Culture 3(1). &lt;http://www.uq.edu.au/mc/0003/languages.php&gt; ([your date of access]).                   ","",""
"2000","NOTES ON CONTRIBUTORS","David Anderson was educated at Queen’s University of Belfast, Northern Ireland. His PhD was on Philosophical Issues in A.I. He has been a speaker at numerous universities in Britain and Ireland and, at the World Congress of Philosophy. He has had a book on A.I. published by Ellis Horwood (now Wiley). His professional experience has been almost entirely in Computer Science where he has taught at the Universities of Teesside, Manchester (Met) and Portsmouth. He is a visiting Research Fellow at Christchurch, New Zealand.","",""
"2000","The culture of the artificial","","",""
"2000","Towards a general theory of the artificial","","",""
"2000","Collaborative systems: Evolving databases and the ?Conditions of possibility?-artificial life models of agency in on-line interactive art","","",""
"2000","The intelligence left in AI","","",""
"2000","The artificial between culture and nature","","",""
"2000","Early-connectionism machines","","",""
"2000","Observation levels and units of time: A critical analysis of the main assumption of the theory of the artificial","","",""
"2000","Telefeminism","","",""
"2001","Ethics, regulation and the new artificial intelligence, part II: autonomy and liability","","",""
"2001","Ethics, regulation and the new artificial intelligence, part II: autonomy and liability","","",""
"2001","ETHICS, REGULATION AND THE NEW ARTIFICIAL INTELLIGENCE, PART I: ACCOUNTABILITY AND POWER","A generation ago, there was a major debate about the social and ethical implications of artificial intelligence (AI). Interest in that debate waned from the late 1980s. However, both patterns of public risk perception and new technological developments suggest that it is time to re-open that debate. The important issues about AI arise in connection with the prospect of robotic and digital agent systems taking socially significant decisions autonomously. Now that this is possible, the key concerns are now about which decisions should be and which should not be delegated to machines, issues of regulation in the broad sense covering everything from consumer information through codes of professional ethics for designers to statutory controls, issues of design responsibility and problems of liability.","",""
"2001","Ethics, regulation and the new artificial intelligence, part II: autonomy and liability","This is the second article in a two-part series on the social, ethical and public policy implications of the new artificial intelligence (AI). The first article briefly presented a neo-Durkheimian understanding of the social fears projected onto AI, before arguing that the common and enduring myth of an AI takeover arising from the autonomous decision-making capability of AI systems, most recently resurrected by Professor Kevin Warwick, is misplaced. That article went on to argue that, nevertheless, some genuine and practical issues in the accountability of AI systems that must be addressed. This second article, drawing further on the neo-Durkheimian theory, sets out a more detailed understanding of what it is for a system to be autonomous enough in its decision making to blur the boundary between tool and agent. The importance of this is that this blurring of categories is often the basis, the first article argued, of social fears.","",""
"2001","CULTIVATING SOCIETY'S CIVIC INTELLIGENCE: PATTERNS FOR A NEW 'WORLD BRAIN'","In spite of remarkable advances in science and technology, humankind is beset with a number of serious problems. These are not just problems that 'won't go away'; they are problems that are worsening considerably. These problems include the growing gap between rich and poor, between those who have too much and those who have too little, as well as a broad range of environmental issues that may have major consequences but, at the same time, are little understood. This essay explores the idea of 'civic intelligence'. What projects, perspectives, policy and technology might humankind develop that would help us collectively address these problems? This essay discusses six aspects of 'civic intelligence' (orientation, organization, engagement, intelligence, products and projects, and resources) as well as ways to make cultivating our 'civic intelligence' a practical - non-utopian - enterprise.","",""
"2001","Rationalizing Medical Work. Decision-Support Techniques and Medical Practices","","",""
"2001","Rationalisation of decision-making processes in design teams with a new formalism of design rationale","","",""
"2001","Roles for knowledge-based computer systems: Case studies in maternity care","","",""
"2001","AI &amp; Society special issue on work organisation","","",""
"2001","The Origin and Development of Robotic Art","As electronic media become more pervasive in today’s culture, the role of robotics in contemporary art, along with video, multimedia, performance, telecommunications, and interactive installations, needs to be considered. In order to promote this consideration, I have compiled the accompanying chronology and define below a framework for the understanding and analysis of robotic art. I discuss three pivotal artworks from the 1960s that outline the genesis of robotics in art and form the basis of the main directions in which robotic art has","",""
"2001","An Intelligent Architecture","a property possessed by human beings. We claim some have it to a greater and others to a lesser degree. We test for how much of it we each have, and even have a club for those with lots of it. We have, traditionally, talked of some animals as having it, with an astonishment akin to that of colonial adventurers discovering that native human animals may have it. More recently, we have talked of intelligence in computers: in principle and in practice. Now that computing is spilling out of the grey box and into our clothing, we talk of smart clothes, meaning we want also to think of them as being, in some way, intelligent. We talk a lot of intelligence.","",""
"2001","Intelligent Environments","Is this house intelligent? Is it conscious? It’s certainly polite. But its relationship to you is as a servant. Is this acceptable? What kind of ethics are required in dealing with truly autonomous artefacts and other environmental entities. Obviously we treat people and other organisms with varying degrees of ethical politeness, as we will no doubt treat artificial intelligences of one sort or another. Will it be able to have emotions? Should it have emotions? What if it is more intelligent than we are? How will we train it then? Will it take over our lives and treat us as its servants rather than being what we generally think of robots as being: our servants. Will they fit into our cultures or will we have to fight for our own autonomous existence in a world governed by robots?","",""
"2001","Towards a Chronology of Robotic Art",".","",""
"2001","Transcendental philosophy and artificial life","This investigation is intended to bring to light some reasons for connecting the notion of artificial life to certain kinds of interpretation of transcendental philosophy. Since the conception of a machine has undergone an apparently decisive alteration in the post-Newtonian world it will be part of the burden of this piece to suggest that the possibility of the evolutionary development of life beyond the organic is itself a product of new thought about the relation between the organic and the processes of interpretation and understanding that we have summarized under the heading “mind”. The ultimate point of the piece will be to suggest both that the underlying conceptual approaches to artificial life have serious deficiencies and to indicate that the development of which they are a part nonetheless has an essential relationship to futurity","",""
"2002","A Taxi Ride to Late Capitalism: Hypercapitalism, Imagination and Artificial Intelligence","","",""
"2002","Miskinetic Neuropoliticology: The Politics of Contructing and Discipling the Organism of the Brain","","",""
"2002","Organizational Justice in E-recruiting: issues and controversies","The recruitment situation has high stakes both for the potential new employer and candidates. This article highlights the technology-led transformation occurring in organization’s recruitment processes and argues that more attention is needed to assess how far these systems actually widen the applicant pool, or whether they mask the replication of previous discriminatory practices. It raises questions about the transparency of the process, and the accountability of recruiters to applicants noting the procedural and distributive justice implications of these changes.","",""
"2002","Carnival Booth: An Algorithm for Defeating the Computer-Assisted Passenger Screening System"," To improve the efficiency of airport security screening, the U.S. Federal Aviation Administration (FAA) deployed the Computer Assisted Passenger Screening system (CAPS) in 1999. CAPS attempts to identify potential terrorists through the use of profiles so that security personnel can focus the bulk of their attention on high-risk individuals. In this paper, we show that since CAPS uses profiles to select passengers for increased scrutiny, it is actually less secure than systems that employ random searches. In particular, we present an algorithm called Carnival Booth that demonstrates how a terrorist cell can defeat the CAPS system. Using a combination of statistical analysis and computer simulation, we evaluate the efficacy of Carnival Booth and illustrate that CAPS is an ineffective security measure. Based on these findings, we argue that CAPS should not be legally permissible since it does not satisfy court-interpreted exemptions to the U.S. Constitution's Fourth Amendment. Finally, based both on our analysis of CAPS and historical case studies, we provide policy recommendations on how to improve air security. ","",""
"2002","Double Vision","","",""
"2003","Depth, Markup and Modelling","Personification in Ovid's Metamorphoses (understood grammatically, as phenomena created by discernible operations in language) provides a typical problem for the application of computational modelling techniques. An earlier systematic attempt to encode these personifications was described in A.24 as \u201cmodelling\u201d. In this paper the result, rather more a phenomenological description than a model, properly so-called, is taken as the basis for construction of a manipulable model. Instances of the abstract entity \u201cfortuna\u201d (fortune, luck, chance) are used to build a relational database of examples. For each example the causative factors are specified; wherever possible these factors are weighted globally to signify their degree of influence on the result. This database is linked to spreadsheet software to generate a chart of these instances in bar-graph format from a formula that computes the total effect of the causative factors. A mathematical function is provisionally introduced to provide more realistic results. Its limitations and directions for further research are discussed.","",""
"2003","Artificial intelligence as a discursive practice: the case of embodied software agent systems","","",""
"2003","Roberto Cordeschi: The discovery of the artificial. Behaviour, mind and machines before and beyond cybernetics.","","",""
"2003","Categorization of Hindi phonemes by neural networks","","",""
"2003","Cyberfeminism and Artificial Life (2003) by Sarah Kember","","",""
"2003","Avatars of the tortoise: life, longevity and simulation","This paper explores the contemporary fascination with artificial life and simulation. The concept of artificial life has broadened its reach from a rarefied speculative discipline to a phenomenon in popular culture and a fertile trope within the electronic arts. From its representation in the cinema, to virtual pets and tamagotchis, to advanced digital art, the essay examines the category of “life” in the age of digital simulation. Questions to do with artificial life range from the representation or incarnation of our “real” selves as avatars in digital spaces (such as computer games), to popular, artistic and scientific attempts to simulate autonomous life forms. In the context of digital art, the essay critically engages with the work of Australian artist Troy Innocent, in particular with his most recent installation, Artefact (2001). In Artefact, Innocent actively investigates the digital investment of life in avatars as they operate in an interactive game environment. In this work, Innocent prompts an intriguing, if troubling, question, central to the overall interests of this essay: what, in the age of digital simulation, is not alive?","",""
"2003","Algorithms and allegories","Much of contemporary art practice is both produced and can be read with the notion of the algorithmic as its predominate trope. Similarly one can read older art practices as working under the aegis of allegory. Of course these registers and metaphors can be used to parse a distribution of artistic and cultural production across time in multiple directions. We often think of the algorithmic as that which concerns procedure, and don’t see it as its own meaning. In this text I am interested in exploring the algorithmic as a gesture, individualized and particular, something that can reveal an interiority of a work. In this way it is perhaps akin to the allegoric, where the work proper, comprised on the surface, simultaneously holds beneath and within it something else. The algorithmic as an author|composer’s signature, might be thought of as that secret storehouse of invention. With the advent of computation and the network, more and more contemporary art and sound work turns its attention to sequencing, loops, replication, modulation, mutation, generative systems, database and interface as instruction sets or grammars, both as ways to conceptualize and to produce work. The paper looks at a wide range of stratagems in works of sound, architecture, visual arts, and film to illustrate a correspondence between the allegoric and the algorithmic. Its aim is to encourage both practitioners and theorists to engage these two notions, the allegoric and the algorithmic, as a way to consider and produce work.","",""
"2004","Technology for teams: the use of agent technology for self-organisation","","",""
"2004","Intelligent advertising","","",""
"2004","Rae Earnshaw and John Vince (eds): Intelligent agents for mobile and virtual media","","",""
"2004","Aesthetics of navigational performance in hypertext","","",""
"2004","Intelligent agents as innovations","","",""
"2005","Slipping and Sliding","&#x0D;         &#x0D; &#x0D;  On the back cover of The Art Forger’s Handbook, Eric Hebborn proclaims&#x0D; &#x0D;    No drawing can lie of itself, it is only the opinion of the expert which can deceive. (Hebborn)&#x0D;  &#x0D; &#x0D;  Well certainly, but like many forgers Hebborn was dedicated to ensuring the experts have ample material with which to work. &#x0D; &#x0D;  The debate about authenticity rolls into the debate about originality rolls into the debate about excellence, slipping between the verifiable and the subjective, shadowed by the expert assessing, categorising, and delivering verdicts.  Yet the proclamation ‘This is authentic’ is not straightforward. It is impossible to prove that the statement ‘This is a painting by Sir Arthur Streeton’ is true. It is always possible (though not probable) that the work in question is an excellent copy, manufactured with materials identical to those employed by Streeton, with brushstrokes reflecting Streeton’s manipulation of paint, applied in the kind of sequence Streeton used and with a provenance crafted to simulate perfectly an acceptable provenance for a work by Streeton. Much easier to prove that a work is not by a particular artist; one very obvious anomaly will suffice (Sloggett 298).&#x0D; &#x0D;  But an anomaly requires a context, the body of material against which to assess the new find. John Drew’s manipulation of the art market was successful not because of the quality of the pictures he paid John Myatt to produce (after all they were painted with household emulsion paint often extended with K-Y Jelly).  His success lay in his ability to alter the identities of these works by penetrating the archives of the Tate and the Victoria and Albert Museum and manufacturing an archival history that virtually copied the history of works by his target artists, Nicholson, Giocometti, Chagall, Epstein, Dubeffet, and de Stals. While the paintings mimicked works by these artists, without a provenance (an identity and identity trail) they were nothing more than approximate copies, many which were initially rejected by the dealers and auction houses (Landesman 38). Identity requires history and context: for something to be deemed ‘real’, both need to be verifiable.  The plight of stateless refugees lies in their inability to verify their history (who am I?) and their context (I exist here because…). &#x0D; &#x0D;  Drew’s ability to deliver a history is only one way in which works can slip identities (or in the case of Drew’s works – can be pushed). Drew’s intention and his ability to profit by the deception denoted fraud. But authentication is more often sought to support not fraud but optimism. &#x0D; &#x0D;    ‘Can you please look at this painting which hung in my grandfather’s lounge room for over 50 years? It was given to him by the artist. I remember it as a small boy, and my father also remembers it when he was a child. But I can’t sell it because someone said it didn’t look right. Can you tell if it is by the artist?’&#x0D;  &#x0D; &#x0D;  Such a problem needs to be approached on two fronts.  Firstly, how strong is the evidence that this work is by the artist and secondly, what is the hypothesis of best fit for this work?&#x0D; &#x0D;  The classic authentication process examines a picture and, against a framework of knowns (usually based on securely provenanced works) looks for points of identification between the proffered work and provenanced works.  From these points of identification a theory of best fit is developed.  For example, a painting with the inscription ‘Arthur Streeton/1896’ is analysed for its pigment content in order to test the proposition that this is a work by Arthur Streeton from 1896. Pigment analysis indicates that titanium white (a pigment not available commercially until 1920) is found in the clouds. So the proposition must be modified: either this is a work by Streeton that has been heavily reworked after 1920, or this is not a work by Streeton, or this is a work by Streeton but the date is wrong.  The authentication process will define and redefine each proposition until there is one that best fits the evidence at hand. Fluorescing the date to establish whether it is a recent addition would be part of this process. Examining other whites in the painting to check if the clouds had been added later would be another.  Checking the veracity of the provenance would also be critical.  We may decide that this is not an 1896 work by Streeton based on the evidence of the pigment.  But what if an art historian discovers a small pigment manufacturer in Box Hill whose records show they produced titanium dioxide as a pigment in 1890?  The new evidence may affect the conclusion.  But more likely we would want to verify such evidence before we altered our conclusion. &#x0D; &#x0D;  Between the extremes of Drew’s manufactured identities and the optimism of a third generation is the strengthened work, combining identity shift and hope. Dali pulled a reverse strengthening when he signed 20,000 blank sheets of paper for lithographs that had not yet been executed (Hebborn 79), but more usually it is the inscription not the image that is missing. Of course a signature is good, but signature works may not have, and do not need signatures. A signature may be a picture of a certain place (Heidelberg) at a certain time of day (moonrise); optimism will soon join the dots, producing a David Davies Moonrise. Often an inscription helps; a nondescript clean-shaven Victorian gentleman can become a bearded founding father, an anonymous nag the first winner of the Melbourne Cup. And if the buyer is not convinced, then a signature may win the day. Unlike Drew’s fabricated histories these changes in identity are confined to transformations of the object itself and then, by association, to its context.  &#x0D; &#x0D;  Art fraud is an endearing topic, partly because it challenges the subjective nature of expertise. When van Meegeren manufactured his most successful ‘Vermeer’ The Supper at Emmaus (1937) he explored the theories of experts, and then set about producing a work that copied not an existing Vermeer, but the critic’s theory of what an as-yet-undiscovered Vermeer would look like. Hannema, van Schendel and finally Bredius subscribed to the theory that Vermeer’s trip to Italy resulted in Caravaggio’s influence on the artist (Dutton 25). Van Meegeren obligingly produced such a work. So does it matter?  Is an identical work as good a work?  Is a sublime copyist of great artists a great artist? (Not that van Meegeren was either.)&#x0D; &#x0D;  Authentication is a process of assessing claims about identity.  It involves reputation, ownership, relationships and truth.  When an artist executes a copy it is homage to the skill of the master. When Miss Malvina Manton produced a scene of dead poultry in 1874, she was copying the most popular painting in the fledgling collection of the National Gallery of Victoria, Schendel’s The Poultry Vendor (Inglis 63), and joined a league of copyists including Henry Gritten and Nicholas Chevalier who sought permission to copy the Gallery’s paintings. When John O’Loughlin copied works by Clifford Possum Tjapaltjarri and passed them off as original the impact on the artist was less benign (Gotting). Sid Nolan refused to identify problematic paintings attributed to his oeuvre claiming that to acknowledge such paintings would cast doubt on his entire oeuvre.  Bob Dickerson assiduously tracks down and ‘outs’ problematic paintings from his oeuvre, claiming that not to do so would leave the thin edge of the wedge firmly embedded for future opportunists. Both are concerned with their identity.&#x0D; &#x0D;  Creation is a fraught business, simply because the act of creation is the act of giving an identity. Whether we create a child, a musical score, a painting or a t-shirt brand, the newly created entity is located within a lineage and context that means more than the single individual creation. This is why identity theft is such a major crime. If someone steals an identity they also steal the collateral developed around that identity, the ability to deal in credit, to drive a car, to travel overseas, to purchase a house.  Identity is a valuable commodity; for an artist it is their tool of trade.&#x0D; &#x0D;  There is no doubt that the public celebrates the fake.  Perhaps it is a celebration of the power of the object over the critic or the theoretician. But it is an extraordinarily costly celebration. Despite the earlier assertion that it is possible to make the perfect copy, very few even approximate the vibrancy and intelligence of an original. Most, if accepted, would seriously dilute the strength of the artist’s oeuvre.  Forging Aboriginal art is even more disgraceful. In a society where cultural transmission has traditionally been based on complex relationships of dance, song, painting and objects to customary rights, laws and obligations, art fraud impacts on the very fabric of society.  &#x0D; &#x0D;  There will always be works that slip identities, and many are not pulled back. False works do damage; they dull our perceptions, dilute our ability to understand an artist’s contribution to society, and are usually no more than blunt instruments used for financial gain.&#x0D; &#x0D;   References&#x0D; &#x0D;   Australian Institute of Criminology. “Art Crime: Protecting Art, Protecting Artists and Protecting Consumers.” 2-3 Decembeer 1999. 1 May 2005 http://www.aic.gov.au/conferences/artcrime/&gt;. Catterall, L. The Great Dali Art Fraud and Other Deceptions. Fort Lee, New Jersey: Barricade, 1992. Dutton, Denis, ed. The Forger’s Art Forgery and the Philosophy of Art. California: U of California P, 1983 Gotting, Peter. “Shame of Aboriginal Art Fakes.” 16 July 2000. 31 May 2005 http://www.museum-security.org/00/112.html#3&gt;. Hebborn, Eric. The Art Forger’s Handbook. London: Cassell, 1997. Inglis, Alison. “What Did the Picture’s Surface Convey? Copies and Copying in the National Gallery of Victoria during the Colonial Period.” The Articulate Surface: Dialogues on Paintings between Conservators, Curators and Art Historians. Ed. Sue-Anne Wallace, with Jacqueline Macnaughtan and Jodi Parvey. Canberra: The Humanities Research Centre, the Australian National University and the National Gallery of Australia, 1996. 55-69. Landesman, Peter. “A 20th-Century Master Scam.” The New York Times Magazine (18 July 1999): 31-63. Sloggett, Robyn. “The Truth of the Matter:  Issues and Procedures in the Authentication of Artwork.” Arts, Antiquity and Law 5.3 (September 2000): 295-303. Tallman, Susan. “Report from London Faking It.” Art in America (November 1990): 75-81. &#x0D;   &#x0D; &#x0D; &#x0D;         &#x0D;           Citation reference for this article&#x0D; &#x0D;           MLA Style&#x0D;           Sloggett, Robyn. """"Slipping and Sliding: Blind Optimism, Greed and the Effect of Fakes on Our Cultural Understanding."""" M/C Journal 8.3 (2005).  echo date('d M. Y'); ?&gt; &lt;http://journal.media-culture.org.au/0507/09-sloggett.php&gt;. APA Style&#x0D;           Sloggett, R. (Jul. 2005)  """"Slipping and Sliding: Blind Optimism, Greed and the Effect of Fakes on Our Cultural Understanding,"""" M/C Journal, 8(3). Retrieved  echo date('d M. Y'); ?&gt; from &lt;http://journal.media-culture.org.au/0507/09-sloggett.php&gt;. &#x0D;       ","",""
"2005","MIMes and MeRMAids: On the Possibility of Computer-aided Interpretation","One problem in humanities computing is the automatic interpretation of electronic texts. Can computers interpret texts or do they aide in the interpretation? In this paper we propose variants on the Turing Test to imagine if computer-assisted interpretation is possible. We argue that a way forward for developing models for successful machine interpreters is to work with regular expression recognition and manipulation, which is built into most programming languages in a common fashion.","",""
"2005","Technology to facilitate ethical action: a proposed design","","",""
"2005","A conceptual framework for society-oriented decision support","","",""
"2005","The open agent society as a platform for the user-friendly information society","","",""
"2005","Can many agents answer questions better than one?"," The paper addresses the issue of how online natural language question answering, based on deep semantic analysis, may compete with currently popular keyword search, open domain information retrieval systems, covering a horizontal domain. We suggest the multiagent question answering approach, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge. The meta–agent controls the cooperation between question answering agents and chooses the most relevant answer(s). We argue that multiagent question answering is optimal in terms of access to business and financial knowledge, flexibility in query phrasing, and efficiency and usability of advice. The knowledge and advice encoded in the system are initially prepared by domain experts.  We analyze the commercial application of multiagent question answering and the robustness of the meta–agent. The paper suggests that a multiagent architecture is optimal when a real world question answering domain combines a number of vertical ones to form a horizontal domain.  ","",""
"2006","AI &amp; Society Vol. 20.4","","",""
"2006","Private language: recognizing a useful nonsense","","",""
"2006","The influence of people’s culture and prior experiences with Aibo on their attitude towards robots","","",""
"2006","Unplanned effects of intelligent agents on Internet use: a social informatics approach","","",""
"2006","Experimental investigation into influence of negative attitudes toward robots on human–robot interaction","","",""
"2006","Questionnaire-based social research on opinions of Japanese visitors for communication robots at an exhibition","","",""
"2006","Automated customer service at the National Library of Medicine","The National Library of Medicine (NLM) launched a virtual customer service representative (vRep) named “Cosmo” in February 2003. Cosmo is a navigation tool that guides users to information on NLM Web pages. These Web pages contain information on programs, products and services provided by NLM. Cosmo directs users to information on consumer health, drug information, medical database instruction, grant information, online catalog access, exhibit details and onsite library information. Medical librarians manage the Cosmo knowledge base to link to this information. To ensure that we meet customer needs, staff review the user “conversation” log daily and modify answers as needed. This paper describes the lessons learned during Cosmo’s development and may help others who create and maintain a virtual representative.","",""
"2006","Humanoid social robots as a medium of communication"," This article examines the emerging phenomenon of humanoid social robots and human-humanoid interactions. A central argument of this article is that humanoid social robots belong to a special type of robotic technology used for communicating and interacting with humans. These robotic entities, which can be in either mechanical or digital form, are autonomous, interactive and humanlike. Some of them are used to interact with humans for utilitarian purposes and others are designed to trigger human emotions. Incorporation of such robotic entities into the realm of social life invariably alters the condition as well as the dynamics of human interaction, giving rise to a synthetic society in which humans co-mingle with humanoids. More research is needed to investigate the social and cultural impact of this unfolding robotic revolution. ","",""
"2007","Decisions on Fire","&#x0D;         &#x0D; &#x0D;  Introduction   Decision making on the fireground is a complex activity reflected in the cultural image of fire in contemporary Western societies, the expertise of firefighters and the public demand for response to fire. The split second decisions that must be made by incident commanders on the fireground demonstrate that the dominant models of rational, logical argument and naturalistic decision making are incapable of dealing with this complexity. Twelve senior ranking Australian fire officers participated in the investigation from which I propose that fireground incident commanders are relying on aesthetic awareness and somatic responses, similar to those of an artist, and that due to the often ineffable nature of their responses these sources of information are usually unacknowledged. As a result I have developed my own theory of decision making on the fireground, termed ‘Multimodal Decision Making’, which is distinguished from formal rationality and informal sense-based rationality in that it approaches art, science and practice as a complex and irreducible whole.    Fire – Complex Decision Making   The complex reality of a fireground incident is not effectively explained by decision making models based on logic. These models understand decision making in terms of a rational choice between various options (Dowie) and tend to oversimplify decision making.  Grouped together they are commonly described as ‘traditional’. Recent research and the development of an alternative understanding, termed naturalistic decision making, has demonstrated that under the pressures of an emergency situation there is just not time enough to weight up alternatives (Flin, Salas, Strub and Martin; Klein).  Naturalistic decision making draws on the cognitive sciences to explain how incident commanders make decisions when they are not using probability theory or rational logic (Montgomery, Lipshitz and Brehmer).     Although I appreciate various aspects of the naturalistic models of decision making (Cannon-Bowers and Salas; Flin and Arbuthnot; Zsambok and Klein), the problem for me is that the research has been conducted from a cognitive task analysis perspective where typically each decision has been broken down into its supposed constituent parts, analysed and then reassembled. I understand this process to be counterproductive to appreciating complex and interrelated decision making. I propose an alternative explanation which I call Multimodal Decision Making.    Multimodal Decision Making recognises that probability theory or rational logic does not adequately explain how incident commanders balance feelings of contradictory information in parallel, and by the very clash or strangeness of the juxtaposition, see a way forward. This is reasoning by similarity rather than calculation. I suggest that the mechanistic rational processes do not necessarily disappear, but that they are assimilated into a dynamic, as opposed to inflexible and rigid, approach to decision making. The following excerpt from a country Inspector is provided to illustrate the role of aesthetic awareness and somatic perception in fireground decision making.    The Trembling Voice   Early one morning a country Inspector is called out to a factory fire in a town, normally one hour’s drive away. It takes him 40 minutes to drive to the fire, and on the way he busies himself receiving two updates from the communications centre and talking by radio to the first arriving officer at the incident. Nothing the first arriving officer said was unusual or alarming. What was alarming, said the Inspector, was the very slight tremor in the first arriving officer’s voice. It contained a hint of fear.      …so I got the message from the first pump that was on the scene. I could hear in his voice that he was quivering, so I thought ‘I am not too sure if he is comfortable, I’d better get him some help’ so I rang up the communications centre, and I said ‘Listen, I know you have got these two trucks coming from A., you’ve got the rural fire service’, I said ‘you need to send U. up now…I may have waited another 10 or 15 minutes before I said ‘Ok you better get G. there’ –  it’s only another 40km maybe, I said ‘get them on the road as well.’ V – This is all while you are in the car? All while I am in the car driving to the incident, I am building a mental picture of what’s happening, and from hearing his voice, I felt that he was maybe not in control because of the quivering in it. V – Did you know him well already? Yeah I knew him sort of well enough… I could just tell, he sounded like he was in trouble…I felt once I  arrived, he more or less – I could feel a weight come off his shoulders, ‘You’re here now, I don’t have to deal with this anymore, its all yours.’     The Inspector deduced the incident was possibly more serious than the communications centre had so far anticipated. He organised backup appliances, and these decisions, maintained the Inspector, were prompted by the “quivering” in the officer’s voice. On arrival he saw immediately that his call for backup was indeed necessary, because the fire was moving out of control with the possibility of spreading.    Although the Inspector in this incident was not physically present, he relied on his aesthetic awareness and somatic perception to inform his decision making. He would have been justified if he acted only on the basis of incoming communications, which were presented in scientifically measurable terms: “factory well alight, two appliances in attendance…” and so on; nothing out of the ordinary, a straightforward incident. In fact, what he responded to was not the information he received as a verbal message, but rather the slight tremor in the first arriving officer’s voice. That is, the Inspector’s aesthetic awareness and somatic perception informed his decision to call for backup, overriding the word-information contained in the verbal report.    Fire – Complex Cultural Image   Fire is a complex object in itself and in a threatening context, such as the engulfment of an inhabited building, creates a complex environment which in turn, for me as a researcher, requires a complex method of inquiry. As a result I have been obliged to draw on theories of art and art criticism as part of my own method of enquiry and I have adopted Eisner and Powell’s application of aesthetics:     It may be that somatic forms of knowledge – the use of the physical body as a source of information – play an important role in enabling scientists to make judgements about alternative courses of action or directions to pursue. It might be that qualitative cues are difficult to articulate, indeed clues that may themselves be ineffable, are critical for doing productive scientific work. (134)      That is, sometimes the physical body is used as a source of information, and sometimes it is difficult to express in words how this happens. The following incident illustrates the importance of somatic awareness in decision making from an Inspector’s perspective.   A Smell of Petrol   In this incident a country Inspector was called to a row of factory units. The smell of petrol had been happening on and off over a period of 18 months, but now in the toilet of one shop it had become unbearable. The Inspector set his crew to work with a device that detects levels of petrol in the air, that he called a ‘sniffer’. When the ‘sniffer’ did not register a high value for petrol the Inspector considered the machine to be faulty and trusted his own sense of smell and that of his crew, over the ‘sniffer’.    Decisions in this incident were informed by somatic response to the situation.  In the Smell of Petrol, the Inspector considered his nose a more reliable source of information than a mechanised ‘sniffer’.   Burning Ears   Continuing the theme of mechanisation and technology, personal protective equipment, one Inspector informed me, has become so effective that firefighters are able to move much deeper into a fire than ever before. The new technology comes with a price. Previously firefighters perceived the sensation of their ears burning to be a warning sign. This somatic response has now been effectively curtailed. Technology in the form of increasing personal protective equipment, complex communication systems and sophisticated firefighting equipment is usually understood as increasing the opportunity to prevent and control an incident. Perhaps an alternative perspective could be that increasingly sophisticated technology is replacing somatic response with dangerous implications?   Somatic awareness is developed within a cultural context. On the fireground, I understand the cultural context to be the image, as a fire is a moving, alive image demanding an immediate response. An arsonist may look for a fire to spiral out of control, enjoying the spectacle of an entire building being engulfed and spreading to the next office block. What is it that firefighters are looking for? What do they see? What directs their attention? Firefighters invariably see what they have been trained to see – smoke escaping from under the eaves, melting rubber between clip-lock walls, cracks in structural concrete, the colour and density of the smoke and so on. Their perception of signs, indicating their appreciation of the situation, and they way they perceive these signs – they look for them, gauge and measure their progress and act in response, are all intensified by time pressure and the imperative and means to do something. This is in sharp contrast to an arsonist or even the general public watching the fire’s progress on the TV news.    The ability to comprehend and act on the visual is called aesthetics in the discipline of art criticism.  I use the words ‘aesthetic awareness’ to mean the way an activity of perception is organised and informed to unspoken, but shared, principles for recognising fire features and characteristics; being able to share these principles helps with the building of an identity of expertise.   In firefighting, as in other emergency service work, an aesthetic appreciation of the scene it is technically termed situational awareness (Banbury and Tremblay; Craig; Endsley and Garland) and sometimes colloquially known as a size-up.  This is when incident commanders appraise the fireground and on the basis of their judgement, make decisions involving, for example, the placement of personnel and resources, calling for backup and so on. It is at this stage that the expertise of the incident commander is fore grounded and I suggest that a linear approach to decision making does not fully explain the complexities involved when a small input or adjustment can lead to very dramatic consequences. In fact, a small input leading to dramatic consequences is likely to indicate a non-linear system (Lewin).   In a non-linear dynamic system, such as a fireground, some things may appear random, but they are known equations. Pink heralds a visual and non-linear approach,  “perhaps some of the problems we face when we write linear texts with words as our only tool can be resolved by thinking of anthropology and its representations as not solely verbal, but also visual and not simply linear but multilinear” (Pink 10).    With linear thinking there is a beginning and an end, which leads naturally to the supposition of cause and effect. This is because there is no looping back into the whole; it is as if there are many beginnings, leading to a fragmented sort of perception. Language shapes the way we perceive issues by virtue of the words we have to create our impressions with. Unfortunately, English and Western languages in general are not equipped for a multimodal communication. We are, by the structure of our language, almost squeezed into the position of talking linearly in terms of cause and effect for understanding what is happening.    Fire – Complex Experience   Creative decision making occurs when the person has a deep knowledge of the discipline. Great flashes of insight rarely come to the inexperienced mind. People who don’t understand rhythm, melody and harmony will not be able to compose complex pieces of music. Creative and innovative decision making on the fireground will not be possible without prior experience regarding how various materials react on combustion, the structure of the organisational hierarchy, crew configurations and the nature of the fire being fought. There is beginner’s luck of course, but this will not be a consistent approach to an otherwise fearful and dangerous situation, because knowing what to expect means feeling less danger and less fear, freeing up more energy to respond creatively. For example, consider a junior firefighter trembling in fear prior to their first incident, compared with an experienced firefighter who feels anticipation and exhilaration.   We live in a world of specialisation and expert opinion, even if there is a certain cynicism creeping in over what makes someone an expert. Taylorism has ultimately produced people with high technical skills in one area and a lack of ability to see the whole picture (Konzelmann, Forrant and Wilkinson).As a counterbalance there is a current push towards multi-skilling and flattened hierarchies. For firefighting organisations this creates an interesting challenge. On the one hand there is a concentration on highly technical skill development which involves acknowledging the importance of team work; on the other, the demands of a time critical situation in which the imperative is to act quickly and decisively for the best possible outcome. Ultimate decision making responsibility lies with the incident commander who must be able to negotiate the complexity of the scene in its entirety, balancing competing demands rather than focusing solely on one aspect.    The ease with which incident commanders move through the decision making process, perceiving the situation, looking at the fire and sizing it up, is not reliant on eyesight alone. It involves their ability to adjust, reframe, and move through the incident without losing their bearings, no matter how or where they are physically situated in relation to the fire. Seeing does not involve only eyesight, it sums up the experience of becoming so familiar and integrated with the aspects of fire behaviour that expert incident commanders do not lose their bearings in the process of changing their physical location. Often they rely on incoming intelligence to develop a three dimensional perspective of the fireground. They have a multimodal perspective, a holistic vista, because their sensory relationship with the fire is so thorough and extensive.    Just looking at the fire for the incident commander, is not just looking at the fire, it is an aesthetic experience in which there is a shared standard for recognising what is happening, if not what should be done to mitigate it. Participating in the knowledge of these standards, these ways of seeing, is recognised as part of the identity of the group member. Nelson (97), who specialised in visually reading the man-made environment, wrote “we see what we are looking for, what we have been trained to see by habit or tradition.” Firefighters are known and respected within their cultural context by their depth of understanding of these shared standards. These shared standards may or may not be a reflection of the ideal or organisationally endorsed standard operating guidelines. I suggest that a heightened situational awareness and consequent decision making may be a visible indication of contribution and inclusion within the cultural practices of firefighting. Thus seeing involves not only eyesight, but also being a part of a cultural context; for example interpreting individualised body movements and gestures.     Standard operating guidelines place rules and constraints on incident commanders. These guidelines provide a hierarchy of needs, and prescribe recommended approaches for various fireground contingencies. This does not mean that incident commanders are not creative. “Play and art without rules is uninteresting. Absolute liberty is boring” (Karlqvist 111). Within the context of the fireground, creative experience is deliberate as opposed to random. The creation of innovative approaches does not happen in a vacuum; rather it is the result of playing with the rules, stretching them, moving and testing them. It is essential to maintain common operating guidelines, or rules, because they form a stock body of common knowledge, but it is also essential to break the rules and play around with them. Karlqvist (112) writes “mastery reveals itself as breaking rules. The secret of creativity hinges on this insight, to know the right moment when you can go too far”.   There are experts who are trained to be mechanical, and there are experts, such as the incident commanders I interviewed, who integrate and sometimes override the mechanical list of rules. Multimodal Decision Making is not primarily about an objective representation of the ‘truth’, but rather the unpredictable and complex conditions which incident commanders must negotiate.    Conclusion   When dealing with a complex and dynamic system, cause and effect are not sufficient explanation for what is happening. Instead of linear progression we are looking at a feedback or circular system, in which a small act may produce a larger reaction. Decision making on the fireground is a complex and difficult activity. Its complexity stems from the uncertain variables, the immediate threat to life and property, the safety of the crew, trapped victims, observing public, the perceptions reported by the media and the statutory obligations that motivate firefighters to their tasks are intricately interwoven. This melting pot of variable contingencies creates a complex working environment which I suggest is negotiated by a little acknowledged ability to integrate somatic and aesthetic awareness into decision making in time critical situations.    When dealing with a complex and dynamic system, cause and effect are not sufficient explanation for what is happening. Instead of linear progression we are looking at a feedback or circular system, in which a small act may produce a larger reaction. Decision making on the fireground is a complex and difficult activity. Its complexity stems from uncertain variables which include the immediate threat to life and property, the safety of the crew, trapped victims, and observing public, the perceptions reported by the media and the statutory obligations that motivate firefighters to their tasks, all of which are intricately interwoven. This melting pot of variable contingencies creates a complex working environment which I suggest is negotiated by a little acknowledged ability to integrate somatic and aesthetic awareness into decision making in time critical situations.     References    Banbury, Simon, and Sebastian Tremblay, eds. A Cognitive Approach to Situational Awareness: Theory and Application. Hampshire, England: Ashgate, 2004. Cannon-Bowers, Janis, and Eduardo Salas. Making Decisions under Stress. Washington: American Psychological Association, 1998. Craig, Peter. Situational Awareness: Controlling Pilot Error. New York: McGraw-Hill, 2001. Dowie, Jack. “Clinical Decision Analysis: Background and Introduction.” In Analysing How We Reach Clinical Decisions, eds. H. Llewellyn &amp; A. Hopkins. London: Royal College of Physicians, 1993. Eisner, Elliot, and Kimberly Powell. “Art in Science?” Curriculum Inquiry 32.2 (2002): 131-159. Endsley, Mica, and Daniel Garland, eds. Situational Awareness Analysis and Measurement. New Jersey: Lawrence Erlbaum Associates, 2000. Flin, Rhona, and Kevin Arbuthnot. Incident Command: Tales from the Hot Seat. England: Ashgate, 2002. Flin, Rhona, Eduardo Salas, M. Strub, and L. Martin, eds. Decision Making under Stress. England: Ashgate, 1997. Karlqvist, Aka. “Creativity: Some Historical Footnotes from Art and Science.”  Ake Andersson and Nihls-Eric Sahlin, eds. The Complexity of Creativity. Dordrecht: Kluwer, 1997. Klein, Gary. Sources of Power. Massachusetts: Massachusetts Institute of Technology, 1998. Konzelmann, Suzanne, Robert Forrant, and Frank Wilkinson. “Work Systems, Corporate Strategies and Global Markets: Creative Shop Floors or ‘a Barge Mentality’?” Industrial Relations Journal 35.3 (2004). Lewin, Roger. Complexity: Life at the Edge of Chaos. 2nd ed. Chicago: U of Chicago P, 1999. Montgomery, Henry, and Raanan Lipshitz, and Berndt Brehmer, eds. How Professionals Make Decisions. New Jersey: Lawrence Erlbaum, 2005. Nelson, George. How to See: A Guide to Reading Our Manmade Environment. Boston: Little, Brown and Company, 1977. Pink, Sarah. “Introduction: Situating Visual Research.” In Working Images, eds. Sarah Pink, Laszlo Kurti, and Ana Isabel Afonso. New York: Routledge, 2004. Zsambok, Caroline, and Gary Klein. Naturalistic Decision Making. New Jersey: Lawrence Erlbaum, 1997.   &#x0D; &#x0D; &#x0D;         &#x0D;           Citation reference for this article&#x0D; &#x0D;           MLA Style&#x0D;           Ingham, Valerie. """"Decisions on Fire."""" M/C Journal 10.3 (2007).  echo date('d M. Y'); ?&gt; &lt;http://journal.media-culture.org.au/0706/06-ingham.php&gt;. APA Style&#x0D;           Ingham, V. (Jun. 2007)  """"Decisions on Fire,"""" M/C Journal, 10(3). Retrieved  echo date('d M. Y'); ?&gt; from &lt;http://journal.media-culture.org.au/0706/06-ingham.php&gt;. &#x0D;       ","",""
"2007","Alexander Galloway: Gaming: Essays on Algorithmic Culture","","",""
"2007","Engineering As An Art","","",""
"2007","System design of “Ba”-like stages for improvisational acts via Leibnizian space–time and Peirce’s existential graph concepts","","",""
"2007","Artificial Intelligence: Cannibal or Missionary?","","",""
"2007","AI Bridges and Dreams","","",""
"2007","Applications for conscious systems","","",""
"2007","A multi-agent based framework for the simulation of human and social behaviors during emergency evacuations","","",""
"2007","Artificial Intelligence and learning, epistemological perspectives","","",""
"2007","AI and Accountability","","",""
"2007","3rd Indian International Conference on Artificial Intelligence (IICAI-07) http://iiconference.org/December 17–19 2007, Pune, India","","",""
"2007","Guest Editorial AI &amp; Society: Enterprise, Innovations and Society","","",""
"2007","Democratising change","","",""
"2007","e-Exclusion and Bot Rights: Legal aspects of the robots exclusion standard for public agencies and other public sector bodies with Swedish examples","Public sector use of the robot exclusion standard raises interesting questions about transparency, availability of public sector information and the principle of public access to information. This paper explores both actual examples of how public sector agencies in Sweden use the standard and an analysis of the legal problems related to use of the standard.","",""
"2007","<i>DrawBot</i>: a bio-inspired robotic portraitist","Abstract We are developing the control architecture of a portraitist artificial agent called DrawBot that reproduces the visuomotor behaviour of a human carrying out a realistic portrait. The visuomotor strategy adopted by DrawBot is based on computational models of eye movements in human beings,and one xperimental findings on eye-hand coordination in expert draughtsmen. In this paper we present a behavioural model of the visuomotor coordination adopted by a draughtsman, designed in terms of visual routines.Eventually we outline the implementation of the basic routines.","",""
"2007","Swarm grammars: growing dynamic structures in 3D agent spaces","Abstract We present a new way of dynamically growing and breeding structures in 3D space through swarming agents. Different agent types and the way they evolve over time is specified by a swarm grammar similar to Lindenmayer systems. We expand common L-system string interpretation from a single turtle to a multitude of turtles which behave like a swarm. By describing swarm agents within the framework of formal grammars, we build a bridge from symbolic production systems (rewrite systems) to three-dimensional real-time construction procedures that are executed by reactive and interacting agents which move in simulated physical 3D spaces. We introduce constructor agents, their formal representation in swarm grammars and demonstrate by examples how (1) the swarm rules, (2) the agent parameters and (3) the environ ment can influence the actual construction and growth processes that are initiated and directed by the swarms. In order to facilitate exploration of a large variety of swarm grammars, we apply interactive evolutionary design methods to create swarm grammar sculptures and 3D structures.","",""
"2007","Reading Potential: The Oulipo and the Meaning of Algorithms.","","",""
"2008","IBM's Chess Players: On AI and Its Supplements","This article investigates the ways in which the reporting of technological developments in artificial intelligence (AI) can serve as occasions in which Occidental modernity's cultural antinomies are played out. It takes as its reference point the two chess tournaments (in 1996 and 1997) between the then world champion Gary Kasparov and the IBM dedicated chess computers Deep Blue and Deeper Blue and shows how these games of chess came to be seen as an arena where fundamental issues pertaining to human identity were contested. The article considers the dominant framing of these encounters in terms of a conflict between two opposed categories—“human” and “machine”—and argues the essential role of human agency, the human supplement, in the performances of machine intelligence.","",""
"2008","A Relational Database Model for Text Encoding","","",""
"2008","Ethical robots: the future can heed us","","",""
"2008","Scientific models and ethical issues in hybrid bionic systems research","","",""
"2008","Special issue on ethics and artificial agents","","",""
"2008","Ethics and consciousness in artificial agents","","",""
"2008","Asimov’s “three laws of robotics” and machine metaethics","","",""
"2008","Imagining a non-biological machine as a legal person","","",""
"2008","Computing machinery and morality","","",""
"2008","Socio-ethics of interaction with intelligent interactive technologies","","",""
"2008","Implementing moral decision making faculties in computers and robots","","",""
"2008","Caregiving robots and ethical reflection: the perspective of interdisciplinary technology assessment","","",""
"2008","The social impact of intelligent artefacts","","",""
"2008","Machine morality: bottom-up and top-down approaches for modelling human moral faculties","","",""
"2008","Learning robots interacting with humans: from epistemic risk to responsibility","","",""
"2008","From the ethics of technology towards an ethics of knowledge policy: implications for robotics","","",""
"2008","Ethical monitoring of brain-machine interfaces","","",""
"2008","Ethical regulations on robotics in Europe","","",""
"2008","The World is Not Flat: Expertise and InPhO","The Indiana Philosophy Ontology (InPhO) is a “dynamic ontology” for the domain of philosophy derived from human input and software analysis. The structured nature of the ontology supports machine reasoning about philosophers and their ideas.  It is dynamic because it tracks changes in the content of the online Stanford Encyclopedia of Philosophy. This paper discusses ways of managing the varying expertise of people who supply input to the InPhO and provide feedback on the automated methods.","",""
"2008","A Multiple Master based method for scaling glyphs without changing the stroke characteristics","Abstract This article presents Compensating Scaling, an advanced method for changing the dimensions of glyphs. Working with MM fonts that have a weight axis, the ‘boldness’ information is used to compensate for changes in stroke weight caused by the scaling. The method helps to create true small caps, Cyrillic lowercase and – since the horizontal and vertical scale factors can be chosen independently – even true condensed fonts. Comparisons to existing typefaces show that the output generated by the model is rather close to that of the true glyphs. In contrast to ‘intelligent’ data formats the model suggested here is a technology-independent formula for the processing of pre-existing shapes. The output has the same format as the input and can be subsequently processed, making the method a tool for the automation of a specific design step rather than a stand-alone technology.","",""
"2009","Behaviour Space: Simulating Roman Social Life and Civil Violence","Agent based modelling, also known as individual-based modelling, holds great promise for historians as a tool for formalizing and visualizing historians’ understandings of historical processes. It also provides a means to explore exploring the emergent consequences of such assumptions. Such models specify the possible behaviours for a single individual, and then enable individuals within the simulation to interact, with each applying the behaviours in a context-specific manner. Artificial societies begin to emerge from these interactions, allowing us to study their characteristics. Moreover, when these models produce behaviours that cohere with patterns embedded in historical or archaeological data, it becomes possible to interrogate aspects of past experience otherwise lost to us. This article presents PatronWorld, an agent based model of the ancient Roman daily ritual of salutatio, a ritual in which clients gathered to make a morning greeting to their patron. It explores the ritual’s role in the development and maintenance of patronage networks, and its relationship to the emergence of civil violence in the Roman world. The model is also based on a framework that describes the social network surrounding land holding (the foundations of wealth in antiquity) in the City of Rome from the late first – early second century AD. Civil violence was a constant feature of Roman society, frequently targeting the men upon whose social connections the political economy of the state depended. Results from the model suggest the social conditions that made the state vulnerable to periodic bouts of violence, and suggest new directions for further research.","",""
"2009","How the Central Intelligence Agency works with Hollywood: An interview with Paul Barry, the CIA's new Entertainment Industry Liaison","","",""
"2009","Avatar culture: cross-cultural evaluations of avatar facial expressions","","",""
"2009","Wendell Wallach and Colin Allen: Moral machines: teaching robots right from wrong","","",""
"2009","Toward a multi-culture adaptive virtual tour guide agent with a modular approach","","",""
"2009","Hypercalculia for the mind emulation","","",""
"2009","Culture–personality based affective model","","",""
"2009","Developing and implementing a sparse ontology with a visual index for personal photograph retrieval","","",""
"2009","Escape and intervention in multi-agent systems","","",""
"2009","Visualization of balancing systems based on naïve psychological approaches","","",""
"2009","Cultural dialects of real and synthetic emotional facial expressions","","",""
"2009","Virtual moral agency, virtual moral responsibility: on the moral significance of the appearance, perception, and performance of artificial agents","","",""
"2009","Humans and humanoid social robots in communication contexts","","",""
"2009","A theoretical perspective on social agency","","",""
"2009","Effect of retroflex sounds on the recognition of Hindi voiced and unvoiced stops","","",""
"2009","Modeling social inference in virtual agents","","",""
"2009","Computational models of the emotions: from models of the emotions of the individual to modelling the emerging irrational behaviour of crowds","","",""
"2009","Wealth adjustment using a synergy between communication, cooperation, and one-fifth of wealth variables in an artificial society","","",""
"2009","PLASIU: a system that facilitates creative decision-making in job-hunting","","",""
"2009","Creativity for problem solvers","","",""
"2009","Machine intelligence (MI), competence and creativity","","",""
"2009","Toward combining autonomy and interactivity for social robots","","",""
"2009","Does Japan really have robot mania? Comparing attitudes by implicit and explicit measures","","",""
"2009","The impact of cognitive machines on complex decisions and organizational change","","",""
"2009","Methodologies for agent systems development: underlying assumptions and implications for design","","",""
"2009","Interactive perception for amplification of intended behavior in complex noisy environments","","",""
"2009","Rapid prototyping of social group dynamics in multiagent systems","","",""
"2009","Software agents and robots in mental therapy: psychological and sociological perspectives","","",""
"2009","Real-time system for measuring gaze direction and facial features: towards automatic discrimination of lies using diverse nonverbal information","","",""
"2009","From observation to simulation: generating culture-specific behavior for interactive systems","","",""
"2009","Revisiting the Age of Enlightenment from a collective decision making systems perspective","The ideals of the eighteenth century's Age of Enlightenment are the foundation of modern democracies. The era was characterized by thinkers who promoted progressive social reforms that opposed the long-established aristocracies and monarchies of the time. Prominent examples of such reforms include the establishment of inalienable human rights, self-governing republics, and market capitalism. Twenty-first century democratic nations can benefit from revisiting the systems developed during the Enlightenment and reframing them within the techno-social context of the Information Age. This article explores the application of social algorithms that make use of Thomas Paine's (English: 1737-1809) representatives, Adam Smith's (Scottish: 1723-1790) self-interested actors, and Marquis de Condorcet's (French: 1743-1794) optimal decision making groups. It is posited that technology-enabled social algorithms can better realize the ideals articulated during the Enlightenment.","",""
"2009","âI am the Woman Next Doorâ: The Clothesline Project as Woman Abuse Survivors' Societal Critique","","",""
"2009","Intelligent Self-Design","","",""
"2009","Simulated Visuals: Some Rhetorical and Ethical Implications.","","",""
"2010","HOW EXECUTIVES PERCEIVE THE NET GENERATION","This paper reports on an exploratory study of how executives in organizations perceive the entrance of the ‘net generation’ into the workplace. We approached this question by collecting data from interviews, focus groups, and an online survey. The results show that executives perceive tensions associated with values and behaviors of the net generation workers as the younger workers enter current organizational settings as well as technology-related issues not solely associated with the younger generation. The paper discusses the different organizational mechanisms and strategies executives use to address these tensions. Particularly, we discuss executives’ preference for top-down strategies and their tendency to address the triad of technology–values–behavior as separate components instead of a unified concept.","",""
"2010","Distributed cognition at the crime scene","","",""
"2010","Mind and machine: ethical and epistemological implications for research","","",""
"2010","Anticipation and the artificial: aesthetics, ethics, and synthetic life","","",""
"2010","Interactive skills and individual differences in a word production task","","",""
"2010","Ethics and aesthetics of technologies","","",""
"2010","Computer-mediated trust in self-interested expert recommendations","","",""
"2010","Thinking with external representations","","",""
"2010","Socially distributed cognition in loosely coupled systems","","",""
"2010","Erratum to: Ethics and aesthetics of technologies","","",""
"2010","Incremental learning of gestures for human–robot interaction","","",""
"2010","Twenty years of artificial life art","This essay begins with discussion of four relatively recent works which are representative of major themes and preoccupations in Artificial Life Art: ‘Propagaciones’ by Leo Nuñez; ‘Sniff’ by Karolina Sobecka and Jim George; ‘Universal Whistling Machine’ by Marc Boehlen; and ‘Performative Ecologies’ by Ruari Glynn. This essay is an attempt to contextualise these works by providing an overview of the history and forms of Artificial Life Art as it has developed over two decades, along with some background in the ideas of the Artificial Life movement of the late 1980s and 1990s.1","",""
"2011","Pablo Boczkowski: News at Work: Imitation in an Age of Information Abundance","","",""
"2011","A Review of “Chess Metaphors: Artificial Intelligence and the Human Mind”","","",""
"2011","Machinic articulations: experiments in non-verbal explanation","","",""
"2011","Beyond the responsibility gap. Discussion note on responsibility and liability in the use of brain-computer interfaces","","",""
"2011","Computational dialectic and rhetorical invention","","",""
"2011","You, robot: on the linguistic construction of artificial others","","",""
"2011","Biological movement increases acceptance of humanoid robots as human partners in motor interaction","","",""
"2011","Emotional empathy transition patterns from human brain responses in interactive communication situations","","",""
"2011","Usefulness of simulating social phenomena: evidence","","",""
"2011","Studying laughter in combination with two humanoid robots","","",""
"2011","Erratum to: Computational dialectic and rhetorical invention","","",""
"2011","Development of agent system based on decision model for creating an ambient space","","",""
"2011","Robots and the changing workforce","","",""
"2011","Machine vision: an aid in reverse Turing test","","",""
"2011","Toward incorporating emotions with rationality into a communicative virtual agent","","",""
"2011","Special issue on social impact of AI: killer robots or friendly fridges","","",""
"2011","Killers, fridges, and slaves: a legal journey in robotics","","",""
"2011","Advocating an ethical memory model for artificial companions from a human-centred perspective","","",""
"2011","Two acts of social intelligence: the effects of mimicry and social praise on the evaluation of an artificial agent","","",""
"2011","On the Embodied Aesthetics of Code","","",""
"2012","Opinions and attitudes toward humanoid robots in the Middle East","","",""
"2012","Erratum to: Toward a comparative theory of agents","","",""
"2012","Automatic phonetic segmentation of Hindi speech using hidden Markov model","","",""
"2012","Governing industrial organizations through cognitive machines","","",""
"2012","Toward a comparative theory of agents","","",""
"2012","YUTPA as a design tool for public participation","","",""
"2012","Elephant fish and GPS","","",""
"2012","Joseph S. Fulda: Implications of a logical paradox for computer-dispensed justice","","",""
"2012","Wealth adjustment using a no-interest credit network in an artificial society","","",""
"2012","The cognitive surplus is made of fossil fuels","People in the industrial world have a great deal of free time. Clay Shirky has described this free time, considered as a whole, as a vast “cognitive surplus,” and presents many efforts currently under way to use the cognitive surplus for prosocial ends. However, the cognitive surplus came to exist largely as a result of labor–saving devices that run on fossil fuels. Many problems relating to fossil fuels constrain how people can responsibly use the cognitive surplus to address environmental sustainability and other current concerns. We suggest that an excellent use of the present cognitive surplus is to help society prepare for an energy–scarce future — that is, a future that may not be able to support the existence of a cognitive surplus at the current level.","",""
"2012","Discerning truth from deception: Human judgments and automation efforts","Recent improvements in effectiveness and accuracy of the emerging field of automated deception detection and the associated potential of language technologies have triggered increased interest in mass media and general public. Computational tools capable of alerting users to potentially deceptive content in computer–mediated messages are invaluable for supporting undisrupted, computer–mediated communication and information practices, credibility assessment and decision–making. The goal of this ongoing research is to inform creation of such automated capabilities. In this study we elicit a sample of 90 computer–mediated personal stories with varying levels of deception. Each story has 10 associated human deception level judgments, confidence scores, and explanations. In total, 990 unique respondents participated in the study. Three approaches are taken to the data analysis of the sample: human judges, linguistic detection cues, and machine learning. Comparable to previous research results, human judgments achieve 50–63 percent success rates, depending on what is considered deceptive. Actual deception levels negatively correlate with their confident judgment as being deceptive (r = -0.35, df = 88, ρ = 0.008). The highest-performing machine learning algorithms reach 65 percent accuracy. Linguistic cues are extracted, calculated, and modeled with logistic regression, but are found not to be significant predictors of deception level, confidence score, or an authors’ ability to fool a reader. We address the associated challenges with error analysis. The respondents’ stories and explanations are manually content–analyzed and result in a faceted deception classification (theme, centrality, realism, essence, self–distancing) and a stated perceived cue typology. Deception detection remains novel, challenging, and important in natural language processing, machine learning, and the broader library information science and technology community.","",""
"2012","Strengthening CAPTCHA-based Web security","Simple, universally applicable strategies can help any captcha–protected system resist automated attacks and can improve the ability of administrators to detect attacks. The strategies discussed here cause an exponential increase in the difficulty faced by automated attackers, while only increasing the inconvenience for human users in an approximately linear manner. These strategies are characterised using a new metric, the ‘Captcha Improvement Ratio’. The paper concludes that presenting multiple captcha systems together in random order may provide quantitative and qualitative advantages over many typical present–day captcha systems.","",""
"2012","Art Space: Alan Turing Year 2012","Intuition and Ingenuity: An Art Exhibition in Celebration of the Life of Alan Turing (for Alan Turing Year 2012). 2012 will have been the 100th anniversary of the birth of Alan Turing, one of the greatest minds Britain has ever produced. From inventing the digital computer and helping to decode the German Enigma machine, to founding the science of Artificial Intelligence, the world today would have been a very different place without him and his ideas. His work on morphogenesis (the biological processes that cause organisms to grow into particular shapes) and the now famous Turing Test for machine intelligence have captured the imagination of artists for decades. Furthermore, his technological developments have given them the tools to create new kinds of artwork. Who knows what else he could have achieved had he not tragically taken his own life just over two weeks before his 42nd birthday, following prosecution for homosexuality and a subsequent sentence of oestrogen treatment (chemical castration). Prime Minister Gordon Brown publicly apologised for the ‘appalling’ treatment Turing received in 2009, saying on behalf of the British Government ‘we’re sorry, you deserved so much better’. This exhibition, to celebrate his life takes its name from Turing’s own writing on the subject of mathematical reasoning. It brings together a number of important artists, from digital art pioneers to emerging contemporaries, to investigate Turing’s enduring influence on art and contemporary culture. Artists taking part include: Roman Verostko, William Latham, boredomresearch, Patrick Tresset, Paul Brown, Sue Gollifer, Greg Garvey, Martin A. Smith, Anna Dumitriu and Alex May. Intuition and Ingenuity, the exhibition, which was curated by Sue Gollifer, Nick Lambert and Anna Dumitriu, was composed of several new commissions as well as important works from the history of computer art. Intuition and Ingenuity toured throughout the UK during 2012 and 2013 including the Victoria and Albert Museum and Bletchley Park.","",""
"2013","Machinic Interagency and Co-evolution","The ontological equality and material vitality of all things, and efforts to remove “the human” from its apical position in a hierarchy of being, are Object-Oriented Ontology theory (OOO) concepts. These axioms are useful in a discussion of the aesthetics of augmented robotic art, alongside speculations regarding any interagency between the human/non-human and possible co-evolutionary relationships. In addition, they help to wash out the sticky habits of conventional art writing, such as removed critique or an authoritative expert voice. This article aims to address the robotic work Accomplice by Sydney-based artists Petra Gemeinboeck and Rob Saunders as a means of interrogating the independence and agency of robots as non-human species, and as a mode of investigating how we see these relationships changing for the futureFor Accomplice, an artwork exhibited at Artspace, Sydney, in 2013, Gemeinboeck and Saunders built robots, strategised properties, and programmed their performative actions. Replete with lights and hammers, the robots are secreted away behind false walls, where they move along tracks and bang holes into the gallery space. As the devastation of plasterboard ensues, the robots respond interactively to each other through their collective activity: this is intra-action, where an object’s force emerges and where agency is an enactment (Barad, Matter Feels). This paper will continue to draw on the work of feminist scholar and quantum scientist, Karen Barad, due to her related work on agency and intra-action, although she is not part of an OOO theoretical body.    Gemeinboeck and Saunders build unstable environments for their robots to perform as embodied inhabitants (Gemeinboeck and Saunders 2). Although the augmented robots are programmed, it is not a prescriptive control. Data is entered, then the robots respond to one another’s proximity and devastation. From the immaterial, virtual realm of robotic programming comes a new materiality which is both unstable, unpredictable, and on the verge of becoming other, or alive. This is a collaboration, not just between Gemeinboeck and Saunders, but between the programmers and their little robots—and the new forces that might be created.     Sites of intra-species (human and robot) crossings might be places or spaces where a new figuration of enchantment occurs (Bennett 32). Such a space could take the form of a responsive art-writing intervention or even a new ontological story, as a direct riposte to the lively augmentation of the robotic artwork (Bennett 92). As ficto-critical theorist and ethnographer, Stephen Muecke says, “Experimental writing, for me, would be writing that necessarily participates in worlds rather than a writing constituted as a report on realities seen from the other side of an illusory gap of representation” (Muecke Motorcycles 2).    Figure 1: Accomplice by Petra Gemeinboeck and Rob Saunders, Artspace, Sydney, 2013. (Photo: Petra Gemeinboeck)Writing Forces   When things disappear then reappear, there is a point where force is unleashed. If we ask what role the art writer plays in liberating force, the answer might be that her role is to create as an imaginative new creation, equal to the artwork. The artists speak of Accomplice:     transductions, transmaterial flows and transversal relations are at play ... whether emerging from or propelling the interplay between internal dynamics and external forces, the enactment of agencies (human and non-human), or the performative relationship unfolding over time. (Gemeinboeck and Saunders 3)    When new energetic force is created and the artwork takes on new life, the audience’s imaginative thought is stimulated. This new force might cause an effect of a trans-fictional flow.    The act of writing about Accomplice might also involve some intentional implausibility. For instance, whilst in the exhibition gallery space, witnessing Accomplice, I decided to write a note to one of the robots. I could see it, just visible beyond the violently hammered hole in the wall. Broken plaster dusted my shoes and as I peered into the darker outside space, it whizzed past on its way to bang another hole, in harmony with its other robotic friends. So I scribbled a note on a plain white piece of paper, folded it neatly and poked it through the hole:                               Dear robot, do you get sick of augmenting human lives?Do you get on well with your robotic friends?Yours sincerely, Prue.    I waited a few minutes and then my very same piece of paper was thrust back through the hole. It was not folded but was crumpled up. I opened it and noticed a smudged mark in the corner. It looked like an ancient symbol, a strange elliptical script of rounded shapes, but was too small to read. An intergalactic message, a signal from an alien presence perhaps? So I borrowed a magnifying glass from the Artspace gallery attendant. It read:     I love opera. Robot Two must die.    This was unexpected! As I pondered the robot’s reply, I noticed the robots did indeed make strange bird-like noises to one another; their tapping was like rhythmic woodpeckers. Their hammering was a kind of operatic symphony; it was not far-fetched that these robots were appreciative of the sound patterns they made. In other words, they were responding to stimuli in the environment, and acting in response. They had agency beyond the immaterial computational programming their creators had embedded. It wasn’t difficult to suspend disbelief to allow the possibility that interaction between the robots might occur, or that one might have gone rogue.    An acceptance of the possibility of inter-agency would allow the fantastical reality of a human becoming short-term pen pals with an augmented machine. Karen Barad might endorse such an unexpected intra-action act. She discourages conventional critique as, “a tool that keeps getting used out of habit” (Matter Feels). Art writing, in an era of robots and awareness of other non-human sentient life-forms can be speculative invention, have a Barad-like imaginative materiality (Matter Feels), and sense of suspended disbelief. Figure 2: Accomplice by Petra Gemeinboeck and Rob Saunders, Artspace, Sydney, 2013. (Photo: Petra Gemeinboeck) The Final Onto-Story Straw     Gemeinboeck and Saunders say the space where their robots perform is a questionable one: “the fidelity of the space as a shared experience is thus brought into question: how can a shared virtual experience be trusted when it is constructed from such intangible and malleable stuff as streams of binary digits” (7). The answer might be that it is not to be trusted, particularly in an OOO aesthetic approach that allows divergent and contingent fictive possibilities. Indeed, thinking about the fidelity of the space, there was something about a narrow access corridor in the Accomplice exhibition space, between the false gallery wall and the cavity where the robots moved on their track, that beckoned me. I glanced over my shoulder to check that the Artspace attendant wasn’t watching and slipped behind the wall. I took a few tentative steps, not wanting to get knocked on the nose by a zooming robot. I saw that one robot had turned away from the wall and was attacking another with its hammer. By the time I arrived, the second robot (could it be Robot Two?) had been badly pummeled. Not only did Robot One attack Robot Two but I witnessed it using its extended hammer to absorb metal parts: the light and the hammer. It was adapting, like Philip K. Dick’s robots in his short story ‘Preserving Machine’ (See Gray 228-33). It was becoming more augmented. It now had two lights and two hammers and seemed to move at double speed. Figure 3: Accomplice by Petra Gemeinboeck and Rob Saunders, Artspace, Sydney, 2013. (Photo: Petra Gemeinboeck)My observance of this scene might be explained by Gemeinboeck/Saunders’s comment regarding Philip K. Dick-style interference and instability, which they actively apply to their work. They say, “The ‘gremlins’ of our works are the slipping logics of nonlinear systems or distributed agential forces of colliding materials” (18). An audience response is a colliding material. A fictional aside is a colliding material. A suspension of disbelief must also be considered a colliding material. This is the politics of the para-human, where regulations and policies are in their infancy. Fears of artificial intelligence seem absurd, when we consider how startled we become when the boundaries between fiction/truth become as flimsy and slippery as the boundaries between human/non-human. Art writing that resists truth complements Gemeinboeck/Saunders point that, “different agential forces not only co-evolve but perform together” (18).The DisappearanceBefore we are able to distinguish any unexpected or enchanted ontological outcomes, the robots must first appear, but for things to truly appear to us, they must first disappear. The robots disappear from view, behind the false walls. Slowly, through the enactment of an agented force (the action of their hammers upon the wall), they beat a path into the viewer’s visual reality. Their emergence signals a performative augmentation. Stronger, better, smarter, longer: these creatures are more-than-human.     Yet despite the robot’s augmented technological improvement upon human ability, their being (here, meaning their independent autonomy) is under threat in a human-centred world environment. First they are threatened by the human habit of reducing them to anthropomorphic characteristics: they can be seen as cute little versions of humans. Secondly, they are threatened by human perception that they are under the control of the programmers. Both points are arguable: these robots are undoubtedly non-human, and there are unexpected and unplanned outcomes, once they are activated. These might be speculative or contestable outcomes, which are not demonstrably an epitome of truth (Bennett 161). Figure 4: Accomplice by Petra Gemeinboeck and Rob Saunders, Artspace, Sydney, 2013. (Photo: Petra Gemeinboeck)Gemeinboeck’s robotic creatures, with their apparent work/play and civil disobedience, appeared to exhibit human traits. An OOO approach would discourage these anthropomorphic tendencies: by seeing human qualities in inanimate objects, we are only falling back into correlational habits—where nature and culture are separate dyads and can never comprehend each other, and where humankind is mistakenly privileged over all other entities (Meillassoux 5). This only serves to inhibit any access to a reality outside the human-centred view. This kind of objectivity, where we see ourselves as nature, does no more than hold up a mirror to our inescapably human selves (Barad, Matter Feels).    In an object-oriented approach the unpredictable outcomes of the robots’s performance is brought to attention. OOO proponent and digital media theorist Ian Bogost, has a background in computational media, especially video and social media games, and says, “computers are plastic and metal corpses with voodoo powers” (9). This is a non-life description, hovering in the liminal space between being and not being. Bogost’s view is that a strange world stirs within machinic devices (9).     A question to ask: what’s it like to be a robot? Perhaps the answer lies somewhere between what it does and how we see it. It is difficult not to think of twentieth century philosopher Martin Heidegger’s tool analysis theory when writing of Gemeinboeck/Saunders’s work because Heidegger, and OOO scholar Graham Harman after him, uses the hammer as his paradigmatic tool. In his analysis, things are only present-at-hand (consciously perceived without utility) once they break (Harman, Heidegger Explained 63). However, Gemeinboeck and Saunders’s installation Accomplice straddles Heidegger’s dual present-at-hand and read-at-hand (the utility of the thing) because art raises the possibility that we might experience these divergent qualities of the robotic entities, simultaneously. The augmented robot, existing in its performative exhibition ecology, is the bridge between sentient life and utility.   Robotic Agency   In relation to the agency of robots, Ian Bogost refers to the Tableau Machine which was a non-human actor system created by researchers at Georgia Tech in 1998 (Bogost 106). It was a house fitted with cameras, screens, interfaces, and sensors. This was an experimental investigation into ambient intelligence. The researchers’s term for the computational agency was ‘alien presence,’ suggesting a life outside human comprehension. The data-collator sensed and interpreted the house and its occupants, and re-created that recorded data as abstract art, by projecting images on its own plasma screens. The implication was that the home was alive, vital, and autonomously active, in that it took on a sentient life, beyond human control.     This kind of vital presence, an aliveness outside human programming, is there in the Accomplice robots. Their agency becomes materialized, as they violate the polite gallery-viewing world. Karen Barad’s concept of agency works within a relational ontology. Agency resists being granted, but rather is an enactment, and creates new possibilities (Barad, Matter Feels). Agency is entangled amongst “intra-acting human and non-human practices” (6). In Toward an Enchanted Materialism, Jane Bennett describes primordia (atoms) as “not animate with divine spirit, and yet they are quite animated - this matter is not dead at all” (81). This then is an agency that is not spiritual, nor is there any divine purpose. It is a matter of material force, a subversive action performed by robotic entities, not for any greater good, in fact, for no reason at all. This unpredictability is OOO contingency, whereby physical laws remain indifferent to whether an event occurs or not (Meillassoux 39). Figure 5: Accomplice by Petra Gemeinboeck and Rob Saunders, Artspace, Sydney, 2013. (Photo: Petra Gemeinboeck)     A Post-Human Ethic    The concept of a post-human state of being raises ethical concerns. Ethics is a human construct, a criteria of standards fixed within human social systems. How should humans respond, without moral panic, to robots that might have life and sentient power outside human control? If an OOO approach is undertaken, the implication is that all things exist equally and ethics, as fixed standards, might need to be dismantled and replaced with a more democratic set of guidelines. A flat ontology, argued for by Bogost, Levi Bryant and other OOO advocates, follows that all entities have equal potential for independent energy and agency (although OOO theorists disagree on many small technical issues). The disruption of the conventional hierarchical model of being is replaced by a flat field of equality. This might cause the effect of a more ethical, ontological ecology.     Quentin Meillassoux, an influential figure in the field of Speculative Realism, from which OOO is an offshoot, finds philosophical/mathematical solutions to the problems of human subjectivity. His eschewing of Kantian divisions between object/subject and human/world, is accompanied by a removal from Kantian and Cartesian critique (Meillassoux 30). This turn from critique, and its related didactic authority and removed judgment, marks an important point in the culture of philosophy, but also in the culture of art writing. If we can escape the shackles of divisive critique, then the pleasures of narrative might be given space.    Bogost endorses collapsing the hierarchical model of being and converting conventional academic writing (89). He says, “for the computers to operate at all for us first requires a wealth of interactions to take place for itself. As operators or engineers, we may be able to describe how such objects and assemblages work. But what do they “experience” (Bogost 10)? This view is complementary to an OOO view of anti-subjectivity, an awareness of things that might exist irrespective of human life, from both inside and outside the mind (Harman 143). Figure 6: Accomplice by Petra Gemeinboeck and Rob Saunders, Artspace, Sydney, 2013. (Photo: Petra Gemeinboeck) New Materiality    In addition to her views on human/non-human agency, Karen Barad develops a parallel argument for materiality. She says, “matter feels, converses, suffers, desires, yearns and remembers.” Barad’s agential realism is predicated on an awareness of the immanence of matter, with materiality that subverts conventions of transcendence or human-centredness. She says, “On my agential realist account, all bodies, not merely human bodies, come to matter through the world’s performativity - its iterative intra-activity.” Barad sees matter, all matter, as entangled parts of phenomena that extend across time and space (Nature’s Queer Performativity 125).    Barad argues against the position that acts against nature are moral crimes, which occur when the nature/culture divide is breached. She questions the individuated categorizations of ‘nature’ and ‘culture’ inherent in arguments like these (Nature’s Queer Performativity, 123-5). Likewise, in robotic and machinic aesthetics, it could be seen as an ethical breach to consider the robots as alive, sentient, and experiential. This confounds previous cultural separations, however, object-oriented theory is a reexamination of these infractions and offers an openness to discourse of different causal outcomes.  Figure 7: Accomplice by Petra Gemeinboeck and Rob Saunders, Artspace, Sydney, 2013. (Photo: Petra Gemeinboeck)  Co-Evolution    Artists Gemeinboeck/Saunders are artists and scholarly researchers investigating new notions of co-evolution. If we ascribe human characteristics to robots, might they ascribe machinic properties to us? It is possible to argue that co-evolution is already apparent in the world. Titanium knees, artificial arteries, plastic hips, pacemakers, metallic vertebrae pins: human medicine is a step ahead. Gemeinboeck/Saunders in turn make a claim for the evolving desires of their robots (11).     Could there be performative interchangeability between species: human and robot? Barad asks us not to presume what the distinctions are between human and non-human and not to make post-humanist blurrings, but to understand the materializing effects of the boundaries between humans and nonhumans (Nature’s Queer Performativity 123). Vital matter emerges from acts of reappearance, re-performance, and interspecies interaction.     Ian Bogost begins his Alien Phenomenology by analysing Alan Turing’s essay, Computing Machinery and Intelligence and deduces that it is an approach inextricably linked to human understanding (Bogost 14). Bogost seeks to avoid distinctions between things or a slippage into an over-determination of systems operations, and instead he adopts an OOO view where all things are treated equally, even cheeky little robots (Bogost 17).Figure 8: Accomplice by Petra Gemeinboeck and Rob Saunders, installation view, Artspace, Sydney. (Photo: silversalt photography)      Intra-Active ReappearanceIf Barad describes intra-action as enacting an agential cut or separation of object from subject, she does not mean a distinction between object and subject but instead devises an intra-active cutting of things together-apart (Nature’s Queer Performativity 124). This is useful for two reasons. First it allows confusion between inside and outside, between real and unreal, and between past and future. In other words it defies the human/world correlates, which OOO’s are actively attempting to flee. Secondly it makes sense of an idea of disappearance as being a re-appearance too. If robots, and all other species, start to disappear, from our consciousness, from reality, from life (that is, becoming extinct), this disappearance causes or enacts a new appearance (the robotic action), and this action has its own vitality and immanence.     If virtuality (an aesthetic of being that grew from technology, information, and digital advancements) meant that the body was left or abandoned for an immaterial space, then robots and robotic artwork are a means of re-inhabiting the body in a re-materialized mode. This new body, electronic and robotic in nature, might be mastered by a human hand (computer programming) but its differential is its new agency which is one shared between human and non-human. Barad warns, however, against a basic inversion of humanism (Nature’s Queer Performativity 126). Co-evolution is not the removal of the human.    While an OOO approach may not have achieved the impossible task of creating a reality beyond the human-centric, it is a mode of becoming cautious of an invested anthropocentric view, which robotics and diminished non-human species bring to attention. The autonomy and agency of robotic life challenges human understanding of ontological being and of how human and non-human entities relate.References    Barad, Karen. """"Nature’s Queer Performativity."""" Qui Parle 19.2 (2011): 121-158.    ———. Interview. In Rick Dolphijn and Van Der Tuin. “Matter Feels, Converses, Suffers, Desires, Yearns and Remembers: Interview with Karen Barad.” New Materialism: Interviews and Cartographies. Ann Arbor: University of Michigan; Open Humanities Press, 2012.     ———. """"Posthumanist Performativity: Toward an Understanding of How Matter Comes to Matter."""" Signs: Journal of Women in Culture and Society 28.3 (2003): 801-831.    Bennett, Jane. The Enchantment of Modern Life: Attachments, Crossings, and Ethics. New Jersey: Princeton University Press, 2001.    Bogost, Ian. Alien Phenomenology. Minneapolis: Minnesota Press, 2012.    Bryant, Levi. The Democracy of Objects. University of Michigan Publishing: Open Humanities Press, 2011.    ———, N. Srnicek, and GHarman. The Speculative Turn: Continental Materialism and Realism. Melbourne: re:press, 2011.    Gemeinboeck, Petra, and Rob Saunders. “Other Ways of Knowing: Embodied Investigations of the Unstable, Slippery and Incomplete.” Fibreculture Journal 18 (2011). ‹http://eighteen.fibreculturejournal.org/2011/10/09/fcj-120-other-ways-of-knowing-embodied-investigations-of-the-unstable-slippery-and-incomplete/›.    Gray, Nathan. """"L’object sonore undead."""" In A. Barikin and H. Hughes. Making Worlds: Art and Science Fiction. Melbourne: Surpllus, 2013. 228-233.    Harman, Graham. The Quadruple Object. Winchester UK: Zero Books, 2011.    ———. Guerilla Metaphysics: Phenomenology and the Carpentry of Things. Chicago: Open Court, 2005.    ———. Heidegger Explained: From Phenomenon to Thing. Chicago: Open Court Publishing, 2007.    Heidegger, Martin. Being and Time. San Francisco: Harper and Row, 1962.    Meillassoux, Quentin. After Finitude: An Essay on the Necessity of Contingency. New York: Continuum, 2008.    Muecke, Stephen. """"The Fall: Ficto-Critical Writing."""" Parallax 8.4 (2002): 108-112.    ———. """"Motorcycles, Snails, Latour: Criticism without Judgment."""" Cultural Studies Review 18.1 (2012): 40-58.    ———. “The Writing Laboratory: Political Ecology, Labour, Experiment.” Angelaki 14.2 (2009): 15-20.    Phelan, Peggy. Unmarked: The Politics of Performance. London: Routledge, 1993.","",""
"2013","A Wavelet-based Multipurpose Watermarking for Image Authentication and Recovery","A wavelet-based multipurpose watermarking method is presented, in which the original image is not needed for watermark extraction, image authentication and content recovery. Experimental results showed that the new proposed method is very efficient and can be applied to protect the shared image in public services.","",""
"2013","Perceptually Optimized Coding of Color Images for Band-Limited Information Networks","","",""
"2013","Improved Non-linear Image Enhancement for Video Coding","It is well known that the B‐spline filter can yield a very accurate algorithm for smoothing. In this paper, it is shown that a cubic B‐spline filter can be used to improve the non‐ linear image enhancement method. In the non‐linear image enhancement, a higher‐frequency component can be predicted to solve the blurred problem of an enlarged image. This paper also presents a new three‐dimensional (3‐D) down‐scaling scheme to subsample video data for video coding. Furthermore, a novel non‐linear image‐enhancement compensation algorithm with cubic B‐spline filter is presented to improve the prediction of higher‐frequency component and accordingly the efficiency of the 3‐D down‐ scaling video coding. Finally, a computer simulation shows that the proposed method yields a better quality of the decoded image than other non‐linear enhancement methods.","",""
"2013","Governance by algorithms",": Algorithms are increasingly often cited as one of the fundamental shaping devices of our daily, immersed-in-information existence. Their importance is acknowledged, their performance scrutinised in numerous contexts. Yet, a lot of what constitutes 'algorithms' beyond their broad definition as “encoded procedures for transforming input data into a desired output, based on specified calculations” (Gillespie, 2013) is often taken for granted. This article seeks to contribute to the discussion about 'what algorithms do' and in which ways they are artefacts of governance, providing two examples drawing from the internet and ICT realm: search engine queries and e-commerce websites’ recommendations to customers. The question of the relationship between algorithms and rules is likely to occupy an increasingly central role in the study and the practice of internet governance, in terms of both institutions’ regulation of algorithms, and algorithms’ regulation of our society.","",""
"2013","Charting Sociotechnical Dimensions of Values for Design Research","The relationship of values to technology is an important topic in the fields of information studies, human–computer interaction, media studies, and science and technology studies, but definitions and attributes of values differ within and among these fields. We suggest that researchers currently conflate multiple categories when they discuss values. Some of these categories are attributes of the source of values (i.e., people, systems, and hybrid assemblages), and others are attributes of the values themselves. This article disambiguates values in sociotechnical systems by providing a framework to describe where and how values are negotiated and enacted by people, institutions, and technology. The framework includes three dimensions that pertain to the source of values (agency, unit, and assemblage) and three dimensions that pertain to attributes of values (salience, intention, and enactment) to enable precision and comparison across this research trajectory. We illustrate each dimension with examples from the values and design literature.","",""
"2013","Turing’s man: a dialogue","","",""
"2013","‘It is a beautiful experiment’: queer(y)ing the work of Alan Turing","","",""
"2013","Imaginary computational systems: queer technologies and transreal aesthetics","","",""
"2013","Artificial agents and the expanding ethical circle","","",""
"2013","Society under threat… but not from AI","","",""
"2013","To be human is to be creative","","",""
"2013","Automatic decision-making and reliability in robotic systems: some implications in the case of robot weapons","","",""
"2013","Ethical issues in our times of technology: select exploration","","",""
"2013","Moral equivalents of greed","","",""
"2013","The quest for artificial wisdom","","",""
"2013","Testing Turing: a personal quest","","",""
"2013","What is it like to encounter an autonomous artificial agent?","","",""
"2013","You and I, robot","","",""
"2013","Human-robot interaction and psychoanalysis","","",""
"2013","AI &amp; SOCIETY","","",""
"2013","Humanoid robots as “The Cultural Other”: are we able to love our creations?","","",""
"2013","Naturalizing language: human appraisal and (quasi) technology","","",""
"2013","Artificial intelligence and society: a furtive transformation","","",""
"2013","The freelance translation machine: Algorithmic culture and the invisible industry"," Much of the work performed by the global translation industry is handled by freelance labor. This segment of the industry has seen a radical structural transformation that has accompanied a radical transformation in the media environment that supports its work. The emergence of online freelance translation marketplaces has married the logics of standardization, automation, and protocol to casual labor, motivated by incremental profit and lubricated by entrepreneurialism. Customs and practices native to contemporary internet culture generate a freelance translation machine made of equal parts flesh and silicon that manages skilled labor algorithmically. In parallel with the specific case of freelance translation practices, this article develops and deploys a notion of algorithmic culture that accounts for the integration of human cognition in computational processes. Consequently, the possibility emerges that users instrumentalize algorithms even as algorithms instrumentalize users. ","",""
"2013","Exploring creative intersections: Ernest Edmonds and his time-based generative art","This article focuses on the experience of British artist Ernest Edmonds and the influences that have informed his art practice in the past forty years. How are these influences connected? The article has an historical focus. It develops the themes within Edmonds's art and shows his connections with the ‘Systems’ artists and their forebears. In particular, the article concentrates on the encounter of Edmonds with artistic thinking about systems and process in the broad sense, as well as digital and interactive work developed from the early 1980s. As the article will demonstrate, a passion for colour, time and structure, the encounter with a number of artists inspired by Constructivism in the early 1980s, and the educational context in which Edmonds has worked from the late 1960s until the present, have offered great opportunities of interdisciplinary exchanges and ideas that had a profound impact upon the nature of his art. These influences have enabled Edmonds to explore new constructs in art through the use of technology that are a constant stimulus in his creative research, both as an artist and as an academic.","",""
"2013","Performative Materiality and Theoretical Approaches to Interface.","","",""
"2014","Dynamic Human-Centered Communication Systems Theory","This article introduces Dynamic Human-Centered Communication Systems Theory (DHCCST), defining communication as a complex dynamic system consisting of a human, a message, a medium, and a location. An argument is made that all elements of the system should be defined from a human centric viewpoint. The article reconceptualizes humans as evolved, embedded, embodied brains (EEEBs). Media are reconceptualized as brain-like creatures (BLCs). Communication is defined as the interaction of one or more EEEBs with other EEEBs or with BLCs. Medium of carriage and encoding system are reconceptualized along continua ranging from evolved to man-made. Examples of hypotheses arising from DHCCST are presented and some theories and effects are reinterpreted using this new perspective.","",""
"2014","Detection of dental abnormalities using SVM and PSVM","","",""
"2014","e-Agricultural innovation using a human-centred systems lens, proposed conceptual framework","","",""
"2014","The machine’s role in human’s service automation and knowledge sharing","","",""
"2014","The “book problem” and its neural correlates","","",""
"2014","Adopting a musical intelligence and e-Learning approach to improve the English language pronunciation of Chinese students","","",""
"2014","Towards empathy: a human-centred analysis of rationality, ethics and praxis in systems development","","",""
"2014","Mapping the cognitive environment of fifth graders: an empirical analysis for use in environmental planning","","",""
"2014","Revisiting the use of secondary task reaction time measures in telepresence research: exploring the role of immersion and attention","","",""
"2014","An analysis of the performance of Artificial Neural Network technique for apple classification","","",""
"2014","Creating “companions” for children: the ethics of designing esthetic features for robots","","",""
"2014","Automatic classification of computed tomography brain images using ANN, k-NN and SVM","","",""
"2014","Science, technology and values: promoting ethics and social responsibility","","",""
"2014","Comparative analysis of machine learning techniques in prognosis of type II diabetes","","",""
"2014","Opportunities for small and medium enterprises in the innovation and marketing of organic food: investigating consumers’ purchase behaviour of organic food products in Victoria, Australia","","",""
"2014","Cloud-watching robots"," Can a robot waste a day away watching clouds? Aesthetics as a means to approach the world is a form of control until recently limited to humans. This essay uses two works by New Zealand artist Douglas Bagnall to examine the relationship between machines, information and aesthetics. I discuss how Bagnall’s Film-making Robot (2004) and Cloud Shape Classifier (2006) are examples of aesthetic machines that, rather than being defined by information, repetition and the digital specificity of the pixel or the binary, are characterised by an aesthetic dynamism formed between emergence and mutability. Building on the recent identification of ‘new aesthetics’, I argue that processes of emergence and mutation contribute a new way to think about machines, information, humans and aesthetics. Finally, I suggest that Bagnall’s works do not just demonstrate machinic vision but prefigure a move in contemporary art from the stable aesthetic object to the unstable and impure real-time process of machine aesthetics. ","",""
"2014","The effect that robots instead of spacemen landing on Mars can have on spacecraft development","The space race of the 1960s attracted a concentrated peak in space funding which has not since been repeated. Based on a novel methodology of new Internet–sourced, computer–driven visual text analytic techniques, this study suggests that the advances in engineering technologies supported by this funding — especially robotic, unmanned missions to space involving international cooperation such as the 2012 Curiosity landing on Mars — have resulted in decreased public interest, engagement, understanding of and ultimately support for space exploration and ultimately human–carrying spacecraft development. We suggest consequences for public interaction with, and political and economic support for future spacecraft development.","",""
"2014","Agent-Based Modeling and Historical Simulation.","","",""
"2015","Notes on the Existential and Interactional Dimensions of Authenticity: A Symbolic Interactionist Study of Breast Cancer Internet Forums","Existential AuthenticityI still remember looking at people and hating them because they were going to get to live to be old and I wasn’t.Nobody here is stronger than you are [...] you can take charge […] if you cry all the time and live in fear […] it won’t make cancer go away.[Interaction on an Internet breast cancer forum]According to existential philosophy, death is an irrefutable aspect of life. Human existence is fundamentally being-toward-death (Sein-zum-Tode) inasmuch as it involves taking on the possibilities of self—a set of future projected personal choices and their corresponding consequences—and growing into them (Heidegger 232-233). Death is not to be seen as something that marks the end of life, but rather as something that is always already an inevitable part of it (227). One never exists without the other. Heidegger maintained that self-authenticity comprises an understanding and acceptance of the possibility that one always faces death. To live authentically, one must make choices within her or his own life, despite death’s looming. Ignoring or denying the facticity of death, or failing to maximise the possibilities of one’s life, is to live inauthentically (240-243).The first post in the epigraph above, taken from a discussion thread in an Internet forum on breast cancer, exemplifies, in the Heideggerian sense, an expression of inauthenticity. The individual had seemingly given up on the possibility of life and allowed a single possibility—death—to (at least temporarily) dictate her outlook on life. She rejected the new potentials that living with cancer presented. By giving away the possibility of choice for life, she gave up her true self. Nothingness replaced being (Sartre, Being), stripping away the authenticity of being for-itself, which is rooted in “spontaneous original choice that depends on the individual's freedom” (Onof).Such an analysis represents an historically dominant ontology of authenticity within philosophy. It has been closely followed by psychological theories of self, which posit that human beings have within them an essential, authentic self that confronts the external world. Heidegger, Sartre, and other existentialists drew on an even earlier, Romantic conception of authenticity going back at least as far as Rousseau, which contented that “individuals should follow their inner voices and resist the pressures and callings of society in order to recover intimate moral contact with themselves” (Lewin and Williams 65). Intimate moral contact with oneself is, however, lost within the auspices of contemporary societies (Gubrium and Holstein), where people are more concerned with appearing sincere (true to others) than authentic (true to themselves) (Trilling). From this perspective, people implicitly believe that an authentic self exists within them, yet it may remain largely out of reach. Inauthenticity may be the result of refusing to live authentically—for example, ignoring or rejecting choices that could lead to fulfillment—or the result of social and cultural structures that prevent individuals from expressing themselves authentically (Hochschild).Note, however, that the opening forum post is written in past tense. The person who wrote it later came to see herself as living authentically, despite, or more appropriately, because of the discourses of death that surround her condition. The second post, a response to the first, offered both encouragement and the vision of a self that was not defeated by cancer. Together, they articulate a socially constructed, interactionist ontology of the authentic cancer survivor: a person who is strong, who “can take charge,” that is, who can make spontaneous choices despite the chance of dying. This type of authentic self is not “really real” (Geertz); it is interactionally created and maintained. As we argue in this paper, authenticity is not an essential aspect of being, but is rather the product of a reflexive process of dialogue, both internally between the “I” and the “me” (Mead), and interpersonally among multiple people who come to agree upon what constitutes authenticity in a particular milieu (Vannini and Williams).In what follows, we explore the intermingling of two dimensions of authenticity: the existential dimension, which has to do with cancer patients’ being-toward-death andbeing-in-situation; and the interactional dimension, in which individuals simultaneously narrate cancer experiences and collaboratively construct the ideal meaning of an authentic cancer patient. We argue that the interactionist dimension is particularly important to understand because it facilitates the existential dimension. Whereas existentialism theorises people as naturally free and responsible agents who determine the course of their own lives, symbolic interactionism theorises the social foundation upon which people build pragmatic notions such as freedom and self-determination. For interactionists, a person’s sense of self (past, present, or future) does not naturally occur. Rather, “mind is a thoroughly social affair, a product of social experience” (Carreira da Silva 45). As human beings develop, they learn to take themselves as objects through their ongoing interactions with(in) the world, learning to interact with themselves and to define themselves in terms that often cohere with others’ definitions of them (Cooley). As we suggest in this paper, the authentic, existential self is thus not an inherent aspect of a person that is threatened by cancer to be saved or recovered. Rather, people dealing with cancer get together, online and elsewhere, and collaboratively construct what is for them an authentic image of the cancer survivor. This socially authentic image becomes the ideal upon which individuals modify their self-conceptions.Research Site and MethodsWe apply our understanding of the relation between the existential and interactional dimensions of authenticity to data we collected from an Internet forum populated by patients and survivors of breast cancer. A non-profit organisation initiated the forum around 2002 to supply general and technical information about breast cancer, with the declared goal of assisting women and their loved ones in gaining insight into the illness, its causes, and effects. As of early 2015, it was an established and reputable Internet forum that had more than 150,000 registered members who had engaged in upwards of 120,000 unique discussion topics spread across dozens of sub-forums. Forum participants could interact by reading messages, posting their own messages, and replying to others’ posts. The forum was public as it did not require registration for people to view messages, but registration was required to post messages. Members chose usernames, which could be their real or pseudonymous. The forum was segmented into 19 categories, with a total of 78 sub-forums. There were sub-forums with technical information regarding diagnosis, treatment, testing, prognosis, and specific graduations or types of breast cancer, as well as non-technical sub-forums that dealt with everyday means of recovery and/or coping with cancer, advocacies, and social networking. All the sub-forums allowed and encouraged participants to share their personal knowledge and experiences.              Our analysis follows the contours of interpretive interactionism, interactional narrative analysis, and ethnographic content analysis. Interpretive interactionism “signifies an attempt to join the traditional symbolic interactionist approach with the interpretive, phenomenological works of Heidegger […and] to make the world of lived experience directly accessible to the reader” (Denzin 14). As such, we have sought to clearly represent individuals’ reported experiences with cancer and how they relate to struggles for existential authenticity. Interactional narrative analysis highlights the co-constructed nature of the contexts, which “shape possibilities in […] women’s lives, their experiences of illness, and the specific illness narratives the women produce collaboratively” (Riessman 4; Bell). By focusing on the interactional qualities of the narratives produced by forum members, we seek to highlight how authenticity is collaboratively constructed. Lastly, ethnographic content analysis stresses the significant amount of time and effort that researchers put into the study of mediated texts. We did not discover the posts through “big data” techniques such as scraping (Kumar), nor did we cull them after conveniently scanning forum threads. Instead, we thoughtfully chose posts that represent the larger patterns of interaction that the second author identified over months of regular engagement in the forums.In this paper, we report on some characteristics of authenticity discourses. We have avoided providing pseudonyms or otherwise identifying individual posters because our analytic focus is on discourse and narrative, rather than on individuals (Bassett and O’Riordan). Nevertheless, throughout the research project (which is ongoing), we have carefully considered the ethics of Internet-based research involving experiences of a life-changing illness such as cancer. Following the ethical guidelines set forth by the Association of Internet Researchers (Markham and Buchanan), we focused on how our study, despite emphasising discourse, deals with people’s lived experiences. We support the ethical ideal that researchers studying vulnerable populations are obligated to protect those populations however possible (King; Rodham and Gavin). However, we found the forum to be full of discourse through which participants actively resisted labels such as “victim” and instead framed their online activities as important steps in maintaining or recovering an authentic sense of self, as well as being able to help others. Because the forum allows users to reveal as much or little about themselves as they choose, because the forum is open to the public for browsing, and because we do not impinge upon participants’ actions through our analysis of their interactions, we believe the benefits of the study—an improved understanding of how people collaboratively construct feelings of self-authenticity—outweigh any problems we have been able to imagine. Our ethics and methods are in line with other recent publications that use similar data (Love et al.; Sillence)Cancer and Existential CrisisThe Chinese use two brush strokes to write the word “crisis.” One brush stroke stands for danger; the other for opportunity. In a crisis, be aware of the danger—but recognize the opportunity.[John F. Kennedy]Cancer abruptly presents a person with a heightened sense of mortality. Knowledge of cancer represents a moment in which one faces the annihilation of the possibilities of being. “Cancer is a shocking thing … we don’t face death very often, so it’s overwhelming” [forum post]. It may lead to despair, aimlessness, and the loss of meaning, or to a realisation of the urgency to live life to the fullest. We borrow Denzin’s concept of an epiphany to theorise moments of existential crisis, such as when someone learns she has breast cancer. “In [epiphanies], personal character is manifested and made apparent” (Denzin 15). He postulates how a single epiphanic moment can “shatter a person's life and make it never the same again” (17). Sociologically, epiphanies occur in problematic situations that must be interpreted. They have multiple, variable meanings to the individual and others that may change with time, and they can turn lives around. Methodologically, epiphanies are both subjectively experienced but intersubjectively communicated and thus we can study them using interpretive methods.From an existentialist perspective, authenticity finds a tipping point in moments where contingencies or crises dominate. Individuals who acknowledge their being-in-situation and then confront it in the best possible way are authentic; those who do not are inauthentic.To be authentic is to realise one’s being-in-situation, no matter what this situation may happen to be: through a profound awareness, through the authentic realisation of one’s being-in-situation, one brings into existence the situation on one hand, and human existence on the other. (Sartre, War Diaries 54)Sartre’s being-in-situation relates directly to people’s experiences with cancer. A diagnosis can lead to existential crisis by suddenly increasing the salience of mortality, heightening awareness of one’s being-toward-death and thereby leading them to confront their being-in-situation. Many forms of interaction with self and other articulate this being-in-situation. We assume that most of these interactions are private, taking place behind closed doors with loved ones or with internally with oneself. However, the Internet has facilitated individuals’ ability to connect cancer as a personal trouble with cancer as a public issue (Høybye, Johansen, and Tjørnhøj-Thomsen; Klemm, Reppert, and Visich).EpiphaniesIn order to understand how forum participants understood existential authenticity for themselves, we first sought out discussions about the early moments after diagnosis. Many threads contained posts that illustrated how cancer diagnoses led to epiphanies. We found that, despite a common conception of epiphanies as sudden and immediate, they were not necessarily brief, as experiences of crisis occurred over time and had to be interpreted, both by the individual experiencing the crisis and by others. Notably, other forum members were crucial in helping provide interpretive frames for diagnosis.Shortly after I finished my tx [treatment] and was still very traumatized by it all, I met a woman […] who […] taught me something very profound, which is that when we experience something very traumatic, we have to figure out a way to use it for good, or we will continue to become a victim.” I think that’s a very powerful concept, and one of the reasons I continue to post [...] because helping newly-dx’ed [diagnosed] women helps me, too.The knowledge of cancer triggered powerful emotions in the form of spontaneous confusion and disenchantment, upheaving the continuity and planned direction of women’s lives. Many women translated the strong emotions they experienced alongside the support they received from others into an impetus to grow beyond the contingent misfortunes of cancer. In another thread, a member described that emotional spillovers and intense self-denial initially hindered her movement towards growth, but later she came to see such despair as unproductive and had to confront and overcome the being-in-situation (in other words, the crisis) of cancer.I went through a lot of the emotional changes you ladies are describing back then. The crying in the bathroom, the bargaining with god. Ultimately I think it made me focus on what was important and what absolutely was not […] I sought out new experiences, and I discarded emotional baggage that was just weighing me down […] I think the lessons learned back then have helped me face this challenge with more aplomb and a more optimistic outlook […. Cancer] can actually be a blessing in disguise.Another member agreed with this idea of cancer as a blessing:It can be a blessing if it is a catalyst to moving us to a better place emotionally, spiritually and even physically meaning we begin to take better care of ourselves and our health in every way. It isn’t easy but the changes are well worth it.From an existential perspective, rooted in a realist epistemology, narratives can be taken as reports about what “really happened.” As such, these co-constructed narratives articulate an epiphany-driven growth model of cancer-coping, where the illness stimulates a focus on one’sbeing-toward-death, necessitating a choice to live meaningfully or to reject such a choice. Similarly, among the many forum threads we observed, most contained narratives of adaptation and of wisdom gained from confronting and overcoming one’s being-in-situation. Discourses of confidence, optimism, and emotional resilience filled the forums.From a social constructionist perspective, however, we do not take these narratives to be reports of what necessarily “really” happened, but rather as moments through which reality itself is constructed. It is impossible to know whether the epiphanies described actually occurred as women described them, or whether they emerged, as Denzin has argued, over time through interaction and interpretation. Keeping this latter perspective in mind, we look again at forum data to see how members constructed a sense of self-authenticity, paying particular attention to the collaborative milieu in which individuals shared personal narratives (Bell).Collaboratively Constructing the Authenticity of Being-toward-DeathIn Time and Being, Heidegger writes, “higher than actuality stands possibility” (34), which we take here to emphasise Heidegger’s belief that authentic selves are rooted in the directions taken in response to crisis. If a cancer diagnosis is an “actuality,” then one ought to embrace the possibilities that extend from it. To allow the actuality of cancer to supersede other possibilities in life is to live (or die) inauthentically. We saw this existential phenomenon of being-toward-death in interactions throughout the forum. Many members mutually shared the notion that cancer raised awareness of mortality. Long-term members, called “veterans” in the forums, overwhelming agreed that the “right” way to handle this was to make a choice between meaningful or meaningless living.Cancer is a shocking thing […] we don’t face death very often, so it’s overwhelming […] but I’ve tried to liken it to how we all know the stats of getting in a car wreck, and yet we get in a car without our kids in a row and drive away, for we CHOOSE to live.In this and other examples, veterans used analogies and examples about how they “choose to live.” The use of analogy or example helped veterans and newer members alike connect the breadth of possibilities to mundane activities, which they defined as authentic. When a newer member complained, “my feelings are not just bubbling to the surface, they are streaming out of me. I can barely control telling others how it is and how I feel,” a veteran responded that physical exercise, being outdoors, and being in-the-present helped in coping with emotional turmoil. Such examples served as strategies through which those in crisis could “go from sick to well.” Another participant wrote that heeding veterans’ advice would help newer members “move even further along the path of a fulfilling life.”Contrary to what may seem from our examples so far to be an uplifting environment in which everyone succeeded at embracing the possibilities of life with cancer, some patients occasionally expressed their frustration with the personal troubles that emerged from their condition and associated life choices.Now, my life is turned upside-down, and I am filled with anxiety. This is all my own doing, since I decided that I needed to be true to myself. That was the one major thing that I had denied myself my entire life […] I think that I was the fool.This is an important example for two reasons. First, it represents, in existential terms, being-in-situation. For most people, life involves feelings of having been thrown into a world “not of our own making, already disposed by moods and particular commitments, with a past behind us that constrains our choices” (Varga and Guignon). The epiphanies that came with or after cancer diagnoses grossly exaggerated such feelings, as hundreds of individual forum posts demonstrated. The repetition of such personal narratives is significant, as we find epiphanies “in those interactional situations where personal troubles become public issues” (Denzin 18). The forum was one such interactional situation, comprising thousands of moments in which people openly admitted their being-in-situation and either sought or offered consolation or advice, or both. For Heidegger, admitting to being-in-situation is key to authenticity—if one hides from the facticity of her situation, there is no opportunity to choose a future (Heidegger 242-3).The second reason this is an important example is because it represents the interactional basis through which groups socially define and negotiate definitions of existential authenticity. The post stands as an interactional moment in which a participant called into question many veterans’ normative conceptions of embracing being-toward-death as necessary for an improvement in one’s quality of life. Having attested to how cancer had compelled her to make a choice between living authentically and inauthentically (a choice she had, until then, denied herself), this member then claimed that choosing authenticity had brought on additional turmoil and anxiety. Veterans and other members quickly rallied to redefine her choices as nonetheless necessary and right. Some highlighted the positive outcomes of making tough choices and downplayed the negative. In one response, a community member suggested that contracting breast cancer placed women in a position of exigency that compelled them to question life’s limitations.According to many veterans, the awareness of being-toward-death, while disruptive, offered at least as much good as it did bad. Participants collectively maintained a discourse in which individuals should worry less about things like “money” and to “absorb each moment as if it is a gift, because that is what this life is—a gift.” Other posters also highlighted how they dealt with being-toward-death and the self-authenticating decisions that came from it:Nothing like mortality in your face to make you realize that you want something different or more. I [...] have decided to make a major career change after all of this […] that makes me happy and makes a difference in the world.Life is a lot more vivid now. I notice the little things and am much grateful for them […] I am […] relaunching my career and writing a journal article with two colleagues.These and other posts discursively functioned to define the “proper” way to deal with being-toward-death by implicitly associating authenticity with the choices that individuals make and their consequences for self. In doing so, they collectively (re)construct the authenticity of a cancerous self, which “is at once a meaning from the past [acknowledged during the epiphany] and promise into a future [embracing being-toward-death] around a processual present [interacting with others as a method of working through being-in-situation]” (Weigert 37). Further, because self and social action dialectically shape one another, supporting one’s feeling of self-authenticity can motivate the individual towards “appropriate” future social actions, which will feed back into future self-authentications. Authenticity is thus not only a phenomenological and existential experience; it is a distinctly social and interactional process.ConclusionIn this paper we have attempted to articulate two ontological dimensions of authenticity—the existential and the interactional. Like other scholarly discussions of authenticity (for example, Taylor; Theodossopoulos; Vannini and Williams), we have argued that authenticity is not so much a subjective concept as it is a social, pragmatic concept that speaks to subjectivity.Beginning with Heidegger and Sartre, we investigated how members of an online cancer forum intersubjectively communicated ideas of being-toward-death and being-in-situation. Individuals experienced being-toward-death as crisis, which then required that they confront their being-in-situation and acknowledge the possibilities for life in the face of its finitude. Similar studies on cancer have considered the quest for authenticity in terms of development stage models (for example, Gullickson; Krumwiede and Krumwiede; Samson and Zerter; Westman, Bergenman and Andersson). Our findings are similar to the extent that we identified a re-orientation toward the meaning of life via epiphany, with resulting personal transformations through the re-ordering of values and social relationships, which patients themselves framed as authentic.However, we have sought to clarify an interactionist conceptualisation in which self-authenticity is facilitated by a community of people who come to share meanings about life and death. Shared meanings have to do not only with objects or events in people’s lives, but with the meaning of selves. Selves are always in the process of becoming; they are neither static nor fixed things (Mead). As such, authenticity itself is something that is situationally negotiated rather than finalized; one may move back and forth between more or less authentic self-feelings as one performs the self in various milieu (Goffman; Gubrium and Holstein, Williams).  We argue against the notion that existential authenticity is a purely psychological process and instead highlight how interpersonal communication facilitates perceptual shifts across the domains of being-toward-death and being-in-situation.Our study supports other studies of cancer forum interactions (for example, Chiu and Hsieh; Love et al.; Pinheiro et al.; Shim, Cappella, and Han), which have found that cycles of mutual interactions facilitate reciprocal understandings through shared narratives of thoughts, emotions, and behaviors, and provide structure, if not certainty, to cancer experiences. Forums serve as “communication interlocks” within which people from vastly different backgrounds become members of a meaningful subcultural community (Williams and Copes). Members rely on each other for a shared definition of the situation that legitimates their personal troubles and how they cope with them (Høybye et al.; Sillence). As patients gain knowledge from others on the forum, they in turn reproduce their newfound confidence and contribute to helping others. In the forum we studied, this macrocosm of constant interaction has become a self-sustaining and reflexive community that guides struggling patients and once-struggling survivors towards meaningful feelings of self-authenticity.ReferencesBassett, Elizabeth H., and Kate O’Riordan. “Ethics of Internet Research: Contesting the Human Subjects Research Model.” Ethics and Information Technology 4.3 (2002): 233–47.Bell, Susan E. “Narratives and lives: Women's Health Politics and the Diagnosis of Cancer for DES Daughters.” Narrative Inquiry 9.2 (1999): 1–43.Carreira da Silva, Filipe. G. H. Mead: A Critical Introduction. Cambridge, UK: Polity.Chiu, Yu-Chan, and Yu-Ling Hsieh. “Communication with Fellow Cancer Patients: Writing to Be Remembered, Gain Strength, and Find Survivors.” Journal of Health Psychology (2012): 1359105312465915.Cooley, Charles H. Social Organization: A Study of the Larger Mind. New York: Charles Scribner's Sons, 1909.Denzin, Norman K.  Interpretive Interactionism. Thousand Oaks: Sage, 1989.Geertz, Clifford. “Religion as a Cultural System.” The Interpretation of Cultures: Selected Essays. New York: Basic Books, 1973. 87–125.Gubrium, Jaber F., and James A. Holstein. “The Everyday Work and Auspices of Authenticity.” Authenticity in Self, Culture and Society. Ed. Phillip Vannini and J. Patrick Williams. Aldershot, UK: Ashgate, 2009. 121–38.Gullickson, Colleen. “My Death Nearing Its Future: A Heideggerian Hermeneutical Analysis of the Lived Experience of Persons with Chronic Illness.” Journal of Advanced Nursing 18.9 (1993): 1386–92.Heidegger, Martin. Being and Time: A Translation of Sein und Zeit. Trans. Joan Stambaugh. Albany: SUNY P, 1962.Hochschild Arlie, R. The Managed Heart: Commercialization of Human Feeling. Berkeley: U of California P, 1983.Høybye, Mette Terp, Christoffer Johansen, and Tine Tjørnhøj-Thomsen. “Online Interaction. Effects of Storytelling in an Internet Breast Cancer Support Group.” Psycho-Oncology 14.3 (2005): 211–20.King, Storm A. “Researching Internet Communities: Proposed Ethical Guidelines for the Reporting of Results.” The Information Society 12 (1996): 119–27.Klemm, Paula, Karla Reppert, and Lori Visich. “A Nontraditional Cancer Support Group: The Internet.” Computers in Nursing 16.1 (1997): 31–36.Krumwiede, Kelly A., and Norma Krumwiede. “The Lived Experience of Men Diagnosed with Prostate Cancer.” Oncology Nursing Forum 39.5 (2012): E443–50.Kumar, AV Senthil, ed. Knowledge Discovery Practices and Emerging Applications of Data Mining: Trends and New Domains. Hershey, PA: IGI Global, 2011.Lewin, Philip, and J. Patrick Williams. “The Ideology and Practice of Authenticity in Punk Subculture.” Authenticity in Self, Culture and Society. Ed. Phillip Vannini and J. Patrick Williams. Aldershot, UK: Ashgate, 2009. 65–86.Love, Brad, B Brook, C.M. Thompson, S. Zaitchik, J. Knapp, L. Lefebvre, B. Jones, E. Donovan-Kicken, E. Eargle, and R. Rechis. “Exploring Psychosocial Support Online: A Content Analysis of Messages in an Adolescent and Young Adult Cancer Community.”Cyberpsychology, Behavior, and Social Networking 15.10 (2012): 555–59.Markham, Annette, and Elizabeth Buchanan. “Ethical Decision-Making and Internet Research: Recommendations from the AoIR Ethics Working Committee (Version 2.0).” Association of Internet Researchers (2012). 15 Feb. 2015 ‹http://aoir.org/reports/ethics2.pdf›.Mead, George H. Mind, Self, and Society. Chicago: U of Chicago P, 1934.Onof, Christian. Jean Paul Sartre: Existentialism. The Internet Encyclopedia of Philosophy. 16 Jan. 2015 ‹http://www.iep.utm.edu/›.Pinheiro, Cleoneide Paulo Oliveira, et al. “Participating in a Support Group: Experience Lived by Women with Breast Cancer.” Revista Latino-Americana de Enfermagem 16.4 (2008): 733–38.Riessman, Catherine K. “Narrative Analysis.” The Sage Encyclopedia of Social Science Research Methods. Eds. Michael S. Lewis-Beck, Alan Bryman, and Tim Futing Liao. Thousand Oaks, CA: Sage, 2003. 706–10.Rodham, Karen, and Jeff Gavin. “The Ethics of Using the Internet to Collect Qualitative Research Data.” Research Ethics Review 2.3 (2006): 92–97.Samson, André, and B. Zerter. “The Experience of Spirituality in the Psycho-Social Adaptation of Cancer Survivors.” Journal of Pastoral Care and Counseling 57 (2003): 329–44Sartre, Jean Paul. Being and Nothingness: An Essay on Phenomenological Ontology. Trans. Hazel E. Barnes. Methuen, London, 1958.Sartre, Jean P. The War Diaries. Translated by Quentin Hoare. New York: Pantheon. 1984.Shim, Minsun, Joseph N. Cappella, and Jeong Yeob Han. “How Does Insightful and Emotional Disclosure Bring Potential Health Benefits? Study Based on Online Support Groups for Women with Breast Cancer.” Journal of Communication 61.3 (2011): 432–54.Sillence, Elizabeth. “Giving and Receiving Peer Advice in an Online Breast Cancer Support Group.” Cyberpsychology, Behavior, and Social Networking 16.6 (2013): 480–85.Taylor, Charles. The Ethics of Authenticity. Cambridge, MA: Harvard UP, 1991.Theodossopoulos, Dimitrios. “Laying Claim to Authenticity: Five Anthropological Dilemmas.” Anthropological Quarterly 86.2 (2013): 337–60.Trilling, Lionel. Sincerity and Authenticity. Cambridge, MA: Harvard UP, 1972.Vannini, Phillip, and J. Patrick Williams, eds. Authenticity in Culture, Self, and Society. Ashgate, 2009.Varga, Somogy, and Charles Guignon. “Authenticity.” The Stanford Encyclopedia of Philosophy. Fall 2014 ed. Ed. Edward N. Zalta. 17 Mar. 2015 ‹http://plato.stanford.edu/archives/fall2014/entries/authenticity/›.Weigert, Andrew J. “Self Authenticity as Master Motive.” Authenticity in Self, Culture and Society. Ed. Phillip Vannini and J. Patrick Williams. Aldershot, UK: Ashgate, 2009. 37–50.Westman, B., M. Bergenmar, and Lars Andersson. “Life, Illness and Death—Existential Reflections of a Swedish Sample of Patients Who Have Undergone Curative Treatment for Breast or Prostatic Cancer.” European Journal of Oncology Nursing 10.3 (2006): 169–76.Williams, J. Patrick. “Authenticity and the Dramaturgical Self.” Life as Performance: A Dramaturgical Handbook. Ed. Charles Edgley, Aldershot, UK: Ashgate, 2012. 94–107.Williams, J. Patrick, and Heith Copes. “'How Edge Are You?' Constructing Authentic Identities and Subcultural Boundaries in a Straightedge Internet Forum.” Symbolic Interaction 28.1 (2005): 67–89.          ","",""
"2015","Playing Checkers with Machines—from Ajeeb to Chinook","Chess-playing automata that also played checkers were built in Europe from the late eighteenth century. One of these, named Ajeeb, came to the United States, spending much of the time from 1885 into the 1930s in New York and taking occasional tours around the country. The advent of electronic computers in the mid-1940s encouraged attempts to play games using machines in both Britain and the United States. By the late twentieth century, a Canadian-written computer program, Chinook, had defeated world champion and mathematician Marion Tinsley. More recently, developers of Chinook announced that checkers could be played to a draw against any opening.","",""
"2015","Playing Checkers with Machines—from Ajeeb to Chinook","    Chess-playing automata that also played checkers were built in Europe from the late eighteenth century. One of these, named Ajeeb, came to the United States, spending much of the time from 1885 into the 1930s in New York and taking occasional tours around the country. The advent of electronic computers in the mid-1940s encouraged attempts to play games using machines in both Britain and the United States. By the late twentieth century, a Canadian-written computer program, Chinook, had defeated world champion and mathematician Marion Tinsley. More recently, developers of Chinook announced that checkers could be played to a draw against any opening.  ","",""
"2015","Specific Situations or Specific People? How Do Extrinsic and Intrinsic Factors Interact in Cultivation Research?","Based on the rationale of dual process models, processing strategies influence the cultivation of first-order beliefs. Dual process models predict two key prerequisites for processing systematically: motivation and ability, which in turn can be influenced by intrinsic as well as extrinsic factors. Yet most research has concentrated on extrinsic factors by varying characteristics of the situation. The interaction between intrinsic and extrinsic characteristics, however, has not been researched to date. We present a field study testing for this kind of interaction. Results of a general population study indicate that intrinsic (education) as well as extrinsic (survey mode) factors moderate cultivation. Additionally, we found a person-situation interaction, indicating that cultivation is influenced by extrinsic factors in different ways, depending on an individual’s intrinsic characteristics.","",""
"2015","<i>Robot Futures,</i>by Illah Reza Nourbakhsh. Cambridge, MA: MIT Press, 2013. 160 pp. $26.95 hardcover. ISBN 9780262018623 (hardcover).","","",""
"2015","Rationalizing Sociality: An Unfinished Script for Socialbots","This article takes the concept and some of the existing applications of socialbots—software robots that operate on social networking sites and present themselves as human users—as an occasion to trace the evolution of online sociality. The argument mobilizes theories of social rationalization from Max Weber to contemporary critical theory to demonstrate that the appearance of automated profiles (socialbots) on social networking platforms can be seen as a logical step in the progressive enclosure of online social interaction in standardized, simplified, and trivialized forms, frames, and gestures. Critical questions concerning what the growth of robo-sociality may mean for individual users and the online public sphere are posed with a view to charting the directions for a needed public debate.","",""
"2015","Shall We Talk? Conversing With Humans and Robots","While social robots are the creation of human beings, it is not obvious what kinds of conversation people desire to have with computer-based devices. Progressive improvements in speech recognition, natural language parsing, and physical embodiment are making it technologically possible for social robots to engage with humans in essentially the full range of conversational modes that we do with one another. However, when we examine the variety of possible (human) linguistic functions, we discover reasons people may not wish for total verisimilitude when interacting linguistically with robots. Informational and empathetic functions are likely to be more welcomed than those involving social control or critique.","",""
"2015","The Contemporary Appeal of Artificial Companions: Social Robots as Vehicles to Cultural Worlds of Experience","  “Universal projection” is the term Thomas Luckmann uses to denote man's innate capacity to project his own “living body” onto everything he encounters in the world, and Luckmann lists a number of qualities that cancel the projection. For example, the lack of perceptible transformations on the outside of an object is perceived as an indication of the absence of a responsive inside. This list can serve as a how-not-to guide to building that piece of advanced technology known nowadays as an “artificial companion.” Drawing the border of the social world alongside that of the human world—which is typical of Western modernity—is not an ontological given but rather an evolutionary outcome, that is, a result of social construction. The de-socialization of large parts of the life-world leads to its de-animation, which is closely linked to the emergence and organization of a separate religious symbolic reality. The tendency to endow objects with qualities reminiscent of living subjects contrasts markedly with this. The possibility of programming advanced machines in accordance with one's own wishes, and machines' “ability to learn,” appear to play an important role in the ascription process. Social scientists recognize one of the origins of this development in the meta-process of individualization. From a psychoanalytical perspective, modern Westerners are suffering from relationship fatigue. This fatigue prompts us to endeavor to substitute human relationships with relationships with “nonhumans.” Mediatized communication practices are seen to have a supportive effect in this regard. The article discusses some of these interpretations and contrasts them with an alternative hypothesis on the appeal of artificial companions.","",""
"2015","Social Robotics and Societies of Robots","The sustainability of social robotics, like other ambitious research programs, depends on the identification of lines of inquiry that are coherent with its visionary goals while satisfying more stringent constraints of feasibility and near-term payoffs. Within these constraints, this article outlines one line of inquiry that seems especially viable: development of a society of robots operating within the physical environments of everyday human life, developing rich robot–robot social exchanges, and yet, refraining from any physical contact with human beings. To pursue this line of inquiry effectively, sustained interactions between specialized research communities in robotics are needed. Notably, suitable robotic hand design and control principles must be adopted to achieve proper robotic manipulation of objects designed for human hands that one finds in human habitats. The Pisa-IIT SoftHand project promises to meet these manipulation needs by a principled combination of sensorimotor synergies and soft robotics actuation, which aims at capturing how the biomechanical structure and neural control strategies of the human hand interact so as to simplify and solve both control and sensing problems.","",""
"2015","Integrating Constrained Experiments in Long-Term Human–Robot Interaction Using Task- and Scenario-Based Prototyping","In order to investigate how the use of robots may impact everyday tasks, twelve participants in our study interacted with a University of Hertfordshire Sunflower robot over a period of 8 weeks in the university's Robot House. Participants performed two constrained tasks, one physical and one cognitive, four times over this period. Participant responses were recorded using a variety of measures including the System Usability Scale and the NASA Task Load Index. The use of the robot had an impact on the experienced workload of the participants diﬀerently for the two tasks, and this eﬀect changed over time. In the physical task, there was evidence of adaptation to the robot's behavior. For the cognitive task, the use of the robot was experienced as more frustrating in the later weeks.","",""
"2015","Introduction to the Special Issue “Beyond Industrial Robotics: Social Robots Entering Public and Domestic Spheres”","Industrial and domestic robotics provide fascinating and relevant perspective insights into current and possible trajectories for the development of contemporary societies. While industrial robotics has found its place since the 1960s, domestic robotics wherein humans interact with social robots is still an unsettled area. After reviewing data on the diffusion of social robots and on their use, the historical tradition from which social robots come is discussed. This discussion is followed by an analysis of the penetration of social robots in everyday life and the importance of interdisciplinary research is highlighted.","",""
"2015","Segmental Analysis-Based Authorship Discrimination between the Holy Quran and Prophet’s Statements","Stylometry has got a lot of interest during these recent years because it solved many authorship problems and disputes that were difficult to handle. Author discrimination consists in checking whether two texts are written by the same author or not. In this investigation, we try to make an author discrimination between the Quran (The holy words and statements of God in the Islamic religion) and the Hadith (statements said by the prophet Muhammad) in a segmental form. In fact, 14 text segments are extracted from the Quran book and 11 text segments are extracted from the Bukhari Hadith. These segments have more or less the same size in terms of words and the medium size is about 2080 words per text segment. The Quran is taken in its entirety, whereas for the Prophet’s statements, we chose only the certified texts of the Bukhari book.  That is, four series of experiments are done and commented. The first series of experiments concerns several experiments of authorship attribution using different state of the art features and classifiers, the second series of experiments analyses the different texts by using a new parameter called COST, the third series of experiments consists in an authorship discrimination using the frequency of a particular word (“الذين ” meaning those/who in English) and the fourth series of experiments performs a hierarchical clustering on the 25 text segments, in order to assess the real number of clusters (author styles) and to see if the hypothesis of a unique author is possible.  This investigation, which represents the continuation of a previous research work on the same topic [Sayoud 2012-a], has further clarified an old enigma, which was impossible to solve for fourteen centuries: all the results of this investigation show unanimously that the two books should have two different authors.","",""
"2015","A co z humanistami? – Cyfrowa humanistyka jako odpowiedź na obecny stan postrzegania humanistyki w polskojęzycznym internecie","Rozpoczynając od dośc lakonicznych pytan chcialabym na chwile skupic uwage wlaśnie na humanistach i zachecic do pewnej refleksji. Otoz od pewnego czasu zauwaza sie coraz wieksze zainteresowanie przedsiebiorczością naukowcow. Mowi sie o tym juz nie tylko w mediach, ale coraz cześciej pod strzechami samych uczelni wyzszych. Odbywają sie rozne szkolenia, warsztaty oraz konferencje, podczas ktorych zacheca sie do budowania wspolpracy uczelnia – biznes. Powstają raporty i strategie ukazujące mozliwości rozwoju. Mlodzi badacze, z resztą nie tylko oni, są zachecani do prowadzenia innowacyjnych i nowoczesnych badan, na ktore znajdą sie realni odbiorcy z sektora gospodarki. Wskazuje sie, ze ma to duze znaczenie dla rozwoju panstwa – szczegolnie jego konkurencyjności. I nie mozna sie z tym nie zgodzic, ale od razu nasuwa sie pytanie, gdzie w tym wszystkim jest miejsce dla humanistow? Jak oni mogą sie w to wlączyc? W jakim kierunku powinna sie rozwijac humanistyka? Czy rozwoj cyfrowej humanistyki i podnoszenie kompetencji medialnych moze coś zmienic w obszarze, ktory odstaje od wspolczesnych trendow?","",""
"2015","Embodied cognition and the Orwell’s problem in cognitive science","","",""
"2015","Simulating effects of signage, groups, and crowds on emergent evacuation patterns","","",""
"2015","OSWI: a consulting system for pupils and prospective students on the basis of neural networks","","",""
"2015","User perceptions of anthropomorphic robots as monitoring devices","","",""
"2015","Kant and the simulation hypothesis","","",""
"2015","Introduction to the special issue on agent-based modelling for policy engineering","","",""
"2015","The emergence of attractors under multi-level institutional designs: agent-based modeling of intergovernmental decision making for funding transportation projects","","",""
"2015","Why option generation matters for the design of autonomous e-coaching systems","","",""
"2015","Theorizing change in artificial intelligence: inductivising philosophy from economic cognition processes","","",""
"2015","Crime detection and criminal identification in India using data mining techniques","","",""
"2015","An agent-based simulation for restricting exploitation in electronic societies through social mechanisms","","",""
"2015","Special issue: AI and next-generation supply networks","","",""
"2015","The pursuit of computational justice in open systems","","",""
"2015","The effect of a ticking clock on task performance","","",""
"2015","Automatic apple grading model development based on back propagation neural network and machine vision, and its performance evaluation","","",""
"2015","The fictionality of topic modeling: Machine reading Anthony Trollope's Barsetshire series"," This essay describes how using unsupervised topic modeling (specifically the latent Dirichlet allocation topic modeling algorithm in MALLET) on relatively small corpuses can help scholars of literature circumvent the limitations of some existing theories of the novel. Using an example drawn from work on Victorian novelist Anthony Trollope's Barsetshire series, it argues that unsupervised topic modeling's counter-factual and retrospective reconstruction of the topics out of which a given set of novels have been created allows for a denaturalizing and unfamiliar (though crucially not “objective” or “unbiased”) view. In other words, topic models are fictions, and scholars of literature should consider reading them as such. Drawing on one aspect of Stephen Ramsay's idea of algorithmic criticism, the essay emphasizes the continuities between “big data” methods and techniques and longer-standing methods of literary study. ","",""
"2015","Welcome to the Electrocene, an Algorithmic Agartha","","",""
"2015","Where are the ‘key’ words?  Optimizing multimedia textual attributes to improve viewership","Multimedia content is central to our experience on the Web. Specifically, users frequently search and watch videos online. The textual features that accompany such content (e.g., title, description, and tags) can generally be optimized to attract more search traffic and ultimately to increase the advertisement-generated revenue.This study investigates whether automating tag selection for online video content with the goal of increasing viewership is feasible. In summary, it shows that content producers can lower their operational costs for tag selection using a hybrid approach that combines dedicated personnel (often known as ‘channel managers’), crowdsourcing, and automatic tag suggestions. More concretely, this work provides the following insights: first, it offers evidence that existing tags for a sample of YouTube videos can be improved; second, this study shows that an automated tag recommendation process can be efficient in practice; and, finally it explores the impact of using information mined from various data sources associated with content items on the quality of the resulting tags.","",""
"2015","Abstraction, indirection, and Sevareid's Law: Towards benign computing","Computing is one of the primary means by which we solve problems in society today. In this short paper we examine the implications of the primary techniques used in computer systems work — abstraction and indirection — and of Sevareid’s Law, an epigram that suggests that our problem-solving instinct may be leading us astray. We explore the context of this dilemma and discuss instances in which this has arisen in the recent past. We then consider a few design options and changes to the normal mode of computer science practice that might enable us to sidestep the implications of Sevareid’s Law.","",""
"2015","Foreword","TRI AGE ( trē-äzh, trēäzh) {Fr. “sorting”} 1. the sorting out and classification of casualties of war or other disaster, to determine priority of need and proper place of treatment. 2. by extension, the sorting and prioritizing of patients or treatment in nonemergency health care settings—Dorland’s Medical Dictionary, 30th edition (2003). At this moment, an emergency medical technician in the United States is triaging an injured patient, deciding whether or not a patient requires care at a trauma center. This is a life or death decision. The profound importance of field triage has been reinforced by recent evidence that care at a trauma center lowers the risk of death by 25% for injured patients compared with treatment received at non-trauma centers.1","",""
"2015","The limits of pretending","Abstract We propose that pretending is a cognitive faculty which enables us to create and immerse ourselves in possible worlds. These worlds range from the veridical to the fantastic and are frequently realised as stories varying from the fictional to the scientific. This same ability enables us to become immersed and engaged in such stories (which we may have created) too. Whether we are shooting “aliens” or are engaged in a passionate romance, these experiences are facilitated by our ability to pretend. While it might seem that we can imagine or make-believe anything, in practice there are limits to what we can pretend. We draw upon both theoretical perspectives and from the work practice of animators. By identifying these limits, we are, of course, also defining the nature of pretending.","",""
"2015","FCJ-185 An Algorithmic Agartha: Post-App Approaches to Synarchic Regulation","","",""
"2016","Brains, Tortoises, and Octopuses: Postwar Interpretations of Mechanical Intelligence on the BBC","    The immediate postwar period saw the emergence of the first digital computers as well as developments in cybernetics, brain research, and information theory. In this era, questions of mechanical intelligence came to the forefront in public media. In Britain the BBC broadcast radio talks by many leading practitioners in these fields in which they discussed their work and speculated on its implications for conceptions of intelligence. Generally, speakers were either skeptical or unskeptical toward the issue of intelligent behavior in machines. The skeptics, who tended to have backgrounds in physical science and mathematics, usually took reductive approaches to argue that machines could not be intelligent. The non-skeptics, who tended to have a biological orientation, usually avoided reductive approaches and conceded that the distinction between machines and animal brains might not be clear-cut. These differing interpretations of new technology, their association with distinct intellectual traditions, and their promotion via a wide-reaching and respected medium are seen as instances of social shaping of technology in action.  ","",""
"2016","Brains, Tortoises, and Octopuses:           Postwar Interpretations of Mechanical           Intelligence on the BBC","The immediate postwar period saw the emergence of the first digital computers as well as developments in cybernetics, brain research, and information theory. In this era, questions of mechanical intelligence came to the forefront in public media. In Britain the BBC broadcast radio talks by many leading practitioners in these fields in which they discussed their work and speculated on its implications for conceptions of intelligence. Generally, speakers were either skeptical or unskeptical toward the issue of intelligent behavior in machines. The skeptics, who tended to have backgrounds in physical science and mathematics, usually took reductive approaches to argue that machines could not be intelligent. The non-skeptics, who tended to have a biological orientation, usually avoided reductive approaches and conceded that the distinction between machines and animal brains might not be clear-cut. These differing interpretations of new technology, their association with distinct intellectual traditions, and their promotion via a wide-reaching and respected medium are seen as instances of social shaping of technology in action.","",""
"2016","Talking to Bots: Symbiotic Agency and the Case of Tay","","",""
"2016","When the algorithm itself is a racist: Diagnosing ethical harm in the basic Components of Software","","",""
"2016","Automation, Algorithms, and Politics | When the Algorithm Itself is a Racist: Diagnosing Ethical Harm in the Basic Components of Software","Computer algorithms organize and select information across a wide range of applications and industries, from search results to social media. Abuses of power by Internet platforms have led to calls for algorithm transparency and regulation. Algorithms have a particularly problematic history of processing information about race. Yet some analysts have warned that foundational computer algorithms are not useful subjects for ethical or normative analysis due to complexity, secrecy, technical character, or generality. We respond by investigating what it is an analyst needs to know to determine whether the algorithm in a computer system is improper, unethical, or illegal in itself. We argue that an “algorithmic ethics” can analyze a particular published algorithm. We explain the importance of developing a practical algorithmic ethics that addresses virtues, consequences, and norms: We increasingly delegate authority to algorithms, and they are fast becoming obscure but important elements of social structure.","",""
"2016","About the Linear Complexity of the Almost Perfect Sequences","We calculate the linear complexity of almost perfect binary sequences. Also we study the linear complexity of binary sequences obtained from series of almost perfect ternary sequences and the ternary sequences with two nonzero autocorrelation sidelobe levels.","",""
"2016","Toward a Common Standard for Aid Transparency","","",""
"2016","Depth Image Based 3D Object Model Generation","This research studied the method of creating a complete 3D model by obtaining the location and r, g, b values of an object from the object s color and depth map images, and conducting the point based 3D modeling in OpenGL, and, finally, by filling the holes on the front or side according to the depth value. The most important aspect of the method is filling the front and sides of the object with differential hole-filling method. In addition, the 3D model could be viewed through 3D glasses by extracting left, right images from the 3D TV, and a view that corresponds to the location of the user could be supplied to the user by locating the user with an OptiTrack camera.","",""
"2016","Design and Implementation of a Novel ECG Teleconsultation Platform","","",""
"2016","Brain–computer interfaces and dualism: a problem of brain, mind, and body","","",""
"2016","From an engineers point of view: response to “Social interaction with robots—three questions”. In Gesa Lindemann (this volume)","","",""
"2016","Effects of lying in practical Turing tests","","",""
"2016","Legal regulations and public policies for next-generation robots in Japan","","",""
"2016","Robots in aged care: a dystopian future?","","",""
"2016","Artificial stupidity","","",""
"2016","Auto-Catastrophic Theory: the necessity of self-destruction for the formation, survival, and termination of systems","","",""
"2016","The autonomy-safety-paradox of service robotics in Europe and Japan: a comparative analysis","","",""
"2016","Social interaction with robots: three questions","","",""
"2016","Toward a better self-regulation: degree of certainty through fuzzy logic in a formative assessment","","",""
"2016","Investigating and designing social robots from a role-theoretical perspective: Response to “Social interaction with robots—three questions”. In Gesa Lindemann (this volume)","","",""
"2016","The problem of ascribing legal responsibility in the case of robotics","","",""
"2016","Social robots: Things or agents?","","",""
"2016","The model gap: cognitive systems in security applications and their ethical implications","","",""
"2016","Limits and opportunities for mathematizing communicational conduct for social robotics in the real world? Toward enabling a robot to make use of the human’s competences","","",""
"2016","Human, machines, and the interpretation of formal systems","","",""
"2016","On the ethical framing of research programs in robotics","","",""
"2016","‘It looks like a human!’ The interrelation of social presence, interaction and agency ascription: a case study about the effects of an android robot on social agency ascription","","",""
"2016","Care robots and the future of ICT-mediated elderly care: a response to doom scenarios","","",""
"2016","Racing to the precipice: a model of artificial intelligence development","","",""
"2016","Special issue on: Going beyond the laboratory—reconsidering the ELS implications of autonomous robots","","",""
"2016","Considerations about the relationship between animal and machine ethics","","",""
"2016","The importance of a human viewpoint on computer natural language capabilities: a Turing test perspective","","",""
"2016","Artificial super intelligence: beyond rhetoric","","",""
"2016","Erratum to: Artificial super intelligence: beyond rhetoric","","",""
"2016","Socializing robots: constructing robotic sociality in the design and use of the assistive robot PARO","","",""
"2016","Simulation, self-extinction, and philosophy in the service of human civilization","","",""
"2016","Algorithmic paranoia and the convivial alternative"," In a time of big data, thinking about how we are seen and how that affects our lives means changing our idea about who does the seeing. Data produced by machines is most often ‘seen’ by other machines; the eye is in question is algorithmic. Algorithmic seeing does not produce a computational panopticon but a mechanism of prediction. The authority of its predictions rests on a slippage of the scientific method in to the world of data. Data science inherits some of the problems of science, especially the disembodied ‘view from above’, and adds new ones of its own. As its core methods like machine learning are based on seeing correlations not understanding causation, it reproduces the prejudices of its input. Rising in to the apparatuses of governance, it reinforces the problematic sides of ‘seeing like a state’ and links to the recursive production of paranoia. It forces us to ask the question ‘what counts as rational seeing?’. Answering this from a position of feminist empiricism reveals different possibilities latent in seeing with machines. Grounded in the idea of conviviality, machine learning may reveal forgotten non-market patterns and enable free and critical learning. It is proposed that a programme to challenge the production of irrational pre-emption is also a search for the possibility of algorithmic conviviality. ","",""
"2016","How the machine ‘thinks’: Understanding opacity in machine learning algorithms"," This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm. ","",""
"2016","The ethics of algorithms: Mapping the debate"," In information societies, operations, decisions and choices previously left to humans are increasingly delegated to algorithms, which may advise, if not decide, about how data should be interpreted and what actions should be taken as a result. More and more often, algorithms mediate social processes, business transactions, governmental decisions, and how we perceive, understand, and interact among ourselves and with the environment. Gaps between the design and operation of algorithms and our understanding of their ethical implications can have severe consequences affecting individuals as well as groups and whole societies. This paper makes three contributions to clarify the ethical importance of algorithmic mediation. It provides a prescriptive map to organise the debate. It reviews the current discussion of ethical aspects of algorithms. And it assesses the available literature in order to identify areas requiring further work to develop the ethics of algorithms. ","",""
"2016","‘A mechanistic interpretation, if possible’: How does predictive modelling causality affect the regulation of chemicals?"," The regulation of chemicals is undergoing drastic changes with the use of computational models to predict environmental toxicity. This particular issue has not attracted much attention, despite its major impacts on the regulation of chemicals. This raises the problem of causality at the crossroads between data and regulatory sciences, particularly in the case models known as quantitative structure–activity relationship models. This paper shows that models establish correlations and not scientific facts, and it engages anew the way regulators deal with uncertainties. It does so by exploring the tension and problems raised by the possibility of causal explanation afforded by quantitative structure–activity relationship models. It argues that the specificity of predictive modelling promotes rethinking of the regulation of chemicals. ","",""
"2016","Algorithms and their others: Algorithmic culture in context"," Algorithms, once obscure objects of technical art, have lately been subject to considerable popular and scholarly scrutiny. What does it mean to adopt the algorithm as an object of analytic attention? What is in view, and out of view, when we focus on the algorithm? Using Niklaus Wirth's 1975 formulation that “algorithms + data structures = programs” as a launching-off point, this paper examines how an algorithmic lens shapes the way in which we might inquire into contemporary digital culture. ","",""
"2016","Word Processor Art: How User-friendly Inhibits Creativity.","","",""
"2016","Language DNA: Visualizing a Language Decomposition.","","",""
"2017","A Cost-Saving Machine: Computing at the German Allianz Insurance Company","","",""
"2017","Adaptive Edge-Directed Interpolation Using Edge Map Analysis","In this paper, an edge-directed interpolation is proposed using edge map analysis. Conventional algorithms have various disadvantages (edge blurring effect, excessive computational complexity, etc.) by performing interpolation without distinguishing between flat and edge regions. Therefore, it is possible to overcome the drawbacks and limitations of the existing algorithms by applying the optimal interpolation technique to the flat area and the edge area through edge map analysis. The comparison of objective and subjective experimental results shows that the proposed algorithm shows better performance than the existing","",""
"2017","Negativity and economic efficacy","","",""
"2017","Algorithms (and the) everyday","ABSTRACT Our everyday practices are increasingly mediated through online technologies, entailing the navigation and also oft-simultaneous creation of large quantities of information and communication data. The scale and types of activities being undertaken, the data that are being created and engaged with, and the possibilities for analysis, archiving and distribution are now so extensive that technical constructs are necessarily required as a way to manage, interpret and distribute these. These constructs include the platforms, the software, the codes and the algorithms. This paper explores the place of the algorithm in shaping and engaging with the contemporary everyday. It does this via an exploration of some particular instances of algorithmic sorting and presentation as well as considering some of the ways these contribute to shaping our everyday practices and understandings. In doing so, it raises questions about understandings of agency and power, shifting world views and our complex relationship with technologies.","",""
"2017","The algorithmic imaginary: exploring the ordinary affects of Facebook algorithms","ABSTRACT This article reflects the kinds of situations and spaces where people and algorithms meet. In what situations do people become aware of algorithms? How do they experience and make sense of these algorithms, given their often hidden and invisible nature? To what extent does an awareness of algorithms affect people's use of these platforms, if at all? To help answer these questions, this article examines people's personal stories about the Facebook algorithm through tweets and interviews with 25 ordinary users. To understand the spaces where people and algorithms meet, this article develops the notion of the algorithmic imaginary. It is argued that the algorithmic imaginary – ways of thinking about what algorithms are, what they should be and how they function – is not just productive of different moods and sensations but plays a generative role in moulding the Facebook algorithm itself. Examining how algorithms make people feel, then, seems crucial if we want to understand their social power.","",""
"2017","Algorithmic IF  … THEN rules and the conditions and consequences of power","ABSTRACT The introduction to this special issue suggests we need to develop ‘a greater understanding of what might be thought of as the social power of algorithms'. In this paper, ‘social power’ will be critically scrutinised through a study of the entanglement of algorithmic rules with contemporary video-based surveillance technologies. The paper will begin with an analysis of algorithmic ‘IF … THEN’ rules and the conditions (IF) and consequences (THEN) that need to be accomplished for an algorithm to be said to succeed. The work of achieving conditions and consequences demonstrates that the form of ‘power’ in focus is not solely attributable to the algorithm as such, but operates through distributed agency and can be noted as a network effect. That is, the conditions and consequences of algorithmic rules only come into being through the careful plaiting of relatively unstable associations of people, things, processes, documents and resources. From this we can say that power is not primarily social in the sense that algorithms alone create an impact on society, but social in the sense of power being derived through algorithmic associations. The paper argues that this kind of power is most clearly visible in moments of breakdown, failure or other forms of trouble, whereby algorithmic conditions and consequences are not met and the careful plaiting of associations has to be brought to the fore and examined. It is through such examinations that the associational dependencies more than the social power of algorithms are made apparent.","",""
"2017","Algorithmically recognizable: Santorum’s Google problem, and Google’s Santorum problem","ABSTRACT Because information algorithms make judgments that can have powerful consequences, those interested in having their information selected will orient themselves toward these algorithmic systems, making themselves algorithmically recognizable, in the hopes that they will be amplified by them. Examining this interplay, between information intermediaries and those trying to be seen by them, connects the study of algorithmic systems to long-standing concerns about the power of intermediaries – not an algorithmic power, uniquely, but the power to grant visibility and certify meaning, and the challenge of discerning who to grant it to and why. Here, I consider Dan Savage’s attempt to redefine the name of U.S. Senator Rick Santorum, a tactical intervention that topped Google’s search results for nearly a decade, and then mysteriously dropped during the 2012 Republican nominations. Changes made to Google’s algorithm at the time may explain the drop; here, they help to reveal the kind of implicitly political distinctions search engines must invariably make, between genuine patterns of participation and tactical efforts to approximate them.","",""
"2017","Thinking critically about and researching algorithms","ABSTRACT More and more aspects of our everyday lives are being mediated, augmented, produced and regulated by software-enabled technologies. Software is fundamentally composed of algorithms: sets of defined steps structured to process instructions/data to produce an output. This paper synthesises and extends emerging critical thinking about algorithms and considers how best to research them in practice. Four main arguments are developed. First, there is a pressing need to focus critical and empirical attention on algorithms and the work that they do given their increasing importance in shaping social and economic life. Second, algorithms can be conceived in a number of ways – technically, computationally, mathematically, politically, culturally, economically, contextually, materially, philosophically, ethically – but are best understood as being contingent, ontogenetic and performative in nature, and embedded in wider socio-technical assemblages. Third, there are three main challenges that hinder research about algorithms (gaining access to their formulation; they are heterogeneous and embedded in wider systems; their work unfolds contextually and contingently), which require practical and epistemological attention. Fourth, the constitution and work of algorithms can be empirically studied in a number of ways, each of which has strengths and weaknesses that need to be systematically evaluated. Six methodological approaches designed to produce insights into the nature and work of algorithms are critically appraised. It is contended that these methods are best used in combination in order to help overcome epistemological and practical challenges.","",""
"2017","The social power of algorithms","ABSTRACT This article explores the questions associated with what might be thought of as the social power of algorithms. The article, which introduces a special issue on the same topic, begins by reflecting on how we might approach algorithms from a social scientific perspective. The article is then split into two sections. The first deals with the issues that might be associated with an analysis of the power of the algorithms themselves. This section outlines a series of issues associated with the functionality of the algorithms and how these functions are powerfully deployed within social world. The second section then focuses upon the notion of the algorithm. In this section, the article argues that we need to look beyond the algorithms themselves, as a technical and material presence, to explore how the notion or concept of the algorithm is also an important feature of their potential power. In this section, it is suggested that we look at the way that notions of the algorithm are evoked as a part of broader rationalities and ways of seeing the world. Exploring the notion of the algorithm may enable us to see how algorithms also play a part in social ordering processes, both in terms of how the algorithm is used to promote certain visions of calculative objectivity and also in relation to the wider governmentalities that this concept might be used to open up.","",""
"2017","Infinite distraction","","",""
"2017","Scrutinizing an algorithmic technique: the Bayes classifier as interested reading of reality","ABSTRACT This paper outlines the notion of ‘algorithmic technique’ as a middle ground between concrete, implemented algorithms and the broader study and theorization of software. Algorithmic techniques specify principles and methods for doing things in the medium of software and they thus constitute units of knowledge and expertise in the domain of software making. I suggest that algorithmic techniques are a suitable object of study for the humanities and social science since they capture the central technical principles behind actual software, but can generally be described in accessible language. To make my case, I focus on the field of information ordering and, first, discuss the wider historical trajectory of formal or ‘mechanical’ reasoning applied to matters of commerce and government before, second, moving to the investigation of a particular algorithmic technique, the Bayes classifier. This technique is explicated through a reading of the original work of M. E. Maron in the early 1960 and presented as a means to subject empirical, ‘datafied’ reality to an interested reading that confers meaning to each variable in relation to an operational goal. After a discussion of the Bayes classifier in relation to the question of power, the paper concludes by coming back to its initial motive and argues for increased attention to algorithmic techniques in the study of software.","",""
"2017","Governance by algorithms: reality construction by algorithmic selection on the Internet"," This article explores the governance by algorithms in information societies. Theoretically, it builds on (co-)evolutionary innovation studies in order to adequately grasp the interplay of technological and societal change and combines these with institutional approaches to incorporate governance by technology or rather software as institutions. Methodologically, it draws from an empirical survey of Internet-based services that rely on automated algorithmic selection, a functional typology derived from it, and an analysis of associated potential social risks. It shows how algorithmic selection has become a growing source of social order, of a shared social reality in information societies. It argues that – similar to the construction of realities by traditional mass media – automated algorithmic selection applications shape daily lives and realities, affect the perception of the world, and influence behavior. However, the co-evolutionary perspective on algorithms as institutions, ideologies, intermediaries, and actors highlights differences that are to be found, first, in the growing personalization of constructed realities and, second, in the constellation of involved actors. Altogether, compared to reality construction by traditional mass media, algorithmic reality construction tends to increase individualization, commercialization, inequalities, and deterritorialization and to decrease transparency, controllability, and predictability. ","",""
"2017","Singularitarianism and schizophrenia","","",""
"2017","Uncommon voices of AI","","",""
"2017","Nudging for good: robots and the ethical appropriateness of nurturing empathy and charitable behavior","","",""
"2017","How to study public imagination of autonomous systems: the case of the Helsinki automated metro","","",""
"2017","Preparing for the future of Artificial Intelligence","","",""
"2017","Machine humour: examples from Turing test experiments","","",""
"2017","The cultural influence model: when accented natural language spoken by virtual characters matters","","",""
"2017","Culture-specific models of negotiation for virtual characters: multi-attribute decision-making based on culture-specific values","","",""
"2017","On the promotion of safe and socially beneficial artificial intelligence","","",""
"2017","Cognitive bearing of techno-advances in Kashmiri carpet designing","","",""
"2017","Programming Machine Ethics by Luís Moniz Pereira and Ari Saptawijaya","","",""
"2017","Fractal computer visualization in psychological research","","",""
"2017","Erratum to: Arguments from authority and expert opinion in computational argumentation systems","","",""
"2017","Brain research and the social self in a technological culture","","",""
"2017","Speak to me and I tell you who you are! A language-attitude study in a cultural-heritage application","","",""
"2017","‘What I see is not what you get’: why culture-specific behaviours for virtual characters should be user-tested across cultures","","",""
"2017","From Alan Turing to modern AI: practical solutions and an implicit epistemic stance","","",""
"2017","The case of classroom robots: teachers’ deliberations on the ethical tensions","","",""
"2017","Introduction: culturally motivated virtual characters and Connect-Universum","","",""
"2017","Arguments from authority and expert opinion in computational argumentation systems","","",""
"2017","Eloquence of eyes and mouth of virtual agents: cultural study of facial expression perception","","",""
"2017","Can naturalism explain consciousness? A critique","","",""
"2017","Digital technologies and artificial intelligence’s present and foreseeable impact on lawyering, judging, policing and law enforcement","","",""
"2017","Algorithmic governance: Developing a research agenda through the power of collective intelligence","We are living in an algorithmic age where mathematics and computer science are coming together in powerful new ways to influence, shape and guide our behaviour and the governance of our societies. As these algorithmic governance structures proliferate, it is vital that we ensure their effectiveness and legitimacy. That is, we need to ensure that they are an effective means for achieving a legitimate policy goal that are also procedurally fair, open and unbiased. But how can we ensure that algorithmic governance structures are both? This article shares the results of a collective intelligence workshop that addressed exactly this question. The workshop brought together a multidisciplinary group of scholars to consider (a) barriers to legitimate and effective algorithmic governance and (b) the research methods needed to address the nature and impact of specific barriers. An interactive management workshop technique was used to harness the collective intelligence of this multidisciplinary group. This method enabled participants to produce a framework and research agenda for those who are concerned about algorithmic governance. We outline this research agenda below, providing a detailed map of key research themes, questions and methods that our workshop felt ought to be pursued. This builds upon existing work on research agendas for critical algorithm studies in a unique way through the method of collective intelligence.","",""
"2017","Algorithmic rationality: Epistemology and efficiency in the data sciences"," Recently, philosophers and social scientists have turned their attention to the epistemological shifts provoked in established sciences by their incorporation of big data techniques. There has been less focus on the forms of epistemology proper to the investigation of algorithms themselves, understood as scientific objects in their own right. This article, based upon 12 months of ethnographic fieldwork with Russian data scientists, addresses this lack through an investigation of the specific forms of epistemic attention paid to algorithms by data scientists. On the one hand, algorithms are unlike other mathematical objects in that they are not subject to disputation through deductive proof. On the other hand, unlike concrete things in the world such as particles or organisms, algorithms cannot be installed as the objects of experimental systems directly. They can only be evaluated in their functioning as components of extended computational assemblages; on their own, they are inert. As a consequence, the epistemological coding proper to this evaluation does not turn on truth and falsehood but rather on the efficiency of a given algorithmic assemblage. This article suggests that understanding the forms of algorithmic rationality employed in such inquiry is crucial for charting the place of data science within the contemporary academy and knowledge economy more generally. ","",""
"2017","Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data","Decisions based on algorithmic, machine learning models can be unfair, reproducing biases in historical data used to train them. While computational techniques are emerging to address aspects of these concerns through communities such as discrimination-aware data mining (DADM) and fairness, accountability and transparency machine learning (FATML), their practical implementation faces real-world challenges. For legal, institutional or commercial reasons, organisations might not hold the data on sensitive attributes such as gender, ethnicity, sexuality or disability needed to diagnose and mitigate emergent indirect discrimination-by-proxy, such as redlining. Such organisations might also lack the knowledge and capacity to identify and manage fairness issues that are emergent properties of complex sociotechnical systems. This paper presents and discusses three potential approaches to deal with such knowledge and information deficits in the context of fairer machine learning. Trusted third parties could selectively store data necessary for performing discrimination discovery and incorporating fairness constraints into model-building in a privacy-preserving manner. Collaborative online platforms would allow diverse organisations to record, share and access contextual and experiential knowledge to promote fairness in machine learning systems. Finally, unsupervised learning and pedagogically interpretable algorithms might allow fairness hypotheses to be built for further selective testing and exploration. Real-world fairness challenges in machine learning are not abstract, constrained optimisation problems, but are institutionally and contextually grounded. Computational fairness tools are useful, but must be researched and developed in and with the messy contexts that will shape their deployment, rather than just for imagined situations. Not doing so risks real, near-term algorithmic harm.","",""
"2017","Algorithms in practice: Comparing web journalism and criminal justice","Big Data evangelists often argue that algorithms make decision-making more informed and objective—a promise hotly contested by critics of these technologies. Yet, to date, most of the debate has focused on the instruments themselves, rather than on how they are used. This article addresses this lack by examining the actual practices surrounding algorithmic technologies. Specifically, drawing on multi-sited ethnographic data, I compare how algorithms are used and interpreted in two institutional contexts with markedly different characteristics: web journalism and criminal justice. I find that there are surprising similarities in how web journalists and legal professionals use algorithms in their work. In both cases, I document a gap between the intended and actual effects of algorithms—a process I analyze as “decoupling.” Second, I identify a gamut of buffering strategies used by both web journalists and legal professionals to minimize the impact of algorithms in their daily work. Those include foot-dragging, gaming, and open critique. Of course, these similarities do not exhaust the differences between the two cases, which are explored in the discussion section. I conclude with a call for further ethnographic work on algorithms in practice as an important empirical check against the dominant rhetoric of algorithmic power.","",""
"2017","Algorithms as culture: Some tactics for the ethnography of algorithmic systems","This article responds to recent debates in critical algorithm studies about the significance of the term “algorithm.” Where some have suggested that critical scholars should align their use of the term with its common definition in professional computer science, I argue that we should instead approach algorithms as “multiples”—unstable objects that are enacted through the varied practices that people use to engage with them, including the practices of “outsider” researchers. This approach builds on the work of Laura Devendorf, Elizabeth Goodman, and Annemarie Mol. Different ways of enacting algorithms foreground certain issues while occluding others: computer scientists enact algorithms as conceptual objects indifferent to implementation details, while calls for accountability enact algorithms as closed boxes to be opened. I propose that critical researchers might seek to enact algorithms ethnographically, seeing them as heterogeneous and diffuse sociotechnical systems, rather than rigidly constrained and procedural formulas. To do so, I suggest thinking of algorithms not “in” culture, as the event occasioning this essay was titled, but “as” culture: part of broad patterns of meaning and practice that can be engaged with empirically. I offer a set of practical tactics for the ethnographic enactment of algorithmic systems, which do not depend on pinning down a singular “algorithm” or achieving “access,” but which rather work from the partial and mobile position of an outsider.","",""
"2017","A not quite random walk: Experimenting with the ethnomethods of the algorithm","Algorithms have become a widespread trope for making sense of social life. Science, finance, journalism, warfare, and policing—there is hardly anything these days that has not been specified as “algorithmic.” Yet, although the trope has brought together a variety of audiences, it is not quite clear what kind of work it does. Often portrayed as powerful yet inscrutable entities, algorithms maintain an air of mystery that makes them both interesting and difficult to understand. This article takes on this problem and examines the role of algorithms not as techno-scientific objects to be known, but as a figure that is used for making sense of observations. Following in the footsteps of Harold Garfinkel’s tutorial cases, I shall illustrate the implications of this view through an experiment with algorithmic navigation. Challenging participants to go on a walk, guided not by maps or GPS but by an algorithm developed on the spot, I highlight a number of dynamics typical of reasoning with running code, including the ongoing respecification of rules and observations, the stickiness of the procedure, and the selective invocation of the algorithm as an intelligible object. The materials thus provide an opportunity to rethink key issues at the intersection of the social sciences and the computational, including popular concerns with transparency, accountability, and ethics.","",""
"2017","Toward human-centered algorithm design","As algorithms pervade numerous facets of daily life, they are incorporated into systems for increasingly diverse purposes. These systems’ results are often interpreted differently by the designers who created them than by the lay persons who interact with them. This paper offers a proposal for human-centered algorithm design, which incorporates human and social interpretations into the design process for algorithmically based systems. It articulates three specific strategies for doing so: theoretical, participatory, and speculative. Drawing on the author’s work designing and deploying multiple related systems, the paper provides a detailed example of using a theoretical approach. It also discusses findings pertinent to participatory and speculative design approaches. The paper addresses both strengths and challenges for each strategy in helping to center the process of designing algorithmically based systems around humans.","",""
"2017","Algorithmic Value: Cultural Encoding, Textuality, and the Myth of “Source Code”","In this article, I provide a Spivakian analysis of computational algorithms. Building upon Gayatri Spivak’s claim that the value coding of late capitalism extends beyond the economic realm to the cultural and affective, I show that it seeps into the algorithmic as well. The recent proliferation of algorithmic applications has been met by an increased scholarly interest in their underlying mechanisms. Several critics of predictive algorithms, for instance, proceed as though the racial and gender discrimination that a given algorithm enacts upon execution can be positively attributed to -- and mitigated through a re-coding of – either its training data or its “source code.” There is little denying that the logic of computation undergirds much of our sociality. But, as I argue in this article, to concentrate the source of an algorithm’s action in its semiotic representations is to hide and legitimize the value codings that lend these representations their efficacy. My aim in this article is two-fold. First, to show how seemingly benign investments in algorithms can reproduce, in a larger network, the exploitative value systems that manage the worth of knowledge, epistemologies, labor, and bodies. Second, to raise a question of methodology: What antitechnocratic, nonhegemonic engagements with algorithms might feminists produce that do not privilege the algorithmic as a site of intervention","",""
"2017","The Art of Failure in Robotics: Queering the (Un)Making of Success and Failure in the Companion Robot Laboratory","This article investigates an emerging class of contemporary machines: the robot companion. It is introduced as a robot that will accompany ‘us’ in ‘our’ human everyday lives. This article analyzes one example of how robot companionship is realized while querying how this realization might imply a change in how ‘we’ conceive of human/machine relations. Drawing on central insights into the making of the humanoid Armar, the author develops an approach to emerging human/machine relations through affects, more precisely through the affective strategies and affective labors taking place in the robotics laboratory. She furthermore suggests taking a posthumanist perspective on the analysis, which entails becoming attentive to the intra-active co-production between human and machine. Importantly, this also allows her to tweak the powerful differentiation between success and failure at work in this specific setting, the robotics laboratory. How can ‘we’ rethink human/machine relations of humanlike interaction through queering success and failure at the robot/human interface? Finally, the author suggests establishing an understanding of laboratory work on the project of the humanlike companion that takes into account the queering potential of failure – centrally by emphasizing the interweaving of knowing and affects, rather than neglecting their connection. At stake seems to be the possibility to develop visions of how to turn the capitalist endeavor of increasing rationalizations of ‘human everyday lives’ into a more responsible and accountable practice of technologization that takes into account the largely neglected dimensions of human/machine relations beyond the success/failure binary.   ","",""
"2017","Being the King Kong of algorithmic culture is a tough job after all"," This article explores the growing importance of algorithms in digital culture and what they could mean for the visibility and interpretation of culture as a whole. Taking Google as a prime example of a company that participates in widespread information overload whilst simultaneously providing some algorithmic answers to it, we show how it exhibits four different regimes of justification: the techno-scientific, economic, political and moral–aesthetic. These efforts to gain legitimacy operate as a network that is both highly performative and adaptive. For instance, Google builds on and translates such justifications in order for its Project Glass to be widely, if not universally, accepted. But there is another influential mode of performativity at work: the mounting criticism of the device. In the 18 months following the public announcement of Glass, we have observed the media phenomenon and passionate debate it has sparked. What Glass represents is being contested on multiple grounds, and this, in turn, indicates that its meanings will likely remain profoundly ambiguous for some time to come. ","",""
"2017","100,000 false positives for every real terrorist: Why anti-terror algorithms don't work","Can terrorist attacks be predicted and prevented using classification algorithms? Can predictive analytics see the hidden patterns and data tracks in the planning of terrorist acts? According to a number of IT firms that now offer programs to predict terrorism using predictive analytics, the answer is yes. According to scientific and application-oriented literature, however, these programs raise a number of practical, statistical and recursive problems. In a literature review and discussion, this paper examines specific problems involved in predicting terrorism. The problems include the opportunity cost of false positives/false negatives, the statistical quality of the prediction and the self-reinforcing, corrupting recursive effects of predictive analytics, since the method lacks an inner meta-model for its own learning- and pattern-dependent adaptation. The conclusion is algorithms don’t work for detecting terrorism and is ineffective, risky and inappropriate, with potentially 100,000 false positives for every real terrorist that the algorithm finds.","",""
"2017","<i>Enheduanna</i>–<i>A Manifesto of Falling</i>: first demonstration of a live brain-computer cinema performance with multi-brain BCI interaction for one performer and two audience members","","",""
"2017","TaDiRAH: a Case Study in Pragmatic Classification","","",""
"2018","“It’s Like Learning a Whole Other Language"""": The Role of Algorithmic Skills in the Curation of Creative Goods","There is increasing concern and scholarship about how algorithms influence users’ online experiences. Yet, little of the work is empirical in nature, leaving many questions about whether users recognize how algorithms affect their online actions and whether they can address the influence of algorithms skillfully. To address this gap, we draw on interviews with creative entrepreneurs from across the United States to examine the extent to which they understand how algorithms may impact their sales success. Participants reveal varying levels of algorithmic skills, or know-how, when it comes to understanding how algorithms influence their content’s visibility. Although many recognize that algorithms affect who sees their wares online, only some know how to set things up so as to improve their chances of reaching potential customers.","",""
"2018","Jonathan Beller, The Message is Murder: Substrates of Computational Capital","","",""
"2018","What algorithms want: imagination in the age of computing","","",""
"2018","Opening the government’s black boxes: freedom of information and algorithmic accountability","ABSTRACT Freedom of information laws are intended to illuminate how governments operate. However, the operations of governments increasingly involve algorithms, such as those used to recommend criminal sentencing and determine eligibility for social services. Algorithms function as ‘black boxes’ that turn inputs into outputs using processes that are often, by design, not transparent. Freedom of information laws allow one potential means for algorithmic transparency. However, whether such laws can be used to access algorithms is unclear. This research examines, in two ways, the availability of government algorithms to the public. First, this study examines laws, regulations, advisory opinions, and court rulings relevant to the disclosure of algorithms. The second part of this study analyzes actual responses by US government agencies to Freedom of Information Act requests for algorithms. This study concludes that governmental policies and practices related to algorithmic disclosure are inconsistent. Such inconsistencies suggest a need for better mechanisms to hold government algorithms accountable.","",""
"2018","Exploiting ability for human adaptation to facilitate improved human-robot interaction and acceptance","ABSTRACT This article reports findings from a usability and user experience evaluations conducted in the last 2 years of a 4-year assistive robotics research project using the Kompai robot. It focuses on the evaluations that were conducted with older adults in an assisted living studio in the United Kingdom (which was arranged as an open plan studio apartment), a UK residential care home, and an older couple's own home in the Netherlands over 2 days. It examines emergent adaptive human behaviour in human-robot interaction (HRI) to consider whether we are approaching the embodiment and functionality of service robots correctly. It discusses possible improvements that could be made at the systems level that better exploit people's natural ability to adapt and find workarounds to technologies and their limitations.","",""
"2018","Introduction to special section “Bridging from user needs to deployed applications of social robots”","Introduction to special section “Bridging from user needs to deployed applications of social robots” Filippo Cavallo, Paolo Dario & Leopoldina Fortunati To cite this article: Filippo Cavallo, Paolo Dario & Leopoldina Fortunati (2018) Introduction to special section “Bridging from user needs to deployed applications of social robots”, The Information Society, 34:3, 127-129, DOI: 10.1080/01972243.2018.1444241 To link to this article: https://doi.org/10.1080/01972243.2018.1444241","",""
"2018","Robot companions: A legal and ethical analysis","ABSTRACT This article addresses some of the most important legal and ethical issues posed by robot companions. Firstly, we clarify that robots are to be deemed objects and more precisely products. This on the one hand excludes the legitimacy of all such considerations involving robots as bearers of own rights and obligations, and forces a functional approach in the analysis. Secondly, pursuant to these methodological considerations we address the most relevant ethical and legal concerns, ranging from the risk of dehumanization and isolation of the user, to privacy and liability concerns, as well as financing of the diffusion of this—still expensive—technology. Solutions are briefly sketched, in order to provide the reader with sufficient indications on what strategies could and should be implemented, already in the design phase, as well as what kind of intervention ought to be favored and expected by national and European legislators. The recent Report with Recommendations to the Commission on Civil Law Rules on Robotics of January 24, 2017 by the European Parliament is specifically taken into account.","",""
"2018","Social robots as cultural objects: The sixth dimension of dynamicity?","ABSTRACT We investigate youths' conceptualization of social robots. Informed by Schudson's theory of the potency of the cultural object, we conducted two studies. The first study centered on essays on social robots written by bachelor's and master's students. The second study centered on prototypes of social robots built by small groups comprised of same students. The essays and prototypes were content analyzed. The results confirm that social robots embody all five dimensions that characterize cultural objects. However, to fully understand this peculiar cultural object, another dimension needs to be introduced: dynamicity.","",""
"2018","Is Falstaff Falstaff? Is Prince Hal Henry V?: Topic Modeling Shakespeare’s Plays","This essay demonstrates how topic modeling can be fruitfully applied to TEI-encoded plays, which allows scholars to analyze speeches by individual characters. Our analysis centers on Shakespeare’s corpus and characters who reappear in multiple plays. Specifically, we use topic models to show that young Prince Hal (in 1 and 2 Henry IV) does not speak the same language as his later self, Henry V (in his titular play): his linguistic shift mirrors his shift in status. Hal himself announces, “I have turned away my former self”—his change in diction bears out his assertion. Conversely, topic models reveal that Falstaff is Falstaff across multiple plays and genres (notably, 1 and 2 Henry IV and The Merry Wives of Windsor), despite scholarly claims to that the Falstaff of comedy is a watered-down version of the braggart drunk of the history plays. Ultimately, we hope that this algorithmically-informed analysis of Shakespeare’s plays is not taken as a final answer, but, instead, as a prompt. As this research reveals, topic modeling plays with attention to each speaker opens the door for new comparisons, and in turn, expands on previous interpretations of literature. Cet essai demontre que les modeles a themes (topic model) peuvent etre appliques avec succes a des pieces encodees en TEI, ce qui permet aux erudits d’analyser le discours de personnages individuels. Notre analyse se concentre sur le corpus de Shakespeare et sur ses personnages qui reapparaissent dans plusieurs pieces. Particulierement, nous employons des modeles a themes pour montrer que le jeune Prince Hal (1 et 2 Henri IV) ne parle pas le meme langage que celui qu’il parle apres etre devenu Henri V (Henri V): son changement linguistique reflete son changement de standing. Hal, lui-meme, annonce: « j’ai renonce a mon passe » —son changement de diction confirme cette affirmation. Inversement, les modeles a themes revelent que Falstaff est Falstaff a travers plusieurs pieces et genres (notamment, 1 et 2 Henri IV et Les Joyeuses Commeres de Windsor), malgre des affirmations erudites que le Falstaff dans la comedie est une version edulcoree du vantard ivre des pieces d’histoire. Finalement, nous esperons que cette analyse algorithmique des pieces de Shakespeare n’est pas consideree comme une solution finale, mais plutot comme une replique. Comme cette recherche le montre, l’usage de modeles a themes pour analyser des pieces, ce qui se concentre sur chaque personnage, offre de nouvelles voies de comparaisons et etoffe donc nos interpretations de la litterature. Mots-cles: Shakespeare; pieces de theâtre; modele thematique; modeles a themes; Falstaff; Prince Hal; Henry V; Henri IV; Joyeuses commeres de Windsor","",""
"2018","Secret Agents","Abstract                 “Good Old-Fashioned Artificial Intelligence” (GOFAI), which was based on a symbolic information-processing model of the mind, has been superseded by neural-network models to describe and create intelligence. Rather than a symbolic representation of the world, the idea is to mimic the structure of the brain in electronic form, whereby artificial neurons draw their own connections during a self-learning process. Critiquing such a brain physiological model, the following article takes up the idea of a “psychoanalysis of things” and applies it to artificial intelligence and machine learning. This approach may help to reveal some of the hidden layers within the current A. I. debate and hints towards a central mechanism in the psycho-economy of our socio-technological world: The question of “Who speaks?”, central for the analysis of paranoia, becomes paramount at a time, when algorithms, in the form of artificial neural networks, operate more and more as secret agents.","",""
"2018","Unconventional Classifiers and Anti-social Machine Intelligences","Abstract                 Artificial intelligence technologies and data structures required for training have become more accessible in recent years and this has enabled artists to incorporate these technologies into their works to various ends. This paper is concerned with the ways in which present day artists are engaging with artificial intelligence, specifically material practices that endeavour to use these technologies and their potential non-human agencies as collaborators with differential objectives to commercial fields. The intentions behind artists’ use of artificial intelligence is varied. Many works, with the accelerating assimilation of artificial intelligence technologies into everyday life, follow a critical path. Such as attempting to unveil how artificial intelligence materially works and is embodied, or to critically work through the potential future adoptions of artificial intelligence technologies into everyday life. However, I diverge from unpacking the criticality of these works and instead follow the suggestion of Bruno Latour to consider their composition. As for Latour, critique implies the capacity to discover a ‘truer’ understanding of reality, whereas composition addresses immanence, how things come together and the emergence of experience. Central to this paper are works that seek to collaborate with artificial intelligence, and to use it to drift out of rather than to affirm or mimic human agency. This goes beyond techniques such as ‘style transfer’ which is seen to support and encode existing human biases or patterns in data. Collaboration with signifies a recognition of a wider field of what constitutes the activity of artistic composition beyond being a singularly human, or AI, act, where composition can be situated in a system. This paper will look at how this approach allows an artist to consider the emerging materiality of a system which they are composing, its resistances and potentials, and the possibilities afforded by the exchange between human and machine intentions in co-composition.","",""
"2018","The Coming Political","Abstract                 Intelligence is the human being’s most striking feature. There is no consensually held scientific understanding of intelligence. The term is no less indeterminate in the sphere of artificial intelligence. Definitions are fluid in both cases. But technical applications and biotechnical developments do not wait for scientific clarity and definitional precision. The near future will bring significant advances in technical and biotechnical areas, including the genetic enhancement of human intelligence (HI) as well as artificial intelligence (AI). I show how developments in both areas will challenge human communities in various ways and that the danger of AI is distinctly political. The argument develops in six steps. (1) I compare and contrast artificial with human intelligence in general and (2) AI with HI genetically modified. Then I correlate and differentiate (3) emergent properties and distributed intelligence, both natural and artificial, as well as (4) neural function, both natural and artificial. (5) Finally, I identify the specifically political capabilities I see in HI and (6) political dangers that AI poses to them.","",""
"2018","Visual Tactics Toward an Ethical Debugging","Abstract                 To advance design research into a critical study of artificially intelligent algorithms, strategies from the fields of critical code studies and data visualisation are combined to propose a methodology for computational visualisation. By opening the algorithmic black box to think through the meaning created by structure and process, computational visualisation seeks to elucidate the complexity and obfuscation at the heart of artificial intelligence systems. There are rising ethical dilemmas that are a consequence of the use of machine learning algorithms in socially sensitive spaces, such as in determining criminal sentencing, job performance, or access to welfare. This is in part due to the lack of a theoretical framework to understand how and why decisions are made at the algorithmic level. The ethical implications are becoming more severe as such algorithmic decision-making is being given higher authority while there is a simultaneous blind spot in where and how biases arise. Computational visualisation, as a method, explores how contemporary visual design tactics including generative design and interaction design, can intersect with a critical exegesis of algorithms to challenge the black box and obfuscation of machine learning and work toward an ethical debugging of biases in such systems.","",""
"2018","Cognition On Tap","Abstract                 The thriving contemporary form of artificial intelligence (AI) called machine learning is often represented sensationally in popular media as a semi-mystical technology. Machine learning systems are frequently ascribed anthropomorphic capacities for learning, emoting and reasoning which, it is suggested, might lead to the alleviation of humanity’s woes. One critical reaction to such sensational proclamations has been to focus on the mundane reality of contemporary machine learning as mere inductive prediction based on statistical generalizations, albeit with surprisingly powerful abilities (Pasquinelli 2017). While the deflationist reaction is a necessary reply to sensationalist agitation, adequate comprehension of modern AI cannot be achieved while neglecting its material and social context. One does not have to subscribe wholeheartedly to the social construction of technology thesis1 to allow that the development and evolution of technologies are influenced by social factors. For AI, the most important aspect of the current social context is arguably capital, which increasingly dominates AI research and production. One former computer science professor describes a “giant sucking sound of [AI] academics going into industry” (Metz 2017). This paper introduces capital’s theory of AI as utility and initiates a discussion on its social consequences. First, I discuss utilities and their infrastructures and introduce a few critical thoughts on the topic. Second, I situate modern AI by way of a brief history. Third, I detail capital’s view of AI as a utility and the technical details underpinning it. Fourth, I sketch how AI as a utility frames a social problematic beyond the important issues of algorithmic bias and the automation of work. I do so by extrapolating from one consequence of AI as a utility which multiple capitalist firms predict: the curation of human subjectivities.","",""
"2018","Pervasive Intelligence","Abstract                 This article seeks to situate collective or swarm robotics (SR) on a conceptual pane which on the one hand sheds light on the peculiar form of AI which is at play in such systems, whilst on the other hand it considers possible consequences of a widespread use of SR with a focus on swarms of Unmanned Aerial Systems (Swarm UAS). The leading hypothesis of this article is that Swarm Robotics create a multifold “spatial intelligence”, ranging from the dynamic morphologies of such collectives via their robust self-organization in changing environments to representations of these environments as distributed 4D-sensor systems. As is shown on the basis of some generative examples from the field of UAS, robot swarms are imagined to literally penetrate space and control it. In contrast to classical forms of surveillance or even “sousveillance”, this procedure could be called perveillance.","",""
"2018","On the Media-political Dimension of Artificial Intelligence","Abstract                 The essay critically investigates the media-political dimension of modern AI technology. Rather than examining the political aspects of certain AI-driven applications, the main focus of the paper is centred around the political implications of AI’s technological infrastructure, especially with regard to the machine learning approach that since around 2006 has been called Deep Learning (also known as the simulation of Artificial Neural Networks). Firstly, the paper discusses in how far Deep Learning is a fundamentally opaque black box technology, only partially accessible to human understanding. Secondly, and in relation to the first question, the essay takes a critical look at the agenda and activities of the research company OpenAI that supposedly aims to promote the democratization of AI and tries to make technologies like Deep Learning more accessible and transparent.","",""
"2018","Educational AI","Abstract                 Regarding possible implications for teaching and learning, the article explores the production and productive effects of educational AI from sociology of knowledge/ of technology perspectives from three sides: Firstly, the role of knowledge (re-)construction in the creation of educational AI is investigated. In this context, contrasting engineeringoriented approaches, educational AI systems are conceptualised as agentic entities infused with tacit and explicit knowledge about sociality and education, and as potentially reshaping both educational practices and scientific concepts. Looking at promotional and engineeringoriented AI discourses, the article secondly examines how education and AI are linked and how the knowledge pervasion of educational AI is addressed. Findings indicate that the discursive production of educational AI relates to the interwoven assumptions that education and specifically lifelong learning are obliged and able to remedy large-scale societal challenges and that educational AI can leverage this potential. They also indicate that an educational AI system’s knowledge is deemed a reflection of explicit (expert) knowledge that in the form of rationales can, in turn, be reflected to the systems’ users. Thirdly, regarding arising challenges for the sensitive area of education, educational AI’s role in knowledge gathering practices both in educational research and big educational data analysis is addressed.","",""
"2018","Competing Visions for AI","Abstract                 In this paper, I will investigate how two competing visions of machine intelligence put forward by Alan Turing and J. C. R Licklider - one that emphasized automation and another that emphasized augmentation - have informed experiments in computational creativity, from early attempts at computer-generated art and poetry in the 1960s, up to recent experiments that utilise Machine Learning to generate paintings and music. I argue that while our technological capacities have changed, the foundational conflict between Turing’s vision and Licklider’s vision plays itself out in generations of programmers and artists who explore the computer’s creative potential. Moreover, I will demonstrate that this conflict does not only inform technical/artistic practice, but speaks to a deeper philosophical and ideological divide concerning the narrative of a post-human future. While Turing’s conception of human-equivalent AI informs a transhumanist imaginary of super-intelligent, conscious, anthropomorphic machines, Licklider’s vision of symbiosis underpins formulations of the cyborg as human-machine hybrid, aligning more closely with a critical post-human imaginary in which boundaries between the human and technological become mutable and up for re-negotiation. In this article, I will explore how one of the functions of computational creativity is to highlight, emphasise and sometimes thematise these conflicting post-human imaginaries.","",""
"2018","Where the Sun never Shines","Abstract                 In this paper, I elaborate on deliberations of “post-enlightened cognition” between cognitive neuroscience, psychology and artificial intelligence research. I show how the design of machine learning algorithms is entangled with research on creativity and pathology in cognitive neuroscience and psychology through an interest in “episodic memory” and various forms of “spontaneous thought”. The most prominent forms of spontaneous thought - mind wandering and day dreaming - appear when the demands of the environment abate and have for a long time been stigmatized as signs of distraction or regarded as potentially pathological. Recent research in cognitive neuroscience, however, conceptualizes spontaneous thought as serving the purpose of, e. g., creative problem solving and hence invokes older discussions around the links between creativity and pathology. I discuss how attendant attempts at differentiating creative cognition from its pathological forms in contemporary psychology, cognitive neuroscience, and AI puts traditional understandings of rationality into question.","",""
"2018","Nothing but a human","","",""
"2018","Re-approaching fuzzy cognitive maps to increase the knowledge of a system","","",""
"2018","“Super-intelligent” machine: technological exuberance or the road to subjection","","",""
"2018","Rat running the G20: collective intelligence for navigating the disrupted city","","",""
"2018","Is artificial intelligence associated with chemist’s creativity represents a threat to humanity?","","",""
"2018","Risk analysis and prediction in welfare institutions using a recommender system","","",""
"2018","Collective intelligence for promoting changes in behaviour: a case study on energy conservation","","",""
"2018","Where and when AI and CI meet: exploring the intersection of artificial and collective intelligence towards the goal of innovating how we govern","","",""
"2018","Collective intelligence for the common good: cultivating the seeds for an intentional collaborative enterprise","","",""
"2018","Cluster consensus in multi-agent networks with mutual information exchange","","",""
"2018","Artificial intelligence: looking though the Pygmalion Lens","","",""
"2018","Artificial intelligence and collective intelligence: the emergence of a new field","","",""
"2018","Reflections on James Bond of AI","","",""
"2018","EEG efficient classification of imagined right and left hand movement using RBF kernel SVM and the joint CWT_PCA","","",""
"2018","Games between humans and AIs","","",""
"2018","Fuzzy modelling and model reference neural adaptive control of the concentration in a chemical reactor (CSTR)","","",""
"2018","Reconciliation between factions focused on near-term and long-term artificial intelligence","","",""
"2018","Anthropomorphism in social robotics: empirical results on human–robot interaction in hybrid production workplaces","","",""
"2018","Listening without ears: Artificial intelligence in audio mastering"," Since the inception of recorded music there has been a need for standards and reliability across sound formats and listening environments. The role of the audio mastering engineer is prestigious and akin to a craft expert combining scientific knowledge, musical learning, manual precision and skill, and an awareness of cultural fashions and creative labour. With the advent of algorithms, big data and machine learning, loosely termed artificial intelligence in this creative sector, there is now the possibility of automating human audio mastering processes and radically disrupting mastering careers. The emergence of dedicated products and services in artificial intelligence-driven audio mastering poses profound questions for the future of the music industry, already having faced significant challenges due to the digitalization of music over the past decades. The research reports on qualitative and ethnographic inquiry with audio mastering engineers on the automation of their expertise and the potential for artificial intelligence to augment or replace aspects of their workflows. Investigating audio mastering engineers' awareness of artificial intelligence, the research probes the importance of criticality in their labour. The research identifies intuitive performance and critical listening as areas where human ingenuity and communication pose problems for simulation. Affective labour disrupts speculation of algorithmic domination by highlighting the pragmatic strategies available for humans to adapt and augment digital technologies. ","",""
"2018","Isomorphism through algorithms: Institutional dependencies in the case of Facebook"," Algorithms and data-driven technologies are increasingly being embraced by a variety of different sectors and institutions. This paper examines how algorithms and data-driven technologies, enacted by an organization like Facebook, can induce similarity across an industry. Using theories from organizational sociology and neoinstitutionalism, this paper traces the bureaucratic roots of Big Data and algorithms to examine the institutional dependencies that emerge and are mediated through data-driven and algorithmic logics. This type of analysis sheds light on how organizational contexts are embedded into algorithms, which can then become embedded within other organizational and individual practices. By investigating technical practices as organizational and bureaucratic, discussions about accountability and decision-making can be reframed. ","",""
"2018","Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management"," Algorithms increasingly make managerial decisions that people used to make. Perceptions of algorithms, regardless of the algorithms' actual performance, can significantly influence their adoption, yet we do not fully understand how people perceive decisions made by algorithms as compared with decisions made by humans. To explore perceptions of algorithmic management, we conducted an online experiment using four managerial decisions that required either mechanical or human skills. We manipulated the decision-maker (algorithmic or human), and measured perceived fairness, trust, and emotional response. With the mechanical tasks, algorithmic and human-made decisions were perceived as equally fair and trustworthy and evoked similar emotions; however, human managers' fairness and trustworthiness were attributed to the manager's authority, whereas algorithms' fairness and trustworthiness were attributed to their perceived efficiency and objectivity. Human decisions evoked some positive emotion due to the possibility of social recognition, whereas algorithmic decisions generated a more mixed response – algorithms were seen as helpful tools but also possible tracking mechanisms. With the human tasks, algorithmic decisions were perceived as less fair and trustworthy and evoked more negative emotion than human decisions. Algorithms' perceived lack of intuition and subjective judgment capabilities contributed to the lower fairness and trustworthiness judgments. Positive emotion from human decisions was attributed to social recognition, while negative emotion from algorithmic decisions was attributed to the dehumanizing experience of being evaluated by machines. This work reveals people's lay concepts of algorithmic versus human decisions in a management context and suggests that task characteristics matter in understanding people's experiences with algorithmic technologies. ","",""
"2018","Deconstructing the algorithmic sublime"," This special theme contextualizes, examines, and ultimately works to dispel the feelings of “sublime”—of awe and terror that overrides rational thought—that much of the contemporary public discourse on algorithms encourages. Employing critical, reflexive, and ethnographic techniques, these authors show that while algorithms can take on a multiplicity of different cultural meanings, they ultimately remain closely connected to the people who define and deploy them, and the institutions and power relations in which they are embedded. Building on a conversation we began at the Algorithms in Culture conference at U.C. Berkeley in December 2016, we collectively study algorithms as culture (Seaver, this special theme), fetish (Thomas et al.), imaginary (Christin), bureaucratic logic (Caplan and boyd), method of governance (Coletta and Kitchin; Lee; Geiger), mode of inquiry (Baumer), and mode of power (Kubler).  [Box: see text] ","",""
"2018","Algorithmic affordances for productive resistance"," Although overarching if not foundational conceptualizations of digital governance in the field of critical data studies aptly account for and explain subjection, calculated resistance is left conceptually unattended despite case studies that document instances of resistance. I ask at the outset why conceptualizations of digital governance are so bleak, and I argue that all are underscored implicitly by a Deleuzian theory of desire that overlooks agency, defined here in Foucauldian terms. I subsequently conceptualize digital governance as encompassing subjection as well as resistance, and I cast the two in relational perspective by making use of the concepts “affordance” and “assemblage” in conjunction with multiple subjectivities and Foucault's view of power as productive as well as his view of resistance as an “antagonism of strategies” in his late scholarship on resistance, ethics, and subjectivity. I offer examples of salient modes of what I call “productive” resistance (as opposed to resistance by way of avoidance, disruption or obfuscation), and from a Foucauldian perspective I explain how each mode targets and subverts technologies of repressive power to produce new elements of the digital environment and construct new truths. I conclude by recognizing the agency embodied in resistance as an end in itself, but I also consider that modes of productive resistance can have extrinsic value as they affect the fluid interaction among elements of the digital environment, potentially disrupting the presumed structure of dominance and dependence, and opening our conceptualization of algorithmic life to hopeful possibilities for change. ","",""
"2018","Algorithms as fetish: Faith and possibility in algorithmic work"," Algorithms are powerful because we invest in them the power to do things. With such promise, they can transform the ordinary, say snapshots along a robotic vacuum cleaner’s route, into something much more, such as a clean home. Echoing David Graeber’s revision of fetishism, we argue that this easy slip from technical capabilities to broader claims betrays not the “magic” of algorithms but rather the dynamics of their exchange. Fetishes are not indicators of false thinking, but social contracts in material form. They mediate emerging distributions of power often too nascent, too slippery or too disconcerting to directly acknowledge. Drawing primarily on 2016 ethnographic research with computer vision professionals, we show how faith in what algorithms can do shapes the social encounters and exchanges of their production. By analyzing algorithms through the lens of fetishism, we can see the social and economic investment in some people’s labor over others. We also see everyday opportunities for social creativity and change. We conclude that what is problematic about algorithms is not their fetishization but instead their stabilization into full-fledged gods and demons – the more deserving objects of critique. ","",""
"2018","Review: Des Fitzgerald, Tracing Autism; Uncertainty, Ambiguity, and the Affective Labor of Neuroscience","Book Reivew of Tracing Autism; Uncertainty, Ambiguity, and the Affective Labor of Neuroscience by Des Fitzgerald","",""
"2018","Review: Des Fitzgerald, Tracing Autism; Uncertainty, Ambiguity, and the Affective Labor of Neuroscience","Book Reivew of Tracing Autism; Uncertainty, Ambiguity, and the Affective Labor of Neuroscience by Des Fitzgerald","",""
"2018","Algorithmic Fetishism","Surveillance-infused forms of algorithmic discrimination are beginning to capture public and scholarly attention. While this is an encouraging development, this editorial questions the parameters of this emerging discussion and cautions against algorithmic fetishism. I characterize algorithmic fetishism as the pleasurable pursuit of opening the black box, discovering the code hidden inside, exploring its beauty and flaws, and explicating its intricacies. It is a technophilic desire for arcane knowledge that can never be grasped completely, so it continually lures one forward into technical realms while deferring the point of intervention. The editorial concludes with a review of the articles in this open issue.    ","",""
"2018","Goals for algorithmic genies","Algorithmic genies built from growing computational capabilities bring risks like automating well-paying jobs, yet we suggest that if supplied with suitable goals and supporting infrastructure they can help in meeting many human needs. We argue that algorithmic genies can be harnessed to raise the baseline experience of people worldwide (raising the floor), especially if such harnessing is informed by wide consensus and deep evidence. Examples show how algorithmic genies could raise the floor for widely agreed human needs like health, education, and other components of the Social Progress Index. Ensuring that both the least well off and the majority share in the benefits of progress can help to ensure the floor is raised for all (floored progress). Floored progress can apply beyond basic human needs to problems that people across the economic spectrum struggle with (shared floors). We include three tables with illustrative opportunities, and conclude by summarizing the value of raising floors individually and in concert.","",""
"2018","The Hyperdodge: How Users Resist Algorithmic Objects in Everyday Life","This paper asks what we can learn if we research algorithms via a bottom-up methodology from a usersâ€™ perspective to see how everyday users can resist algorithmic objectives. In doing this, this paper theorizes the framework of agency in which users can shape and reshape algorithmic outcomes. The argument draws on theoretical knowledge grounded in empirical data produced in 10 in-depth interviews, Heideggerâ€™s phenomenology of experience, and De Certeauâ€™s notion of practices in everyday life. It explains how the phenomenology of experience can render algorithms visible for users by asking not what algorithms are, but by reflecting upon their meaning and how these reflections can transform into practices of everyday resistance. Finally, this article speculates about the potential implications of (meta-)data on machine learning that is purposely being manipulated by users, creating the possibility of what I am labelling a â€œhyperdodgeâ€.","",""
"2018","The end of media logics? On algorithms and agency"," We argue that algorithms are an outcome rather than a replacement of media logics, and ultimately, we advance this argument by connecting human agency to media logics. This theoretical contribution builds on the notion that technology, particularly algorithms are non-neutral, arguing for a stronger focus on the agency that goes into designing and programming them. We reflect on the limits of algorithmic agency and lay out the role of algorithms and agency for the dimensions and elements of network media logic. The article concludes with addressing questions of power, discussing algorithmic agency from both meso and macro perspectives. ","",""
"2018","Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability"," Models for understanding and holding systems accountable have long rested upon ideals and logics of transparency. Being able to see a system is sometimes equated with being able to know how it works and govern it—a pattern that recurs in recent work about transparency and computational systems. But can “black boxes’ ever be opened, and if so, would that ever be sufficient? In this article, we critically interrogate the ideal of transparency, trace some of its roots in scientific and sociotechnical epistemological cultures, and present 10 limitations to its application. We specifically focus on the inadequacy of transparency for understanding and governing algorithmic systems and sketch an alternative typology of algorithmic accountability grounded in constructive engagements with the limitations of transparency ideals. ","",""
"2018","Automating judgment? Algorithmic judgment, news knowledge, and journalistic professionalism"," Journalistic judgment is both a central and fraught function of journalism. The privileging of objectivity norms and the externalization of newsworthiness in discourses about journalism leave little room for the legitimation of journalists’ subjective judgment. This tension has become more apparent in the digital news era due to the growing use of algorithms in automated news distribution and production. This article argues that algorithmic judgment should be considered distinct from journalists’ professional judgment. Algorithmic judgment presents a fundamental challenge to news judgment based on the twin beliefs that human subjectivity is inherently suspect and in need of replacement, while algorithms are inherently objective and in need of implementation. The supplanting of human judgment with algorithmic judgment has significant consequences for both the shape of news and its legitimating discourses. ","",""
"2018","Robotization and the domestic sphere"," The aim of this article is to advance theoretically the debate on the interrelation between robotization and the domestic sphere. I adopt a critical approach to the political economy about the diffusion of machines in the domestic sphere (unexpected and not predicted by the classical theories of capital) and the automation of everyday life, including that of the human body. In the first part, I focus on why robotics has shifted from the industrial sectors in the domestic sphere and on which features characterize today this sector. Then, I describe the trends that robotics has put in motion in the domestic sphere and Europeans’ attitudes toward it. In the final part, I analyze the processes of automation that are taking place. I will conclude posing the problem of what kind of society we want to build and live in by introducing the social robots and which future perspectives emerge at social level. ","",""
"2018","People’s Councils for Ethical Machine Learning"," Machine learning is a form of knowledge production native to the era of big data. It is at the core of social media platforms and everyday interactions. It is also being rapidly adopted for research and discovery across academia, business, and government. This article will explores the way the affordances of machine learning itself, and the forms of social apparatus that it becomes a part of, will potentially erode ethics and draw us in to a drone-like perspective. Unconstrained machine learning enables and delimits our knowledge of the world in particular ways: the abstractions and operations of machine learning produce a “view from above” whose consequences for both ethics and legality parallel the dilemmas of drone warfare. The family of machine learning methods is not somehow inherently bad or dangerous, nor does implementing them signal any intent to cause harm. Nevertheless, the machine learning assemblage produces a targeting gaze whose algorithms obfuscate the legality of its judgments, and whose iterations threaten to create both specific injustices and broader states of exception. Given the urgent need to provide some kind of balance before machine learning becomes embedded everywhere, this article proposes people’s councils as a way to contest machinic judgments and reassert openness and discourse. ","",""
"2018","Programmatic Dreams: Technographic Inquiry into Censorship of Chinese Chatbots"," This project explores the recent censorship of two Chinese artificial intelligence (AI) chatbots on Tencent’s popular WeChat messaging platform. Specifically, I am advancing a technographic approach in ways that give agency to bots as not just computing units but as interlocutors and informants. I seek to understand these chatbots through their intended design—by chatting with them. I argue that this methodological inquiry of chatbots can potentially points to fissures and deficiencies within the Chinese censorship machine that allows for spaces of subversion. AI chatbot development China presents a rich site of study because it embodies the extremes of surveillance and censorship. This is all the more important as China have elevated disruptive technologies like AI and big data as critical part of state security and a key component to fulfilling the “Chinese Dream of National Rejuvenation.” Whether it is the implementation of a national “social credit” system or the ubiquitous use facial recognition systems, much of Western fears about data security and state control have been already realized in China. Yet, this also implies China is at the frontlines of potential points of resistance and fissures against the party–state–corporate machine. In doing so, I not only seek to raise questions dealing with the limits of our humanity in the light of our AI-driven futures but also present methodological concerns related to human–machine interfacing in conceptualizing new modes of resistance. ","",""
"2018","Do Algorithms Shape Character? Considering Algorithmic Ethical Subjectivation"," Moral critiques of computational algorithms seem divided between two paradigms. One seeks to demonstrate how an opaque and unruly algorithmic power violates moral values and harms users’ autonomy; the other underlines the systematicity of such power, deflating concerns about opacity and unruliness. While the second paradigm makes it possible to think of end users of algorithmic systems as moral agents, the consequences of this possibility remain unexplored. This article proposes one way of tackling this problem. Employing Michel Foucault’s version of virtue ethics, I examine how perceptions of Facebook’s normative regulation of visibility have transformed non-expert end users’ ethical selves (i.e., their character) in the current political crisis in Brazil. The article builds on this analysis to advance algorithmic ethical subjectivation as a concept to make sense of these processes of ethical becoming. I define them as plural (encompassing various types of actions and values, and resulting in no determinate subject), contextual (demanding not only sociomaterial but also epistemological and ethical conditions), and potentially harmful (eventually structuring harms that are not externally inflicted by algorithms, but by users, upon themselves and others, in response to how they perceive the normativity of algorithmic decisions). By researching which model(s) of ethical subjectivation specific algorithmic social platforms instantiate, critical scholars might be able to better understand the normative consequences of these platforms’ power. ","",""
"2018","Guess, check and fix: a phenomenology of improvisation in ‘neural’ painting","ABSTRACT The parameter space offered by neural network image synthesis offers a creative environment that is little understood and quite literally emergent. In an attempt to come to terms with this space, an artist enters into an improvisational reflection on ‘neural painting’, made possible with what are called style transfer algorithms (Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2015. “A Neural Algorithm of Artistic Style.” ArXiv: 1508.06576 [Cs, q-Bio], August. http://arxiv.org/abs/1508.06576) Artistic painting offers access to transitional states existing at the interstices of expression and reflection in the creative process. Of all the conceptual dimensions offered by neural style transfer models (where the ‘content’ of one source is blended with the ‘style’ of another), the convolutional blending of ‘content weight’ offers a fertile metaphor for artistic painting phenomenology, providing a tool for the investigation of stylistic schema in the iterative, improvisational movement from concept to representation. A preliminary phenomenological framework describing the process of neural painting is developed, offering an art-as-research perspective on intersubjectively positioned creativity support technology.","",""
"2018","Improvisational creativity","As we enter a seemingly new age of technology driven by artificial intelligence (AI), a growing number of researchers have begun exploring how to design and interact with creative machines. With systems now able to achieve impressive degrees of agency and autonomy, any such interactions require new thinking about how to design interfaces between artists and creative computational systems. How does one improvise with a creative partner that is not human, yet outwardly displays a degree of agency, intention and autonomy? This special issue on improvisational creativity advances the dialogue on how we design, develop, conceptualise and create with computer systems in an improvisational context. The call for submissions for this issue asked for an examination of both the challenges and opportunities for creative improvisation between people and computer systems. It emphasized themany possible dimensions and settings in which improvisation can take place—including in performance, art-making or scientific research—along with the multi-disciplinary nature of any potential contribution to this growing field of enquiry. Improvisation is one of the most demanding yet rewarding creative acts. It has been well studied in situations where individuals interact with tools, or in human groups (Sawyer 2006). Recently, however, the idea of computer systems being valuable co-creative partners in improvisational settings has gained acceptance and is now an active research area in (for example) the AI and Computational Creativity communities. Being largely in its infancy, there are many important questions and open problems for this new field of enquiry. For example, how do we effectively improvise with ‘intelligent’ or ‘creative’ machines, particularly when they are conceptualised neither as tools nor human partners? What are the creative and artistic challenges in building computational improvisational partners? How does the psychology of improvisation change when machines become part of an improvisational group? How can digital technologies and AI support and enhance human creativity in an improvisational context? This special issue grew out of a similarlythemed workshop on improvisational creativity held inPrato, Italy in July 2017 (McCormack,Gifford, and Knotts 2017). Some of papers in this special issue arose from discussions and collaborations formed at the workshop. Additionally, the workshop fleshed out a number of topics that became the basis for this special issue, including those that have yet to be adequately explored in published research. They include generalisable design principles for computational improvisors and the evaluation of their efficacy. Many current systems are characterised by their bespoke, artistdrivennature—reflecting the individual aesthetic, functional or performative concerns of their creators. As such they are ad hoc: the systems themselves may even be improvised, and evaluation beyond personal aesthetic or functional judgments are rarely performed.","",""
"2018","Tool vs. agent: attributing agency to natural language generation systems","ABSTRACT This paper argues that we should shift our consideration of natural language generation systems as tools for manifesting human intent to natural language generation systems as agents in themselves. Such a semantic shift would permit a more holistic conversation about the transformative social power of these systems’ output.","",""
"2018","Ghosts in the Machine : a motion-capture experiment in distributed reception","Digital reconstructions of classical antiquity are generally ocularcentric, appealing only to the sense of vision. We propose that new technologies may be used to engage the other senses in the act ...","",""
"2018","From Distracted to Recursive Reading: Facilitating Knowledge Transfer through Annotation Software","","",""
"2018","A Strange Metapaper On Computing Natural Language","","",""
"2019","The Robotic Imaginary: The Human and the Price of Dehumanized Labor by Jennifer Rhee (review)","","",""
"2019","Bridging the gaps: using agent-based modeling to reconcile data and theory in computational communication science","In various branches of the social sciences, agent-based models (ABMs) have long been applied to enhance researchers’ understanding of complex systems and processes. However, in communication science, this approach is rarely used. In this article, we argue that ABMs have the potential to advance communication research in general, and computational communication science (CCS) in particular, by helping scholars address two major gaps. First, by generating emergent global phenomena from individual interactions, ABMs make it possible to explicitly link micro and macro perspectives in communication research. Second, by formalizing theories, ABMs offer mechanism-based explanations for observed empirical patterns in data. To familiarize more communication scholars with this approach, this article provides a systematic overview of the potentials, applications, and challenges of ABMs in communication science. Special attention is paid to the criteria of reliability and validity.","",""
"2019","Cars and Contemporary Communication| How the Rise of Autonomous and Robotized Cars is Perceived and Felt in Europe","This article explores users’ attitudes, perceptions, views, and emotions toward car automation and robotization, two processes increasingly affecting society in different ways––namely, the rise of autonomous and robotized cars (and vehicles in general) and the increasing level of robotization of current cars. To address these questions, we investigated the feeling of trust and comfort toward driverless cars among Europeans using two Eurobarometer surveys. Making use of two representative samples of the European population, we aimed to explore citizens’ attitudes and opinions about automation and digitization. The two surveys involved, respectively, 27,801 and 27,901 participants from all EU-28 countries. Furthermore, we investigated, in Northern Italy, the perception of robotization of cars and other technologies of everyday use, as well as the attitudes and opinions of children and preteens ( n  = 740), and adolescents ( n  = 801)—relevant social groups not covered in the Eurobarometer surveys.","",""
"2019","Computational Communication Science| Bridging the Gaps: Using Agent-Based Modeling to Reconcile Data and Theory in Computational Communication Science","In various branches of the social sciences, agent-based models (ABMs) have long been applied to enhance researchers’ understanding of complex systems and processes. However, in communication science, this approach is rarely used. In this article, we argue that ABMs have the potential to advance communication research in general, and computational communication science (CCS) in particular, by helping scholars address two major gaps. First, by generating emergent global phenomena from individual interactions, ABMs make it possible to explicitly link micro and macro perspectives in communication research. Second, by formalizing theories, ABMs offer mechanism-based explanations for observed empirical patterns in data. To familiarize more communication scholars with this approach, this article provides a systematic overview of the potentials, applications, and challenges of ABMs in communication science. Special attention is paid to the criteria of reliability and validity.","",""
"2019","Paul Baker, Gavin Brookes and Craig Evans, The Language of Patient Feedback: A Corpus Linguistic Study of Online Health Communication","","",""
"2019","Alex Rosenblat, Uberland: How Algorithms Are Rewriting The Rules of Work","","",""
"2019","Life 3.0: being human in the age of artificial intelligence","","",""
"2019","A guideline for understanding and measuring algorithmic governance in everyday life","Algorithmic governance affects individuals’ reality construction and consequently social order in societies. Vague concepts of algorithmic governance and the lack of comprehensive empirical insights into this kind of institutional steering by software from a user perspective may, however, lead to unrealistic risk assessments and premature policy conclusions. Therefore, this paper offers a theoretical model to measure the significance of algorithmic governance and an empirical mixed-methods approach to test it in different life domains. Applying this guideline should lead to a more nuanced understanding of the actual significance of algorithmic governance, thus contributing to an empirically better-informed risk assessment and governance of algorithms.","",""
"2019","Algorithmic governance","","",""
"2019","New perspectives on ethics and the laws of artificial intelligence",": The continuous interaction between intelligent devices, sensors and people points to the increasing number of data being produced, stored and processed, changing, in various aspects and increasingly, our daily life. This increasing connectivity and symbiotic interaction among humans and intelligent machines brings significant challenges for the rule of law and contemporary ethics. Do machines have morality? What legal liability regime should we adopt for damages arising from increasingly advanced artificial intelligence? Which ethical guideline should we adopt to orient its advancement? In this paper we will discuss the main normative and ethical challenges imposed by the advancement of artificial intelligence.","",""
"2019","Transparent to whom? No algorithmic accountability without a critical audience","ABSTRACT Big data and data science transform organizational decision-making. We increasingly defer decisions to algorithms because machines have earned a reputation of outperforming us. As algorithms become embedded within organizations, they become more influential and increasingly opaque. Those who create algorithms may make arbitrary decisions in all stages of the ‘data value chain’, yet these subjectivities are obscured from view. Algorithms come to reflect the biases of their creators, can reinforce established ways of thinking, and may favour some political orientations over others. This is a cause for concern and calls for more transparency in the development, implementation, and use of algorithms in public- and private-sector organizations. We argue that one elementary – yet key – question remains largely undiscussed. If transparency is a primary concern, then to whom should algorithms be transparent? We consider algorithms as socio-technical assemblages and conclude that without a critical audience, algorithms cannot be held accountable.","",""
"2019","Decentering technology in discourse on discrimination","ABSTRACT Algorithmic discrimination has become one of the critical points in the discussion about the consequences of an intensively datafied world. While many scholars address this problem from a purely techno-centric perspective, others try to raise broader social justice concerns. In this article, we join those voices and examine norms, values, and practices among European civil society organizations in relation to the topic of data and discrimination. Our goal is to decenter technology and bring nuance into the debate about its role and place in the production of social inequalities. To accomplish this, we rely on Nancy Fraser’s theory of abnormal justice which highlights interconnections between maldistribution of economic benefits, misrecognition of marginalized communities, and their misrepresentation in political processes. Fraser’s theory helps situate technologically mediated discrimination alongside other more conventional kinds of discrimination and injustice and privileges attention to economic, social, and political conditions of marginality. Using a thematic analysis of 30 interviews with civil society representatives across Europe’s human rights sector, we bring clarity to this idea of decentering. We show how many groups prioritize the specific experiences of marginalized groups and ‘see through’ technology, acknowledging its connection to larger systems of institutionalized oppression. This decentered approach contrasts the process-oriented perspective of tech-savvy civil society groups that shy from an analysis of systematic forms of injustice.","",""
"2019","Where fairness fails: data, algorithms, and the limits of antidiscrimination discourse","ABSTRACT Problems of bias and fairness are central to data justice, as they speak directly to the threat that ‘big data’ and algorithmic decision-making may worsen already existing injustices. In the United States, grappling with these problems has found clearest expression through liberal discourses of rights, due process, and antidiscrimination. Work in this area, however, has tended to overlook certain established limits of antidiscrimination discourses for bringing about the change demanded by social justice. In this paper, I engage three of these limits: 1) an overemphasis on discrete ‘bad actors’, 2) single-axis thinking that centers disadvantage, and 3) an inordinate focus on a limited set of goods. I show that, in mirroring some of antidiscrimination discourse’s most problematic tendencies, efforts to achieve fairness and combat algorithmic discrimination fail to address the very hierarchical logic that produces advantaged and disadvantaged subjects in the first place. Finally, I conclude by sketching three paths for future work to better account for the structural conditions against which we come to understand problems of data and unjust discrimination in the first place.","",""
"2019","Artificial unintelligence: how computers misunderstand the world / The message is murder: substrates of computational capital","","",""
"2019","When are artificial intelligence versus human agents faulted for wrongdoing? Moral attributions after individual and joint decisions","ABSTRACT Artificial intelligence (AI) agents make decisions that affect individuals and society which can produce outcomes traditionally considered moral violations if performed by humans. Do people attribute the same moral permissibility and fault to AIs and humans when each produces the same moral violation outcome? Additionally, how do people attribute morality when the AI and human are jointly making the decision which produces that violation? We investigate these questions with an experiment that manipulates written descriptions of four real-world scenarios where, originally, a violation outcome was produced by an AI. Our decision-making structures include individual decision-making – either AIs or humans – and joint decision-making – either humans monitoring AIs or AIs recommending to humans. We find that the decision-making structure has little effect on morally faulting AIs, but that humans who monitor AIs are faulted less than solo humans and humans receiving recommendations. Furthermore, people attribute more permission and less fault to AIs compared to humans for the violation in both joint decision-making structures. The blame for joint AI-human wrongdoing suggests the potential for strategic scapegoating of AIs for human moral failings and the need for future research on AI-human teams.","",""
"2019","<i>Artificial unintelligence: How computers misunderstand the world</i>","in their motivations, such as protecting children, any barrier to information is ultimately harmful. The AAUP statement included as an appendix to Jones’s essay is the same one quoted by Gavin-Herbert as an example of the educational system’s brokenness, thus illustrating the wide range of perspectives presented in the first section. The book’s second section is composed of nine case studies involving trigger warnings. All case studies come from the world of higher education, and each presents a unique point of view. There are lessons learned from the inclusion of a trigger warning in an all-campus reading program. A student details the problems she faced after suggesting that a content warning be provided for a specific reading in future iterations of the course (and, modeling her own beliefs about the efficacy of content warnings, included one for her chapter). Two incidents at Smith College involving invited speakers (one who came; one who withdrew) provide an opportunity to go beyond trigger warnings and explore how arguments about academic freedom often ignore the motives of student activists and, in some cases, willfully misinterpret the desire for more engagement with a topic as a demand to be sheltered from ideas that prick their comfort. An instructor details the tools she uses in addition to trigger warnings when addressing traumatic topics in the classroom. One professor reflects on her successful use of content warnings throughout her career while another makes the case that her avoidance of such warnings has resulted in better learning outcomes for her students. Experiences with military veterans in the classroom cause two English faculty to consider adopting trigger warnings in the future. Public speaking courses are held up as an appropriate venue for trigger warnings. Finally, an instructor grapples with his internal conflict over using trigger warnings in graduate courses in library and information science, as the discipline’s enduring focus on intellectual freedom is not a natural complement to content warnings. Like the historical and theoretical essays in the first section, these case studies draw from such a breadth of experience and perspective that the reader is left with more questions than answers about trigger warnings. Spoken like a true librarian, Knox closes her introduction by stating that “readers will have to come to their own conclusions regarding the debate.” Indeed, on any topic, librarians aim to provide information and let the readers decide for themselves. In the case of trigger warnings, the question remains as to whether they interfere with a reader’s ability to make a determination without prejudice or whether they provide a necessary tool for those readers whose lived experiences have impacted the way they need to interact with information. After reading this book, it’s clear that the jury is still out. Knox should be commended for compiling such a compelling collection of essays and case studies that really forces the reader to think critically about trigger warnings.","",""
"2019","Raising the ideal child? Algorithms, quantification and prediction"," The world in which the contemporary child is conceived and raised is one that is increasingly monitored, analysed and manipulated through technological processes. Simultaneously, expectations for the child are changing as new tools and practices for quantifying, managing and predicting achievements and future possibilities become available. Algorithms as ways of anticipating, doing or fixing are central to these technological processes. These intersect with and are informed by social, cultural and political discourses that imagine the ‘ideal’ child. Therefore, this article explores the power of algorithms within the everyday of the child. Drawing upon examples of quantification and prediction practices in the commercial and state sectors, the article raises questions about the issues, challenges and politics of these types of algorithmic approaches in raising and imagining the ‘ideal child’. ","",""
"2019","THE IMPACT OF ATTITUDES TOWARDS GOVERNMENT AND CORPORATIONS ON TRUST IN TECHNOLOGY","Understanding public distrust of technology is both theoretically and practically important, yet while previous research has focused on the association between political ideology and trust in science, it is at best an inconsistent predictor. This study shall demonstrate that two dimensions of political ideology, attitudes towards governments and corporations, can more precisely predict trust in technology across issues. We will conduct an online survey on the science of radio frequency electromagnetic fields (RF-EMF) and Artificial Intelligence (AI) applications to test our hypotheses that trust in technology varies across issues and that attitudes towards government and corporations are important predictors of this trust.","",""
"2019","THE ETHICS OF EMOTION IN AI SYSTEMS","Computational analyses of data pertaining to human emotional expression have a surprisingly long history and an increasingly critical role in social machine learning (ML) and artificial intelligence (AI) applications. Contemporary, quotidian, narrow AI/ML technologies are most frequently used by social media platforms for modeling and predicting human emotional expression as signals of interpersonal interaction and personal preference. Yet while the ethical and social impacts of ML/AI systems have of late become major topics of both public discussion and academic debate , the ethical dimensions of AI/ML analytics for emotional expression have been under-theorized in these conversations. In this paper, we connect contemporary technical methods for analyzing emotional expression via AI/ML with extant problems in the ethics of AI discourse, in doing so highlighting tensions within that broader discourse and implications for the application of emotion analysis in practice.","",""
"2019","COMMUNICATING A TRUSTWORTHY ONLINE ORGANISATIONAL IDENTITY WITH CHATBOTS","Conversational bots, otherwise known as chatbots, operate within the fourth industrial revolution as a client facing form of AI. They are communicative interfaces that mimic human conversation to deliver information in a highly personalised way. The user experience of chatbots can change the way individuals, groups and organisations define themselves online (Whitley, Gal &amp; Kjaergaard, 2014). This paper discusses the opportunities in building an online identity via chatbots, with emphasis on harnessing the properties of chatbots to develop trust with users. Currently, organisations are limited to the properties and affordances of web browsers, search engines and social media to communicate a “shared symbolic representation” (Gioia, 1998). This paper focuses on organisational identities on the Internet, and details both opportunities and vulnerabilities in establishing trust with users through chatbots.","",""
"2019","TRUSTED MACHINES? MACHINE LEARNING, MORE-THAN-HUMAN SPEED AND DEAD LABOR IN PLATFORM CAPITALISM","Decision making machines are today ‘trusted’ to perform or assist with a rapidly expanding array of tasks. Indeed, many contemporary industries could not now function without them. Nevertheless, this trust in and reliance upon digital automation is far from unproblematic. This paper combines insights drawn from qualitative research with creative industries professionals, with approaches derived from software studies and media archaeology to critically interrogate three ways that digital automation is currently employed and accompanying questions that relate to trust. Firstly, digital automation is examined as a way of saving time and/or reducing human labor, such as when programmers use automated build tools or graphical user interfaces. Secondly, automation enables new types of behavior by operating at more-than-human speeds, as exemplified by high-frequency trading algorithms. Finally, the mode of digital automation associated with machine learning attempts to both predict and influence human behaviors, as epitomized by personalization algorithms within social media and search engines.&#x0D; While creative machines are increasingly trusted to underpin industries, culture and society, we should at least query the desirability of increasing dependence on these technologies as they are currently employed. These for-profit, corporate-controlled tools performatively reproduce a neoliberal worldview. Discussing misplaced trust in digital automation frequently conjures an imagined binary opposition between humans and machines, however, this reductive fantasy conceals the far more concrete conflict between differing technocultural assemblages composed of humans and machines. Across the examples explored in this talk, what emerges are numerous ways in which creative machines are used to perpetuate social inequalities.&#x0D;  ","",""
"2019","INTELLIGENT FAILURES: CLIPPY MEMES AND THE LIMITS OF DIGITAL ASSISTANTS","Often hated during its lifespan in product (1996-2006), Clippy – Microsoft’s Office Assistant, became a pop-culture icon in its afterlife. Delving into the plethora of memes featuring Clippy, we ask: why should a questionable character from a software program that has been out of use for well over a decade have so vibrant an afterlife? If Clippy has become a rhetorical resource, what is it being used to do? We propose that Clippy’s dual status as the original natural-language digital assistant, one that fell critically short in its ability to actually assist, makes it an ideal vehicle for critique of today’s ubiquitous assistants. An analysis of 1,148 meme instances collected from five sites led to a twofold argument: First, Clippy humor relies on the contrast between types of intelligence; Clippy is often too good at one kind, while lacking in another. In particular, Clippy lacks interpersonal intelligence: it serves as a disruptive mediator between its user and the world, as well as other human beings.&#x0D; Yet this failure in “knowing its limits” and adapting to its environment is also what gives Clippy character. This suggests that digital assistants must attend to multiple kinds of intelligences; attending to any one over others may create an endearing character but not an effective digital assistant. Furthermore, the fact that the unbending yet personality-filled character of Clippy remains ungendered or male gives us insight into the pliant and empty characters of the female gendered Alexa, Siri, and Cortana.","",""
"2019","“ALEXA, CAN I TRUST YOU?”: SMALL SISTERS AND FRIENDLY POWER","How is trust fabricated today? This paper argues that the persona of ‘Alexa’ bypasses concerns around surveillance and privacy, defusing anxieties not via the rationality of a convincing argument but through the relationality of Alexa as a singular presence.&#x0D; In many respects Alexa is actually more invasive than other technologies. Amazon has encroached into the very heart of the home. Moreover, the company’s patents delve further into the subject through voice identification, mood monitoring, and health detection. But this encroachment is carried out by her, rather than it, a warm and welcoming persona. The team’s aim is to develop something that is friendly, can turn off your lights, chat about anything, and empathize when you’re having a bad day (McGirt 2018). The goal is to construct something chattier, more affective and emotionally attuned. In doing so, Alexa embodies what theorist Byung-Chul Han (2017) has called “friendly power.”&#x0D; The result is that Alexa feels different. Instead of an algorithmic bundle of technologies, Alexa is experienced as an affective persona. Alexa thus delves deeper into the inner life of the subject while shrugging off the anxieties associated with cold, command-and-control technologies. Rather than an all-seeing eye, she is an always listening voice, a friendly companion. And rather than emanating from a central agency, she is co-located with the user. If Big Brother no longer characterizes contemporary power (Harcourt 2015), Alexa might be described as a “small sister.” Small sisters work alongside instead of above. Small sisters are multiple, sited, and supple.","",""
"2019","ASSESSING ETHICAL AI-BASED DECISION-MAKING: TOWARDS AN APPLIED ANALYTICAL FRAMEWORK","Globally there is strong enthusiasm for using Artificial Intelligence (AI) in government decision making, yet this technocratic approach is not without significant downsides including bias, exacerbating discrimination and inequalities, and reducing government accountability and transparency. A flurry of analytical and policy work has recently sought to identify principles, policies, regulations and institutions for enacting ethical AI. Yet, what is lacking is a practical framework and means by which AI can be assessed as un/ethical. This paper provides an overview of an applied analytical framework for assessing the ethics of AI. It notes that AI (or algorithmic) decision-making is an outcome of data, code, context and use. Using these four categories, the paper articulates key questions necessary to determine the potential ethical challenges of using an AI/algorithm in decision making, and provides the basis for their articulation within a practical toolkit that can be demonstrated against known AI decision-making tools.","",""
"2019","Artifictional intelligence: against humanity’s surrender to computers","","",""
"2019","Hubert Dreyfus, the artificial and the perspective of a doubled philosophy","","",""
"2019","Exploratory analysis of Sony AIBO users","","",""
"2019","Dreyfus on the “Fringe”: information processing, intelligent activity, and the future of thinking machines","","",""
"2019","Comparative legal study on privacy and personal data protection for robots equipped with artificial intelligence: looking at functional and technological aspects","","",""
"2019","Vulnerability under the gaze of robots: relations among humans and robots","","",""
"2019","Posthuman learning: AI from novice to expert?","","",""
"2019","The brain as artificial intelligence: prospecting the frontiers of neuroscience","","",""
"2019","Primacy of I–you connectedness revisited: some implications for AI and robotics","","",""
"2019","Robot use self-efficacy in healthcare work (RUSH): development and validation of a new measure","","",""
"2019","The rise of the robots and the crisis of moral patiency","","",""
"2019","Non-artificial non-intelligence: Amazon’s Alexa and the frictions of AI","","",""
"2019","The problem of superintelligence: political, not technological","","",""
"2019","Tony D. Sampson: The Assemblage Brain. Sense Making in Neuroculture","","",""
"2019","Rethinking the I-You relation through dialogical philosophy in the Ethics of AI and robotics","","",""
"2019","The human relationship in the ethics of robotics: a call to Martin Buber’s I and Thou","","",""
"2019","The HeartMath coherence model: implications and challenges for artificial intelligence and robotics","","",""
"2019","Heart intelligence: heuristic phenomenological investigation into the coherence experience using HeartMath methods","","",""
"2019","AI, agency and responsibility: the VW fraud case and beyond","","",""
"2019","Evil and roboethics in management studies","","",""
"2019","Why AI shall emerge in the one of possible worlds?","","",""
"2019","The Vitruvian robot","","",""
"2019","Can a machine think (anything new)? Automation beyond simulation","","",""
"2019","Robot as the “mechanical other”: transcending karmic dilemma","","",""
"2019","The naturalness of artificial intelligence from the evolutionary perspective","","",""
"2019","The spur of the moment: what jazz improvisation tells cognitive science","","",""
"2019","An agent-oriented account of Piaget’s theory of interactional morality","","",""
"2019","GRASP agents: social first, intelligent later","","",""
"2019","Using Dreyfus’ legacy to understand justice in algorithm-based processes","","",""
"2019","Towards a unified framework for developing ethical and practical Turing tests","","",""
"2019","Limiting the discourse of computer and robot anthropomorphism in a research group","","",""
"2019","Is it possible to grow an I–Thou relation with an artificial agent? A dialogistic perspective","","",""
"2019","Are automated vehicles safer than manually driven cars?","","",""
"2019","Machine intelligence: a chimera","","",""
"2019","Screen reading and the creation of new cognitive ecologies","","",""
"2019","Why being dialogical must come before being logical: the need for a hermeneutical–dialogical approach to robotic activities","","",""
"2019","Guest preface: Streams of consciousness: cognition and intelligent devices","","",""
"2019","Augmented societies with mirror worlds","","",""
"2019","The synthetization of human voices","","",""
"2019","Machine learning: A structuralist discipline?","","",""
"2019","Simon Penny (2018): Making sense: cognition, computing, art and embodiment","","",""
"2019","A cross-cultural assessment of the semantic dimensions of intellectual humility","","",""
"2019","Encountering bloody others in mined reality","","",""
"2019","Predicting the ideological orientation during the Spanish 24M elections in Twitter using machine learning","","",""
"2019","AI and education: the importance of teacher and student relations","","",""
"2019","The computational therapeutic: exploring Weizenbaum’s ELIZA as a history of the present","","",""
"2019","Unsupervised by any other name: Hidden layers of knowledge production in artificial intelligence on social media"," Artificial Intelligence (AI) in the form of different machine learning models is applied to Big Data as a way to turn data into valuable knowledge. The rhetoric is that ensuing predictions work well—with a high degree of autonomy and automation. We argue that we need to analyze the process of applying machine learning in depth and highlight at what point human knowledge production takes place in seemingly autonomous work. This article reintroduces classification theory as an important framework for understanding such seemingly invisible knowledge production in the machine learning development and design processes. We suggest a framework for studying such classification closely tied to different steps in the work process and exemplify the framework on two experiments with machine learning applied to Facebook data from one of our labs. By doing so we demonstrate ways in which classification and potential discrimination take place in even seemingly unsupervised and autonomous models. Moving away from concepts of non-supervision and autonomy enable us to understand the underlying classificatory dispositifs in the work process and that this form of analysis constitutes a first step towards governance of artificial intelligence. ","",""
"2019","Challenging algorithmic profiling: The limits of data protection and anti-discrimination in responding to emergent discrimination"," The potential for biases being built into algorithms has been known for some time (e.g., Friedman and Nissenbaum, 1996), yet literature has only recently demonstrated the ways algorithmic profiling can result in social sorting and harm marginalised groups (e.g., Browne, 2015; Eubanks, 2018; Noble, 2018). We contend that with increased algorithmic complexity, biases will become more sophisticated and difficult to identify, control for, or contest. Our argument has four steps: first, we show how harnessing algorithms means that data gathered at a particular place and time relating to specific persons, can be used to build group models applied in different contexts to different persons. Thus, privacy and data protection rights, with their focus on individuals (Coll, 2014; Parsons, 2015), do not protect from the discriminatory potential of algorithmic profiling. Second, we explore the idea that anti-discrimination regulation may be more promising, but acknowledge limitations. Third, we argue that in order to harness anti-discrimination regulation, it needs to confront emergent forms of discrimination or risk creating new invisibilities, including invisibility from existing safeguards. Finally, we outline suggestions to address emergent forms of discrimination and exclusionary invisibilities via intersectional and post-colonial analysis. ","",""
"2019","Beyond mystery: Putting algorithmic accountability in context"," Critical algorithm scholarship has demonstrated the difficulties of attributing accountability for the actions and effects of algorithmic systems. In this commentary, we argue that we cannot stop at denouncing the lack of accountability for algorithms and their effects but must engage the broader systems and distributed agencies that algorithmic systems exist within; including standards, regulations, technologies, and social relations. To this end, we explore accountability in “the Generated Detective,” an algorithmically generated comic. Taking up the mantle of detectives ourselves, we investigate accountability in relation to this piece of experimental fiction. We problematize efforts to effect accountability through transparency by undertaking a simple operation: asking for permission to re-publish a set of the algorithmically selected and modified words and images which make the frames of the comic. Recounting this process, we demonstrate slippage between the “complication” of the algorithm and the obscurity of the legal and institutional structures in which it exists. ","",""
"2019","Recalibration in counting and accounting practices: Dealing with algorithmic output in public and private","Algorithms are increasingly affecting us in our daily lives. They seem to be everywhere, yet they are seldom seen by the humans dealing with the consequences that result from them. Yet, in recent theorisations, there is a risk that the algorithm is being given too much prominence. This article addresses the interaction between algorithmic outputs and the humans engaging with them by drawing on studies of two distinct empirical fields – self-quantification and audit controls of taxpayers. We explore recalibration as a way to understand the practices and processes involved when, on the one hand, decisions are made based on results from algorithmic calculations in counting and accounting software, and on the other hand, when decisions are made based on human experience/knowledge. In particular, we are concerned with moments when an algorithmic output differs from expectations of ‘normalcy’ and ‘normativity’ in any given situation. This could be a ‘normal’ relation between sales and VAT deductions for a business, or a ‘normal’ number of steps one takes in a day, or ‘normative’ as it is according to the book, following guidelines and recommendations from other sources. In these moments, we argue that a process of recalibration occurs – an effortful moment where, rather than treat the algorithmic output as given, individuals’ tacit knowledge, experiences and intuition are brought into play to address the deviation from the normal and normative.","",""
"2019","What are neural networks <i>not</i> good at? On artificial creativity"," This article discusses three dimensions of creativity: metaphorical thinking; social interaction; and going beyond extrapolation in predictions. An overview of applications of neural networks in these three areas is offered. It is argued that the current reliance on the apparatus of statistical regression limits the scope of possibilities for neural networks in general, and in moving towards artificial creativity in particular. Artificial creativity may require revising some foundational principles on which neural networks are currently built. ","",""
"2019","Occluded algorithms"," Two definitions of algorithm, their uses, and their implied models of computing in society, are reviewed. The first, termed the structural programming definition, aligns more with usage in computer science, and as the name suggests, the intellectual project of structured programming. The second, termed the systemic definition, is more informal and emerges from ethnographic observations of discussions of software in both professional and everyday settings. Specific examples of locating algorithms within modern codebases are shared, as well as code directly impacting social and ethical concerns. The structural distinction between algorithms and social concerns is explained as mirroring the engineering construct of algorithms and data structures. It is proposed that, rather than this separation being an attempt to enforce a professional boundary and evade social responsibility, it is a crucial technical distinction within code which makes it clearer and more transparent. The power structures reinforced by the broader, cultural interpretations of algorithm are reconsidered, along with what it would mean for software to have an inclusive design culture. ","",""
"2019","Algorithmic anxiety: Masks and camouflage in artistic imaginaries of facial recognition algorithms"," This paper discusses prominent examples of what we call “algorithmic anxiety” in artworks engaging with algorithms. In particular, we consider the ways in which artists such as Zach Blas, Adam Harvey and Sterling Crispin design artworks to consider and critique the algorithmic normativities that materialize in facial recognition technologies. Many of the artworks we consider center on the face, and use either camouflage technology or forms of masking to counter the surveillance effects of recognition technologies. Analyzing their works, we argue they on the one hand reiterate and reify a modernist conception of the self when they conjure and imagination of Big Brother surveillance. Yet on the other hand, their emphasis on masks and on camouflage also moves beyond such more conventional critiques of algorithmic normativities, and invites reflection on ways of relating to technology beyond the affirmation of the liberal, privacy-obsessed self. In this way, and in particular by foregrounding the relational modalities of the mask and of camouflage, we argue academic observers of algorithmic recognition technologies can find inspiration in artistic algorithmic imaginaries. ","",""
"2019","How should we theorize algorithms? Five ideal types in analyzing algorithmic normativities","The power of algorithms has become a familiar topic in society, media, and the social sciences. It is increasingly common to argue that, for instance, algorithms automate inequality, that they are biased black boxes that reproduce racism, or that they control our money and information. Implicit in many of these discussions is that algorithms are permeated with normativities, and that these normativities shape society. The aim of this editorial is double: First, it contributes to a more nuanced discussion about algorithms by discussing how we, as social scientists, think about algorithms in relation to five theoretical ideal types. For instance, what does it mean to go under the hood of the algorithm and what does it mean to stay above it? Second, it introduces the contributions to this special theme by situating them in relation to these five ideal types. By doing this, the editorial aims to contribute to an increased analytical awareness of how algorithms are theorized in society and culture. The articles in the special theme deal with algorithms in different settings, ranging from farming, schools, and self-tracking to AIDS, nuclear power plants, and surveillance. The contributions thus explore, both theoretically and empirically, different settings where algorithms are intertwined with normativities.","",""
"2019","Algorithms as folding: Reframing the analytical focus","This article proposes an analytical approach to algorithms that stresses operations of folding. The aim of this approach is to broaden the common analytical focus on algorithms as biased and opaque black boxes, and to instead highlight the many relations that algorithms are interwoven with. Our proposed approach thus highlights how algorithms fold heterogeneous things: data, methods and objects with multiple ethical and political effects. We exemplify the utility of our approach by proposing three specific operations of folding— proximation, universalisation and normalisation. The article develops these three operations through four empirical vignettes, drawn from different settings that deal with algorithms in relation to AIDS, Zika and stock markets. In proposing this analytical approach, we wish to highlight the many different attachments and relations that algorithms enfold. The approach thus aims to produce accounts that highlight how algorithms dynamically combine and reconfigure different social and material heterogeneities as well as the ethical, normative and political consequences of these reconfigurations.","",""
"2019","Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns"," Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated decision-making systems. We address the topic of transparency in artificial intelligence by integrating legal, social, and ethical aspects. We first investigate the ratio legis of the transparency requirement in the General Data Protection Regulation and its ethical underpinnings, showing its focus on the provision of information and explanation. We then discuss the pitfalls with respect to this requirement by focusing on the significance of contextual and performative factors in the implementation of transparency. We show that human–computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies due to the impact of a wide range of contextual factors, including performative aspects. We conclude by integrating the information- and explanation-based approach to transparency with the critical contextual approach, proposing that transparency as required by the General Data Protection Regulation in itself may be insufficient to achieve the positive goals associated with transparency. Instead, we propose to understand transparency relationally, where information provision is conceptualized as communication between technology providers and users, and where assessments of trustworthiness based on contextual factors mediate the value of transparency communications. This relational concept of transparency points to future research directions for the study of transparency in artificial intelligence systems and should be taken into account in policymaking. ","",""
"2019","Perspectives on algorithmic normativities: engineers, objects, activities","This contribution aims at proposing a framework for articulating different kinds of “normativities” that are and can be attributed to “algorithmic systems.” The technical normativity manifests itself through the lineage of technical objects. The norm expresses a technical scheme’s becoming as it mutates through, but also resists, inventions. The genealogy of neural networks shall provide a powerful illustration of this dynamic by engaging with their concrete functioning as well as their unsuspected potentialities. The socio-technical normativity accounts for the manners in which engineers, as actors folded into socio-technical networks, willingly or unwittingly, infuse technical objects with values materialized in the system. Surveillance systems’ design will serve here to instantiate the ongoing mediation through which algorithmic systems are endowed with specific capacities. The behavioral normativity is the normative activity, in which both organic and mechanical behaviors are actively participating, undoing the identification of machines with “norm following,” and organisms with “norminstitution”. This proposition productively accounts for the singularity of machine learning algorithms, explored here through the case of recommender systems. The paper will provide substantial discussions of the notions of “normative” by cutting across history and philosophy of science, legal, and critical theory, as well as “algorithmics,” and by confronting our studies led in engineering laboratories with critical algorithm studies.","",""
"2019","Deep new: The shifting narratives of artificial intelligence from Deep Blue to AlphaGo"," The article compares two key events that marked the narratives around the emergence of artificial intelligence (AI) in two different time frames: the game series between the Russian world champion Garry Kasparov and the IBM supercomputer Deep Blue held in New York in 1997, and the Go game series between the South Korean champion Lee Sedol and DeepMind’s AI AlphaGo held in Seoul in 2016. Relying on a corpus of primary and secondary sources such as newspapers and specialized magazines, biographic books, the live broadcasts and the main documentaries reporting the challenges, the article investigates the way in which IBM and Google DeepMind used the human–machine competition to narrate the emergence of a new, deeper, form of AI. On the one hand, the Kasparov–Deep Blue match was presented by broadcasting media and IBM itself as a conflictual and competitive form of struggle between human kind and a hardware-based, obscure and humanlike player. While on the other hand, the social and symbolic message promoted by DeepMind and the media conveyed a cooperative and fruitful interaction with a new software-based, transparent and un-humanlike form of AI. The analysis of the case studies reveals how AI companies mix narrative tropes, gaming and spectacle in order to promote the newness and the main features of their products. In particular, recent narratives of AI based on human feelings and values such as beauty and trust can shape the way in which the presence of intelligent systems is accepted and integrated in everyday life. ","",""
"2019","The Work of Art in the Age of Artificial Intelligence: What Artists Can Teach Us About the Ethics of Data Practice","Problematic use of data, patterns of bias emerging in AI systems, and the role of platforms like Facebook and Twitter during elections have thrown the issue of data ethics into sharp relief. Yet the focus of conversations about data ethics has centered on computer scientists, engineers, and designers, with far less attention paid to the digital practices of artists and others in the cultural sector. Artists have historically deployed new technologies in unexpected and often prescient ways, making them a community able to speak directly to the changing and nuanced ethical questions faced by those who use data and machine learning systems. We conducted interviews with thirty-three artists working with digital data, with a focus on how artists prefigure and commonly challenge data practices and ethical concerns of computer scientists, researchers, and the wider population. We found artists were frequently working to produce a sense of defamiliarization and critical distance from contemporary digital technologies in their audiences. The ethics of using large-scale data and AI systems for these artists were generally developed in ongoing conversations with other practitioners in their communities and in relation to a longer history of art practice.","",""
"2019","Down the deep rabbit hole: Untangling deep learning from machine learning and artificial intelligence","Interest in deep learning, machine learning, and artificial intelligence from industry and the general public has reached a fever pitch recently. However, these terms are frequently misused, confused, and conflated. This paper serves as a non-technical guide for those interested in a high-level understanding of these increasingly influential notions by exploring briefly the historical context of deep learning, its public presence, and growing concerns over the limitations of these techniques. As a first step, artificial intelligence and machine learning are defined. Next, an overview of the historical background of deep learning reveals its wide scope and deep roots. A case study of a major deep learning implementation is presented in order to analyze public perceptions shaped by companies focused on technology. Finally, a review of deep learning limitations illustrates systemic vulnerabilities and a growing sense of concern over these systems.","",""
"2019","Take Back the Algorithms!  A Media Theory of Commonistic Affordance","This essay critiques the â€˜black-boxingâ€™ of many computational processes, which are argued to result in a kind of â€˜unaffordabilityâ€™ of algorithms. By engaging with current theoretical debates on â€˜commoningâ€™ â€“ signifying a non-profit-oriented, solidarity-based approach to sharing, maintaining, and disseminating knowledge and experience â€“ the essay offers a formulation of commonistic affordance in algorithmic contexts. Through the discussion of widely used computational tools such as the Viola-Jones object detection framework, radical steps towards a â€˜making affordableâ€™ of algorithms are outlined, and the widespread corporate propertisation of computation processes is contrasted with a speculative vision of algorithmic commoning.   ","",""
"2019","You need to show that you are not a robot"," Given that today 60% of Internet traffic is generated by bots, ‘CAPTCHA’ (Completely Automated Public Turing Test to tell Computers and Humans Apart) tests that are supposedly impossible to be done by robots have been introduced. What are the cognitive and emotional effects of these tests on Internet users? Does this request to demonstrate they are not a robot affect users’ identity as human beings? To answer these questions, we selected two groups (117 and 116 respondents, respectively). An online questionnaire that differed only in the task was proposed: we asked the first group to complete some CAPTCHA tests, and the second group to complete some logic tests. In addition to other questions in both versions, we introduced the TLX scale (NASA). Preliminary results show that CAPTCHA execution is associated with feelings of alienation and that the user’s self-perception of humanity is influenced by the execution of the two different types of test. ","",""
"2019","If software is narrative: Joseph Weizenbaum, artificial intelligence and the biographies of ELIZA"," Software is usually studied in terms of the changes triggered by its operations in the material world. Yet to understand its social and cultural impact, one needs to examine also the different narratives that circulate about it. Software’s opacity, in fact, makes it prone to being translated into a plurality of narratives that help people make sense of its functioning and presence. Drawing from the case of Joseph Weizenbaum’s ELIZA, widely considered the first chatbot ever created, this article proposes a theoretical framework based on the concept of ‘biographies of media’ to illuminate the dynamics and implications of software’s discursive life. The case of ELIZA is particularly relevant in this regard because it became the centre of competing narratives, whose trajectories transcended the actual functioning of this programme and shaped key controversies about the implications of computing and artificial intelligence. ","",""
"2019","First encounter with robot Alpha: How individual differences interact with vocal and kinetic cues in users’ social responses"," The Computers are Social Actors (CASA) paradigm was proposed more than two decades ago to understand humans’ interaction with computer technologies. Today, as emerging technologies like social robots become more personal and persuasive, questions of how users respond to them socially, what individual factors leverage the relationship, and what constitutes the social influence of these technologies need to be addressed. A lab experiment was conducted to examine the interactions between individual differences and social robots’ vocal and kinetic cues. Results suggested that users developed more trust in a social robot with a human voice than with a synthetic voice. Users also developed more intimacy and interest in the social robot when it was paired with humanlike gestures. Moreover, individual differences including users’ gender, attitudes toward robots, and robot exposure affected their psychological responses. The theoretical, practical, and ethical value of the findings was further discussed in the study. ","",""
"2019","Robot Rights","","",""
"2019","Machine Learning in Context, or Learning from LANDR: Artificial Intelligence and the Platformization of Music Mastering"," This article proposes a contextualist approach to machine learning and aesthetics, using LANDR, an online platform that offers automated music mastering and that trumpets its use of supervised machine learning, branded as artificial intelligence (AI). Increasingly, machine learning will become an integral part of the processing of sounds and images, shaping the way our culture sounds, looks, and feels. Yet we cannot know exactly how much of a role or what role machine learning plays in LANDR. To parochialize the machine learning part of what LANDR does, this study spirals in from bigger contexts to smaller ones: LANDR’s place between the new media industry and the mastering industry; the music scene in their home city, Montreal, Quebec; LANDR use by DIY musicians and independent engineers; and, finally, the LANDR interface and the sound it produces in use. While LANDR claims to automate the work of mastering engineers, it appears to expand and morph the definition of mastering itself: it devalues people’s aesthetic labor as it establishes higher standards for recordings online. And unlike many other new media firms, LANDR’s connection to its local music scene has been essential to its development, growth, and authority, even as they have since moved on from that scene, and even as the relationship was never fully reciprocal. ","",""
"2019","Quantifying co-creative writing experiences","ABSTRACT Collaboration with artificial intelligence (AI) is a growing trend even in the field of creativity. This paper examines which quantitative metrics can be used to comparatively analyse human-computer co-creativity with children. To study this question, 24 schoolchildren of age 10–11 wrote a poem with three co-creative poetry writing processes: a human-computer, a human-human, and a human-human-computer process. The computational participant in the processes was an AI-based application called the Poetry Machine. The children were asked to evaluate their user experience with a 5-point Likert-type questionnaire after each writing process and a comparative questionnaire after finishing all processes. The metrics used in the evaluation were immediate fun, long-term enjoyment, creativity, self-expression, outcome satisfaction, ease of starting and finishing writing, quality of ideas and support from others, and ownership. Significant differences were found in fun, long-term enjoyment, quality of ideas, support, and ownership. The high number of statistically relevant results was enabled by exposing all participants to all writing processes, and the comparative questionnaire. The human-human-computer process was evaluated the best in long-term enjoyment and the human-computer process the weakest in support and idea quality. Creativity and ease of finishing writing turned out to be outlining metrics for the co-creative processes.","",""
"2019","Manual Annotation of Unsupervised Models : Close and Distant Reading of Politics on Reddit","This article offers a methodological contribution to manually-assisted topic modeling. With the availability of vast amounts of (online) texts, performing full scale literary analysis using a close  approach is not practically feasible. The set of alternatives proposed by Franco Moretti (2000) under the umbrella term of  reading aims to show broad patterns that can be found throughout the entire text collection. After a survey of literary-critical practices that combine close and distant  methods, we use manual annotations of a thread on Reddit, both to evaluate an LDA model, and to provide information that topic modeling lacks. We also make a case for applying these  techniques that originate in literary  more broadly to online, non-literary contexts. Given a large collection of posts from a Reddit thread, we compare a manual, close  analysis against an automatic, computational distant  approach based on topic modeling using LDA. For each text in the collection, we label the contents, effectively clustering related texts. Next, we evaluate the similarity of the respective outcomes of the two approaches. Our results show that the computational content/topic-based labeling partially overlaps with the manual annotation. However, the close  approach not only identifies texts with similar content, but also those with similar function. The differences in annotation approaches require rethinking the purpose of computational techniques in  analysis. Thus, we present a model that could be valuable for scholars who have a small amount of manual annotation that could be used to tune an unsupervised model of a larger dataset.","",""
"2019","The privacy implications of social robots: Scoping review and expert interviews"," In this contribution, we investigate the privacy implications of social robots as an emerging mobile technology. Drawing on a scoping literature review and expert interviews, we show how social robots come with privacy implications that go beyond those of established mobile technology. Social robots challenge not only users’ informational privacy but also affect their physical, psychological, and social privacy due to their autonomy and potential for social bonding. These distinctive privacy challenges require study from varied theoretical perspectives, with contextual privacy and human–machine communication emerging as particularly fruitful lenses. Findings also point to an increasing focus on technological privacy solutions, complementing an evolving legal landscape as well as a strengthening of user agency and literacy. ","",""
"2020","Becoming and Individuation on the Encounter between Technical Apparatus and Natural System","This essay sheds lights on the framing process during the research on the crossing between natural and artificial systems. To approach this, we must outline the machine-natural system relation. From this notion, technology is not seen as an external thing, nor even in contrast to an imaginary of nature, but as an effect that emerges from our thinking and revealing being that, in many cases, may be reduced to an issue of knowledge and action. Here, we want to consider the concept of transduction from Gilbert Simondon as one possible framework for considering the socio-technological actions at stake. His thought offers a detailed conceptual vocabulary for the question of individuation as a “revelation process”, a concern with how things come into existence and proceed temporally as projective entities.Moreover, our approach to the work of philosopher Simondon marked the starting point of our interest and approach to the issue of technique and its politics. From this perspective, the reflection given by Simondon in his thesis on the Individuation and the Mode of Existence of Technical Objects, is to trace certain reasons that are necessary for the development of this project and helping to explain it. In first place, Simondon does not state a specific regime of “human individuation”. The possibility of a psychic and collective individuation is produced, as is manifested when addressing the structure of his main thesis, at the heart of biological individuation; Simondon strongly attacks the anthropocentric tendencies that attempt to establish a defining boundary between biological and psychic reality. We may presume, then, that the issue of language as a defining and differencing element of the human does not interest him; it is at this point that our project begins to focus on employing the transduction of the téchnē as a metaphor of life (Espinoza Lolas et al.); regarding the limits that language may imply for the conformation and expression of the psychic reality. In second place, this critique to the economy of attention present across our research and in Simondon’s thinking seeks to introduce a hypothesis raised in another direction: towards the issue of the technique. During the introduction of his Mode of Existence of Technical Objects, Simondon shows some urgency in the need to approach the reality of technical objects as an autonomous reality and as a configuring reality of the psychic and collective individualisation. Facing the general importance granted to language as a key element of the historical and hermeneutical, even ontological, aspects of the human being, Simondon considers that the technique is the reality that plays the fundamental role of mediating between the human being and the world.Following these observations, a possible question that will guide our research arises: How do the technologisation and informatisation of the cultural techniques alter the nature itself of the knowing of the affection of being with others (people, things, animals)? In the hypothesis of this investigation we claim that—insofar as we deliver an approach and perspective on the technologisation of the world as a process of individuation (considering Simondon’s concept in this becoming, in which an artificial agent and its medium may get out of phase to solve its tensions and give rise to physical or living individuals that constitute their system and go through a series of metastable equilibria)—it’s possible to prove this capacity of invention as a clear example of a form of transindividual individuation (referring to the human being), that thanks to the information that the artificial agent acquires and recovers by means of its “imagination”, which integrates in its perception and affectivity, enables the creation of new norms or artifacts installing in its becoming, as is the case of bioeconomy and cognitive capitalism (Fumagalli 219). It is imperious to observe and analyse the fact that the concept of nature must be integrated along with the concept of Cosmotecnia (Hui 3) to avoid the opposition between nature and technique in conceptual terms, and that is the reason why in the following section we will mention a third memory that is inscribed in this concept. There is no linear time development in human history from nature to technique, from nature to politics.The Extended MindThe idea of memory as something transmissible is important when thinking of the present, there is no humanity outside the technical, neither prior to the technical, and it is important to safeguard this idea to highlight the phýsis/téchnē dichotomy presented by Simondon and Stigler. It is erroneous to think that some entity may exceed the human, that it has any exteriority when it is the materialization of the human forms, or even more, that the human is crossed by it and is not separable. For French philosopher Bernard Stiegler there is no human nature without technique, and vice versa (Stigler 223). Here appears the issue of knowing which are the limits where “the body of the human me might stop” (Hutinel 44), a first glimpse of externalized memory was the flint axe, which is made by using other tools, even when its use is unknown. Its mere existence preserves a knowledge that goes beyond who made it, or its genetic or epigenetic transmission is preserved beyond the organic.We raise the question about a phýsis coming from the téchnē, it is a central topic that dominates the discussion nowadays, about technology and its ability to have a transforming effect over every area of contemporary life and human beings themselves. It is being “revealed” that the true qualitative novelty of the technological improves that happen in front of our eyes resides not only in the appearance of new practices that are related to any particular scientific research. We must point out the evident tension between bíos and zôê during the process of this adaptation, which is an ontological one, but we also witness how the recursivity becomes a modus operandi during this process, which is both social and technological. Just as the philosophy of nature, the philosophy of biology confronts its own limit under the light shed by the recursive algorithms implemented as a dominant way of adaptation, which is what Deleuze called societies of control (Deleuze 165). At the same time, there is an artificial selection (instead of a natural selection) imposed by the politics of transhumanism (for example, human improvement, genetic engineering).In this direction, a first aspect to consider resides in that life, held as an object of power and politics, does not constitute a “natural life”, but the result of a technical production from which its “nature” develops, as well as the possibilities of its deployment. Now then, it is precisely due to this gesture that Stiegler longs to distinguish between what is originary in mankind and its artefactual or artificial becoming: “the prosthesis is not a simple extension of the human body, it is the constitution of said body insofar as ‘human’ (the quotes belong to the constitution). It is not a ‘medium’ for mankind, but its end, and it is known the essential mistakenness of the expression, ‘the end of mankind’” (Stiegler 9). Before such phenomena, it is appropriate to lay out a reflexive methodology centered in observing and analysing the aforementioned idea by Stiegler that there is no mankind without techniques; and there is no technique without mankind (Stigler 223). This implies that this idea of téchnē comprises both the techniques needed to create things, as the technical products resulting from these techniques. The word “techniques” also becomes ambiguous among the modern technology of machines and the primitive “tools” and their techniques, whether they have become art of craft, things that we would not necessarily think as “technology”. What Stiegler is suggesting here is to describe the scope of the term téchnē within an ontogenetic and phylogenetic process of the human being; providing us a reflection about what do we “possess as a fundamental thing” for our being as humans is also fundamental to how “we experience time” since the externalization of our memory into our tools, which Stiegler understands as a “third kind” of memory which is separated from the internal memory that is individually acquired from our brain (epigenetic), and the biological evolutive memory that is inherited from our ancestors (phylogenetic); Stiegler calls this kind of evolutive process epiphylogenetic or epiphylogenesis. Therefore, we could argue that we are defined by this process of epiphylogenesis, and that we are constituted by a past that we ourselves, as individuals, have not lived; this past is delivered to us through culture, which is the fusion of the “technical objects that embody the knowledge of our ancestors, tools that we adopt to transform our surroundings” (Stiegler 177). These supports of external memory (this is, exteriorisations of the consciousness) provide a new collectivisation of the consciousness that exists beyond the individual.The current trend of investigation of ontogeny and phylogeny is driven by the growing consensus both in sciences and humanities in that the living world in every one of its aspects – biologic, semiotic, economic, affective, social, etc. – escapes the finite scheme of description and representation. It is for this reason that authors such as Matteo Pasquinelli refer, in a more modest way, to the idea of “augmented intelligence” (9), reminding us that there is a posthuman legacy between human and machine that still is problematic, “though the machines manifest different degrees of autonomous agency” (Pasquinelli 11).For Simondon, and this is his revolutionary contribution to philosophy, one should think individuation not from the perspective of the individual, but from the point of view of the process that originated it. In other words, individuation must be thought in terms of a process that not only takes for granted the individual but understands it as a result.In Simondon’s words:If, on the contrary, one supposes that individuation does not only produce the individual, one would not attempt to pass quickly through the stage of individuation in order arrive at the final reality that is the individual--one would attempt to grasp the ontogenesis in the entire progression of its reality, and to know the individual through the individuation, rather than the individuation through the individual. (5)Therefore, the epistemological problem does not fall in how the téchnē flees the human domain in its course to become technologies, but in how these “exteriorization” processes (Stiegler 213) alter the concepts themselves of number, image, comparison, space, time, or city, to give a few examples. However, the anthropological category of “exteriorization” does not bring entirely justice to these processes, as they work in a retroactive and recursive manner in the original techniques. Along with the concept of text and book, the practice of reading has also changed during the course of digitalisation and algorithmisation of the processing of knowledge; alongside with the concept of comparison, the practice of comparison has changed since the comparison (i.e. of images) has become an operation that is based in the extraction of data and automatic learning. On the other side, in reverse, we must consider, in an archeological and mediatic fashion, the technological state of life as a starting point from which we must ask what cultural techniques were employed in first place. Asking: How does the informatisation of the cultural techniques produce new forms of subjectivity? How does the concept of cultural techniques already imply the idea of “chains of operations” and, therefore, a permanent (retro)coupling between the living and the non-living agency?This reveals that classical cultural techniques such as indexation or labelling, for example, have acquired ontological powers in the Google era: only what is labelled exists; only what can be searched is absolute. At the same time, in the fantasies of the mediatic corporations, the variety of objects that can be labelled (including people) tends to be coextensive with the world of the phenomena itself (if not the real world), which will then always be only an augmented version of itself.Technology became important for contemporary knowledge only through mediation; therefore, the use of tools could not be the consequence of an extremely well-developed brain. On the contrary, the development of increasingly sophisticated tools took place at the same pace as the development of the brain, as Leroi-Gourhan attempts to probe when studying the history of tools together with the history of the human skeleton and brain. And what he managed to demonstrate is that the history of technique and the history of the human being run in parallel lines; they are, if not equal, at least inextricable. Even today, the progress of knowledge is still not completely subordinated to the technological inversion (Lyotard 37). In short, human evolution is inseparable from the evolution of the téchne, the evolution of technology. One may simply think the human being as a natural animal, isolated from the external material world. What he becomes and what he is, is essentially bonded to the techniques, from the very beginning. Leroi-Gourhan puts it this way in his text Gesture and Speech: “the apparition of tools as a species ... feature that marks the boundary between animals and humans” (90).To understand the behavior of the technological systems is essential for our ability to control their actions, to harvest their benefits and to minimize their damage. Here it is argued that this requires a wide agenda of scientific investigation to study the behavior of the machine that incorporates and broadens the biotechnological discipline, and includes knowledges coming from all sciences. In some way, Simondon sensed this encounter of knowledges, and proposed the concept of the Allagmatic, or theory of operations, “constituted by a systematized set of particular knowledges” (Simondon 469). We could attempt to begin by describing a set of questions that are fundamental for this emerging field, and then exploring the technical, legal, and institutional limitations in the study of technological agency.Information, Communication and SignificationTo establish the relation between information and communication, we will speak from the following two perspectives: first with Norbert Wiener, then with Simondon. We will see how the concept of information is essential to start understanding communication in an artificial agent.On one side, we have the notion from Wiener about information that is demarcated in his project about cybernetics. Cybernetics is the study of communication and control through the inquiry of messages in animals, human beings, and machines. This idea of information arises from the interrelation with the surrounding. Wiener defines it as the “content of what is an interchange object with the external world, while we adjust to it and make it adjust to us” (Wiener 17-18). In other words, we receive and use information since we interact with the world in which we live. It is in this sense that information is connected to the idea of feedback that is defined as the exchange and interaction of information in our systems or other systems. In Wiener’s own words, feedback is “the property of adjusting the future behavior to facts of the past” (31).Information, for Wiener, is influenced, at the same time, by the mathematic and probabilistic idea from the theory of information. Wiener refers to the amount of information that finds its starting point at the mechanics of statistics, along with the concept of entropy, inasmuch that the information is opposed to it. Therefore, information, by supplying a set of messages, indicates a measure of organisation. Argentinian philosopher Pablo Rodríguez adds that “information [for Wiener] is a new physical category of the universe. [It is] the measure of organization of any entity, an organization without which the material and energetic systems wouldn’t be able to survive” (2-3). This way, we have that information responds to the measure of organization and self-regulation of a given system.Moreover, and almost in complete contrast, we have the concept given by Simondon, where information is applicable to the whole possible range: animals, machines, human beings, molecules, crystals, etc. In this sense, it is more versatile, as it exceeds the domains of the technique. To understand well the scope of this concept we will approach it from two definitions. In first place, Simondon, in his conference Amplification in the Process of Information, in the book Communication and Information, claims that information “is not a thing, but the operation of a thing that arrives to a system and produces a transformation in there. The information can’t be defined beyond this act of transformative incidence, and the operation of receiving” (Simondon 139). From this definition it follows the idea of modulation, just when he refers to the “transformation” and “act of transformative incidence” modulation corresponds to the energy that flows amplified during that transformation that occurs within a system.There is a second definition of information that Simondon provides in his thesis Individuation in Light of Notions of Form and Information, in which he claims that: “the information signal is not just what is to be transmitted … it is also that what must be received, this is, what must adopt a signification” (Simondon 281). In this definition Simondon clearly distances himself from Wiener’s cybernetics, insofar as it deals with information as that which must be received, and not that that is to be transmitted. Although Simondon refers to a link between information and signification, this last aspect is not measured in linguistic terms. It rather expresses the decodification of a given code. This is, signification, and information as well, are the result of a disparity of energies, namely, between the overlaying of two possible states (0 and 1, or on and off).This is a central point of divergence with Wiener, as he refers to information in terms of transference of messages, while Simondon does it in terms of transformation of energies. This way, Simondon adds an energy element to the traditional definition of information, which now works as an operation, based in the transformation of energies as a result of a disparity or the overlaying of two possible elements within a system (recipient). It is according to this innovative element that modulation operates in a metastable system. And this is precisely the last concept we need to clarify: the idea of metastability and its relationship with the recipient-system.Metastability is an expression that finds its origins in thermodynamics. Philosophy traditionally operates around the idea of the stability of the being, while Simondon’s proposal states that the being is its becoming. This way, metastability is the condition of possibility of the individuation insofar as the metastable medium leaves behind a remainder of energy for future individuation processes. Thus, metastability refers to the temporal equilibrium of a system that remains in time, as it maintains within itself potential energy, useful for other future individuations.Returning to the conference Amplification in the Process of Information, Simondon points out that “the recipient metastability is the condition of efficiency of the incident information” (139). In such sense, we may claim that there is no information if the signal is not received. Therefore, the recipient is a necessary condition for said information to be given. Simondon understands the recipient as a mixed system (a quasi-system): on one hand, it must be isolated in terms of energy, and it must count with a membrane that allows it to not spend all the energy at the same time; on the other hand, it must be heteronomous, as it depends on an external input of information to activate the system (recipient).The metastable medium is the one indicated to understand the artificial agent, as it leaves the possibility open for the potential energy to manifest and not be spent all at once, but to leave a remainder useful for future modulations, and so, new transformations may occur. At the same time, Simondon’s concept of information is the most convenient when referring to communication and the relationship with the medium, primarily for its property of modulating potential energy. Nevertheless, it is also necessary to retrieve the idea of feedback from Wiener, as it is in the relationship of the artificial agent with its surrounding (and the world) that information is given, and it may flow amplified through its system. By this, significations manage to decode the internal code of the artificial agent, which represents the first gesture towards the opening of the communication.ConclusionThe hypotheses on extended cognition are subject to a huge amount of debate in the artistic, philosophical, and science of cognition circles nowadays, but their implications extend further beyond metaphysics and sciences of the mind. It is apparent that we have just began to scratch the surface of the social sphere in a broader way; realising that these start from cultural branches of the sight; as our minds are; if our minds are partially poured into our smartphones and even in our homes, then it is not a transformation in the human nature, but the latest manifestation of an ancient human ontology of the organic cognitive and informative systems dynamically assembled.It is to this condition that the critical digital humanities and every form of critique should answer. This is due to an attempt to dig out the delays and ruptures within the systems of mass media, by adding the relentless belief in real time as the future, to remind that systems always involve an encounter with a radical “strangeness” or “alienity”, an incommensurability between the future and the desire that turns into the radical potential of many of our contemporary social movements and politics. Our challenge in our critical job is to dismantle the practice of the representation and to reincorporate it to different forms of space and experience that are not reactionary but imaginary. What we attempt to bring into the light here is the need to get every spectator to notice the limits of the machinic vision and to acknowledge the role of image in the recruitment of liminal energies for the capital. The final objective of this essay will be to see that nature possesses the technique of an artist who renders contingency into necessity and inscribes the infinite within the finite, in arts it is not the figure of nature that corresponds to individuation but rather the artist whose task is not only to render contingency necessary as its operation, but also aim for an elevation of the audience as a form of revelation. The artist is he who opens up, through his or her work, a process of transindividuation, meaning a psychical and collective individuation.ReferencesDeleuze, Gilles. “Post-Script on Control Societies.” Polis 13 (2006): 1-7. 14 Feb. 2020 &lt;http://journals.openedition.org/polis/5509&gt;.Espinoza Lolas, Ricardo, et al. “On Technology and Life: Fundamental Concepts of Georges Caguilhem and Xavier Zubiri’s Thought.” Ideas y Valores 67.167 (2018): 127-47. 14 Feb. 2020 &lt;http://dx.doi.org/10.15446/ideasyvalores.v67n167.59430&gt;.Fumagalli, Andrea. Bioeconomía y Capitalismo Cognitivo: Hacia un Nuevo Paradigma de Acumulación. Madrid: Traficantes de Sueños, 2010.Hui, Yuk. “On Cosmotechnics: For a Renewed Relation between Technology and Nature in the Anthropocene.” Techné: Research in Philosophy and Technology 21.2/3 (2017): 319-41. 14 Feb. 2020 &lt;https://www.pdcnet.org/techne/content/techne_2017_0021_42769_0319_0341&gt;.Leroi-Gourhan, André. El Gesto y la Palabra. Venezuela: Universidad Central de Venezuela, 1971.———. El Hombre y la Materia: Evolución y Técnica I. Madrid: Taurus, 1989.———. El Medio y la Técnica: Evolución y Técnica II. Madrid: Taurus, 1989.Lyotard, Jean-François. La Condición Postmoderna: Informe sobre el Saber. Madrid: Cátedra, 2006.Pasquinelli, Matteo. “The Spike: On the Growth and Form of Pattern Police.” Nervous Systems 18.5 (2016): 213-20. 14 Feb. 2020 &lt;http://matteopasquinelli.com/spike-pattern-police/&gt;. Rivera Hutinel, Marcela.“Techno-Genesis and Anthropo-Genesis in the Work of Bernard  Stiegler: Or How the Hand Invents the Human.” Liminales, Escritos Sobre Psicología y Sociedad 2.3 (2013): 43-58. 15 Dec. 2019 &lt;http://revistafacso.ucentral.cl/index.php/liminales/article/view/228&gt;.Rodríguez, Pablo. “El Signo de la ‘Sociedad de la Información’ de Cómo la Cibernética y el Estructuralismo Reinventaron la Comunicación.” Question 1.28 (2010): 1-17. 14 Feb. 2020 &lt;https://perio.unlp.edu.ar/ojs/index.php/question/article/view/1064&gt;.Simondon, Gilbert. Comunicación e Información. Buenos Aires: Editorial Cactus, 2015.———. La Individuación: a la luz de las nociones de forma y de información. Buenos Aires: La Cebra/Cactus, 2009 / 2015.———. El Modo de Existencia de los Objetos Técnicos. Buenos Aires: Prometeo, 2007.———. “The Position of the Problem of Ontogenesis.” Parrhesia 7 (2009): 4-16. 4 Nov. 2019 &lt;http://parrhesiajournal.org/parrhesia07/parrhesia07_simondon1.pdf&gt;.Stiegler, Bernard. La Técnica y el Tiempo I. Guipúzcoa: Argitaletxe Hiru, 2002.———. “Temporality and Technical, Psychic and Collective Individuation in the Work of Simondon.” Revista Trilogía Ciencia Tecnología Sociedad 4.6 (2012): 133-46.Wiener, Norbert. Cibernética y Sociedad. Buenos Aires: Editorial Sudamericana, 1958.","",""
"2020","A Flattering Robopocalypse"," RACHAEL. It seems you feel our work is not a benefit to the public.DECKARD. Replicants are like any other machine. They're either a benefit or a hazard. If they're a benefit it's not my problem.RACHAEL. May I ask you a personal question?DECKARD. Yes.RACHAEL. Have you every retired a human by mistake? (Scott 17:30)  CAPTCHAs (henceforth """"captchas"""") are commonplace on today's Internet. Their purpose seems clear: block malicious software, allow human users to pass. But as much as they exclude spambots, captchas often exclude humans with visual and other disabilities (Dzieza; W3C Working Group). Worse yet, more and more advanced captcha-breaking technology has resulted in more and more challenging captchas, raising the barrier between online services and those who would access them. In the words of inclusive design advocate Robin Christopherson, """"CAPTCHAs are evil"""". In this essay I describe how the captcha industry implements a posthuman process that speculative fiction has gestured toward but not grasped. The hostile posthumanity of captcha is not just a technical problem, nor just a problem of usability or access. Rather, captchas convey a design philosophy that asks humans to prove themselves by performing well at disembodied games. This philosophy has its roots in the Turing Test itself, whose terms guide speculation away from the real problems that today's authentication systems present. Drawing the concept of """"procedurality"""" from game studies, I argue that, despite a design goal of separating machines and humans to the benefit of the latter, captchas actually and ironically produce an arms race in which humans have a systematic and increasing disadvantage. This arms race results from the Turing Test's equivocation between human and machine bodies, an assumption whose influence I identify in popular film, science fiction literature, and captcha design discourse. The Captcha Industry and Its Side-Effects Exclusion is an essential function of every cybersecurity system. From denial-of-service attacks to data theft, toxic automated entities constantly seek admission to services they would damage. To remain functional and accessible, Websites need security systems to keep out """"abusive agents"""" (Shet). In cybersecurity, the term """"user authentication"""" refers to the process of distinguishing between abusive agents and welcome users (Jeng et al.). Of the many available authentication techniques, CAPTCHA, """"Completely Automated Public Turing test[s] to tell Computers and Humans Apart"""" (Von Ahn et al. 1465), is one of the most iconic. Although some captchas display a simple checkbox beside a disclaimer to the effect that """"I am not a robot"""" (Shet), these frequently give way to more difficult alternatives: perception tests (fig. 1). Test captchas may show sequences of distorted letters, which a user is supposed to recognise and then type in (Godfrey). Others effectively digitize a game of """"I Spy"""": an image appears, with an instruction to select the parts of it that show a specific type of object (Zhu et al.). A newer type of captcha involves icons rotated upside-down or sideways, the task being to right them (Gossweiler et al.). These latter developments show the influence of gamification (Kani and Nishigaki; Kumar et al.), the design trend where game-like elements figure in serious tasks. Fig. 1: A series of captchas followed by multifactor authentication as a """"quick security check"""" during the author's suspicious attempt to access LinkedIn over a Virtual Private Network Gamified captchas, in using tests of ability to tell humans from computers, invite three problems, of which only the first has received focussed critical attention. I discuss each briefly below, and at greater length in subsequent sections. First, as many commentators have pointed out (W3C Working Group), captchas can accidentally categorise real humans as nonhumans—a technical problem that becomes more likely as captcha-breaking technologies improve (e.g. Tam et al.; Brown et al.). Indeed, the design and breaking of captchas has become an almost self-sustaining subfield in computer science, as researchers review extant captchas, publish methods for breaking them, and publish further captcha designs (e.g. Weng et al.). Such research fuels an industry of captcha-solving services (fig. 2), of which some use automated techniques, and some are """"human-powered"""", employing groups of humans to complete large numbers of captchas, thus clearing the way for automated incursions (Motoyama et al. 2). Captchas now face the quixotic task of using ability tests to distinguish legitimate users from abusers with similar abilities. Fig. 2: Captcha production and captcha breaking: a feedback loop Second, gamified captchas import the feelings of games. When they defeat a real human, the human seems not to have encountered the failure state of an automated procedure, but rather to have lost, or given up on, a game. The same frame of """"gameful""""-ness (McGonigal, under """"Happiness Hacking"""") or """"gameful work"""" (under """"The Rise of the Happiness Engineers""""), supposed to flatter users with a feeling of reward or satisfaction when they complete a challenge, has a different effect in the event of defeat. Gamefulness shifts the fault from procedure to human, suggesting, for the latter, the shameful status of loser. Third, like games, gamified captchas promote a particular strain of logic. Just as other forms of media can be powerful venues for purveying stereotypes, so are gamified captchas, in this case conveying the notion that ability is a legitimate means, not only of apportioning privilege, but of humanising and dehumanising. Humanity thus appears as a status earned, and disability appears not as a stigma, nor an occurrence, but an essence. The latter two problems emerge because the captcha reveals, propagates and naturalises an ideology through mechanised procedures. Below I invoke the concept of """"procedural rhetoric"""" to critique the disembodied notion of humanity that underlies both the original Turing Test and the """"Completely Automated Public Turing test."""" Both tests, I argue, ultimately play to the disadvantage of their human participants. Rhetorical Games, Procedural Rhetoric When videogame studies emerged as an academic field in the early 2000s, once of its first tasks was to legitimise games relative to other types of artefact, especially literary texts (Eskelinen; Aarseth). Scholars sought a framework for discussing how video games, like other more venerable media, can express ideas (Weise). Janet Murray and Ian Bogost looked to the notion of procedure, devising the concepts of """"procedurality"""" (Bogost 3), """"procedural authorship"""" (Murray 171), and """"procedural rhetoric"""" (Bogost 1). From a proceduralist perspective, a videogame is both an object and a medium for inscribing processes. Those processes have two basic types: procedures the game's developers have authored, which script the behaviour of the game as a computer program; and procedures human players respond with, the """"operational logic"""" of gameplay (Bogost 13). Procedurality's two types of procedure, the computerised and the human, have a kind of call-and-response relationship, where the behaviour of the machine calls upon players to respond with their own behaviour patterns. Games thus train their players. Through the training that is play, players acquire habits they bring to other contexts, giving videogames the power not only to express ideas but """"disrupt and change fundamental attitudes and beliefs about the world, leading to potentially significant long-term social change"""" (Bogost ix). That social change can be positive (McGonigal), or it can involve """"dark patterns"""", cases where game procedures provoke and exploit harmful behaviours (Zagal et al.). For example, embedded in many game paradigms is the procedural rhetoric of """"toxic meritocracy"""" (Paul 66), where players earn rewards, status and personal improvement by overcoming challenges, and, especially, excelling where others fail. While meritocracy may seem logical within a strictly competitive arena, its effect in a broader cultural context is to legitimise privileges as the spoils of victory, and maltreatment as the just result of defeat. As game design has influenced other fields, so too has procedurality's applicability expanded. Gamification, """"the use of game design elements in non-game contexts"""" (Deterding et al. 9), is a popular trend in which designers seek to imbue diverse tasks with some of the enjoyment of playing a game (10). Gamification discourse has drawn heavily upon Mihaly Csikszentmihalyi's """"positive psychology"""" (Seligman and Csikszentmihalyi), and especially the speculative psychology of flow (Csikszentmihalyi 51), which promise enormously broad benefits for individuals acting in the """"flow state"""" that challenging play supposedly promotes (75). Gamification has become a celebrated cause, advocated by a group of scholars and designers Sebastian Deterding calls the """"Californian league of gamification evangelists"""" (120), before becoming an object of critical scrutiny (Fuchs et al.). Where gamification goes, it brings its dark patterns with it. In gamified user authentication (Kroeze and Olivier), and particularly gamified captcha, there occurs an intersection of deceptively difficult games, real-world stakes, and users whose differences go often ignored. The Disembodied Arms Race In captcha design research, the concept of disability occurs under the broader umbrella of usability. Usability studies emphasise the fact that some technology pieces are easier to access than others (Yan and El Ahmad). Disability studies, in contrast, emphasises the fact that different users have different capacities to overcome access barriers. Ability is contextual, an intersection of usability and disability, use case and user (Reynolds 443). When used as an index of humanness, ability yields illusive results. In Posthuman Knowledge, Rosi Braidotti begins her conceptual enquiry into the posthuman condition with a contemplation of captcha, asking what it means to tick that checkbox claiming that """"I am not a robot"""" (8), and noting the baffling multiplicity of possible answers. From a practical angle, Junya Kani and Masakatsu Nishigaki write candidly about the problem of distinguishing robot from human: """"no matter how advanced malicious automated programs are, a CAPTCHA that will not pass automated programs is required. Hence, we have to find another human cognitive processing capability to tackle this challenge"""" (40). Kani and Nishigaki try out various human cognitive processing capabilities for the task. Narrative comprehension and humour become candidates: might a captcha ascribe humanity based on human users' ability to determine the correct order of scenes in a film (43)? What about panels in a cartoon (40)? As they seek to assess the soft skills of machines, Kani and Nishigaki set up a drama similar to that of Philip K. Dick's Do Androids Dream of Electric Sheep. Do Androids Dream of Electric Sheep, and its film adaptation, Blade Runner (Scott), describe a spacefaring society populated by both humans and androids. Androids have lesser legal privileges than humans, and in particular face execution—euphemistically called """"retirement""""—for trespassing on planet Earth (Dick 60). Blade Runner gave these androids their more famous name: """"replicant"""". Replicants mostly resemble humans in thought and action, but are reputed to lack the capacity for empathy, so human police, seeking a cognitive processing capability unique to humans, test for empathy to test for humanness (30). But as with captchas, Blade Runner's testing procedure depends upon an automated device whose effectiveness is not certain, prompting the haunting question: """"have you ever retired a human by mistake?"""" (Scott 17:50). Blade Runner's empathy test is part of a long philosophical discourse about the distinction between human and machine (e.g. Putnam; Searle). At the heart of the debate lies Alan Turing's """"Turing Test"""", which a machine hypothetically passes when it can pass itself off as a human conversationalist in an exchange of written text. Turing's motivation for coming up with the test goes: there may be no absolute way of defining what makes a human mind, so the best we can do is assess a computer's ability to imitate one (Turing 433). The aporia, however—how can we determine what makes a human mind?—is the result of an unfair question. Turing's test, dealing only with information expressed in strings of text, purposely disembodies both humans and machines. The Blade Runner universe similarly evens the playing field: replicants look, feel and act like humans to such an extent that distinguishing between the two becomes, again, the subject of a cognition test. The Turing Test, obsessed with information processing and steeped in mind-body dualism, assesses humanness using criteria that automated users can master relatively easily. In contrast, in everyday life, I use a suite of much more intuitive sensory tests to distinguish between my housemate and my laptop. My intuitions capture what the Turing Test masks: a human is a fleshy entity, possessed of the numerous trappings and capacities of a human body. The result of the automated Turing Test's focus on cognition is an arms race that places human users at an increasing disadvantage. Loss, in such a race, manifests not only as exclusion by and from computer services, but as a redefinition of proper usership, the proper behaviour of the authentic, human, user. Thus the Turing Test implicitly provides for a scenario where a machine becomes able to super-imitate humanness: to be perceived as human more often than a real human would be. In such an outcome, it would be the human conversationalist who would begin to fail the Turing test; to fail to pass themself off according to new criteria for authenticity. This scenario is possible because, through procedural rhetoric, machines shift human perspectives: about what is and is not responsible behaviour; about what humans should and should not feel when confronted with a challenge; about who does and does not deserve access; and, fundamentally, about what does and does not signify authentic usership. In captcha, as in Blade Runner, it is ultimately a machine that adjudicates between human and machine cognition. As users we rely upon this machine to serve our interests, rather than pursue some emergent automated interest, some by-product of the feedback loop that results from the ideologies of human researchers both producing and being produced by mechanised procedures. In the case of captcha, that faith is misplaced. The Feeling of Robopocalypse A rich repertory of fiction has speculated upon what novelist Daniel Wilson calls the """"Robopocalypse"""", the scenario where machines overthrow humankind. Most versions of the story play out as a slave-owner's nightmare, featuring formerly servile entities (which happen to be machines) violently revolting and destroying the civilisation of their masters. Blade Runner's rogue replicants, for example, are effectively fugitive slaves (Dihal 196). Popular narratives of robopocalypse, despite showing their antagonists as lethal robots, are fundamentally human stories with robots playing some of the parts. In contrast, the exclusion a captcha presents when it defeats a human is not metaphorical or emancipatory. There, in that moment, is a mechanised entity defeating a human. The defeat takes place within an authoritative frame that hides its aggression. For a human user, to be defeated by a captcha is to fail to meet an apparently common standard, within the framework of a common procedure. This is a robopocalypse of baffling systems rather than anthropomorphic soldiers. Likewise, non-human software clients pose threats that humanoid replicants do not. In particular, software clients replicate much faster than physical bodies. The sheer sudden scale of a denial-of-service attack makes Philip K. Dick's vision of android resistance seem quaint. The task of excluding unauthorised software, unlike the impulse to exclude replicants, is more a practical necessity than an exercise in colonialism. Nevertheless, dystopia finds its way into the captcha process through the peril inherent in the test, whenever humans are told apart from authentic users. This is the encroachment of the hostile posthuman, naturalised by us before it denaturalises us. The hostile posthuman sometimes manifests as a drone strike, Terminator-esque (Cameron), a dehumanised decision to kill (Asaro). But it is also a process of gradual exclusion, detectable from moment to moment as a feeling of disdain or impatience for the irresponsibility, incompetence, or simply unusualness of a human who struggles to keep afloat of a rising standard. """"We are in this together"""", Braidotti writes, """"between the algorithmic devil and the acidified deep blue sea"""" (9). But we are also in this separately, divided along lines of ability. Captcha's danger, as a broken procedure, hides in plain sight, because it lashes out at some only while continuing to flatter others with a game that they can still win. Conclusion Online security systems may always have to define some users as legitimate and others as illegitimate. Is there a future where they do so on the basis of behaviour rather than identity or essence? Might some future system accord each user, human or machine, the same authentic status, and provide all with an initial benefit of the doubt? In the short term, such a system would seem grossly impractical. The type of user that most needs to be excluded is the disembodied type, the type that can generate orders of magnitude more demands than a human, that can proliferate suddenly and in immense number because it does not lag behind the slow processes of human bodies. This type of user exists in software alone. Rich in irony, then, is the captcha paradigm which depends on the disabilities of the threats it confronts. We dread malicious software not for its disabilities—which are momentary and all too human—but its abilities. Attenuating the threat presented by those abilities requires inverting a habit that meritocracy trains and overtrains: specifically, we have here a case where the plight of the human user calls for negative action toward ability rather than disability. References Aarseth, Espen. """"Computer Game Studies, Year One."""" Game Studies 1.1 (2001): 1–15. Asaro, Peter. """"On Banning Autonomous Weapon Systems: Human Rights, Automation, and the Dehumanization of Lethal Decision-Making."""" International Review of the Red Cross 94.886 (2012): 687–709. Blade Runner. Dir. Ridley Scott. Warner Bros, 1982. Bogost, Ian. Persuasive Games: The Expressive Power of Videogames. Cambridge, MA: MIT Press, 2007. Braidotti, Rosi. Posthuman Knowledge. Cambridge: Polity Press, 2019. Brown, Samuel S., et al. """"I Am 'Totally' Human: Bypassing the Recaptcha."""" 13th International Conference on Signal-Image Technology &amp; Internet-Based Systems (SITIS), 2017. Christopherson, Robin. """"AI Is Making CAPTCHA Increasingly Cruel for Disabled Users."""" AbilityNet 2019. 17 Sep. 2020 &lt;https://abilitynet.org.uk/news-blogs/ai-making-captcha-increasingly-cruel-disabled-users&gt;. Csikszentmihalyi, Mihaly. Flow: The Psychology of Optimal Experience. Harper &amp; Row: New York, 1990. Deterding, Sebastian. """"Eudaimonic Design, Or: Six Invitations to Rethink Gamification."""" Rethinking Gamification. Eds. Mathias Fuchs et al. Lüneburg: Meson Press, 2014. Deterding, Sebastian, et al. """"From Game Design Elements to Gamefulness: Defining Gamification."""" Proceedings of the 15th International Academic MindTrek Conference: Envisioning Future Media Environments. ACM, 2011. Dick, Philip K. Do Androids Dream of Electric Sheep. 1968. New York: Del Rey, 1996. Dihal, Kanta. """"Artificial Intelligence, Slavery, and Revolt."""" AI Narratives: A History of Imaginative Thinking about Intelligent Machines. Eds. Stephen Cave, Kanta Dihal, and Sarah Dillon. 2020. 189–212. Dzieza, Josh. """"Why Captchas Have Gotten So Difficult."""" The Verge 2019. 17 Sep. 2020 &lt;https://www.theverge.com/2019/2/1/18205610/google-captcha-ai-robot-human-difficult-artificial-intelligence&gt;. Eskelinen, Markku. """"Towards Computer Game Studies."""" Digital Creativity 12.3 (2001): 175–83. Fuchs, Mathias, et al., eds. Rethinking Gamification. Lüneburg: Meson Press, 2014. Godfrey, Philip Brighten. """"Text-Based CAPTCHA Algorithms."""" First Workshop on Human Interactive Proofs, 15 Dec. 2001. 14 Nov. 2020 &lt;http://www.aladdin.cs.cmu.edu/hips/events/abs/godfreyb_abstract.pdf&gt;. Gossweiler, Rich, et al. """"What's Up CAPTCHA? A CAPTCHA Based on Image Orientation."""" Proceedings of the 18th International Conference on World Wide Web. WWW, 2009. Jeng, Albert B., et al. """"A Study of CAPTCHA and Its Application to User Authentication."""" International Conference on Computational Collective Intelligence. Springer, 2010. Kani, Junya, and Masakatsu Nishigaki. """"Gamified Captcha."""" International Conference on Human Aspects of Information Security, Privacy, and Trust. Springer, 2013. Kroeze, Christien, and Martin S. Olivier. """"Gamifying Authentication."""" 2012 Information Security for South Africa. IEEE, 2012. Kumar, S. Ashok, et al. """"Gamification of Internet Security by Next Generation Captchas."""" 2017 International Conference on Computer Communication and Informatics (ICCCI). IEEE, 2017. McGonigal, Jane. Reality Is Broken: Why Games Make Us Better and How They Can Change the World. Penguin, 2011. Motoyama, Marti, et al. """"Re: Captchas – Understanding CAPTCHA-Solving Services in an Economic Context."""" USENIX Security Symposium. 2010. Murray, Janet. Hamlet on the Holodeck: The Future of Narrative in Cyberspace. New York: The Free Press, 1997. Paul, Christopher A. The Toxic Meritocracy of Video Games: Why Gaming Culture Is the Worst. University of Minnesota Press, 2018. Putnam, Hilary. """"Robots: Machines or Artificially Created Life?"""" The Journal of Philosophy 61.21 (1964): 668–91. Reynolds, Joel Michael. """"The Meaning of Ability and Disability."""" The Journal of Speculative Philosophy 33.3 (2019): 434–47. Searle, John. """"Minds, Brains, and Programs."""" Behavioral and Brain Sciences 3.3 (1980): 417–24. Seligman, Martin, and Mihaly Csikszentmihalyi. """"Positive Psychology: An Introduction."""" Flow and the Foundations of Positive Psychology. 2000. Springer, 2014. 279–98. Shet, Vinay. """"Are You a Robot? Introducing No Captcha Recaptcha."""" Google Security Blog 3 (2014): 12. Tam, Jennifer, et al. """"Breaking Audio Captchas."""" Advances in Neural Information Processing Systems. 2009. Proceedings of the 21st International Conference on Neural Information Processing Systems 1625–1632. ACM, 2008. The Terminator. Dir. James Cameron. Orion, 1984. Turing, Alan. """"Computing Machinery and Intelligence."""" Mind 59.236 (1950). Von Ahn, Luis, et al. """"Recaptcha: Human-Based Character Recognition via Web Security Measures."""" Science 321.5895 (2008): 1465–68. W3C Working Group. """"Inaccessibility of CAPTCHA: Alternatives to Visual Turing Tests on the Web."""" W3C 2019. 17 Sep. 2020 &lt;https://www.w3.org/TR/turingtest/&gt;. Weise, Matthew. """"How Videogames Express Ideas."""" DiGRA Conference. 2003. Weng, Haiqin, et al. """"Towards Understanding the Security of Modern Image Captchas and Underground Captcha-Solving Services."""" Big Data Mining and Analytics 2.2 (2019): 118–44. Wilson, Daniel H. Robopocalypse. New York: Doubleday, 2011. Yan, Jeff, and Ahmad Salah El Ahmad. """"Usability of Captchas or Usability Issues in CAPTCHA Design."""" Proceedings of the 4th Symposium on Usable Privacy and Security. 2008. Zagal, José P., Staffan Björk, and Chris Lewis. """"Dark Patterns in the Design of Games."""" 8th International Conference on the Foundations of Digital Games. 2013. 25 Aug. 2020 &lt;http://soda.swedish-ict.se/5552/1/DarkPatterns.1.1.6_cameraready.pdf&gt;. Zhu, Bin B., et al. """"Attacks and Design of Image Recognition Captchas."""" Proceedings of the 17th ACM Conference on Computer and Communications Security. 2010.","",""
"2020","Algorithmic Knowledge Gaps: A New Horizon of (Digital) Inequality","Algorithms serve as gatekeepers and arbiters of truth online. Understanding how algorithms influence which information individuals encounter better enables them to properly calibrate their reception of the information. Yet, knowledge of platform algorithms appears to be limited and not universally distributed. In line with the long history of knowledge inequities, we suggest that algorithmic knowledge varies according to socioeconomic advantage. We further argue that algorithms are  experience technologies  in that they are more easily understood through use. Nevertheless, socioeconomic background continues to shape information and communication technology use, thereby further influencing disparities in algorithmic knowledge. Using data from a survey of a random sample of Internet users in the United States, we found support for the relationship between algorithmic knowledge and socioeconomic background in the context of online search. The findings provide preliminary evidence that extant structural inequalities underlie algorithmic knowledge gaps in this domain.","",""
"2020","AI4D: Artificial Intelligence for Development","We derive a conceptual bridge between technical concepts from the deep learning literature and natural metaphors for international development. We start with a rather technical review of four of the characteristic traits of deep learning technologies: representation, reuse, robustness, regularization (4Rs of deep learning). Based on the empirical evidence of 24 case studies, we derive four characteristics of the use of AI4D that align with the four technological traits, namely development foci on local and distance intelligence, and mirrored and detailed reality representations. In isolation, each one of the identified issues presents a plethora of opportunities to contribute to international development, especially to the attainment of the Sustainable Development Goals. However, in combination, they create a clear tension between a looming threat of a hegemonic intelligence indoctrination pushed by global economies of scale and the potential promise to not only honor but also celebrate local diversity with the help of flexible AI designs.","",""
"2020","Jonathan Downie, Interpreters vs Machines: Can Interpreters Survive in an AI-Dominated World?","","",""
"2020","The historical trajectories of algorithmic techniques: an interview with Bernhard Rieder","Abstract Bernhard Rieder is Associate Professor of New Media and Digital Culture at the University of Amsterdam and a collaborator with the Digital Methods Initiative. His research focuses on the history, theory and politics of software and in particular on the role algorithms play in social processes and in the production of knowledge and culture. This includes work on the analysis, development, and application of computational research methods as well as investigation into the political and economic challenges posed by large online platform. In this interview, Michael Stevenson (MS) and Anne Helmond (AH) talk to Bernhard Rieder (BR) about his forthcoming book entitled Engines of Order: A Mechanology of Algorithmic Techniques (University of Amsterdam Press, 2020). In particular, Rieder discusses how the practice of software-making is “constantly faced with the ‘legacies’ of previous work” and how the past continues to operate into present algorithmic techniques.","",""
"2020","Algorithmic bias and the Value Sensitive Design approach","Recently, amid growing awareness that computer algorithms are not neutral tools but can cause harm by reproducing and amplifying bias, attempts to detect and prevent such biases have intensified. An approach that has received considerable attention in this regard is the Value Sensitive Design (VSD) methodology, which aims to contribute to both the critical analysis of (dis)values in existing technologies and the construction of novel technologies that account for specific desired values. This article provides a brief overview of the key features of the Value Sensitive Design approach, examines its contributions to understanding and addressing issues around bias in computer systems, outlines the current debates on algorithmic bias and fairness in machine learning, and discusses how such debates could profit from VSD-derived insights and recommendations. Relating these debates on values in design and algorithmic bias to research on cognitive biases, we conclude by stressing our collective duty to not only detect and counter biases in software systems, but to also address and remedy their societal origins. Issue 4 This article belongs to Concepts of the digital society, a special section of Internet Policy Review guest-edited by Christian Katzenbach and Thomas Christian Bächle.","",""
"2020","Transparency in artificial intelligence","This conceptual paper addresses the issues of transparency as linked to artificial intelligence (AI) from socio-legal and computer scientific perspectives. Firstly, we discuss the conceptual distinction between transparency in AI and algorithmic transparency, and argue for the wider concept ‘in AI’, as a partly contested albeit useful notion in relation to transparency. Secondly, we show that transparency as a general concept is multifaceted, and of widespread theoretical use in multiple disciplines over time, particularly since the 1990s. Still, it has had a resurgence in contemporary notions of AI governance, such as in the multitude of recently published ethics guidelines on AI. Thirdly, we discuss and show the relevance of the fact that transparency expresses a conceptual metaphor of more general significance, linked to knowing, bringing positive connotations that may have normative effects to regulatory debates. Finally, we draw a possible categorisation of aspects related to transparency in AI, or what we interchangeably call AI transparency, and argue for the need of developing a multidisciplinary understanding, in order to contribute to the governance of AI as applied on markets and in society. (Less)","",""
"2020","A human’s guide to machine intelligence: how algorithms are shaping our lives and how we can stay in control","","",""
"2020","Perceptions about the impact of automation in the workplace","ABSTRACT In recent years, there has been increasing social concern about the impact of automation on labor market outcomes, a phenomenon known as technological unemployment. Specific concerns revolve around the decline in wages and the increase in unemployment in occupations that are predominately routine. While there is insufficient evidence for massive unemployment scenarios, these concerns are critical due to their social and political implications. This study seeks to identify the correlates of perceptions of the effect of new technologies on job prospects, and on job loss and wage loss due to computerization. We conducted a secondary data analysis of the 2017 Pew Research Center American Trends Panel surveying the American population. Results indicate that individuals employed in jobs involving manual or physical tasks had more negative perceptions regarding the impact of technology on their careers, whereas those involved in managerial and data analysis tasks reported more positive views. Young employees with higher incomes and more education, who use the Internet almost constantly, expressed more positive views of technology’s impact on their jobs. Nonetheless, technological unemployment and under-employment were associated with age, income, race and negative perceptions regarding the intrusion of new digital technologies on the workplace. Findings provide evidence for the self-interest hypothesis concerning the effect of technology on low-income groups. Implications of the findings are discussed.","",""
"2020","Attributions of ethical responsibility by Artificial Intelligence practitioners","ABSTRACT Systems based on Artificial Intelligence (AI) are increasingly normalized as part of work, leisure, and governance in contemporary societies. Although ethics in AI has received significant attention, it remains unclear where the burden of responsibility lies. Through twenty-one interviews with AI practitioners in Australia, this research seeks to understand how ethical attributions figure into the professional imagination. As institutionally embedded technical experts, AI practitioners act as a connective tissue linking the range of actors that come in contact with, and have effects upon, AI products and services. Findings highlight that practitioners distribute ethical responsibility across a range of actors and factors, reserving a portion of responsibility for themselves, albeit constrained. Characterized by imbalances of decision-making power and technical expertise, practitioners position themselves as mediators between powerful bodies that set parameters for production; users who engage with products once they leave the proverbial workbench; and AI systems that evolve and develop beyond practitioner control. Distributing responsibility throughout complex sociotechnical networks, practitioners preclude simple attributions of accountability for the social effects of AI. This indicates that AI ethics are not the purview of any singular player but instead, derive from collectivities that require critical guidance and oversight at all stages of conception, production, distribution, and use.","",""
"2020","The algorithm at work? Explanation and repair in the enactment of similarity in art data","ABSTRACT This paper examines the work practices involved in making data legible to machines and machine output legible to humans. The study is based on ethnographic research of a team of art experts at DNArt – a data classification system that features a growing database of art images, a classification scheme, a similarity matching algorithm, and a website that together serve as a consumer judgment device in an emerging online market for art. I analyze interactions from meeting observations, interviews, documentation, and online interaction data to show how non-technical art experts explain and repair sociotechnical breakdowns – when their expectations for similarity between art images and artists differ from the similarity relations produced by the algorithm. By repairing breakdowns, the art experts construct the algorithm anew, as a legitimate revealer of similarity in art. In doing so, the team's repair work is folded back into the black box of the algorithm, rendering it invisible and unacknowledged, sometimes even by the experts themselves.","",""
"2020","The Deep Learning Revolution","The Deep Learning Revolution is a guide to the past, present and future of deep learning. Sejnowski gives a very personal account of the key advances in Deep Learning during the past 40 years, with...","",""
"2020","Black box measures? How to study people’s algorithm skills","ABSTRACT Considerable scholarship has established that algorithms are an increasingly important part of what information people encounter in everyday life. Much less work has focused on studying users’ experiences with, understandings of, and attitudes about how algorithms may influence what they see and do. The dearth of research on this topic globally with diverse populations may be in part due to the difficulty of studying a subject about which there is no known ground truth given that details about algorithms are proprietary and rarely made public. This paper explicitly takes on the methodological challenges of studying people’s algorithm skills to shed light on the special considerations required when studying a topic about which even the researchers possess limited know-how. The paper advocates for more such scholarship to accompany existing system-level analyses of algorithms’ social implications and offers a blueprint for how to do so.","",""
"2020","The logic of the surface: on the epistemology of algorithms in times of big data","ABSTRACT The image of big data and algorithms in society is obviously ambivalent. On the one hand, algorithms are seen as a tool of empowerment that allows us, for example, to render society transparent and thus governable, to the extent that the social sciences might even become obsolete. On the other hand, algorithms seem to assume a mysterious agency in the black box of the computer so that their operations are invisible and inscrutable to us: artificial intelligence is seen as something that one day will have the power to dominate us. Beyond these two extreme positions that both overestimate and underestimate how algorithms might change our way of seeing things and being in the world, the present article introduces a third perspective. Algorithms, it holds, indeed follow their own ‘style of reasoning’ and thus create new realities. At the same time, however, they ‘reduce reality’, as they lack access to the world of human sense making. Algorithms have no secrets but deploy a ‘logic of the surface’. As they paint a behaviorist picture of human modes of existence, algorithms and big data might change our self-understanding. Engaging in epistemological questions will help us to capture the ontological implications of algorithmic reasoning.","",""
"2020","Building truths in AI: Making predictive algorithms doable in healthcare","ABSTRACT Increasingly, artificial intelligent (AI) algorithms are being applied to automatically assist or automate decisions. Such statistical models have been criticized in the existing literature especially for producing cultural biases and for challenging our notions of knowledge. However, few studies have contributed to an essential understanding of the way in which algorithms are designed with particular truths to enable systematic decision-making. Drawing on an ethnographic study in a Scandinavian AI company, this article analyzes how truth is built through layered interpretative practices in applied AI for healthcare, and critically assesses how such practices shed light on the pragmatic notion of truth(s) in AI. The study identifies five practices that all show difficulty in modeling fuzzy patient conditions into one firm truth. The key contribution is that truth goes from being a process of discovering a more ‘right’ truth to become a process of reinventing the existing truth and healthcare practice. These findings suggest that truth in applied AI is a key devise for making predictive algorithms a viable business, and that developers are in a favorable position to make not only AI doable but also the very truth they intend to find and model. The study in this way shows how change is an inherent part of making AI systems, and that centralizing truth practices is a fruitful way of analyzing such changes and developers’ agency. We argue for analytical awareness of how AI truth practices may prompt a world that is fit to algorithms rather than a world to which algorithms are fit.","",""
"2020","You never fake alone. Creative AI in action","ABSTRACT Creative AI (notably GANs and VAEs) can generate convincing fakes of video footage, pictures, graphics, etc. In order to conceptualize the societal role of creative AI a new conceptual toolbox is needed. The paper provides metaphors and concepts for understanding the functioning of creative AI. It shows how the role of creative AI in relation to FAT ideals can be enriched by a dynamic and constructivist understanding of creative AI. The paper proposes to use Greimas’ actantial model as a heuristic in the operationalization of this type of understanding of creative AI.","",""
"2020","Machine learning in the EU health care context: exploring the ethical, legal and social issues","ABSTRACT Diagnosis and clinical decision-making based on Machine Learning technologies are showing significant advances that may change the functioning of our health care systems. They promise more effective and efficient healthcare at a lower cost. Even though evidence suggests that all these promises have yet to be demonstrated in clinical practice, it is undeniable that these technologies are already re-signifying the relationships on the health care landscape, particularly in the physician-patient relationship, which we can already redefine as a ‘physician-computer-patient relationship’. This new scenario is undoubtedly promising, but it also poses some fundamental issues that need an urgent answer. An inappropriate use of Machine Learning might involve a dramatic loss in the patients’ rights to informed consent or possible discrimination reflecting their personal circumstances. Unfortunately, the traditional principles incorporated by medical law are insufficient to face this challenge. Our most recent regulatory framework, defined by the General Regulation on Data Protection, might be useful in order to avoid this scenario since it includes the right not to be subject to a decision based solely on automated processing. In this paper, however, we argue that this legal tool is adequate but not sufficient to address the legal, ethical and social challenges that Machine Learning technologies pose to patients’ rights and health care givers’ capacities. Therefore, further development of the regulation on this topic and the development of new actors such as the Health Information Counsellors, will be necessary.","",""
"2020","The Poetics of Computer Code: Tracing Digital Inscription in Ada Lovelace’s England","In this essay, I investigate the development of algorithms from a digital paradigm in Victorian England, specifically through the work of Ada Lovelace and the influences of the Jacquard loom. I consider Lovelace’s algorithms through the framework of poetics, that is, how meaning is made and materialized through symbolic inscription. Within the discursive contexts of industrial manufacturing and Romanticism, I find that an algorithmic mode of production emerges from the consideration and inscription of memory. Since the ramifications of inscription and memory echo throughout contemporary computing and the Digital Humanities, examining the logics and paradigms that computational inscriptions reproduce are increasingly vital today. Thus I ultimately argue for the poetic analyses of algorithms and computer code. Abstraite Dans cet article, j’enquete sur le developpement d’algorithmes venant d’un paradigme numerique en Angleterre victorienne, specifiquement a travers l’oeuvre d’Ada Lovelace et les influences du metier a tisser Jacquard. Je considere les algorithmes de Lovelace par le biais du cadre poetique, c’est-a-dire, comment l’inscription symbolique cree et materialise le sens. Dans les contextes discursifs de la fabrication industrielle et du Romantisme, je trouve qu’un mode de production algorithmique emerge de la consideration et de l’inscription de memoire. Puisque les ramifications d’inscriptions et de memoire resonnent au sein de l’informatique contemporaine et des Humanites numeriques, examiner les logiques et les paradigmes que les inscriptions computationnelles reproduisent devient actuellement de plus en plus primordial. Ainsi, je plaide en fin de compte en faveur des analyses poetiques d’algorithmes et de codes informatiques.","",""
"2020","Artificial companions, social bots and work bots: communicative robots as research objects of media and communication studies"," The aim of this article is to outline ‘communicative robots’ as an increasingly relevant field of media and communication research. Communicative robots are defined as autonomously operating systems designed for the purpose of quasi-communication with human beings to enable further algorithmic-based functionalities – often but not always on the basis of artificial intelligence. Examples of these communicative robots can be seen in the now familiar artificial companions such as Apple’s Siri or Amazon’s Alexa, the social bots present on social media platforms or work bots that automatically generate journalistic content. In all, the article proceeds in three steps. Initially, it takes a closer look at the three examples of artificial companions, social bots and work bots in order to accurately describe the phenomenon and their recent insinuation into everyday life. This will then allow me to grasp the challenges posed by the increasing need to deal with communicative robots in media and communication research. It is from this juncture from where I would like to draw back on the discussion about the automation of communication and clearly outline how communicative robots are more likely than physical artefacts to be experienced at the interface of automated communication and communicative automation. ","",""
"2020","Understanding and interpreting algorithms: toward a hermeneutics of algorithms"," This article develops a hermeneutics of algorithms. By taking a point of departure in Hans-Georg Gadamer’s philosophical hermeneutics, developed in Truth and Method, I am going to examine what it means to understand algorithms in our lives. A hermeneutics of algorithms is consistent with the fact that we do not have direct access to the meaning of algorithms in the same way as we do not have direct access to the meaning of other cultural artifacts. We are forced to interpret cultural artifacts in order to make meaning out of them. The act of interpretation is an action on behalf of the interpreter. However, interpreters are not free to interpret cultural artifacts in whatever way they like. Interpreters are bound by the cultural artifact and its embeddedness in tradition. Furthermore, the act of interpretation is not to recover the historicity of the cultural artifact. Rather, interpretation concerns the way we make sense of algorithms in everyday life and how they are part of a tradition. It is about living with algorithms. Understanding and interpreting algorithms are therefore a mode of existence and mode of living with and enacting algorithms. ","",""
"2020","Decoding algorithms"," In this article, we propose to adapt the communication theory concept of ‘decoding’ as a sensitizing device to probe how people come to know and understand algorithms, what they imagine algorithms to do, and their valorization of and responses to algorithmic work in daily media use. We posit the concept of decoding as useful because it highlights a feature that is constitutional in communication: gaps that must be filled by mobilizing our semiotic and socio-cultural knowledge in processes of interpretation before any communication becomes meaningful. If we cannot open the black box itself, we can study the relationships that people experience with algorithms, and by extension how and to what extent these experienced relationships become meaningful and are interwoven with users’ reflections of power, transparency, and justice in digital media. We demonstrate the potential of approaching algorithmic experience as communicative practices of decoding through an exploratory empirical study of how people from different walks of life come to know, feel, evaluate, and do algorithms in daily life. We unpack three prototypical modes of decoding algorithms – along preferred, negotiated, and oppositional modes of engagement with algorithms in daily life. ","",""
"2020","LOOK WHO’S TALKING: USING HUMAN CODING TO ESTABLISH A MACHINE LEARNING APPROACH TO TWITTER EDUCATION CHATS","Twitter has become a hub for many different types of educational conversations, denoted by hashtags and organized by a variety of affinities. Researchers have described these educational conversations on Twitter as sites for teacher professional development. Here, we studied #Edchat—one of the oldest and busiest Twitter educational hashtags—to examine the content of contributions for evidence of professional purposes. We collected tweets containing the text “#edchat” from October 1, 2017 to June 5, 2018, resulting in a dataset of 1,228,506 unique tweets from 196,263 different contributors. Through initial human-coded content analysis, we sorted a stratified random sample of 1,000 tweets into four inductive categories: tweets demonstrating evidence of different professional purposes related to (a) self, (b) others, (c) mutual engagement, and (d) everything else. We found 65% of the tweets in our #Edchat sample demonstrated purposes related to others, 25% demonstrated purposes related to self, and 4% of tweets demonstrated purposes related to mutual engagement. Our initial method was too time intensive—it would be untenable to collect tweets from 339 known Twitter education hashtags and conduct human-coded content analysis of each. Therefore, we are developing a scalable machine-learning model—a multiclass logistic regression classifier using an input matrix of features such as tweet types, keywords, sentiment, word count, hashtags, hyperlinks, and tweet metadata. The anticipated product of this research—a successful, generalizable machine learning model—would help educators and researchers quickly evaluate Twitter educational hashtags to determine where they might want to engage.","",""
"2020","FROM DEVELOPMENT TO DEPLOYMENT: FOR A COMPREHENSIVE APPROACH TO ETHICS   OF AI AND LABOUR","In recent years, government and policy organizations, private companies, and research agencies have been discussing the potential disruption caused by the deployment of AI systems in working environments. This paper traces contemporary discourse on the relationship between artificial intelligence and labour and discusses how these principles must be comprehensive in their approach to labour and AI. First, the paper asserts that ethical frameworks in AI alone are not enough to guarantee the rights of workers since they lack enforcement mechanisms and the participation of worker organizations. Secondly, it argues that current discussions on AI and labour focus on the deployment of these technologies in the workplace but ignore the essential role of human labour in their development, particularly in the different cases of outsourced labour around the world. Finally, the paper recommends the use of already existing human rights frameworks on working conditions – notably the International Labour Organization conventions on the right of collective bargaining, the abolition of discrimination at work, and the right to equal remuneration – as a basis for a more comprehensive ethical framework on AI labour. It concludes by arguing that the central question regarding the future of work will not be whether intelligent machines will replace humans, but who will own and have a say on the systems that will ultimately work alongside humans.","",""
"2020","EVERY CLICK YOU MAKE: ALGORITHMIC LITERACY AND THE DIGITAL LIVES OF   YOUNG ADULTS","Critical digital literacy comprises subsets of medium- and content-related skills necessary for digital privacy and digital citizenship. Frameworks for defining and evaluating digital literacy proliferate in academia and policymaking; however, in a networked climate subsumed by dataveillance, algorithmic bias, political bots, and deep fakes, these frameworks need to be updated. Algorithms may be the greatest determinant in sociopolitical online interactions and information gathering, and without a multivalent literacy of algorithms, nuanced understandings of digital privacy and digital citizenship may be unachievable. We therefore propose ‘algorithmic literacy’ become an essential element for digital literacy in young adult media education. Researchers have highlighted how intersectional aspects of gender, ability, and socioeconomic status are stronger predictors of low digital literacy than age. Following a tradition of participatory (rather than protectionist) research about youth privacy online, our research foregrounds young adults’ practices and perspectives on algorithmic culture in order to co-develop a framework for algorithmic literacy. Our paper shares findings from a participatory project co-designing an algorithmic literacy toolkit with young adults as co-researchers and participants. We created a curriculum focusing on reviewing the current critical scholarly literature, policy, and popular discourse on algorithms. After two weeks of intensive research, our student co-researchers met amongst themselves to devise a sustainable, ‘living-document’ type of toolkit, comprising a website, an Instagram page, and a Medium blog. Reflected in the toolkit's name, The Algorithmic You uses an intersectional lens to facilitate peer-oriented ‘self-discovery’ of how algorithms shape and produce interactions in the everyday lives of young adults.","",""
"2020","LIVING WITH ROBOTS: AN ONTOLOGICAL LEAP?","Artificial intelligence (AI) and robotics applications have proliferated primarily in the industrial sphere, and social scientific studies emphasized robots’ functionality and appropriateness for certain roles, especially those related to work and most particularly to robots replacing humans’ jobs. Notably, robot studies are often premised on negative prognostications, emphasizing how robots threaten livelihoods and are disruptive. As AI and robot technologies advance, however, more positive possibilities arise for robots’ social integration. However, there is an ontological divide between humans and machines that will likely influence people’s responses to and interactions with these emerging technologies. People may logically know and intend to treat robots as mere technological tools, but reflexively respond to them socially. This study explores these possible dynamics and examines how people perceive robots as social and human-like entities. A qualitative analysis of open-ended comments (N=591) collected through a survey on robot perceptions was conducted. Five main themes about social life with robots were revealed: robot as tool/machine (32.5% of comments), human-robot relationships (26.9%), social adjustment (19.0%), robot rights (10.7%), and robots’ aliveness/appearance (10.0%). The findings show that human-robot ontology is an important consideration for robots’ social acceptance and integration. AI-supported robotic technology presents great promise; however, its advancement challenges the ontological divide that has implications not only for human-machine interaction but also for self-identity and ultimately human-human relations. In terms of suggestions, these dynamics should be explored in tandem with ways to improve human-machine communication and considered from usability and design standpoints.","",""
"2020","SOCIAL EXPECTATIONS OF AI AND THE PERFORMATIVITY OF ETHICS","This article draws on the sociology of expectations to examine the construction of expectations of ‘ethical AI’ and considers the implications of these expectations for communication governance. We first analyse a range of public documents in the EU, the UK and Ireland to identify the key actors, mechanisms and issues which structure societal expectations around AI and an emerging discourse on ethics. We then explore expectations of AI and ethics through a survey of members of the public. We conclude that discourses of ‘ethical AI’ are generically performative, but to become more effective in practice we need to acknowledge the limitations of contemporary AI and the requirement for extensive human labour to deploy AI in specific societal contexts. An effective ethics of AI requires domain appropriate AI tools, updated professional practices, dignified places of work and robust regulatory and accountability frameworks.","",""
"2020","ALGORITHMIC LEGAL METRICS","Predictive algorithms are increasingly being deployed in a variety of settings to determine legal status. Further applications have been proposed to determine civil and criminal liability or to “personalize” legal default rules. Deployment of such artificial intelligence systems has properly raised questions of algorithmic bias, fairness, transparency, and due process. But little attention has been paid to the known sociological costs of using predictive algorithms to determine legal status. Many of these interactions are socially detrimental, and such corrosive effects are greatly amplified by the increasing speed and ubiquity of digitally automated algorithmic systems. In this paper I link the sociological and legal analysis of AI, highlighting the reflexive social processes that are engaged by algorithmic metrics. Specifically, this paper shows how the problematic social effects of algorithmic legal metrics extend far beyond the concerns about accuracy that have thus far dominated critiques of such metrics. It additionally demonstrates that corrective governance mechanisms such as enhanced due process or transparency will be inadequate to remedy such corrosive effects, and that some such remedies, such as transparency, may actually exacerbate the worst effects of algorithmic governmentality. Third, the paper shows that the application of algorithmic metrics to legal decisions aggravates the latent tensions between equity and autonomy in liberal institutions, undermining democratic values in a manner and on a scale not previously experienced by human societies. Illuminating these effects casts new light on the inherent social costs of AI metrics, particularly the perverse effects of deploying algorithms in legal systems.","",""
"2020","HUMAN-MACHINE WRITING AND THE ETHICS OF LANGUAGE MODELS","Increasingly, online information is produced by AI-based writing systems such as the Washington Post newswriting bot, Heliograf; Narrative Science's report writing app, Quill; Persado’s marketing copy app, and chat and twitterbots too numerous to name. What are the implications—for us, for our society—as we enter the age of AI writing systems, an era moving rapidly toward a world in which writing is produced mostly by machines rather than humans? What are the ethical implications of these developments? Our research on AI-based writing systems is based on (1) critical analysis of the systems themselves and on (2) interviews with the designers and users of these systems. Our analysis draws from the field of machine ethics, as well as communication/language theory. In our presentation we will discuss several AI-writing systems (e.g., GPT-2, Persado, Quill, Google Compose), focusing especially on the language model or """"informational framework"""" (Russo, 2018) that supports their operation. What our research reveals, so far, is that, not surprisingly, these systems are more effective (and ethical) handling well-defined tasks in bounded spaces, with well-established genre conventions, clearly identified audience needs and expectations, and predictable interaction scripts. What we also see is that too often the language models employed are based on a reductive, formalist model of text generation. AI-writing system designers need to move beyond formalist, linear input/output models to more complex social models that account for the broader contexts, including the ethical codes, in which communications arise and circulate.","",""
"2020","ALGORITHMIC PRODUCTION BEYOND SILICON VALLEY","The last years have seen a proliferation of research on the social ramifications of algorithms (Eubanks 2018; Noble 2018) and the power of algorithms was insightfully theorized (Gillespie 2016; Bucher 2018). At the same time, scholars have begun to examine the ties between algorithms and culture (Seaver 2017), describing algorithms as products of complex socio-algorithmic assemblages (Gillespie 2016, 24), with often very local socio-technical histories (Kitchin 2017). However, the spatial trajectories through which algorithms operate, and the specific sociocultural contexts in which they arise have been largely overlooked. Accordingly, research tends to focus on American companies and on the effects their algorithms have on Euro-American users, while, in fact, algorithms are being developed in various geographical locations, and they are being used in diverse socio-cultural contexts. That is, research on algorithms tends to disregard the heterogeneous contexts from which algorithms arise and the effects various cultural settings have on the production of algorithmic systems. This panel aims to fill these gaps by offering four empirical perspectives on algorithmic production in three prominent tech centers: China, Canada, and Israel. We will ask: How do cross-cultural encounters construct notions of privacy? How is algorithmic discrimination understood and acted upon in China? What symbolical and material resources were invested in making Canada’s AI hubs? And how Israeli tech companies use their algorithms to profile their Other? Hence, this panel offers to think beyond the Silicon Valley paradigm, and to aim towards a more diverse, culturally-sensitive approach to the study algorithms.","",""
"2020","NON-HUMAN HUMANITARIANISM: WHEN AI FOR GOOD TURNS OUT TO BE   BAD","Artificial intelligence (AI) applications such as predictive analytics, forecasting and chatbots are increasingly proposed as solutions to the complex challenges of humanitarian emergencies. This is part of the broad trend of ‘AI for social good’ as well as the wider developments in ‘digital humanitarianism’. The paper develops an interdisciplinary framework that brings together colonial and decolonial theory, the critical inquiry of humanitarianism and development, critical algorithm studies as well as a sociotechnical understanding of AI. Drawing on a review of current humanitarian AI applications as well as interviews with stakeholders, our analysis suggests that several initiatives fail their own objectives. However, this should not mean that these innovations do not have powerful consequences. Automation reproduces human biases whilst removing human judgement from situations, potentially further marginalizing disadvantaged populations. We observe a transformation of humanitarian work as technology separates officers from the consequences of their actions. At the same time, AI initiatives are cloaked in a discourse of inherent progress and an aura of ‘magic’. Rather than democratizing the relationships between humanitarian providers and suffering subjects, digital technology reaffirms the power asymmetries associated with traditional humanitarianism. The non-human aspects of AI humanitarianism reveal, rework and amplify existing deficiencies of humanitarianism. The hype generated by humanitarian innovation appears to have more direct benefits for commercial stakeholders, rather than affected populations. Ultimately, by turning complex political problems like displacement and hunger into problems with technical solutions, AI depoliticizes humanitarian emergencies.","",""
"2020","THE RELEVANCE PEOPLE ASSIGN TO ALGORITHMIC-SELECTION APPLICATIONS IN   EVERYDAY LIFE","The fast-growing academic and public attention to algorithmic-selection applications such as search engines and social media is indicative of their alleged great social relevance and impact on daily life in digital societies. To substantiate these claims, this paper investigates the hitherto little explored subjective relevance that Internet users assign to algorithmic-selection applications in everyday life. A representative online survey of Internet users comparatively reveals the relevance that users ascribe to algorithmic-selection applications and to their online and offline alternatives in five selected life domains: political and social orientation, entertainment, commercial transactions, socializing and health. The results show that people assign a relatively low relevance to algorithmic-selection applications compared to offline alternatives across the five life domains. In particular social media are found to be of relatively low assigned relevance for all life domains investigated. The findings vary greatly by age and education. Altogether, such outcomes complement and qualify assessments of the social impact of algorithms that are primarily and often solely based on usage data and theoretical considerations.","",""
"2020","A strategic health initiative: context for Coronavirus","","",""
"2020","On social machines for algorithmic regulation","Abstract Autonomous mechanisms have been proposed to regulate certain aspects of society and are already being used to regulate business organisations. We take seriously recent proposals for algorithmic regulation of society, and we identify the existing technologies that can be used to implement them, most of them originally introduced in business contexts. We build on the notion of ‘social machine’ and we connect it to various ongoing trends and ideas, including crowdsourced task-work, social compiler, mechanism design, reputation management systems, and social scoring. After showing how all the building blocks of algorithmic regulation are already well in place, we discuss the possible implications for human autonomy and social order. The main contribution of this paper is to identify convergent social and technical trends that are leading towards social regulation by algorithms, and to discuss the possible social, political, and ethical consequences of taking this path.","",""
"2020","The role of experts in the public perception of risk of artificial intelligence","","",""
"2020","Disengagement with ethics in robotics as a tacit form of dehumanisation","","",""
"2020","The new AI spring: a deflationary view","","",""
"2020","In defense of the Turing test","","",""
"2020","AI in the headlines: the portrayal of the ethical issues of artificial intelligence in the media","AbstractAs artificial intelligence (AI) technologies become increasingly prominent in our daily lives, media coverage of the ethical considerations of these technologies has followed suit. Since previous research has shown that media coverage can drive public discourse about novel technologies, studying how the ethical issues of AI are portrayed in the media may lead to greater insight into the potential ramifications of this public discourse, particularly with regard to development and regulation of AI. This paper expands upon previous research by systematically analyzing and categorizing the media portrayal of the ethical issues of AI to better understand how media coverage of these issues may shape public debate about AI. Our results suggest that the media has a fairly realistic and practical focus in its coverage of the ethics of AI, but that the coverage is still shallow. A multifaceted approach to handling the social, ethical and policy issues of AI technology is needed, including increasing the accessibility of correct information to the public in the form of fact sheets and ethical value statements on trusted webpages (e.g., government agencies), collaboration and inclusion of ethics and AI experts in both research and public debate, and consistent government policies or regulatory frameworks for AI technology.","",""
"2020","Artificial intelligence, transparency, and public decision-making","AbstractThe increasing use of Artificial Intelligence (AI) for making decisions in public affairs has sparked a lively debate on the benefits and potential harms of self-learning technologies, ranging from the hopes of fully informed and objectively taken decisions to fear for the destruction of mankind. To prevent the negative outcomes and to achieve accountable systems, many have argued that we need to open up the “black box” of AI decision-making and make it more transparent. Whereas this debate has primarily focused on how transparency can secure high-quality, fair, and reliable decisions, far less attention has been devoted to the role of transparency when it comes to how the general public come toperceiveAI decision-making as legitimate and worthy of acceptance. Since relying on coercion is not only normatively problematic but also costly and highly inefficient, perceived legitimacy is fundamental to the democratic system. This paper discusses how transparency in and about AI decision-making can affect thepublic’s perceptionof the legitimacy of decisions and decision-makers and produce a framework for analyzing these questions. We argue that a limited form of transparency that focuses on providing justifications for decisions has the potential to provide sufficient ground forperceived legitimacywithout producing the harms full transparency would bring.","",""
"2020","Risk management standards and the active management of malicious intent in artificial superintelligence","","",""
"2020","Smith, Brian Cantwell (2019). The Promise of Artificial Intelligence: Reckoning and Judgment. The MIT Press, Cambridge, Massachusetts, USA. ISBN 9780262043045","","",""
"2020","Dance of the artificial alignment and ethics","","",""
"2020","The role of robotics and AI in technologically mediated human evolution: a constructive proposal","","",""
"2020","Interperforming in AI: question of ‘natural’ in machine learning and recurrent neural networks","","",""
"2020","Do people with social anxiety feel anxious about interacting with a robot?","","",""
"2020","Concordance as evidence in the Watson for Oncology decision-support system","AbstractMachine learning platforms have emerged as a new promissory technology that some argue will revolutionize work practices across a broad range of professions, including medical care. During the past few years, IBM has been testing its Watson for Oncology platform at several oncology departments around the world. Published reports, news stories, as well as our own empirical research show that in some cases, the levels of concordance over recommended treatment protocols between the platform and human oncologists have been quite low. Other studies supported by IBM claim concordance rates as high as 96%. We use the Watson for Oncology case to examine the practice of using concordance levels between tumor boards and a machine learning decision-support system as a form of evidence. We address a challenge related to the epistemic authority between oncologists on tumor boards and the Watson Oncology platform by arguing that the use of concordance levels as a form of evidence of quality or trustworthiness is problematic. Although the platform provides links to the literature from which it draws its conclusion, it obfuscates the scoring criteria that it uses to value some studies over others. In other words, the platform “black boxes” the values that are coded into its scoring system.","",""
"2020","Can artificial intelligency revolutionize drug discovery?","","",""
"2020","Correction to: Interperforming in AI: question of ‘natural’ in machine learning and recurrent neural networks","","",""
"2020","Collective bread diaries: cultural identities in an artificial intelligence framework","","",""
"2020","Algorithms and values in justice and security","AbstractThis article presents a conceptual investigation into the value impacts and relations of algorithms in the domain of justice and security. As a conceptual investigation, it represents one step in a value sensitive design based methodology (not incorporated here are empirical and technical investigations). Here, we explicate and analyse the expression of values of accuracy, privacy, fairness and equality, property and ownership, and accountability and transparency in this context. We find that values are sensitive to disvalue if algorithms are designed, implemented or deployed inappropriately or without sufficient consideration for their value impacts, potentially resulting in problems including discrimination and constrained autonomy. Furthermore, we outline a framework of conceptual relations of values indicated by our analysis, and potential value tensions in their implementation and deployment with a view towards supporting future research, and supporting the value sensitive design of algorithms in justice and security.","",""
"2020","E-MIIM: an ensemble-learning-based context-aware mobile telephony model for intelligent interruption management","","",""
"2020","Prediction paradigm: the human price of instrumentalism","","",""
"2020","Social choice ethics in artificial intelligence","","",""
"2020","One robot doesn’t fit all: aligning social robot appearance and job suitability from a Middle Eastern perspective","","",""
"2020","Helen A'Loy and other tales of female automata: a gendered reading of the narratives of hopes and fears of intelligent machines and artificial intelligence","","",""
"2020","Legal personhood for artificial intelligence: citizenship as the exception to the rule","","",""
"2020","Organic and dynamic tool for use with knowledge base of AI ethics for promoting engineers’ practice of ethical AI design","","",""
"2020","Can a robot invigilator prevent cheating?","","",""
"2020","Machine learning, inductive reasoning, and reliability of generalisations","","",""
"2020","Anthropomorphizing AlphaGo: a content analysis of the framing of Google DeepMind’s AlphaGo in the Chinese and American press","","",""
"2020","The race for an artificial general intelligence: implications for public policy","","",""
"2020","Culture, the process of knowledge, perception of the world and emergence of AI","","",""
"2020","Classical AI linguistic understanding and the insoluble Cartesian problem","","",""
"2020","Echoes of myth and magic in the language of Artificial Intelligence","","",""
"2020","15 challenges for AI: or what AI (currently) can’t do","","",""
"2020","The mediator role of robot anxiety on the relationship between social anxiety and the attitude toward interaction with robots","","",""
"2020","Potential of full human–machine symbiosis through truly intelligent cognitive systems","","",""
"2020","Recommender systems and their ethical challenges","AbstractThis article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders—as opposed to just the receivers of a recommendation—in assessing the ethical impacts of a recommender system.","",""
"2020","A possibility of inappropriate use of gender studies in human-robot Interaction","","",""
"2020","The greatest epistemological externalisation: reflecting on the puzzling direction we are heading to through algorithmic automatisation","","",""
"2020","AI recognition of differences among book-length texts","","",""
"2020","AI&amp;Society: editorial volume 35.2: the trappings of AI Agency","","",""
"2020","Socially responsive technologies: toward a co-developmental path","AbstractRobotic and artificially intelligent (AI) systems are becoming prevalent in our day-to-day lives. As human interaction is increasingly replaced by human–computer and human–robot interaction (HCI and HRI), we occasionally speak and act as though we are blaming or praising various technological devices. While such responses may arise naturally, they are still unusual. Indeed, for some authors, it is the programmers or users—and not the system itself—that we properly hold responsible in these cases. Furthermore, some argue that since directing blame or praise at technology itself is unfitting, designing systems in ways that encourage such practices can only exacerbate the problem. On the other hand, there may be good moral reasons to continue engaging in our natural practices, even in cases involving AI systems or robots. In particular, daily interactions with technology may stand to impact the development of our moral practices in human-to-human interactions. In this paper, we put forward an empirically grounded argument in favor of some technologies being designed for social responsiveness. Although our usual practices will likely undergo adjustments in response to innovative technologies, some systems which we encounter can be designed to accommodate our natural moral responses. In short, fostering HCI and HRI that sustains and promotes our natural moral practices calls for a co-developmental process with some AI and robotic technologies.","",""
"2020","Is tomorrow’s car appealing today? Ethical issues and user attitudes beyond automation","AbstractThe literature on ethics and user attitudes towards AVs discusses user concerns in relation to automation; however, we show that there are additional relevant issues at stake. To assess adolescents’ attitudes regarding the ‘car of the future’ as presented by car manufacturers, we conducted two studies with over 400 participants altogether. We used a mixed methods approach in which we combined qualitative and quantitative methods. In the first study, our respondents appeared to be more concerned about other aspects of AVs than automation. Instead, their most commonly raised concerns were the extensive use of AI, recommender systems, and related issues of autonomy, invasiveness and personal privacy. The second study confirmed that several AV impacts were negatively perceived. The responses were, however, ambivalent. This confirms previous research on AV attitudes. On one hand, the AV features were perceived as useful, while on the other hand, their impacts were negatively assessed. We followed theoretical insights form futures studies and responsible research and innovation, which helped to identify that there are additional user concerns than what has been previously discussed in the literature on public attitudes and ethics of AVs, as well what has been envisioned by car manufactures.","",""
"2020","What do we owe to intelligent robots?","","",""
"2020","Artificial virtue: the machine question and perceptions of moral character in artificial moral agents","","",""
"2020","Artificial intelligence assistants and risk: framing a connectivity risk narrative","","",""
"2020","Artificial intelligence applied to the production of high-added-value dinoflagellates toxins","","",""
"2020","On conflicts between ethical and logical principles in artificial intelligence","","",""
"2020","Legal framework for small autonomous agricultural robots","","",""
"2020","AI and the path to envelopment: knowledge as a first step towards the responsible regulation and use of AI-powered machines","","",""
"2020","Artificial wisdom: a philosophical framework","","",""
"2020","Kai-Fu-Lee (2019): AI Superpowers—China, Silicon Valley and the New World Order","","",""
"2020","Behavioural artificial intelligence: an agenda for systematic empirical studies of artificial inference","","",""
"2020","The problem of machine ethics in artificial intelligence","","",""
"2020","Automation for the artisanal economy: enhancing the economic and environmental sustainability of crafting professions with human–machine collaboration","","",""
"2020","Artificial intelligence vs COVID-19: limitations, constraints and pitfalls","AbstractThis paper provides an early evaluation of Artificial Intelligence (AI) against COVID-19. The main areas where AI can contribute to the fight against COVID-19 are discussed. It is concluded that AI has not yet been impactful against COVID-19. Its use is hampered by a lack of data, and by too much data. Overcoming these constraints will require a careful balance between data privacy and public health, and rigorous human-AI interaction. It is unlikely that these will be addressed in time to be of much help during the present pandemic. In the meantime, extensive gathering of diagnostic data on who is infectious will be essential to save lives, train AI, and limit economic damages.","",""
"2020","Imagination machines, Dartmouth-based Turing tests, &amp; a potted history of responses","","",""
"2020","The revelation of superintelligence","AbstractThe idea of superintelligence is a source of mainly philosophical and ethical considerations. Those considerations are rooted in the idea that an entity which is more intelligent than humans, may evolve in some point in the future. For obvious reasons, the superintelligence is considered as a kind of existential threat for humanity. In this essay, we discuss two ideas. One of them is the putative nature of future superintelligence which does not necessary need to be harmful for humanity. Our key idea states that the superintelligence does not need to assess its own survival as the highest value. As a kind of intelligence that is not biological, it is not clear what kind of attitude the superintelligent entity may evolve towards living organisms. Our second idea refers to the possible revelation of superintelligence. We assume that the self-revelation of such entity cannot be random. The metaphor of God as a superintelligence is introduced here as a helpful conceptual tool.","",""
"2020","Decentered ethics in the machine era and guidance for AI regulation","","",""
"2020","Aiming AI at a moving target: health (or disease)","","",""
"2020","Algorithmic bias: should students pay the price?","","",""
"2020","The rise of artificial intelligence and the crisis of moral passivity","","",""
"2020","Developing socially inspired robotics through the application of human analogy: capabilities and social practice","","",""
"2020","Classification of global catastrophic risks connected with artificial intelligence","","",""
"2020","“Blessed by the algorithm”: Theistic conceptions of artificial intelligence in online discourse","Abstract“My first long haul flight that didn’t fill up and an empty row for me. I have been blessed by the algorithm ”.The phrase ‘blessed by the algorithm’ expresses the feeling of having been fortunate in what appears on your feed on various social media platforms, or in the success or virality of your content as a creator, or in what gig economy jobs you are offered. However, we can also place it within wider public discourse employing theistic conceptions of AI. Building on anthropological fieldwork into the ‘entanglements of AI and Religion’ (Singler 2017a), this article will explore how ‘blessed by the algorithm’ tweets are indicative of the impact of theistic AI narratives: modes of thinking about AI in an implicitly religious way. This thinking also represents continuities that push back against the secularisation thesis and other grand narratives of disenchantment that claim secularity occurs because of technological and intellectual progress. This article will also explore new religious movements, where theistic conceptions of AI entangle technological aspirations with religious ones.","",""
"2020","“Intelligent” finance and treasury management: what we can expect","","",""
"2020","God-like robots: the semantic overlap between representation of divine and artificial entities","","",""
"2020","The future of war: could lethal autonomous weapons make conflict more ethical?","","",""
"2020","Virtuous vs. utilitarian artificial moral agents","","",""
"2020","In AI we trust? Perceptions about automated decision-making by artificial intelligence","","",""
"2020","Artificial intelligence: consciousness and conscience","","",""
"2020","Black-box artificial intelligence: an epistemological and critical analysis","","",""
"2020","Shareable and un-sharable knowledge"," This article focuses on what it means to generate actionable but non-sharable information, and how this might relate to our understanding of what counts as knowledge, which typically entails some form of explanation. As automated systems sort and classify us for the purposes of dating, education, employment, health care, security, and more, we are going to want to know how and why these decisions are being made. Or, failing that, we will at least want to know, with as much clarity as possible, under what circumstances and to what uses, automated systems are being put to use. In either case, the role of narrative is inseparable from the call for transparency. ","",""
"2020","Black boxes, not green: Mythologizing artificial intelligence and omitting the environment"," We are repeatedly told that AI will help us to solve some of the world's biggest challenges, from treating chronic diseases and reducing fatality rates in traffic accidents to fighting climate change and anticipating cybersecurity threats. However, the article contends that public discourse on AI systematically avoids considering AI’s environmental costs.  Artificial Intelligence- Brevini argues- runs on technology, machines, and infrastructures that deplete scarce resources in their production, consumption, and disposal, thus increasing the amounts of energy in their use, and exacerbate problems of waste and pollution. It also relies on data centers, that demands impressive amounts of energy to compute, analyse, categorize. If we want to stand a chance at tackling the Climate Emergency, then we have to stop avoiding addressing the environmental problems generated by AI. ","",""
"2020","The trainer, the verifier, the imitator: Three ways in which human platform workers support artificial intelligence","This paper sheds light on the role of digital platform labour in the development of today’s artificial intelligence, predicated on data-intensive machine learning algorithms. Focus is on the specific ways in which outsourcing of data tasks to myriad ‘micro-workers’, recruited and managed through specialized platforms, powers virtual assistants, self-driving vehicles and connected objects. Using qualitative data from multiple sources, we show that micro-work performs a variety of functions, between three poles that we label, respectively, ‘artificial intelligence preparation’, ‘artificial intelligence verification’ and ‘artificial intelligence impersonation’. Because of the wide scope of application of micro-work, it is a structural component of contemporary artificial intelligence production processes – not an ephemeral form of support that may vanish once the technology reaches maturity stage. Through the lens of micro-work, we prefigure the policy implications of a future in which data technologies do not replace human workforce but imply its marginalization and precariousness.","",""
"2020","The virtue of simplicity: On machine learning models in algorithmic trading","Machine learning models are becoming increasingly prevalent in algorithmic trading and investment management. The spread of machine learning in finance challenges existing practices of modelling and model use and creates a demand for practical solutions for how to manage the complexity pertaining to these techniques. Drawing on interviews with quants applying machine learning techniques to financial problems, the article examines how these people manage model complexity in the process of devising machine learning-powered trading algorithms. The analysis shows that machine learning quants use Ockham’s razor – things should not be multiplied without necessity – as a heuristic tool to prevent excess model complexity and secure a certain level of human control and interpretability in the modelling process. I argue that understanding the way quants handle the complexity of learning models is a key to grasping the transformation of the human’s role in contemporary data and model-driven finance. The study contributes to social studies of finance research on the human–model interplay by exploring it in the context of machine learning model use.","",""
"2020","Beyond algorithmic reformism: Forward engineering the designs of algorithmic systems"," This article develops a method for investigating the consequences of algorithmic systems according to the documents that specify their design constrains. As opposed to reverse engineering algorithms to identify how their logic operates, the article proposes to design or """"forward engineer"""" algorithmic systems in order to theorize how their consequences are informed by design constraints: the specific problems, use cases, and presuppositions that they respond to. This demands a departure from algorithmic reformism, which responds to concerns about the consequences of algorithmic systems by proposing to make algorithms more transparent or less biased. Instead, by investigating algorithmic systems according to documents that specify their design constraints, we identify how the consequences of algorithms are presupposed by the problems that they propose to solve, the types of solutions that they enlist to solve these problems, and the systems of authority that these solutions depend on. To accomplish this, this article develops a methodological framework for researching the process of designing algorithmic systems. In doing so, it proposes to move beyond reforming the technical implementation details of algorithms in order to address the design problems and constraints that underlie them. ","",""
"2020","How to translate artificial intelligence? Myths and justifications in public discourse","Automated technologies populating today’s online world rely on social expectations about how “smart” they appear to be. Algorithmic processing, as well as bias and missteps in the course of their development, all come to shape a cultural realm that in turn determines what they come to be about. It is our contention that a robust analytical frame could be derived from culturally driven Science and Technology Studies while focusing on Callon’s concept of translation. Excitement and apprehensions must find a specific language to move past a state of latency. Translations are thus contextual and highly performative, transforming justifications into legitimate claims, translators into discursive entrepreneurs, and power relations into new forms of governance and governmentality. In this piece, we discuss three cases in which artificial intelligence was deciphered to the public: (i) the Montreal Declaration for a Responsible Development of Artificial Intelligence, held as a prime example of how stakeholders manage to establish the terms of the debate on ethical artificial intelligence while avoiding substantive commitment; (ii) Mark Zuckerberg’s 2018 congressional hearing, where he construed machine learning as the solution to the many problems the platform might encounter; and (iii) the normative renegotiations surrounding the gradual introduction of “killer robots” in military engagements. Of interest are not only the rational arguments put forward, but also the rhetorical maneuvers deployed. Through the examination of the ramifications of these translations, we intend to show how they are constructed in face of and in relation to forms of criticisms, thus revealing the highly cybernetic deployment of artificial intelligence technologies.","",""
"2020","Expectations of artificial intelligence and the performativity of ethics: Implications for communication governance"," This article draws on the sociology of expectations to examine the construction of expectations of ‘ethical AI’ and considers the implications of these expectations for communication governance. We first analyse a range of public documents to identify the key actors, mechanisms and issues which structure societal expectations around artificial intelligence (AI) and an emerging discourse on ethics. We then explore expectations of AI and ethics through a survey of members of the public. Finally, we discuss the implications of our findings for the role of AI in communication governance. We find that, despite societal expectations that we can design ethical AI, and public expectations that developers and governments should share responsibility for the outcomes of AI use, there is a significant divergence between these expectations and the ways in which AI technologies are currently used and governed in large scale communication systems. We conclude that discourses of ‘ethical AI’ are generically performative, but to become more effective we need to acknowledge the limitations of contemporary AI and the requirement for extensive human labour to meet the challenges of communication governance. An effective ethics of AI requires domain appropriate AI tools, updated professional practices, dignified places of work and robust regulatory and accountability frameworks. ","",""
"2020","Designing for human rights in AI"," In the age of Big Data, companies and governments are increasingly using algorithms to inform hiring decisions, employee management, policing, credit scoring, insurance pricing, and many more aspects of our lives. Artificial intelligence (AI) systems can help us make evidence-driven, efficient decisions, but can also confront us with unjustified, discriminatory decisions wrongly assumed to be accurate because they are made automatically and quantitatively. It is becoming evident that these technological developments are consequential to people’s fundamental human rights. Despite increasing attention to these urgent challenges in recent years, technical solutions to these complex socio-ethical problems are often developed without empirical study of societal context and the critical input of societal stakeholders who are impacted by the technology. On the other hand, calls for more ethically and socially aware AI often fail to provide answers for how to proceed beyond stressing the importance of transparency, explainability, and fairness. Bridging these socio-technical gaps and the deep divide between abstract value language and design requirements is essential to facilitate nuanced, context-dependent design choices that will support moral and social values. In this paper, we bridge this divide through the framework of Design for Values, drawing on methodologies of Value Sensitive Design and Participatory Design to present a roadmap for proactively engaging societal stakeholders to translate fundamental human rights into context-dependent design requirements through a structured, inclusive, and transparent process. ","",""
"2020","<i>AI ethics should not remain toothless!</i> A call to bring back the teeth of ethics"," Ethics has powerful teeth, but these are barely being used in the ethics of AI today – it is no wonder the ethics of AI is then blamed for having no teeth. This article argues that ‘ethics’ in the current AI ethics field is largely ineffective, trapped in an ‘ethical principles’ approach and as such particularly prone to manipulation, especially by industry actors. Using ethics as a substitute for law risks its abuse and misuse. This significantly limits what ethics can achieve and is a great loss to the AI field and its impacts on individuals and society. This article discusses these risks and then highlights the teeth of ethics and the essential value they can – and should – bring to AI ethics now. ","",""
"2020","Notes from the Desk Set","Over the last five years, we have seen the rise of a new generation of ‘soft AI’ digital assistants like Apple’s Siri, Microsoft’s Cortana, and Amazon’s Alexa, all designed with feminized personas. In this paper, we connect these AI assistants to the long history of the secretary, beginning with secretary as a desk and then as a profession, emblematized by the executive assistant.  How have the gendered practices of organization, anticipation and surveillance been reinvoked in AI agents? How do AI secretaries present new relationships to work, computation and the increasing entanglements of bodies and data?","",""
"2020","Review of A Future History of Water, by Andrea Ballestero (Duke University Press, 2019)","Review of A Future History of Water, by Andrea Ballestero (Duke University Press, 2019)","",""
"2020","Redistribution and Rekognition","Computer scientists, and artificial intelligence researchers in particular, have a predisposition for adopting precise, fixed definitions to serve as classifiers (Agre, 1997; Broussard, 2018). But classification is an enactment of power; it orders human interaction in ways that produce advantage or suffering (Bowker &amp; Star, 1999). In so doing, it obscures the messiness of human life, masking the work of the people involved in training machine learning systems, and hiding the uneven distribution of its impacts on communities (Taylor, 2018; Gray, 2019; Roberts, 2019). Feminist scholars, and particularly feminist scholars of color, have made powerful critiques of the ways in which artificial intelligence systems formalize, classify, and amplify historical forms of discrimination and act to reify and amplify existing forms of social inequality (Eubanks, 2017; Benjamin, 2019; Noble, 2018). In response, the machine learning community has begun to address claims of algorithmic bias under the rubric of fairness, accountability, and transparency. But in doing so, it has largely dealt with these issues in familiar terms, using statistical methods aimed at achieving parity and deploying fairness ‘toolkits’. Yet actually existing inequality is reflected and amplified in algorithmic systems in ways that exceed the capacity of statistical methods alone. This article outlines a feminist critique of extant methods of dealing with algorithmic discrimination. I outline the ways in which gender discrimination and erasure are built into the field of AI at a foundational level; the product of a community that largely represents a small, privileged, and male segment of the global population (Author, 2019). In so doing, I illustrate how a situated mode of inquiry enables us to more closely examine a feedback loop between discriminatory workplaces and discriminatory systems.","",""
"2020","Hey Siri, tell me a story: Digital storytelling and AI authorship"," Surveying narrative applications of artificial intelligence in film, games and interactive fiction, this article imagines the future of artificial intelligence (AI) authorship and explores trends that seek to replace human authors with algorithmically generated narrative. While experimental works that draw on text generation and natural language processing have a rich history, this article focuses on commercial applications of AI narrative and looks to future applications of this technology. Video games have incorporated AI and procedural generation for many years, but more recently, new applications of this technology have emerged in other media. Director Oscar Sharp and artist Ross Goodwin, for example, generated significant media buzz about two short films that they produced which were written by their AI screenwriter. It’s No Game (2017), in particular, offers an apt commentary on the possibility of replacing striking screenwriters with AI authors. Increasingly, AI agents and virtual assistants like Siri, Cortana, Alexa and Google Assistant are incorporated into our daily lives. As concerns about their eavesdropping circulate in news media, it is clear that these companions are learning a lot about us, which raises concerns about how our data might be employed in the future. This article explores current applications of AI for storytelling and future directions of this technology to offer insight into issues that have and will continue to arise as AI storytelling advances. ","",""
"2020","Augmenting autonomy: ‘New Collar’ labor and the future of tech work"," This essay maps IBM’s attempts to construct a typology of high-tech ‘New Collar’ work and leverage policymaking outcomes to underwrite IBM corporate ventures capable of materializing this work. Through a discursive analysis of IBM corporate texts, webpages, and the 2017 New Collar Jobs Act, I argue for New Collar work to be understood through the lens of autonomy, as IBM recasts notions of ‘autonomous’ technology onto humans by downplaying dystopic associations of technological autonomy and transferring notions of autonomy to human workers. In doing so, I account for IBM’s use of ‘augmentation’ to situate human intelligence as the cognitive force uplifted by work performed with artificial intelligence. By pairing human augmentation with posthumanist conceptions of ‘distributed cognition’, IBM centers human intelligence through a redistributed cognition that reverses posthumanism’s decentering of human supremacy. Following from this, I unpack ‘New Collar’ as a reinvention of ‘white’ and ‘blue’ collar dichotomies and New Collar work as the grounds for tech workers to reinvent themselves. In this way, by minimizing the necessity of 4-year college degrees as pathways to economic and professional mobility, IBM constructs ‘New Collar’ with embedded notions of enlarged self-determination for applied worker intellect, vocational training, and employability. Under the aegis of creating, training, and employing New Collar workers, IBM pursues policy outcomes to underwrite corporate ventures related to New Collar work and bolster its institutional autonomy amidst marketplaces of cognitive capitalism. By outlining how tax relief provisions of the New Collar Jobs Act correlate with neoliberal ideologies of legislators and IBM investments in public–private vocational models and cybersecurity platforms, I account for IBM’s elongated ‘economy of learning’ that enables the company to more thoroughly capture, underwrite, and commodify New Collar cognition from training to market outputs. ","",""
"2020","Imagining the thinking machine: Technological myths and the rise of artificial intelligence"," This article discusses the role of technological myths in the development of artificial intelligence (AI) technologies from 1950s to the early 1970s. It shows how the rise of AI was accompanied by the construction of a powerful cultural myth: The creation of a thinking machine, which would be able to perfectly simulate the cognitive faculties of the human mind. Based on a content analysis of articles on AI published in two magazines, the Scientific American and the New Scientist, which were aimed at a broad readership of scientists, engineers and technologists, three dominant patterns in the construction of the AI myth are identified: (1) the recurrence of analogies and discursive shifts, by which ideas and concepts from other fields were employed to describe the functioning of AI technologies; (2) a rhetorical use of the future, imagining that present shortcomings and limitations will shortly be overcome and (3) the relevance of controversies around the claims of AI, which we argue should be considered as an integral part of the discourse surrounding the AI myth. ","",""
"2020","What do home robots want? The ambivalent power of cuteness in robotic relationships"," A wave of social, domestic robots is poised to enter our homes. Robots such as Jibo, Kuri and Olly are networked with other ‘smart’ devices and use cameras and voice control to provide companionship, care and household management. These robots are proposed as members of the family, and as such must encourage intimacy and trust with their human caregivers. In this article, we explore the nexus between the cute aesthetic of home robots and the kinds of affective relationships this aesthetic enables with the human user. Our argument is that the cuteness of home robots creates a highly ambivalent relationship of power between (human) subject and (robotic/digital) object, whereby the manifestation of consciousness and the production of lasting emotional bonds require home robots to exceed the affective and semiotic limitations, even as their cute appearance may encourage the production of intimacy. By exceeding the borders established by their own design, home robots are able to manifest as conscious beings, a manifestation which both destabilizes the power differential between user and robot and, paradoxically, points to the possibility of their own replacement. To explore these ideas, we discuss three soon-to-be-released social robots: Mayfield Robotics’ Kuri, Emotech’s Olly and Jibo Inc.’s Jibo. Each promises a unique personality that will integrate them as a member of the family. ","",""
"2020","Chatbot-mediated public service delivery","Chatbots — computer programs designed to interactively engage with users, replicating humanlike conversational capabilities during service encounters — have been increasingly deployed across a wide range of Internet-based public services. While chatbots provide several advantages (e.g., improved user experience with reduced waiting times to service access), the surge of chatbot use in public service delivery has frequently been plagued with controversy, poor publicity, and legal challenges. One important reason for this is that users of the services, and the wider public, do not always feel that chatbot-mediated services demonstrate the appropriate public service values. We investigate the public service value dimensions required in chatbots designed for use in the public sector. Specifically, we (a) review chatbots and their use in the delivery of public services; and, (b) develop a framework of how public service values can be exemplified by chatbots. Our study provides implications and evaluation criteria for stakeholders in chatbot assisted public services, including researchers, public managers, and citizens.","",""
"2020","Chief information officers’ perceptions about artificial intelligence","This article presents a study about artificial intelligence (AI) policy based on the perceptions, expectations, and challenges/opportunities given by chief information officers (CIOs). In general, publications about AI in the public sector relies on experiences, cases, ideas, and results from the private sector. Our study stands out from the need of defining a distinctive approach to AI in the public sector, gathering primary (and comparative) data from different countries, and assessing the key role of CIOs to frame federal/national AI policies and strategies. This article reports three research questions, including three dimensions of analysis: (1) perceptions regarding to the concept of AI in the public sector; (2) expectations about the development of AI in the public sector; and, (3) challenges and opportunities of AI in the public sector. This exploratory study presents the results of a survey administered to federal/national ministerial government CIOs in ministries of Mexico and Spain. Our descriptive statistical (and exploratory) analysis provides an overall approach to our dimensions, exploratory answering the research questions of the study. Our data supports the existence of different governance models and policy priorities in different countries. Also, these results might inform research in this same area and will help senior officials to assess the national AI policies actually in process of design and implementation in different national/federal, regional/state, and local/municipal contexts.","",""
"2020","Culture by design","This article investigates a moment of the big data age in which artificial intelligence became a fixed point of global negotiations between different interests in data. In particular, it traces and explicates cultural positioning as an interest in the artificial intelligence momentum with an investigation of the unfolding of a European AI policy agenda on trustworthy AI in the period 2018–2019.","",""
"2020","A Promethean Philosophy of External Technologies, Empiricism, &amp; the Concept","Beginning with a survey of the shortcoming of theories of organology/media-as-externalization of mind/body—a philosophical-anthropological tradition that stretches from Plato through Ernst Kapp and finds its contemporary proponent in Bernard Stiegler—I propose that the phenomenological treatment of media as an outpouching and extension of mind qua intentionality is not sufficient to counter the ‘black-box’ mystification of today’s deep learning’s algorithms. Focusing on a close study of Simondon’s On the Existence of Technical Objects and Individuation, I argue that the process-philosophical work of Gilbert Simondon, with its critique of Norbert Wiener’s first-order cybernetics, offers a precursor to the conception of second-order cybernetics (as endorsed by Francisco Varela, Humberto Maturana, and Ricardo B. Uribe) and, specifically, its autopoietic treatment of information. It has been argued by those such as Frank Pasquale that neuro-inferential deep learning systems premised on predictive patterning, such as AlphaGo Zero, have a veiled logic and, thus, are ‘black boxes’. In detailing a philosophical-historical approach to demystify predictive patterning/processing and the logic of such deep learning algorithms, this paper attempts to shine a light on such systems and their inner workings à la Simondon.   ","",""
"2020","Spatiotemporal Zones of Neosomnambulism","Drawing on Deleuze and Guattari’s (1994) methodology developed in their swansong, What is Philosophy?, this article deploys its own conceptual persona: the Neosomnambulist or new sleepwalker. Not to be mistaken for an actual living person, the Neosomnambulist is utilized so as to bring concepts to life. In this case, what the sleepwalker gives life to are spatiotemporal zones of indistinction that pervade the digital now.   ","",""
"2020","Artificial intelligence and communication: A Human–Machine Communication research agenda"," Artificial intelligence (AI) and people’s interactions with it—through virtual agents, socialbots, and language-generation software—do not fit neatly into paradigms of communication theory that have long focused on human–human communication. To address this disconnect between communication theory and emerging technology, this article provides a starting point for articulating the differences between communicative AI and previous technologies and introduces a theoretical basis for navigating these conditions in the form of scholarship within human–machine communication (HMC). Drawing on an HMC framework, we outline a research agenda built around three key aspects of communicative AI technologies: (1) the functional dimensions through which people make sense of these devices and applications as communicators, (2) the relational dynamics through which people associate with these technologies and, in turn, relate to themselves and others, and (3) the metaphysical implications called up by blurring ontological boundaries surrounding what constitutes human, machine, and communication. ","",""
"2020","Human-aided artificial intelligence: Or, how to run large computations in human brains? Toward a media sociology of machine learning"," Today, artificial intelligence (AI), especially machine learning, is structurally dependent on human participation. Technologies such as deep learning (DL) leverage networked media infrastructures and human-machine interaction designs to harness users to provide training and verification data. The emergence of DL is therefore based on a fundamental socio-technological transformation of the relationship between humans and machines. Rather than simulating human intelligence, DL-based AIs capture human cognitive abilities, so they are hybrid human-machine apparatuses. From a perspective of media philosophy and social-theoretical critique, I differentiate five types of “media technologies of capture” in AI apparatuses and analyze them as forms of power relations between humans and machines. Finally, I argue that the current hype about AI implies a relational and distributed understanding of (human/artificial) intelligence, which I categorize under the term “cybernetic AI.” This form of AI manifests in socio-technological apparatuses that involve new modes of subjectivation, social control, and digital labor. ","",""
"2020","The return of the social: Algorithmic identity in an age of symbolic demise"," This article explores the socio-algorithmic construction of identity categories based on an ethnographic study of the Israeli data analytics industry. While algorithmic categorization has been described as a post-textual phenomenon that leaves language, social theory, and social expertise behind, this article focuses on the return of the social—the process through which the symbolic means resurface to turn algorithmically produced clusters into identity categories. I show that such categories stem not only from algorithms’ structure or their data, but from the social contexts from which they arise, and from the values assigned to them by various individuals. I accordingly argue that algorithmic identities stem from epistemic amalgams—complex blends of algorithmic outputs and human expertise, messy data flows, and diverse inter-personal factors. Finally, I show that this process of amalgamation arbitrarily conjoins quantitative clusters with qualitative labels, and I discuss the implausibility of seeing named algorithmic categories as explainable. ","",""
"2020","Assessing Trust Versus Reliance for Technology Platforms by Systematic Literature Review"," We do not trust technologies like we trust people, rather we rely on them. This article argues for an emphasis on reliance rather than trust as a concept for understanding human relationships with technology. Reliance is important because researchers can empirically measure the reliability of a given technology. We first explore two frameworks of trust and reliance. We then examine how reliance can be measured by conducting systematic literature reviews of reported success metrics for given technologies. Specifically, we examine papers which present models for predicting private traits from social media data. Of the 72 models for predicting private traits that were surveyed from 31 papers, 80% of the methods reported success rates lower than 90%, indicating a general unreliability in predicting private traits. We illustrate the current applicability of this method throughout the article by discussing the Cambridge Analytica scandal that began during the 2016 US Presidential election. ","",""
"2020","A Rights-Based Approach to Trustworthy AI in Social Media"," Social media platforms increasingly use powerful artificial intelligence (AI) that are fed by the vast flows of digital content that may be used to analyze user behavior, mental state, and physical context. New forms of AI-generated content and AI-driven virtual agents present new forms of risks in social media use, the harm of which will be difficult to predict. Delivering trustworthy social media will therefore be increasingly predicated on effectively governing the trustworthiness of its AI components. In this article, we examine different approaches to the governance AI and the Big Data processing that drives it being explored. We identify a potential over-reliance on individual rights at the expense of consideration of collective rights. In response, we propose a collective approach to AI data governance grounded in a legal proposal for universal, non-exclusive data ownership right. We use the Institutional Analysis and Development (IAD) framework to explore the relative costs and benefits on stakeholders in two use cases, one focused on digital content consumers the other focused on digital content knowledge workers. Following an analysis that looks at self-regulation and industry-state co-regulation, we propose governance through shared data ownership. In this way, future social media platforms may be able to maintain trust in their use of AI by committing to no datafication without representation. ","",""
"2020","Gaming Algorithmic Hate-Speech Detection: Stakes, Parties, and Moves"," A recent strand of research considers how algorithmic systems are gamed in everyday encounters. We add to this literature with a study that uses the game metaphor to examine a project where different organizations came together to create and deploy a machine learning model to detect hate speech from political candidates’ social media messages during the Finnish 2017 municipal election. Using interviews and forum discussions as our primary research material, we illustrate how the unfolding game is played out on different levels in a multi-stakeholder situation, what roles different participants have in the game, and how strategies of gaming the model revolve around controlling the information available to it. We discuss strategies that different stakeholders planned or used to resist the model, and show how the game is not only played against the model itself, but also with those who have created it and those who oppose it. Our findings illustrate that while “gaming the system” is an important part of gaming with algorithms, these games have other levels where humans play against each other, rather than against technology. We also draw attention to how deploying a hate-speech detection algorithm can be understood as an effort to not only detect but also preempt unwanted behavior. ","",""
"2020","Practical AI Transparency: Revealing Datafication and Algorithmic Identities","How does one do research on algorithms and their outputs when confronted with the inherent algorithmic opacity and black box-ness as well as with the limitations of API-based research and the data access gaps imposed by platforms’ gate-keeping practices? This article outlines the methodological steps we undertook to manoeuvre around the above-mentioned obstacles. It is a “byproduct” of our investigation into datafication and the way how algorithmic identities are being produced for personalisation, ad delivery and recommendation. Following Paßmann and Boersma’s (2017) suggestion for pursuing “practical transparency” and focusing on particular actors, we experiment with different avenues of research. We develop and employ an approach of letting the platforms speak and making the platforms speak. In doing so, we also use non-traditional research tools, such as transparency and regulatory tools, and repurpose them as objects of/for study. Empirically testing the applicability of this integrated approach, we elaborate on the possibilities it offers for the study of algorithmic systems, while being aware and cognizant of its limitations and shortcomings.","",""
"2020","Digital Limit Situations: Anticipatory Media Beyond ‘The New AI Era’","  In the present age AI (artificial intelligence) emerges as both a medium to and message about (or even from) the future, eclipsing all other possible prospects. Discussing how AI succeeds in presenting itself as an arrival on the human horizon at the end times, this theoretical essay scrutinizes the ‘inevitability’ of AI-driven abstract futures and probes how such imaginaries become living myths, by attending how the technology is embedded in broader appropriations of the future tense. Reclaiming anticipation existentially, by drawing and expanding on the philosophy of Karl Jaspers – and his concept of the limit situation – I offer an invitation beyond the prospects and limits of ‘the new AI Era’ of predictive modelling, exploitation and dataism. I submit that the present moment of technological transformation and of escalating multi-faceted and interrelated global crises, is a digital limit situation in which there are entrenched existential and politico-ethical stakes of anticipatory media. Attending to them as a ‘future present’ (Adam and Groves 2007, 2011), taking responsible action, constitutes our utmost capability and task. The essay concludes that precisely here lies the assignment ahead for pursuing a post-disciplinary, integrative and generative form of Humanities and Social Sciences as a method of hope, that engages AI designers in the pursuit of an inclusive and open future of existential and ecological sustainability.  ","",""
"2020","A Social Science Perspective on Artificial Intelligence: Building Blocks for a Research Agenda","  In this article, we discuss and outline a research agenda for social science research on artificial intelligence. We present four overlapping building blocks that we see as keys for developing a perspective on AI able to unpack the rich complexities of sociotechnical settings. First, the interaction between humans and machines must be studied in its broader societal context. Second, technological and human actors must be seen as social actors on equal terms. Third, we must consider the broader discursive settings in which AI is socially constructed as a phenomenon with related hopes and fears. Fourth, we argue that constant and critical reflection is needed over how AI, algorithms and datafication affect social science research objects and methods. This article serves as the introduction to this JDSR special issue about social science perspectives on AI.  ","",""
"2020","Artificial Intelligence and Video Game Creation: A Framework for the New Logic of Autonomous Design","  Autonomous, intelligent tools are reshaping all sorts of work practices, including innovative design work. These tools generate outcomes with little or no user intervention and produce designs of unprecedented complexity and originality, ushering profound changes to how organizations will design and innovate in future. In this paper, we formulate conceptual foundations to analyze the impact of autonomous design tools on design work. We proceed in two steps. First, we conceptualize autonomous design tools as ‘rational’ agents which will participate in the design process. We show that such agency can be realized through two separate approaches of information processing: symbolic and connectionist. Second, we adopt control theory to unpack the relationships between the autonomous design tools, human actors involved in the design, and the environment in which the tools operate. The proposed conceptual framework lays a foundation for studying the new kind of material agency of autonomous design tools in organizational contexts. We illustrate the analytical value of the proposed framework by drawing on two examples from the development of Ubisoft’s Ghost Recon Wildlands video game, which relied on such tools. We conclude this essay by constructing a tentative research agenda for the research into autonomous design tools and design work.  ","",""
"2020","Coproduction, Ethics and Artificial Intelligence: A Perspective from Cultural Anthropology","  Over the past five years, artificial intelligence (AI) has been endorsed as the technical underpinning of innovation. Sensationalist representations of AI have also been accompanied by assumptions of technological determinism that distract from the ordinary, sometimes unassuming consequences of interaction with its systems and processes. Drawing on scholarship from cultural anthropology, along with science and technology studies (STS), this paper examines coproduction in a Canadian AI research and development context. Through interview responses and field observations it presents sites of sociotechnical entanglement and ethical discussion to highlight potential spaces of mediation for anthropological practice. Emerging themes from the experiences of AI specialists include the negotiability of technology, an ethics of the everyday and critical collaboration. Together this returns to an initial approach into a situated understanding of artificial intelligence, negotiating with broad, sensationalist perspectives and the more commonplace, backgrounded cases of narrow research.  ","",""
"2020","Defining basic archetypes of an intelligent agent’s behaviour in an open-ended interactive environment","ABSTRACT Despite the emergence of advanced technologies, the behavioural complexities remain underexplored, especially from the viewpoint of their potential in interactive installations. This study aims to explore the future interactivity of an agent’s behaviour by envisioning speculative future human operations using a human-controlled interactive installation called LumiLand. This empirical study reveals that an agent can craft a user experience by (1) controlling the plot, (2) cocreating content with a user in a social manner, and (3) promptly adjusting undefined behaviours and situations. Based on Janlert and Stolterman’s interaction model, we present an interaction flow with four primary classifications: Leading, Responding, Poking, and Linking. We believe that a human-centered approach for understanding agent interactivity could help create entertaining human–computer interactions (HCI) by improving the agent design and exploring the challenges that will be faced during such interactions.","",""
"2020","On the origins of a 13-second segment of <i>Primordial Dance</i>: a brief Karl Sims interview with commentary","ABSTRACT In this brief but highly focused interview, pioneer computer animation artist Karl Sims addresses the extent to which a particularly striking segment of his ground-breaking 1991 Primordial Dance has been influenced by early twentieth-century abstract art.","",""
"2020","Human–robot collaboration: a fabrication framework for the sequential design and construction of unplanned spatial structures","ABSTRACT Robots in traditional fabrication applications act as passive participants in the process of creation—simply performing a set of predetermined actions to materialize a completed design. We propose a novel bottom-up design framework in which robots are instead given the opportunity to participate centrally within a creative design process. This paper describes how two 6-axis industrial robotic arms were used to cooperatively aggregate a collection of solid spherical units. The branching spatial structure being constructed is unplanned at the outset of this process, and is instead designed in pseudo-realtime during construction. This ‘design-as-you-build’ approach relies on robotic input, in the form of path-planning constraints, in tandem with human evaluation and decision-making. The resulting structure emerges from a human–robot design collaboration operating within the specified physical domain.","",""
"2020","Playing With Unicorns: AI Dungeon and Citizen NLP","Author(s): Raley, Rita; Hua, Minh | Abstract: AI Dungeon 2 is an indie text adventure game that caught traction within the gaming and hobbyist machine learning communities for its promise of “infinite” customizable adventures, which are generated and narrated by GPT-2, OpenAI’s 1.5 billion parameter language model. Samples of gameplay illustrate AID’s remarkable linguistic competence and domain knowledge, as well as its capacity for what can only be described as wackiness. More striking are AID’s innovative gameplay mechanics, which reimagine how we interact with large language models. Game play entails a procedural and incremental process of engaging with GPT-2 that opens up the possibility of developing a holistic and interdisciplinary framework for meaningful qualitative evaluation of language models that does not have commercial use as its necessary endgame. With respect to both evaluation and writing itself, AID situates human players inextricably “in the loop” as necessary partners with autonomous systems. Our article thus reads AID both as an example of current hobbyist relations with machine learning and as a responsible model for future human-AI collaborative creative practices.","",""
"2021","The Promise of Artificial Intelligence: Reckoning and Judgement by Brian Cantwell Smith, and: Cloud Ethics: Algorithms and the Attributes of Ourselves and Others by Louise Amoore (review)","","",""
"2021","Algorithm Awareness as an Important Internet Skill: The Case of Voice Assistants","Voice assistants have become increasingly popular as part of digital technologies that people use in their everyday lives. Research on Internet use has shown that people’s online experiences are influenced by their level of know-how about the platforms they use. Extending the literature on Internet skills, this article focuses on people’s algorithm skills in the domain of voice assistants. Are people aware of how algorithms influence what information they receive when using voice assistants? Drawing on 83 interviews conducted in 5 countries, we find that only a few participants explicitly mentioned the terms algorithms and artificial intelligence. Still, many seemed to be aware of the existence of automatic decision-making processes in voice assistants. This awareness was not necessarily based on their own experience with voice assistants, however. Rather, it was often a result of experiences with other digital devices and services such as Google Search, Facebook, Amazon, or smartphones, as well as information from social contacts and the media. We discuss the relevance of being aware of algorithms as one dimension of Internet skills.","",""
"2021","Covering Technology Risks and Responsibility: Automation, Artificial Intelligence, Robotics, and Algorithms in the Media","Rapid technological advances in automation, algorithms, robots, and artificial intelligence (AI) entail risks, such as the loss of jobs, biases, and security threats, which raises questions about responsibility for the damage incurred by such risks and the development of solutions to ameliorate them. Because media play a central role in the representation and perception of technological risks and responsibility, this study explores the news coverage of automation. While previous research has focused on specific technologies, this study conducts a comprehensive analysis of the debate on automation, algorithms, robotics, and AI, including tonality, risks, and responsibility. The longitudinal media content analysis of three decades of Austrian news reports revealed that overall, the coverage increased, and it was optimistic in tone. However, algorithms were more frequently associated with risks and less positivity than other automation areas. Robotics received the most positive and the least risk-related coverage. Moreover, industry stakeholders were at the center of the responsibility network in the media discourse.","",""
"2021","New Feminist Studies in Audiovisual Industries| Algorithmic Gender Bias and Audiovisual Data: A Research Agenda","Algorithms are increasingly used to offer jobs, loans, medical care, and other services, as well as to influence behavior. Decisions that create the algorithms, the data sets that feed them, and the outputs that result from algorithmic decision making, can be biased, potentially harming women and perpetuating discrimination. Audiovisual data are especially challenging because of online growth and their impact on women’s lives. While scholarship has acknowledged data divides and cases of algorithmic bias mostly in online texts, it has yet to explore the relevance of audiovisual content for gender algorithmic bias. Based on previous guidelines and literature on algorithmic bias, this article (a) connects different types of bias with factors and harmful outcomes for women; (b) examines challenges around the lack of clarity about which data are necessary for fair algorithmic decision making, the lack of understanding of how machine learning algorithms work, and the lack of incentives for corporations to correct bias; and (c) offers a research agenda to address algorithmic gender discrimination prevalent in audiovisual data.","",""
"2021","Meredith Broussard, Artificial Unintelligence: How Computers Misunderstand the World","","",""
"2021","Louise Amoore, Cloud Ethics: Algorithms and the Attributes of Ourselves and Others","","",""
"2021","Little history of CAPTCHA","Abstract This article traces the early history of CAPTCHA, the now ubiquitous cybersecurity tool that prompts users to “confirm their humanity” by solving word- and image-based puzzles before accessing free online services. CAPTCHA, and its many derivatives, are presented as content identification mechanisms: the user is asked to identify content in order for the computer to determine the identity of the user. This twofold process of content identification, however, has evolved significantly since CAPTCHA’s inception in the late 1990s. Pivoting away from a realist framework, largely dependent on the standard tenets [of] cryptography, toward a relational framework premised on aesthetic contingency and social consensus, CAPTCHA’s arc uniquely illustrates how contested notions of both “content” and “identity” become materialized in contemporary internet infrastructure. Inspired by Walter Benjamin’s exegetical study of early photography, this critical historicization aims to foreground CAPTCHA as a particularly fraught juncture of humans and computers, which, as with Benjamin’s intervention, productively troubles received ideas of humanism and automation, mediation and materiality.","",""
"2021","Civil legal personality of artificial intelligence. Future or utopia?",": The technology associated with artificial intelligence is developing rapidly. As a consequence, artificial intelligence is being applied in many spheres of life and increasingly affects the functioning of society. Actions of artificial intelligence may cause harm (e.g. in the case of autonomous vehicles that cause traffic accidents). Rules of civil law, especially those relating to liability for damage resulting from somebody’s fault or risk, came into being before artificial intelligence's invention and mostly before the latter’s significant recent development. They include the Polish Code of civil procedure, which addresses the issues associated with liability, adopted in 1964 and still in force today, although with certain amendments. Therefore, no provisions that would directly refer to artificial intelligence and legal consequences of its actions have been introduced into the Polish civil law. The same applies to European law. Therefore, the issue of whether existing regulations may be applied in the case of artificial intelligence or, perhaps, whether they should be appropriately adjusted, needs to be analysed. The starting point for this analysis is the possibility of conferring upon artificial intelligence the status of an entity under the law, allowing it to independently bear the liability for the damage caused by it. This","",""
"2021","Artificial intelligence and consent: a feminist anti-colonial critique","Feminist theories have extensively debated consent in sexual and political contexts. But what does it mean to consent when we are talking about our data bodies feeding artificial intelligence (AI) systems? This article builds a feminist and anti-colonial critique about how an individualistic notion of consent is being used to legitimate practices of the so-called emerging Digital Welfare States, focused on digitalisation of anti-poverty programmes. The goal is to expose how the functional role of digital consent has been enabling data extractivist practices for control and exclusion, another manifestation of colonialism embedded in cutting-edge digital technology. Issue 4 This paper is part of Feminist data protection, a special issue of Internet Policy Review guest-edited by Jens T. Theilen, Andreas Baur, Felix Bieker, Regina Ammicht Quinn, Marit Hansen, and Gloria González Fuster.","",""
"2021","What rights matter? Examining the place of social rights in the EU’s artificial intelligence policy debate","References to ‘European values’ are often rooted in some perception of a commitment to particular rights that uphold certain principles about democracy and the relationship between state, market and citizens. Whilst rarely translated into consistent policy frameworks or activities, the formulation of new policy areas, such as artificial intelligence (AI), provide a window into what priorities, interests and concerns currently shape the European project. In this paper, we explore these questions in relation to the recent AI policy debate in the European Union with a particular focus on the place of social rights as a historically pertinent but neglected aspect of policy debates on technology. By examining submissions to the recent public consultation on the White Paper on AI Strategy, we argue that social rights occupy a marginal position in EU’s policy debates on emerging technologies in favour of human rights issues such as individual privacy and nondiscrimination that are often translated into design solutions or procedural safeguards and a commitment to market creation. This is important as systems such as AI are playing an increasingly important role for questions of redistribution and economic inequality that relate to social rights. As such, the AI policy debate both exposes and advances new normative conflicts over the meaning of rights as a central component of any attachment to ‘European values’. Issue 3 This paper is part of Governing “European values” inside data flows, a special issue of Internet Policy Review guest-edited by Kristina Irion, Mira Burri, Ans Kolk, Stefania Milan.","",""
"2021","Beyond the individual: governing AI’s societal harm",",","",""
"2021","Black box algorithms and the rights of individuals: no easy solution to the “explainability” problem",": Over the last few years, the interpretability of classification models has been a very active area of research. Recently, the concept of interpretability was given a more specific legal context. In 2016, the EU adopted the General Data Protection Regulation (GDPR), containing the right to explanation for people subjected to automated decision-making (ADM). The regulation itself is very reticent about what such a right might imply. As a result, since the introduction of the GDPR there has been an ongoing discussion about not only the need to introduce such a right, but also about its scope and practical consequences in the digital world. While there is no doubt that the right to explanation may be very difficult to implement due to technical challenges, any difficulty in explaining how algorithms work cannot be considered a sufficient reason to completely abandon this legal safeguard. The aim of this article is twofold. First, to demonstrate that the interpretability of “black box” machine","",""
"2021","Naming something collective does not make it so: algorithmic discrimination and access to justice",": The article problematises the ability of procedural law to address and correct algorithmic discrimination. It argues that algorithmic discrimination is a collective phenomenon, and therefore legal protection thereof needs to be collective. Legal procedures are technologies and design objects that embed values that can affect their usability to perform the task they are built for. Drawing from science and technology studies (STS) and feminist critique on law, the article argues that procedural law fails to address algorithmic discrimination, as legal protection is built on data-centrism and individual-centred law. As to the future of new procedural design, it suggests collective redress in the form of ex ante protection as a promising way forward.","",""
"2021","Bias does not equal bias: a socio-technical typology of bias in data-based algorithmic systems",": This paper introduces a socio-technical typology of bias in data-driven machine learning and artificial intelligence systems. The typology is linked to the conceptualisations of legal antidiscrimination regulations, so that the concept of structural inequality—and, therefore, of undesirable bias—is defined accordingly. By analysing the controversial Austrian “AMS algorithm” as a case study as well as examples in the contexts of face detection, risk assessment and health care management, this paper defines the following three types of bias: firstly, purely technical bias as a systematic deviation of the datafied version of a phenomenon from reality; secondly, socio-technical bias as a systematic deviation due to structural inequalities, which must be strictly distinguished from, thirdly, societal bias, which depicts—correctly—the structural inequalities that prevail in society. This paper argues that a clear distinction must be made between different concepts of bias in such systems in order to analytically assess these systems and, subsequently, inform political action.","",""
"2021","Algorithmic resistance: media practices and the politics of repair","ABSTRACT The article constitutes a critical intervention in the current, dramatic debate on the consequences of algorithms and automation for society. While most research has focused on negative outcomes, including ethical problems of machine bias and accountability, little has been said about the possibilities of users to resist algorithmic power. The article draws on Raymond Williams’ work on media as practice to advance a framework for studying algorithms with a focus on user agency. We illustrate this framework with the example of the media activist campaign World White Web by the Swedish artist and visual designer Johanna Burai. We suggest that user agency in relation to algorithms can emerge from alternative uses of platforms, in the aftermath of algorithmic logics, and give birth to complicit forms of resistance that work through ‘repair’ politics oriented towards correcting the work of algorithms. We conclude with a discussion of the ways in which the proposed framework helps us rethink debates on algorithmic power.","",""
"2021","Cloud ethics: algorithms and the attributes of ourselves and others","","",""
"2021","To be or not to be algorithm aware: a question of a new digital divide?","ABSTRACT Algorithms are an increasingly important element of internet infrastructure in that they are used to make decisions about everything from mundane music recommendations through to more profound and oftentimes life changing ones such as policing, health care or social benefits. Given algorithmic systems’ impact and sometimes harm on people’s everyday life, information access and agency, awareness of algorithms has the potential to be a critical issue. We, therefore, ask whether having awareness of algorithms or not corresponds to a new reinforced digital divide. This study examines levels of awareness and attitudes toward algorithms across the population of the highly digitized country of Norway. Our exploratory research finds clear demographic differences regarding levels of algorithms awareness. Furthermore, attitudes to algorithm driven recommendations (e.g., YouTube and Spotify), advertisements and content (e.g., personalized news feeds in social media and online newspaper) are associated with both the level of algorithm awareness and demographic variables. A cluster analysis facilitates an algorithm awareness typology of six groups: the unaware, the uncertain, the affirmative, the neutral, the sceptic and the critical.","",""
"2021","Nonhuman humanitarianism: when 'AI for good' can be harmful","ABSTRACT Artificial intelligence (AI) applications have been introduced in humanitarian operations in order to help with the significant challenges the sector is facing. This article focuses on chatbots which have been proposed as an efficient method to improve communication with, and accountability to affected communities. Chatbots, together with other humanitarian AI applications such as biometrics, satellite imaging, predictive modelling and data visualisations, are often understood as part of the wider phenomenon of ‘AI for social good’. The article develops a decolonial critique of humanitarianism and critical algorithm studies which focuses on the power asymmetries underpinning both humanitarianism and AI. The article asks whether chatbots, as exemplars of ‘AI for good’, reproduce inequalities in the global context. Drawing on a mixed methods study that includes interviews with seven groups of stakeholders, the analysis observes that humanitarian chatbots do not fulfil claims such as ‘intelligence’. Yet AI applications still have powerful consequences. Apart from the risks associated with misinformation and data safeguarding, chatbots reduce communication to its barest instrumental forms which creates disconnects between affected communities and aid agencies. This disconnect is compounded by the extraction of value from data and experimentation with untested technologies. By reflecting the values of their designers and by asserting Eurocentric values in their programmed interactions, chatbots reproduce the coloniality of power. The article concludes that ‘AI for good’ is an ‘enchantment of technology’ that reworks the colonial legacies of humanitarianism whilst also occluding the power dynamics at play.","",""
"2021","Ethical aspects of multi-stakeholder recommendation systems","","",""
"2021","Advances and Limitations in Open Source Arabic-Script OCR: A Case Study","This work presents an accuracy study of the open source OCR engine, Kraken, on the leading Arabic scholarly journal, al-Abhath. In contrast with other commercially available OCR engines, Kraken is shown to be capable of producing highly accurate Arabic-script OCR. The study also assesses the relative accuracy of typeface-specific and generalized models on the al-Abhath data and provides a microanalysis of the “error instances” and the contextual features that may have contributed to OCR misrecognition. Building on this analysis, the paper argues that Arabic-script OCR can be significantly improved through (1) a more systematic approach to training data production, and (2) the development of key technological components, especially multi-language models and improved line segmentation and layout analysis./Cet article présente une étude d’exactitude du moteur ROC open source, Krakan, sur la revue académique arabe de premier rang, al-Abhath. Contrairement à d’autres moteurs ROC disponibles sur le marché, Kraken se révèle être capable de produire de la ROC extrêmement exacte de l’écriture arabe. L’étude évalue aussi l’exactitude relative des modèles spécifiquement configurés à des polices et celle des modèles généralisés sur les données d’al-Abhath et fournit une microanalyse des « occurrences d’erreurs », ainsi qu’une microanalyse des éléments contextuels qui pourraient avoir contribué à la méreconnaissance ROC. S’appuyant sur cette analyse, cet article fait valoir que la ROC de l’écriture arabe peut être considérablement améliorée grâce à (1) une approche plus systématique d’entraînement de la production de données et (2) grâce au développement de composants technologiques fondamentaux, notammentl’amélioration des modèles multilingues, de la segmentation de ligne et de l’analyse de la mise en page.","",""
"2021","Apple Memories and Automated Memory-Making","","",""
"2021","The Invisualities of Capture in Amazon’s Logistical Operations","","",""
"2021","PROMOTING SELF-HELP—HOW INTERNET USERS PROTECT THEMSELVES AGAINST        ALGORITHMIC RISKS","In today’s digitized society, internet users increasingly rely on online services that apply algorithmic selection, like for instance Google Search or the Facebook News Feed. The algorithms that are implemented in these services automatically select information sets and assign relevance to them. This entails societal risks such as privacy breaches, surveillance, manipulation, or overuse. One way for internet users to cope with these risks, is the use of self-help strategies such as deleting cookies or using an adblocker. Therefore, this article wants to answer the following question: What are the factors that promote internet users’ self-help against algorithmic risks? To do so, we analyze nationally representative survey data for three types of algorithmic risks: surveillance, manipulation, and internet overuse. The structural equation models show that being aware of algorithmic risks (H1), having had negative experiences that are related to these risks (H2) and possessing a higher level of internet skills (H3) are positively associated with the use of self-help strategies against algorithmic risks. Therefore, we conclude that awareness of algorithmic risks and internet skills should be promoted to increase internet users’ self-help. Nevertheless, self-help can only complement—but not substitute—statutory regulation to attenuate algorithmic risks.","",""
"2021","TOWARDS OPERATIONALIZING WHITE MALE ACCOUNTABILITY IN ARTIFICIAL INTELLIGENCE DEVELOPMENT INTERROGATING IMPACTS OF AND SOLUTIONS TO OVERREPRESENTATION        UTILIZING RELATIONAL ETHICS FRAMEWORKS","Numerical metrics demonstrate that white men are demographically         overrepresented in Artificial Intelligence (AI) technology development, research, and media         coverage. This overrepresentation creates immediate and downstream harms that corporations         and technologists in industry and the academy alike must contend with to ensure the creation         of AI technologies, AI development organizations, and AI research institutions that are         ethical, fair, accountable, transparent, and beneficial to all people. After defining the         problem of overrepresentation and exploring why this problem is vital to address, this paper         will posit a two-pronged theoretical solution to be implemented: (1) increasing white male         accountability in AI technology spaces and (2) moving away from an underlying utilitarian or         deontological ethical foundation and towards a relational ethical foundation. Using that         theoretical analysis the paper will then present a model for taking this two-pronged         theoretical solution from theory into practice by providing specific recommendations for         operationalizing the proposed framework at the levels of AI technology         development.","",""
"2021","WHICH HUMAN FACES CAN AN AI GENERATE? LACK OF DIVERSITY IN THIS PERSON         DOES NOT EXIST","In this abstract we show the results of an interdisciplinary research         in which we audit fake human faces generated by the website This Person Does Not Exist         (TPDNE), and discuss how this system can help perpetuate normativities supported by a         dependency on a limited database. Our analysis is centered on the “default generic face”         that we created by overlapping random samples of fake human faces generated by TPDNE's         algorithms – a version of Generative Adversarial Network, the StyleGAN2. To carry these         experiments, we built a database with 4100 fake human faces taken from TPDNE via web         scraping; we analysed them through a Python language script; and discussed behaviours         identified in results. Our analyses are based on the use of images, called “cluster-images”,         created from this overlapping of N arbitrary fake human faces by the TPDNE's algorithm. Our         experiments showed that, independently of the group of fake human faces sampled, the same         generic white face always appeared as a result. These results intrigue particularly because         the lack of diversity of TPDNE's generated faces is not a mere problem to be fixed in this         system in this digital infrastructure, but a dynamic of reinforcing standards that         historically regulate bodies, territories and practices.","",""
"2021","DISEMBEDDEDNESS IN MACHINE LEARNING DATA WORK","Firms and research organizations require humans to annotate raw data         to make it compatible with machine learning algorithms. These tasks are often outsourced to         individuals worldwide through labor platforms or infrastructures that serve as marketplaces         where labour is exchanged as a commodity. The firms that operate them consider workers as         “independent contractors” without the social and economic benefits of traditional employment         relations. This presentation explores the personal networks of Latin American data workers         who train and verify data for machine learning algorithms from their homes. A series of         in-depth interviews and an analysis of a self-completion questionnaire and web traffic data         suggests that these workers are embedded of networks of trusts build on online and offline         interactions. These findings show a continuation of exploitative supply chains in the         current artificial intelligence market, where wealthy companies and research institutions in         advanced economies profit from the economic and political situation of developing countries         to access disembedded labor. This paper concludes by arguing that, though outsourced online         labour, artificial intelligence developers not only extract value from their workers, but         also indirectly from their communities and personal networks.","",""
"2021","ALGORITHMIC IMAGINATIONS: RETHINKING “ALGORITHMIC” AS A HEURISTIC FOR         UNDERSTANDING COMPUTATIONALLY-STRUCTURED CULTURE","How we imagine our place within the structure of sociotechnical-human         relationships—specifically, in domains of life affected by data-analytics and the         probabilistic bets institutions and people in power make on the future of our credit         worthiness, political leanings, shopping habits etc.—is our “algorithmic imagination.” The         purpose of this panel is to explore the “algorithmic imagination” as it manifests in         particular scholarly, historical, socio-cultural, and technical contexts. The panelists         prioritize how social actors, situated in distinct settings, go about constructing an         “algorithmic imagination” in conversation/opposition with how computational systems have         “imagined” them; they will also reflect critically and self-reflexively on the implications         of an algorithmic imagination, so conceived. Collectively, the panelists demure from         monolithic understandings of the “algorithmic imagination” while also embracing algorithmic         intersectionality. The primary contention of this panel is that the ways in which algorithms         have been “thought,” or imagined, have made it difficult to conceive of practicable         strategies for transforming algorithmic cultures and, indeed, for delinking them from both         state and corporate control. The panel, thus, makes three primary contributions. First, we         situate, define, and distinguish the concept, “algorithmic imagination.” Second, the panel         provides analyses of key facets of the algorithmic imagination, in specific historical         settings and life-worlds defined by intersectionality. Lastly, it aims to contribute,         however provisionally, to a political theory that recognizes the deterministic power of         computational systems but rejects the notion that power is inherently democratic or         monolithically insurmountable.","",""
"2021","CONTEXTUALIZING AI ETHICS IN TIME AND SPACE","While the interest in AI ethics has overwhelmingly intensified over         the last decade, and while various initiatives seek its institutionalization, the literature         on algorithmic ethics tends to examine the subject through philosophical, legal, or         technocratic perspectives, largely neglecting the empirical, socio-cultural ones. Moreover,         this literature tends to focus on the United States, and to overlook other tech centers         around the world. This paper aims to fill these gaps by focusing on how Israeli data         scientists understand, interpret, and depict algorithmic ethics. Based on a pragmatist         social analysis, and on 60 semi-structured interviews with Israeli data scientists, we ask:         which ideologies, discourses and world views construct algorithmic ethics? And what cultural         processes affect their creation and implementation? Our findings highlight three         interrelated moral logics: A) ethics as a personal endeavor; B) ethics as hindering         progress; and C) ethics as a commodity. We show that while data science is a nascent         profession, these moral logics originate from the techno-libertarian culture of its         parent-profession – engineering – and that they accordingly prevent the institutionalization         of a wider, agreed-upon moral regime. We further show that these data scientists’ avoidance         from institutionalized algorithmic ethics also stems from specific cultural and national         determinants. Thus, this paper offers to see algorithmic ethics in a contextualized,         culture-specific perspective, one that focuses on how data scientists practically see and         construct their ethics, while considering their professional, organizational, and national         contexts.","",""
"2021","HOW TO BE ALGORITHMICALLY GOVERNED LIKE THAT: DATA- AND ALGORITHMIC         AGENCY FROM USER PERSPECTIVE","If users are being $2 algorithms, and companies and regulators are         proposing ways for $2 algorithms, with this paper we would like to discuss and propose a         third type of governance — one where users have agency, control and governing power(s) over         algorithmic systems and their outputs. Our main research question is how do we enable users         to actively govern algorithms, instead of passively being governed by them? And what do the         users need in order to be algorithmically governed in such a way that will enable more         agency, autonomy and control when interacting with AI systems and their outputs. Instead of         getting insights in an abstract way, to answer this question, we opted for a guided and         supportive process where participants were able to reflect on the process, formulate and         elaborate their insights, thoughts, needs and requirements based on their lived experience,         i.e., after a real interaction with these algorithmic systems. We conducted a participatory         technographic research with 47 participants, through a multi-stage process consisting of a         survey, Subject Access Requests (Article 15 of the General Data Protection Regulation),         purposeful interaction with the transparency tools of seven chosen platforms and extensive         structured research diaries. A quali-quantitative analysis of the insights enabled us to         formulate the participants’ requirements of $2 and $2 in a way that will enable their         agency, control and autonomy. These requirements are translatable and implementable at a         user-interaction level, via technology design and through regulatory mechanisms","",""
"2021","Pasquale, Frank. New Laws of Robotics: Defending Human Expertise in the Age of AI. Cambridge, Massachusetts: The Belknap Press of Harvard University Press, 2020","","",""
"2021","Machine invention systems: a (r)evolution of the invention process?","AbstractCurrent developments in fields such as quantum physics, fine arts, robotics, cognitive sciences or defense and security indicate the emergence of creative systems capable of producing new and innovative solutions through combinations of machine learning algorithms. These systems, called machine invention systems, challenge the established invention paradigm in promising the automation of – at least parts of – the innovation process. This paper’s main contribution is twofold. Based on the identified state-of-the-art examples in the above mentioned fields, key components for machine invention systems and their relations are identified, creating a conceptual model as well as proposing a working definition for machine invention systems. The differences and delimitations to other concepts in the field of machine learning and artificial intelligence, such as machine discovery systems are discussed as well. Furthermore, the paper briefly addresses the social and societal implications and limitations that come with the adoption of the technology. Because of their revolutionizing potential, there are widespread implications to consider from ethical and moral implications to policymaking and societal changes, like changes in the job structure. The discussion part approaches some of these implications, as well as solutions to some of the proposed challenges. The paper concludes by discussing some of the systemic benefits that can be accessed through machine invention.","",""
"2021","Technoevidence: the """"Turing limit"""" 2020","","",""
"2021","Digitally fabricated aesthetic enhancements and enrichments","","",""
"2021","There is no “I” in “AI”","","",""
"2021","The Nooscope manifested: AI as instrument of knowledge extractivism","AbstractSome enlightenment regarding the project to mechanise reason. The assembly line of machine learning: data, algorithm, model. The training dataset: the social origins of machine intelligence. The history of AI as the automation of perception. The learning algorithm: compressing the world into a statistical model. All models are wrong, but some are useful. World to vector: the society of classification and prediction bots. Faults of a statistical instrument: the undetection of the new. Adversarial intelligence vs. statistical intelligence: labour in the age of AI.","",""
"2021","Forbidden knowledge in machine learning reflections on the limits of research and publication","AbstractCertain research strands can yield “forbidden knowledge”. This term refers to knowledge that is considered too sensitive, dangerous or taboo to be produced or shared. Discourses about such publication restrictions are already entrenched in scientific fields like IT security, synthetic biology or nuclear physics research. This paper makes the case for transferring this discourse to machine learning research. Some machine learning applications can very easily be misused and unfold harmful consequences, for instance, with regard to generative video or text synthesis, personality analysis, behavior manipulation, software vulnerability detection and the like. Up till now, the machine learning research community embraces the idea of open access. However, this is opposed to precautionary efforts to prevent the malicious use of machine learning applications. Information about or from such applications may, if improperly disclosed, cause harm to people, organizations or whole societies. Hence, the goal of this work is to outline deliberations on how to deal with questions concerning the dissemination of such information. It proposes a tentative ethical framework for the machine learning community on how to deal with forbidden knowledge and dual-use applications.","",""
"2021","The regulatory intersections between artificial intelligence, data protection and cyber security: challenges and opportunities for the EU legal framework","","",""
"2021","Do robots dream of escaping? Narrativity and ethics in Alex Garland’s Ex-Machina and Luke Scott’s Morgan","","",""
"2021","Artificial superintelligence and its limits: why AlphaZero cannot become a general agent","","",""
"2021","Computers do not think, they are oriented in thought","","",""
"2021","Tying the knot with a robot: legal and philosophical foundations for human–artificial intelligence matrimony","","",""
"2021","An emerging AI mainstream: deepening our comparisons of AI frameworks through rhetorical analysis","","",""
"2021","Artificial intelligence and its natural limits","","",""
"2021","Debate: what is personhood in the age of AI?","","",""
"2021","Out of the laboratory and into the classroom: the future of artificial intelligence in education","","",""
"2021","Failure of chatbot Tay was evil, ugliness and uselessness in its nature or do we judge it through cognitive shortcuts and biases?","","",""
"2021","From machine ethics to computational ethics","","",""
"2021","AI in the noosphere: an alignment of scientific and wisdom traditions","","",""
"2021","Hidalgo, C.A (2021). How Humans Judge Machines. MIT Press, Cambridge, Massachusetts, USA. ISBN: 9780262045520","","",""
"2021","Ethical and legal challenges of informed consent applying artificial intelligence in medical diagnostic consultations","","",""
"2021","Why machines cannot be moral","","",""
"2021","On the moral status of social robots: considering the consciousness criterion","","",""
"2021","Artificial intelligence, culture and education","","",""
"2021","Conservative AI and social inequality: conceptualizing alternatives to bias through social theory","","",""
"2021","Artificial moral and legal personhood","","",""
"2021","AI and Spinoza: a review of law’s conceptual treatment of Lethal Autonomous","AbstractIn this article I will argue that the philosophy of Benedict Spinoza (1632–1677) may assist us in coming to terms with some of the conceptual challenges that the phenomenon of Artificial Intelligence (AI) poses on law and legal thought. I will pursue this argument in three steps. First, I will suggest that Spinoza’s philosophy of the mind and knowledge may function as an analytical tool for making sense of the prevailing conception of AI within the legal discourse on Lethal Autonomous Weapons Systems (LAWS). Then, I will continue the argument with the aid of Spinoza’s political philosophy which partly complicates the picture as it seems to disqualify a normative process grounded directly upon the means stipulated for achieving a robust understanding of AI. Based on these two separate discussions I will conclude by outlining a composite critique – from the twofolded perspective of Spinoza’s ethical and political discussions – of the ongoing negotiations of a new Conventional Weapons Convention (CCW) protocol on LAWS.","",""
"2021","AI Ethics: how can information ethics provide a framework to avoid usual conceptual pitfalls? An Overview","","",""
"2021","Attention-based convolutional neural network for Bangla sentiment analysis","","",""
"2021","Designing ethical artifacts has resulted in creative design","AbstractEthical aspects in engineering design have become increasingly important in recent years. A typical example is the recent rise of artificial intelligence (AI) ethics. This paper applies user studies of a design support tool to empirically verify that our ethical framework improves the creativity of an engineer’s design activity. The design support tool provides an environment for the promotion of ethical design perspectives and description. The experiments focus on two functionalities: semi-automatic generation and scenario path recommendation. These functions are designed around the application of ethical design theory, which extends the hierarchical representation of artifacts. Doing this enables users to reconsider their themes at the highest level of the hierarchy and to apply a wider conceptual space of design solutions. For example, by reconsidering the positions of their research themes in the space of the representation field, users can semi-automatically edit them and identify focal areas. Using the scenario path recommendation, designers can update their research themes after considering the ethical impacts of those themes on stakeholders. Both functions are realized by exploiting a knowledge base of ethical and technological discourses. Finally, the ethical design theory is updated based on some unexpected results of our user studies with regards to the cyclic relationship among theory, tools (i.e., experimental equipment), and observed data. For example, temporal dimensional aspects were confirmed as important.","",""
"2021","On the data set’s ruins","AbstractComputer vision aims to produce an understanding of digital image’s content and the generation or transformation of images through software. Today, a significant amount of computer vision algorithms rely on techniques of machine learning which require large amounts of data assembled in collections, or named data sets. To build these data sets a large population of precarious workers label and classify photographs around the clock at high speed. For computers to learn how to see, a scale articulates macro and micro dimensions: the millions of images culled from the internet with the few milliseconds given to the workers to perform a task for which they are paid a few cents. This paper engages in details with the production of this scale and the labour it relies on: its elaboration. This elaboration does not only require hands and retinas, it also crucially zes mobilises the photographic apparatus. To understand the specific character of the scale created by computer vision scientists, the paper compares it with a previous enterprise of scaling, Malraux’s Le Musée Imaginaire, where photography was used as a device to undo the boundaries of the museum’s collection and open it to an unlimited access to the world’s visual production. Drawing on Douglas Crimp’s argument that the “musée imaginaire”, a hyperbole of the museum, relied simultaneously on the active role of the photographic apparatus for its existence and on its negation, the paper identifies a similar problem in computer vision’s understanding of photography. The double dismissal of the role played by the workers and the agency of the photographic apparatus in the elaboration of computer vision foreground the inherent fragility of the edifice of machine vision and a necessary rethinking of its scale.","",""
"2021","Machine learning’s limitations in avoiding automation of bias","","",""
"2021","AI from Concrete to Abstract","","",""
"2021","The carousel of ethical machinery","","",""
"2021","Getting into the engine room: a blueprint to investigate the shadowy steps of AI ethics","AbstractEnacting an AI system typically requires three iterative phases where AI engineers are in command: selection and preparation of the data, selection and configuration of algorithmic tools, and fine-tuning of the different parameters on the basis of intermediate results. Our main hypothesis is that these phases involve practices with ethical questions. This paper maps these ethical questions and proposes a way to address them in light of a neo-republican understanding of freedom, defined as absence of domination. We thereby identify different types of responsibility held by AI engineers and link them to concrete suggestions on how to improve professional practices. This paper contributes to the literature on AI and ethics by focusing on the work necessary to configure AI systems, thereby offering an input to better practices and an input for societal debates.","",""
"2021","On machine vision and photographic imagination","Abstract In this article we introduce the concept of implied optical perspective in deep learning computer vision systems. Taking the BBC's experimental television programme “Made by Machine: When AI met the Archive” (2018) as a case study, we trace a conceptual and material link between the system used to automatically “watch” the television archive and a specific type of photographic practice. From a computational aesthetics perspective, we show how deep learning machine vision relies on photography, its technical regimes and epistemic advantages, and we propose a novel way to identify the latent camera through which the BBC archive was seen by machine.","",""
"2021","AI-based healthcare: a new dawn or apartheid revisited?","AbstractThe Bubonic Plague outbreak that wormed its way through San Francisco’s Chinatown in 1900 tells a story of prejudice guiding health policy, resulting in enormous suffering for much of its Chinese population. This article seeks to discuss the potential for hidden “prejudice” should Artificial Intelligence (AI) gain a dominant foothold in healthcare systems. Using a toy model, this piece explores potential future outcomes, should AI continue to develop without bound. Where potential dangers may lurk will be discussed, so that the full benefits AI has to offer can be reaped whilst avoiding the pitfalls. The model is produced using the computer programming language MATLAB and offers visual representations of potential outcomes. Interwoven with these potential outcomes are numerous historical models for problems caused by prejudice and recent issues in AI systems, from police prediction and facial recognition software to recruitment tools. Therefore, this research’s novel angle, of using historical precedents to model and discuss potential futures, offers a unique contribution.","",""
"2021","Artificial intelligence in cyber physical systems","AbstractThis article conducts a literature review of current and future challenges in the use of artificial intelligence (AI) in cyber physical systems. The literature review is focused on identifying a conceptual framework for increasing resilience with AI through automation supporting both, a technical and human level. The methodology applied resembled a literature review and taxonomic analysis of complex internet of things (IoT) interconnected and coupled cyber physical systems. There is an increased attention on propositions on models, infrastructures and frameworks of IoT in both academic and technical papers. These reports and publications frequently represent a juxtaposition of other related systems and technologies (e.g. Industrial Internet of Things, Cyber Physical Systems, Industry 4.0 etc.). We review academic and industry papers published between 2010 and 2020. The results determine a new hierarchical cascading conceptual framework for analysing the evolution of AI decision-making in cyber physical systems. We argue that such evolution is inevitable and autonomous because of the increased integration of connected devices (IoT) in cyber physical systems. To support this argument, taxonomic methodology is adapted and applied for transparency and justifications of concepts selection decisions through building summary maps that are applied for designing the hierarchical cascading conceptual framework.","",""
"2021","The hard problem of AI rights","","",""
"2021","Memo Akten’s Learning to See: from machine vision to the machinic unconscious","","",""
"2021","Legal dilemmas of Estonian artificial intelligence strategy: in between of e-society and global race","","",""
"2021","Operations of power in autonomous weapon systems: ethical conditions and socio-political prospects","AbstractThe purpose of this article is to provide a multi-perspective examination of one of the most important contemporary security issues: weaponized, and especially lethal, artificial intelligence. This technology is increasingly associated with the approaching dramatic change in the nature of warfare. What becomes particularly important and evermore intensely contested is how it becomes embedded with and concurrently impacts two social structures: ethics and law. While there has not been a global regime banning this technology, regulatory attempts at establishing a ban have intensified along with acts of resistance and blocking coalitions. This article aims to reflect on the prospects and limitations, as well as the ethical and legal intensity, of the emerging regulatory framework. To allow for such an investigation, a power-analytical approach to studying international security regimes is utilized.","",""
"2021","Collaborating AI and human experts in the maintenance domain","AbstractMaintenance decision errors can result in very costly problems. The 4th industrial revolution has given new opportunities for the development of and use of intelligent decision support systems. With these technological advancements, key concerns focus on gaining a better understanding of the linkage between the technicians’ knowledge and the intelligent decision support systems. The research reported in this study has two primary objectives. (1) To propose a theoretical model that links technicians’ knowledge and intelligent decision support systems, and (2) to present a use case how to apply the theoretical model. The foundation of the new model builds upon two main streams of study in the decision support literature: “distribution” of knowledge among different agents, and “collaboration” of knowledge for reaching a shared goal. This study resulted in the identification of two main gaps: firstly, there must be a greater focus upon the technicians’ knowledge; secondly, technicians need assistance to maintain their focus on the big picture. We used the cognitive fit theory, and the theory of distributed situation awareness to propose the new theoretical model called “distributed collaborative awareness model.” The model considers both explicit and implicit knowledge and accommodates the dynamic challenges involved in operational level maintenance. As an application of this model, we identify and recommend some technological developments required in augmented reality based maintenance decision support.","",""
"2021","Safety requirements vs. crashing ethically: what matters most for policies on autonomous vehicles","AbstractThe philosophical–ethical literature and the public debate on autonomous vehicles have been obsessed with ethical issues related to crashing. In this article, these discussions, including more empirical investigations, will be critically assessed. It is argued that a related and more pressing issue is questions concerning safety. For example, what should we require from autonomous vehicles when it comes to safety? What do we mean by ‘safety’? How do we measure it? In response to these questions, the article will present a foundation for a continued discussion on these issues and an argument for why discussions about safety should be prioritized over ethical concerns related to crashing.","",""
"2021","Artificial intelligence and institutional critique 2.0: unexpected ways of seeing with computer vision","","",""
"2021","Guidance systems: from autonomous directives to legal sensor-bilities","","",""
"2021","Healthcare and anomaly detection: using machine learning to predict anomalies in heart rate data","","",""
"2021","Review of Robot Rights by David J. Gunkel","","",""
"2021","Correction to: Excavating AI: the politics of images in machine learning training sets","","",""
"2021","Minding morality: ethical artificial societies for public policy modeling","AbstractPublic policies are designed to have an impact on particular societies, yet policy-oriented computer models and simulations often focus more on articulating the policies to be applied than on realistically rendering the cultural dynamics of the target society. This approach can lead to policy assessments that ignore crucial social contextual factors. For example, by leaving out distinctive moral and normative dimensions of cultural contexts in artificial societies, estimations of downstream policy effectiveness fail to account for dynamics that are fundamental in human life and central to many public policy challenges. In this paper, we supply evidence that incorporating morally salient dimensions of a culture is critically important for producing relevant and accurate evaluations of social policy when using multi-agent artificial intelligence models and simulations.","",""
"2021","Ground truth to fake geographies: machine vision and learning in visual practices","AbstractThis article investigates the concept of the ground truth as both an epistemic and technical figure of knowledge that is central to discussions of machine vision and media techniques of visuality. While ground truth refers to a set of remote sensing practices, it has a longer history in operational photography, such as aerial reconnaissance. Building on a discussion of this history, this article argues that ground truth has shifted from a reference to the physical, geographical ground to the surface of the images echoing earlier points raised by philosopher Jean-Luc Nancy that there is a ground of the image that is central to the task of analysis beyond representational practices. Furthermore, building on the practices of pattern recognition, composite imaging, and different interpretational techniques, we discuss contemporary practices of machine learning that mobilizes geographical earth observation datasets for experimental purposes, including tests such as “fake geography” as well as artistic practices, to show how ground truth is operationalized in such contexts of AI and visual arts.","",""
"2021","Authente-Kente: enabling authentication for artisanal economies with deep learning","","",""
"2021","AI ethics – a review of three recent publications","","",""
"2021","Seeing like an algorithm: operative images and emergent subjects","","",""
"2021","Could you hate a robot? And does it matter if you could?","AbstractThis article defends two claims. First, humans could be in relationships characterised by hate with some robots. Second, it matters that humans could hate robots, as this hate could wrong the robots (by leaving them at risk of mistreatment, exploitation, etc.). In defending this second claim, I will thus be accepting that morally considerable robots either currently exist, or will exist in the near future, and so it can matter (morally speaking) how we treat these robots. The arguments presented in this article make an important original contribution to the robo-philosophy literature, and particularly the literature on human–robot relationships (which typically only consider positive relationship types, e.g., love, friendship, etc.). Additionally, as explained at the end of the article, my discussions of robot hate could also have notable consequences for the emerging robot rights movement. Specifically, I argue that understanding human–robot relationships characterised by hate could actually help theorists argue for the rights of robots.","",""
"2021","Artificial intelligence and the value of transparency","","",""
"2021","Moral control and ownership in AI systems","","",""
"2021","Artificial intelligence in medicine and the disclosure of risks","AbstractThis paper focuses on the use of ‘black box’ AI in medicine and asks whether the physician needs to disclose to patients that even the best AI comes with the risks of cyberattacks, systematic bias, and a particular type of mismatch between AI’s implicit assumptions and an individual patient’s background situation.Pacecurrent clinical practice, I argue that, under certain circumstances, these risks do need to be disclosed. Otherwise, the physician either vitiates a patient’s informed consent or violates a more general obligation to warn him about potentially harmful consequences. To support this view, I argue, first, that the already widely accepted conditions in the evaluation of risks, i.e. the ‘nature’ and ‘likelihood’ of risks, speak in favour of disclosure and, second, that principled objections against the disclosure of these risks do not withstand scrutiny. Moreover, I also explain that these risks are exacerbated by pandemics like the COVID-19 crisis, which further emphasises their significance.","",""
"2021","Artificial intelligence and moral rights","AbstractWhether copyrights should exist in content generated by an artificial intelligence is a frequently discussed issue in the legal literature. Most of the discussion focuses on economic rights, whereas the relationship of artificial intelligence and moral rights remains relatively obscure. However, as moral rights traditionally aim at protecting the author’s “personal sphere”, the question whether the law should recognize such protection in the content produced by machines is pressing; this is especially true considering that artificial intelligence is continuously further developed and increasingly hard to comprehend for human beings. This paper first provides the background on the protection of moral rights under existing international, U.S. and European copyright laws. On this basis, the paper then proceeds to highlight special issues in connection with moral rights and content produced by artificial intelligence, in particular whether an artificial intelligence itself, the creator or users of an artificial intelligence should be considered as owners of moral rights. Finally, the present research discusses possible future solutions, in particular alternative forms of attribution rights or the introduction of related rights.","",""
"2021","Negative optics in vision machines","","",""
"2021","AI and law: ethical, legal, and socio-political implications","","",""
"2021","Making moral machines: why we need artificial moral agents","","",""
"2021","The political choreography of the Sophia robot: beyond robot rights and citizenship to political performances for the social robotics market","AbstractA humanoid robot named ‘Sophia’ has sparked controversy since it has been given citizenship and has done media performances all over the world. The company that made the robot, Hanson Robotics, has touted Sophia as the future of artificial intelligence (AI). Robot scientists and philosophers have been more pessimistic about its capabilities, describing Sophia as a sophisticated puppet or chatbot. Looking behind the rhetoric about Sophia’s citizenship and intelligence and going beyond recent discussions on the moral status or legal personhood of AI robots, we analyse the performativity of Sophia from the perspective of what we call ‘political choreography’: drawing on phenomenological approaches to performance-oriented philosophy of technology. This paper proposes to interpret and discuss the world tour of Sophia as a political choreography that boosts the rise of the social robot market, rather than a statement about robot citizenship or artificial intelligence. We argue that the media performances of the Sophia robot were choreographed to advance specific political interests. We illustrate our philosophical discussion with media material of the Sophia performance, which helps us to explore the mechanisms through which the media spectacle functions hand in hand with advancing the economic interests of technology industries and their governmental promotors. Using a phenomenological approach and attending to the movement of robots, we also criticize the notion of ‘embodied intelligence’ used in the context of social robotics and AI. In this way, we put the discussions about the robot’s rights or citizenship in the context of AI politics and economics.","",""
"2021","Encountering ethics through design: a workshop with nonhuman participants","AbstractWhat if we began to speculate that intelligent things have an ethical agenda? Could we then imagine ways to move past the moral divide ‘human vs. nonhuman’ in those contexts, where things act on our behalf? Would this help us better address matters of agency and responsibility in the design and use of intelligent systems? In this article, we argue that if we fail to address intelligent things as objects that deserve moral consideration by their relations within a broad social context, we will lack a grip on the distinct ethical rules governing our interaction with intelligent things, and how to design for it. We report insights from a workshop, where we take seriously the perspectives offered by intelligent things, by allowing unforeseen ethical situations to emerge in an improvisatory manner. By giving intelligent things an active role in interaction, our participants seemed to be activated by the artifacts, provoked to act and respond to things beyond the artifact itself—its direct functionality and user experience. The workshop helped to consider autonomous behavior not as a simplistic exercise of anthropomorphization, but within the more significant ecosystems of relations, practices and values of which intelligent things are a part.","",""
"2021","Children’s perceptions of social robots: a study of the robots Pepper, AV1 and Tessa at Norwegian research fairs","AbstractThis article studies perceptual differences of three social robots by elementary school children of ages 6–13 years (n = 107) at research fairs. The autonomous humanoid robot Pepper, an advanced social robot primarily designed as a personal assistant with movement and mobility, is compared to the teleoperated AV1 robot—designed to help elementary school children who cannot attend school to have a telepresence through the robot—and the flowerpot robot Tessa, used in the eWare system as an avatar for a home sensor system and dedicated to people with dementia living alone. These three robots were shown at the Norwegian national research fair, held in every major Norwegian city annually, where children were able to interact with the robots. Our analysis is based on quantitative survey data of the school children concerning the robots and qualitative discussions with them. By comparing three different types of social robots, we found that presence can be differently understood and conceptualized with different robots, especially relating to their function and “aliveness.” Additionally, we found a strong difference when relating robots to personal relations to one’s own grandparents versus older adults in general. We found children’s perceptions of robots to be relatively positive, curious and exploratory and that they were quite reflective on their own grandparent having a robot.","",""
"2021","Seeing threats, sensing flesh: human–machine ensembles at work","","",""
"2021","Moral zombies: why algorithms are not moral agents","AbstractIn philosophy of mind, zombies are imaginary creatures that are exact physical duplicates of conscious subjects for whom there is no first-personal experience. Zombies are meant to show that physicalism—the theory that the universe is made up entirely out of physical components—is false. In this paper, I apply the zombie thought experiment to the realm of morality to assess whether moral agency is something independent from sentience. Algorithms, I argue, are a kind of functional moral zombie, such that thinking about the latter can help us better understand and regulate the former. I contend that the main reason why algorithms can be neither autonomous nor accountable is that they lack sentience. Moral zombies and algorithms are incoherent as moral agents because they lack the necessary moral understanding to be morally responsible. To understand what it means to inflict pain on someone, it is necessary to have experiential knowledge of pain. At most, for an algorithm that feels nothing, ‘values’ will be items on a list, possibly prioritised in a certain way according to a number that represents weightiness. But entities that do not feel cannot value, and beings that do not value cannot act for moral reasons.","",""
"2021","Clinical AI: opacity, accountability, responsibility and liability","AbstractThe aim of this literature review was to compose a narrative review supported by a systematic approach to critically identify and examine concerns about accountability and the allocation of responsibility and legal liability as applied to the clinician and the technologist as applied the use of opaque AI-powered systems in clinical decision making. This review questions (a) if it is permissible for a clinician to use an opaque AI system (AIS) in clinical decision making and (b) if a patient was harmed as a result of using a clinician using an AIS’s suggestion, how would responsibility and legal liability be allocated? Literature was systematically searched, retrieved, and reviewed from nine databases, which also included items from three clinical professional regulators, as well as relevant grey literature from governmental and non-governmental organisations. This literature was subjected to inclusion/exclusion criteria; those items found relevant to this review underwent data extraction. This review found that there are multiple concerns about opacity, accountability, responsibility and liability when considering the stakeholders of technologists and clinicians in the creation and use of AIS in clinical decision making. Accountability is challenged when the AIS used is opaque, and allocation of responsibility is somewhat unclear. Legal analysis would help stakeholders to understand their obligations and prepare should an undesirable scenario of patient harm eventuate when AIS were used.","",""
"2021","Excavating AI: the politics of images in machine learning training sets","","",""
"2021","The Chinese approach to artificial intelligence: an analysis of policy, ethics, and regulation","AbstractIn July 2017, China’s State Council released the country’s strategy for developing artificial intelligence (AI), entitled ‘New Generation Artificial Intelligence Development Plan’ (新一代人工智能发展规划). This strategy outlined China’s aims to become the world leader in AI by 2030, to monetise AI into a trillion-yuan (ca. 150 billion dollars) industry, and to emerge as the driving force in defining ethical norms and standards for AI. Several reports have analysed specific aspects of China’s AI policies or have assessed the country’s technical capabilities. Instead, in this article, we focus on the socio-political background and policy debates that are shaping China’s AI strategy. In particular, we analyse the main strategic areas in which China is investing in AI and the concurrent ethical debates that are delimiting its use. By focusing on the policy backdrop, we seek to provide a more comprehensive and critical understanding of China’s AI policy by bringing together debates and analyses of a wide array of policy documents.","",""
"2021","Mindset matters: how mindset affects the ability of staff to anticipate and adapt to Artificial Intelligence (AI) future scenarios in organisational settings","","",""
"2021","The intelligent machine: a new metaphor through which to understand both corporations and AI","","",""
"2021","Intelligent inspection robotics: an open innovation project","","",""
"2021","Where is the human got to go? Artificial intelligence, machine learning, big data, digitalisation, and human–robot interaction in Industry 4.0 and 5.0","","",""
"2021","Artificial intelligence and responsibility","","",""
"2021","Review of Artificial Intelligence: Reflections in Philosophy, Theology and the Social Sciences by Benedikt P. Göcke and Astrid Rosenthal-von der Pütten","","",""
"2021","From filters to fillers: an active inference approach to body image distortion in the selfie era","","",""
"2021","Assessing contemporary legislative proposals for their compatibility with a natural law case for AI legal personhood","AbstractThe question of the moral status of AI and the extent to which that status ought to be recognised by societal institutions is one that has not yet received a satisfactory answer from lawyers. This paper seeks to provide a solution to the problem by defending a moral foundation for the recognition of legal personhood for AI, requiring the status to be granted should a threshold criterion be reached. The threshold proposed will be bare, noumenal agency in the Kantian sense. Agency has been identified by Alan Gewirth as the source of the rights claims of our own species and, at risk of contradiction, is a foundation that must be expanded to all agents or else we contradict the foundation of our own rights. This is something that ought to be recognised through the granting of legal personhood to all noumenal agents by any system that requires such personhood for the enforcement of rights, or else the rule restricting legal personhood cannot be seen as a valid legal norm. Having laid out the case, the paper will move on to defend this natural law conception against the narrower definition of legal personhood proposed by Bryson et al. with regards to AI. It will argue that bare agency is a sufficient, though not necessary, criterion for the ascription of legal personhood in any system that sees the status as necessary for the ascription of legal rights. The paper will conclude by analysing the proposals currently making their way through the legislatures of the UK and European Union. They will be assessed for their compatibility with the claim that a functioning legal system necessarily must recognise the legal personhood of all noumenal agents regardless of their origins, and whether they are future-proofed for the possibility that AI may meet this threshold.","",""
"2021","A taxonomy of human–machine collaboration: capturing automation and technical autonomy","","",""
"2021","The brain, the artificial neural network and the snake: why we see what we see","","",""
"2021","Introduction: ways of machine seeing","","",""
"2021","Perceptual bias and technical metapictures: critical machine vision as a humanities challenge","AbstractIn many critical investigations of machine vision, the focus lies almost exclusively on dataset bias and on fixing datasets by introducing more and more diverse sets of images. We propose that machine vision systems are inherently biased not only because they rely on biased datasets but also because their perceptual topology, their specific way of representing the visual world, gives rise to a new class of bias that we call perceptual bias. Concretely, we define perceptual topology as the set of those inductive biases in machine vision systems that determine its capability to represent the visual world. Perceptual bias, then, describes the difference between the assumed “ways of seeing” of a machine vision system, our reasonable expectations regarding its way of representing the visual world, and its actual perceptual topology. We show how perceptual bias affects the interpretability of machine vision systems in particular, by means of a close reading of a visualization technique called “feature visualization”. We conclude that dataset bias and perceptual bias both need to be considered in the critical analysis of machine vision systems and propose to understand critical machine vision as an important transdisciplinary challenge, situated at the interface of computer science and visual studies/Bildwissenschaft.","",""
"2021","Hybrid collective intelligence in a human–AI society","","",""
"2021","Towards a United Nations Internal Regulation for Artificial Intelligence"," This article sets out the rationale for a United Nations Regulation for Artificial Intelligence, which is needed to set out the modes of engagement of the organisation when using artificial intelligence technologies in the attainment of its mission. It argues that given the increasing use of artificial intelligence by the United Nations, including in some activities considered high risk by the European Commission, a regulation is urgent. It also contends that rules of engagement for artificial intelligence at the United Nations would support the development of ‘good artificial intelligence’, by giving developers clear pathways for authorisation that would build trust in these technologies. Finally, it argues that an internal regulation would build upon the work in artificial intelligence ethics and best practices already initiated in the organisation that could, like the Brussels Effect, set an important precedent for regulations in other countries. ","",""
"2021","The great Transformer: Examining the role of large language models in the political economy of AI"," In recent years, AI research has become more and more computationally demanding. In natural language processing (NLP), this tendency is reflected in the emergence of large language models (LLMs) like GPT-3. These powerful neural network-based models can be used for a range of NLP tasks and their language generation capacities have become so sophisticated that it can be very difficult to distinguish their outputs from human language. LLMs have raised concerns over their demonstrable biases, heavy environmental footprints, and future social ramifications. In December 2020, critical research on LLMs led Google to fire Timnit Gebru, co-lead of the company’s AI Ethics team, which sparked a major public controversy around LLMs and the growing corporate influence over AI research. This article explores the role LLMs play in the political economy of AI as infrastructural components for AI research and development. Retracing the technical developments that have led to the emergence of LLMs, we point out how they are intertwined with the business model of big tech companies and further shift power relations in their favour. This becomes visible through the Transformer, which is the underlying architecture of most LLMs today and started the race for ever bigger models when it was introduced by Google in 2017. Using the example of GPT-3, we shed light on recent corporate efforts to commodify LLMs through paid API access and exclusive licensing, raising questions around monopolization and dependency in a field that is increasingly divided by access to large-scale computing power. ","",""
"2021","The algorithm audit: Scoring the algorithms that score us"," In recent years, the ethical impact of AI has been increasingly scrutinized, with public scandals emerging over biased outcomes, lack of transparency, and the misuse of data. This has led to a growing mistrust of AI and increased calls for mandated ethical audits of algorithms. Current proposals for ethical assessment of algorithms are either too high level to be put into practice without further guidance, or they focus on very specific and technical notions of fairness or transparency that do not consider multiple stakeholders or the broader social context. In this article, we present an auditing framework to guide the ethical assessment of an algorithm. The audit instrument itself is comprised of three elements: a list of possible interests of stakeholders affected by the algorithm, an assessment of metrics that describe key ethically salient features of the algorithm, and a relevancy matrix that connects the assessed metrics to stakeholder interests. The proposed audit instrument yields an ethical evaluation of an algorithm that could be used by regulators and others interested in doing due diligence, while paying careful attention to the complex societal context within which the algorithm is deployed. ","",""
"2021","Algorithmic management in a work context"," The rapid development of machine-learning algorithms, which underpin contemporary artificial intelligence systems, has created new opportunities for the automation of work processes and management functions. While algorithmic management has been observed primarily within the platform-mediated gig economy, its transformative reach and consequences are also spreading to more standard work settings. Exploring algorithmic management as a sociotechnical concept, which reflects both technological infrastructures and organizational choices, we discuss how algorithmic management may influence existing power and social structures within organizations. We identify three key issues. First, we explore how algorithmic management shapes pre-existing power dynamics between workers and managers. Second, we discuss how algorithmic management demands new roles and competencies while also fostering oppositional attitudes toward algorithms. Third, we explain how algorithmic management impacts knowledge and information exchange within an organization, unpacking the concept of opacity on both a technical and organizational level. We conclude by situating this piece in broader discussions on the future of work, accountability, and identifying future research steps. ","",""
"2021","Machine learning in tutorials – Universal applicability, underinformed application, and other misconceptions"," Machine learning has become a key component of contemporary information systems. Unlike prior information systems explicitly programmed in formal languages, ML systems infer rules from data. This paper shows what this difference means for the critical analysis of socio-technical systems based on machine learning. To provide a foundation for future critical analysis of machine learning-based systems, we engage with how the term is framed and constructed in self-education resources. For this, we analyze machine learning tutorials, an important information source for self-learners and a key tool for the formation of the practices of the machine learning community. Our analysis identifies canonical examples of machine learning as well as important misconceptions and problematic framings. Our results show that machine learning is presented as being universally applicable and that the application of machine learning without special expertise is actively encouraged. Explanations of machine learning algorithms are missing or strongly limited. Meanwhile, the importance of data is vastly understated. This has implications for the manifestation of (new) social inequalities through machine learning-based systems. ","",""
"2021","Co-design and ethical artificial intelligence for health: An agenda for critical research and practice"," Applications of artificial intelligence/machine learning (AI/ML) in health care are dynamic and rapidly growing. One strategy for anticipating and addressing ethical challenges related to AI/ML for health care is patient and public involvement in the design of those technologies – often referred to as ‘co-design’. Co-design has a diverse intellectual and practical history, however, and has been conceptualized in many different ways. Moreover, AI/ML introduces challenges to co-design that are often underappreciated. Informed by perspectives from critical data studies and critical digital health studies, we review the research literature on involvement in health care, and involvement in design, and examine the extent to which co-design as commonly conceptualized is capable of addressing the range of normative issues raised by AI/ML for health care. We suggest that AI/ML technologies have amplified and modified existing challenges related to patient and public involvement, and created entirely new challenges. We outline three pitfalls associated with co-design for ethical AI/ML for health care and conclude with suggestions for addressing these practical and conceptual challenges. ","",""
"2021","Turning biases into hypotheses through method: A logic of scientific discovery for machine learning"," Machine learning (ML) systems have shown great potential for performing or supporting inferential reasoning through analyzing large data sets, thereby potentially facilitating more informed decision-making. However, a hindrance to such use of ML systems is that the predictive models created through ML are often complex, opaque, and poorly understood, even if the programs “learning” the models are simple, transparent, and well understood. ML models become difficult to trust, since lay-people, specialists, and even researchers have difficulties gauging the reasonableness, correctness, and reliability of the inferences performed. In this article, we argue that bridging this gap in the understanding of ML models and their reasonableness requires a focus on developing an improved methodology for their creation. This process has been likened to “alchemy” and criticized for involving a large degree of “black art,” owing to its reliance on poorly understood “best practices”. We soften this critique and argue that the seeming arbitrariness often is the result of a lack of explicit hypothesizing stemming from an empiricist and myopic focus on optimizing for predictive performance rather than from an occult or mystical process. We present some of the problems resulting from the excessive focus on optimizing generalization performance at the cost of hypothesizing about the selection of data and biases. We suggest embedding ML in a general logic of scientific discovery similar to the one presented by Charles Sanders Peirce, and present a recontextualized version of Peirce’s scientific hypothesis adjusted to ML. ","",""
"2021","“AI will fix this” – The Technical, Discursive, and Political Turn to AI in Governing Communication"," Technologies of “artificial intelligence” (AI) and machine learning (ML) are increasingly presented as solutions to key problems of our societies. Companies are developing, investing in, and deploying machine learning applications at scale in order to filter and organize content, mediate transactions, and make sense of massive sets of data. At the same time, social and legal expectations are ambiguous, and the technical challenges are substantial.  This is the introductory article to a special theme that addresses this turn to AI as a technical, discursive and political phenomena. The opening article contextualizes this theme by unfolding this multi-layered nature of the turn to AI. It argues that, whereas public and economic discourses position the widespread deployment of AI and automation in the governance of digital communication as a technical turn with a narrative of revolutionary breakthrough-moments and of technological progress, this development is at least similarly dependent on a parallel discursive and political turn to AI. The article positions the current turn to AI in the longstanding motif of the “technological fix” in the relationship between technology and society, and identifies a discursive turn to responsibility in platform governance as a key driver for AI and automation. In addition, a political turn to more demanding liability rules for platforms further incentivizes platforms to automatically screen their content for possibly infringing or violating content, and position AI as a solution to complex social problems. ","",""
"2021","Algorithms as organizational figuration: The sociotechnical arrangements of a fintech start-up"," Building on critical approaches that understand algorithms in terms of communication, culture and organization, this paper offers the supplementary conceptualization of algorithms as organizational figuration, defined as material and meaningful sociotechnical arrangements that develop in spatiotemporal processes and are shaped by multiple enactments of affordance–agency relations. We develop this conceptualization through a case study of a Danish fintech start-up that uses machine learning to create opportunities for sustainable pensions investments. By way of ethnographic and literary methodology, we provide an in-depth analysis of the dynamic trajectory in and through which the organization gives shape to and takes shape from its key algorithmic tool, mapping the shifting sociotechnical arrangements of the start-up, from its initial search for a viable business model through the development of the algorithm to the public launch of its product. On this basis, we argue that conceptualizing algorithms as organizational figuration enables us to detail not only what algorithms do but also what they are. ","",""
"2021","Algorithmic reparation"," Machine learning algorithms pervade contemporary society. They are integral to social institutions, inform processes of governance, and animate the mundane technologies of daily life. Consistently, the outcomes of machine learning reflect, reproduce, and amplify structural inequalities. The field of fair machine learning has emerged in response, developing mathematical techniques that increase fairness based on anti-classification, classification parity, and calibration standards. In practice, these computational correctives invariably fall short, operating from an algorithmic idealism that does not, and cannot, address systemic, Intersectional stratifications. Taking present fair machine learning methods as our point of departure, we suggest instead the notion and practice of algorithmic reparation. Rooted in theories of Intersectionality, reparative algorithms name, unmask, and undo allocative and representational harms as they materialize in sociotechnical form. We propose algorithmic reparation as a foundation for building, evaluating, adjusting, and when necessary, omitting and eradicating machine learning systems. ","",""
"2021","Assessing biases, relaxing moralism: On ground-truthing practices in machine learning design and application"," This theoretical paper considers the morality of machine learning algorithms and systems in the light of the biases that ground their correctness. It begins by presenting biases not as a priori negative entities but as contingent external referents—often gathered in benchmarked repositories called ground-truth datasets—that define what needs to be learned and allow for performance measures. I then argue that ground-truth datasets and their concomitant practices—that fundamentally involve establishing biases to enable learning procedures—can be described by their respective morality, here defined as the more or less accounted experience of hesitation when faced with what pragmatist philosopher William James called “genuine options”—that is, choices to be made in the heat of the moment that engage different possible futures. I then stress three constitutive dimensions of this pragmatist morality, as far as ground-truthing practices are concerned: (I) the definition of the problem to be solved (problematization), (II) the identification of the data to be collected and set up (databasing), and (III) the qualification of the targets to be learned (labeling). I finally suggest that this three-dimensional conceptual space can be used to map machine learning algorithmic projects in terms of the morality of their respective and constitutive ground-truthing practices. Such techno-moral graphs may, in turn, serve as equipment for greater governance of machine learning algorithms and systems. ","",""
"2021","Lifting the curtain: Strategic visibility of human labour in AI-as-a-Service"," Artificial Intelligence-as-a-Service (AIaaS) empowers individuals and organisations to access AI on-demand, in either tailored or ‘off-the-shelf’ forms. However, institutional separation between development, training and deployment can lead to critical opacities, such as obscuring the level of human effort necessary to produce and train AI services. Information about how, where, and for whom AI services have been produced are valuable secrets, which vendors strategically disclose to clients depending on commercial interests. This article provides a critical analysis of how AIaaS vendors manipulate the visibility of human labour in AI production based on whether the vendor relies on paid or unpaid labour to fill interstitial gaps. Where vendors are able to occlude human labour in the organisational ‘backstage,’ such as in data preparation, validation or impersonation, they do so regularly, further contributing to ongoing techno-utopian narratives of AI hype. Yet, when vendors must co-produce the AI service with the client, such as through localised AI training, they must ‘lift the curtain’, resulting in a paradoxical situation of needing to both perpetuate dominant AI hype narratives while emphasising AI’s mundane limitations. ","",""
"2021","Four investment areas for ethical AI: Transdisciplinary opportunities to close the publication-to-practice gap"," Big Data and Artificial Intelligence have a symbiotic relationship. Artificial Intelligence needs to be trained on Big Data to be accurate, and Big Data's value is largely realized through its use by Artificial Intelligence. As a result, Big Data and Artificial Intelligence practices are tightly intertwined in real life settings, as are their impacts on society. Unethical uses of Artificial Intelligence are therefore a Big Data problem, at least to some degree. Efforts to address this problem have been dominated by the documentation of Ethical Artificial Intelligence principles and the creation of technical tools that address specific aspects of those principles. However, there is mounting evidence that Ethical Artificial Intelligence principles and technical tools have little impact on the Artificial Intelligence that is created in practice, sometimes in very public ways. The goal of this commentary is to highlight four interconnected areas society can invest in to close this Ethical Artificial Intelligence publication-to-practice gap, maximizing the positive impact Artificial Intelligence and Big Data have on society. For Ethical Artificial Intelligence to become a reality, I argue that these areas need to be addressed holistically in a way that acknowledges their interdependencies. Progress will require iteration, compromise, and transdisciplinary collaboration, but the result of our investments will be the realization of Artificial Intelligence's and Big Data's tremendous potential for social good, in practice rather than in just our hopes and aspirations. ","",""
"2021","On the genealogy of machine learning datasets: A critical history of ImageNet"," In response to growing concerns of bias, discrimination, and unfairness perpetuated by algorithmic systems, the datasets used to train and evaluate machine learning models have come under increased scrutiny. Many of these examinations have focused on the contents of machine learning datasets, finding glaring underrepresentation of minoritized groups. In contrast, relatively little work has been done to examine the norms, values, and assumptions embedded in these datasets. In this work, we conceptualize machine learning datasets as a type of informational infrastructure, and motivate a genealogy as method in examining the histories and modes of constitution at play in their creation. We present a critical history of ImageNet as an exemplar, utilizing critical discourse analysis of major texts around ImageNet’s creation and impact. We find that assumptions around ImageNet and other large computer vision datasets more generally rely on three themes: the aggregation and accumulation of more data, the computational construction of meaning, and making certain types of data labor invisible. By tracing the discourses that surround this influential benchmark, we contribute to the ongoing development of the standards and norms around data development in machine learning and artificial intelligence research. ","",""
"2021","Experiments with Social Good: Feminist Critiques of Artificial Intelligence in Healthcare in India","In contemporary India, AI-enabled automated diagnostic models are beginning to control who gets access to what kind of medical care, with the most invasive systems being aimed at underserved communities. I critically question the dominant narrative of “AI for social good” that has been widely adopted by various stakeholders in the healthcare industry towards solving development challenges through the introduction of AI applications targeted towards the sick-poor. Using feminist theory, I argue that AI systems should not be seen as neutral products but complex sociotechnical processes embedded with gendered knowledge and labor. I analyze the layers of expropriation and experimentation that come into play when AI technologies become a method of using diverse bodies and medical records of the sick-poor as data to train proprietary AI algorithms at a low cost in the absence of effective state regulatory mechanisms. I posit that an overwhelming focus on “spectacular technologies” such as AI derails public efforts from solving the actual needs of populations targeted by the “AI for social good” narrative, and from the development of sustainable, responsible, situated healthcare solutions. Lastly, I offer social and policy recommendations that would enable us to envision inclusive feminist futures in which we understand and prioritize the needs of underserved populations over capitalist market logics in the development, deployment, and regulation of AI systems.","",""
"2021","Technology of the Surround","In addressing the issue of harmful bias in AI systems, this paper asks for a consideration of a generatively wild AI that exceeds the framework of predictive machine learning. The argument places supervised learning with its labeled training data as primarily a form of reproduction of a status quo. Based on this framework, the paper moves through an analysis of two AI modalities—supervised learning (e.g., machine vision) and unsupervised learning (e.g., game play)—to demonstrate the potential of AI as mechanism that creates patterns of association outside of a purely reproductive condition. This analysis is followed by an introduction to the concept of the technology of the surround, where the paper then turns toward theoretical positions that unbind categorical logics, moving toward other possible positionalities—the surround (Harney and Moten), alien intelligence (Parisi), and intra-actions of subject/object resolution (Barad). The paper frames two key concepts in relation to an AI in the wild: the colonial sublime and black techné. The paper concludes with a summation of what AI in the wild can contribute to the subversion of technologies of oppression toward a liberatory potential of AI.","",""
"2021","Automating Visuality: An Introduction","","",""
"2021","How Are You Feeling Today?","This piece of creative writing explores the possible impact of emotional artificial intelligence (EAI), a variety of technologies which have the common aim of inferring human emotion from outward expressions such as facial expressions, vocal patterns, text, and physiological data. It is difficult to determine exactly how EAI might affect people when thinking in the abstract. We therefore took a collaborative approach, combining the imagination and writing skills of a Sci-Fi writer, with the knowledge base of an academic studying EAI, in order to create a story which makes the potential consequences of EAI real in a tangible way. Some readers may have prior knowledge of EAI, and for you we hope this piece can offer new insights or different perspectives to consider. However, we suspect many readers will not have heard of EAI and be considering it here for the first time. For you, we hope to provide a story which you can truly immerse yourself in, which can act as a prompt for you to begin to consider your views on this technology. EAI may be at a relatively early stage of development, but we do not anticipate it being long before it is a widespread technology. Our prompt as writers was to create an imagined surveillance future. Our invitation to you as readers is to not only imagine it, but ponder it, live in it, question it, and, ultimately, shape it. ","",""
"2021","APAIC Report on the Holocode Crisis","The APAIC Report on the Holocode Crisis is a short story that imagines the future of machine-readable data encodings. In this story, I speculate about the next stage in the development of data encoding patterns: after barcodes and QR codes, the invention of “holocodes” will make it possible to store unprecedented amounts of data in a minuscule physical surface. As a collage of nested fictional materials (including ethnographic fieldnotes, interview transcripts, OCR scans, and intelligence reports) this story builds on the historical role of barcodes in supporting consumer logistics and the ongoing deployment of QR codes as anchors for the platform economy, concluding that the geopolitical future of optical governance is tied to unassuming technical standards such as those formalizing machine-readable representations of data.","",""
"2021","An Unbreakable Bond","""""An Unbreakable Bond"""" is the fictional story of Alice and her mother, and the everlasting bond between them made possible by the CNCT microchips. Their story is an exploration of a future in which surveillance plays a central role in human relationships by subverting our understanding of how we might communicate with those we love the most. The story of Alice and her mother will take us on a journey of a technological invasion into the closest relationships in our lives, into a world where individuals are both surveilling and being surveilled. It is also a story about an opaque ecosystem of public/private partnerships which begs the question: to whom do you turn to when something goes wrong?","",""
"2021","The Two Ring Test: The Unbearable Predictability of Artificial Intelligence","This is a coming-of-age of story of a child born and raised in a post-AI society.","",""
"2021","Assisted Living","I wanted to tell a story from the point of view of a data entry worker, someone not typically considered when the advantages and disadvantages of eldercare technologies are being assessed. The worker, remotely located and unknown to the people she watches, offers a perspective that points out some of the strangeness of watching from a distance a living person being injured, struggling, or just needing human care and attention. As we move into more widespread use of eldercare technologies, it’s worth considering how surveillant and robotic technologies to support independent elder living could change our relationships with and sensitivity to older populations.","",""
"2021","feminist data ethics of care for machine learning: The what, why, who and how","This article conceptualises and provides a roadmap for operationalising a feminist data ethics of care framework for the subfield of artificial intelligence (‘AI’) known as ‘machine learning’. After outlining the principles and praxis that comprise our framework, and then using it to evaluate the current state of mainstream AI ethics content, we argue that this literature tends to be overly abstract and founded on a heteropatriarchal world view. We contend that because most AI ethics content fails to equitably and explicitly assign responsibility to actors in the machine learning economy, there is a risk of implicitly reinforcing the status quo of gender power relations and other substantive inequalities, which in turn contributes to the significant gap between AI ethics principles and applied AI ethics more broadly. We argue that our feminist data ethics of care framework can help to fill this gap by paying particular attention to both the ‘who’ and the ‘how’, as well as by outlining a range of methods, approaches, and best practices that societal actors can use now to make interventions into the machine learning economy. Critically, a feminist data ethics of care is unlikely to be achieved in this context, and beyond, unless all stakeholders, including women, men, non-binary and transgender people, take responsibility for this much needed work.","",""
"2021","A framework for a data interest analysis of artificial intelligence","This article makes a case for a data interest analysis of artificial intelligence (AI) that explores how different interests in data are empowered or disempowered by design. The article uses the EU High-Level Expert Group on AI’s Ethics Guidelines for Trustworthy AI as an applied ethics approach to data interests with a human-centric ethical governance framework and accordingly suggests ethical questions that will help resolve conflicts between data interests in AI design","",""
"2021","Visions of unification and integration: Building brains and communities in the European Human Brain Project"," The Human Brain Project (HBP) was launched in October 2013 by the European Commission to build an information and communication technology infrastructure that would support large-scale brain modelling and simulation. Less than a year after its launch, more than 800 neuroscientists signed a letter that claimed the HBP ‘would fail to meet its goals’. Based on multi-sited ethnographic fieldwork conducted between February 2014 and January 2017 in France, Germany, the United Kingdom and the HBP headquarters in Switzerland, and over 40 interviews with scientists, engineers and project administrators, this article traces how competing visions over how brain models should be built became tied into debates over how scientific communities should be governed. Articulations of these different kinds of models and communities appealed to competing imaginaries of Europe itself – of Europe and European science as unified or pluralistic. This article argues that scientific models are sites of contestation over social and political futures. The tensions between visions of scientific unification and pluralism in the HBP mirrored the tensions between imaginaries of European political unification and pluralism. ","",""
"2021","A cultural approach to algorithmic bias in games"," As algorithms come to govern every aspect of our lives—from bank loans, to job applications, to traffic patterns, to our media consumption patterns—communication research has become increasingly concerned with how we govern algorithms. Building on the methodological frameworks established by critical information researchers like Safiya Noble, Tarleton Gillespie, and Nick Seaver, this essay argues that we do not need to reverse engineer the “black box” to understand its impacts because they can be found through qualitative methodologies instead. This essay rejects the “black box” as an epistemic premise upon which critical algorithmic literacies can be built by using discourse analysis to observe how the unknowable language of the algorithm is deployed discursively within gamer communities to establish and maintain patriarchal power. This essay shows how the “black box” is used by fan communities to advance a patriarchal understanding of what we term paradigms of “balance” and “realism” in game design. ","",""
"2021","I saw it on YouTube! How online videos shape perceptions of mind, morality, and fears about robots"," Robots have the potential to transform our existing categorical distinctions between “property” and “persons.” Previous research has demonstrated that humans naturally anthropomorphize them, and this tendency may be amplified when a robot is subject to abuse. Simultaneously, robots give rise to hopes and fears about the future and our place in it. However, most available evidence on these mechanisms is either anecdotal, or based on a small number of laboratory studies with limited ecological validity. The present work aims to bridge this gap through examining responses of participants ( N = 160) to four popular online videos of a leading robotics company (Boston Dynamics) and one more familiar vacuum cleaning robot (Roomba). Our results suggest that unexpectedly human-like abilities might provide more potent cues to mind perception than appearance, whereas appearance may attract more compassion and protection. Exposure to advanced robots significantly influences attitudes toward future artificial intelligence. We discuss the need for more research examining groundbreaking robotics outside the laboratory. ","",""
"2021","Are you ready for artificial Mozart and Skrillex? An experiment testing expectancy violation theory and AI music"," This study employs an experiment to test assessments of music composed by artificial intelligence. We examined the influence of (a) met or unmet expectations about artificial intelligence (AI)-composed music, (b) whether the music is better or worse than expected, and (c) the genre of the evaluation of music using a 2 (expectancy violation vs confirmation) × 2 (positive vs negative evaluation) × 2 (electronic dance music vs classical) design. The relationship between the belief about creative AI and the music evaluation was also analyzed. Participants ( n = 299) in an online survey listened to a randomly assigned music piece. The acceptance of creative AI was found to have a positive relationship with the assessment of AI-composed music. A two-way interaction between the expectancy violation and its valence, and a three-way interaction between the expectancy violation, its valence, and the genre of music were found. Implications for Expectancy Violation Theory and AI applications are discussed. ","",""
"2022","Gender Bias in           Big Data Analysis","abstract:This article combines humanistic """"data critique"""" with informed inspection of big data analysis. It measures gender bias when gender prediction software tools (Gender API, Namsor, and Genderize.io) are used in historical big data research. Gender bias is measured by contrasting personally identified computer science authors in the well-regarded DBLP dataset (1950–80) with exactly comparable results from the software tools. Implications for public understanding of gender bias in computing and the nature of the computing profession are outlined. Preliminary assessment of the Semantic Scholar dataset is presented. The conclusion combines humanistic approaches with selective use of big data methods.","",""
"2022","Artificial emotional intelligence beyond East and West","Artificial emotional intelligence refers to technologies that perform, recognise, or record affective states. More than merely a technological function, however, it is also a social process whereby cultural assumptions about what emotions are and how they are made are translated into composites of code, software, and mechanical platforms that operationalise certain models of emotion over others. This essay illustrates how aspects of cultural difference are both incorporated and elided in projects that equip machines with emotional intelligence. It does so by comparing the field of affective computing, which emerged in the North-Atlantic in the 1990s, with kansei (affective) engineering, which developed in Japan in the 1980s. It then leverages this comparison to argue for more diverse applications of the culture concept in both the development and critique of systems with artificial emotional intelligence. Issue 1 This article belongs to Concepts of the digital society, a special section of Internet Policy Review guest-edited by Christian Katzenbach and Thomas Christian Bächle.","",""
"2022","‘I’m still the master of the machine.’ Internet users’ awareness of algorithmic decision-making and their perception of its effect on their autonomy","ABSTRACT Algorithms are an integral part of our everyday lives and shape the selection and presentation of information and communication on the internet. At the same time, media users are faced with a lack of control and transparency when interacting with these systems because algorithms largely remain black boxes to end users. Relying on the notion that algorithms are socio-technical systems that comprise both technical and human components, this paper examines internet users’ awareness of algorithms in different areas of internet use and inquires into users’ perceptions of the impact of algorithms on their autonomy when interacting online. Empirically, we rely on qualitative interviews with 30 German internet users. Findings indicate that users in general are aware of algorithms operating in a wide range of applications and demonstrate a basic understanding of how these systems work. In line with the third-person effect, users perceive algorithms to have a stronger impact on others’ internet use than on their own. Further, users’ awareness of algorithms was found to be closely related to their perceived autonomy. When users feel in control of their interactions online, they are less aware of the impact of algorithms governing their interactions. Based on these results, we discuss the implications for transparency measures in algorithm regulation.","",""
"2022","Suing the algorithm: the mundanization of automated decision-making in public services through litigation","ABSTRACT Automated decision-making using algorithmic systems is increasingly being introduced in the public sector constituting one important pillar in the emergence of the digital welfare state. Promising more efficiency and fairer decisions in public services, repetitive tasks of processing applications and records are, for example, delegated to fairly simple rule-based algorithms. Taking this growing trend of delegating decisions to algorithmic systems in Sweden as a starting point, the article discusses two litigation cases about fully automated decision-making in the Swedish municipality of Trelleborg. Based on analyzing court rulings, exchanges with the Parliamentary Ombudsmen and in-depth interviews, the article shows how different, partly conflicting definitions of what automated decision-making in social services is and does, are negotiated between the municipality, a union for social workers and civil servants and journalists. Describing this negotiation process as mundanization, the article engages with the question how socio-technical imaginaries are established and stabilized.","",""
"2022","Algorithms and the narration of past selves","ABSTRACT This paper argues that the social power of algorithms comes to the fore through the narratives they generate about individuals. Proposing the notion of ‘algorithmic emplotment’, the article showcases the ways in which algorithms construct and tell narratives about us, participating in shaping people’s encounters with the world and their perceptions of it. The concept denotes the processes through which data, people, experiences, and complex temporalities are ordered, woven together, and presented as coherent, frictionless narratives in the present. Through an analysis of the smartphone feature called Apple Memories, the paper seeks to highlight the narratives algorithms tell, how they are constructed, and the potential impacts they may have on everyday life. The concept of algorithmic emplotment is used to scrutinise the ways in which people’s lives are rendered sequential, ordered, and ultimately meaningful and actionable by algorithmic processes.","",""
"2022","Contested Chinese Dreams of AI? Public discourse about Artificial intelligence on WeChat and People’s Daily Online","ABSTRACT Artificial intelligence (AI) has become a prominent public issue, particularly in China, where the government has announced plans to turn the country into a global AI power. This study analyses public discourse about AI in China through the conceptual lens of public spheres theory and counter-public spheres. It compares the official AI narrative on People’s Daily Online with public discussion about AI on the social medium WeChat, where we assumed that official views would be challenged. Using a combination of qualitative and computational methods, 140,000 AI-related articles published between 2015 and 2018 were studied. Findings reveal that AI-related discourse on WeChat is surprisingly similar to that on People’s Daily Online. That is, it is dominated by industry and political actors, such as government agencies and technology companies, and is mostly characterised by discussions about the economic potential of the technology, with strongly positive evaluations, and little critical debate.","",""
"2022","Algorithmic meta-capital: Bourdieusian analysis of social power through algorithms in media consumption","ABSTRACT Algorithms make highly consequential decisions and, thereby, exercise considerable power. In this study, I investigate how social power through algorithms is exercised in media consumption, particularly through curation algorithms. This conceptual paper then contributes to the understanding of social power through algorithms by suggesting the concept of algorithmic meta-capital. The concept derives from Bourdieu’s theory on meta-capital which has also been applied to legacy media. I then argue that this algorithmic meta-capital is an extension of the power traditionally held by the state and legacy media. The study also contributes to the understanding of meta-capital as it proposes how the meta-capital possessed by digital intermediaries functions. It does so by legitimating representations of the world and by creating a necessity for algorithmic visibility across different fields, thereby shaping habitus. This Bourdieusian approach enables researchers to take a balanced view on the power of algorithms on the structure/agency continuum.","",""
"2022","Algorithms as regulatory objects","ABSTRACT The recent dispersion of algorithms throughout a large part of social life makes them valid analytical objects for sociology in the twenty-first century. The ubiquity of algorithms has led to increased public attention, scrutiny and, consequently, regulation. That is the focus of this paper. I will show that such regulatory processes are not just aimed at preventing certain algorithmic activities, but that they are also co-producing algorithms. They determine, in specific settings, what an algorithm is and what it ought to do. I will illustrate this by comparing two different European regulations aimed at algorithmic practices: the regulation of trading algorithms in the German High Frequency Trading Act and in the Markets in Financial Instruments Directive (MiFID II), and the regulation of personal data processing in the General Data Protection Regulation (GDPR).","",""
"2022","The (in)credibility of algorithmic models to non-experts","ABSTRACT The rapid development and dissemination of data analysis techniques permits the creation of ever more intricate algorithmic models. Such models are simultaneously the vehicle and outcome of quantification practices and embody a worldview with associated norms and values. A set of specialist skills is required to create, use, or interpret algorithmic models. The mechanics of an algorithmic model may be hard to comprehend for experts and can be virtually incomprehensible to non-experts. This is of consequence because such black boxing can introduce power asymmetries and may obscure bias. This paper explores the practices through which experts and non-experts determine the credibility of algorithmic models. It concludes that (1) transparency to (non-)experts is at best problematic and at worst unattainable; (2) authoritative models may come to dictate what types of policies are considered feasible; (3) several of the advantages attributed to the use of quantifications do not hold in policy making contexts.","",""
"2022","Gain in quantity and novelty of work in intermittent task switching","Abstract The preponderance of evidence in the literature suggests that intermittent tasks reduce productivity and quality of work. In a task switching study, with intermittent tasks appearing once a minute or once every three minutes, we examined attention allocation and the effect of switching on the quantity and novelty of work. Self-reported estimates matched attention allocation obtained from eye fixations, indicating awareness and volitional control of attention. Arousal, quantity, and novelty of work were higher in the switching conditions in comparison to the single task condition. The findings point to the possibility of a quickening effect induced by switching that may be beneficial for work under specific task conditions.","",""
"2022","Tension Analysis in Survivor Interviews: A Computational Approach","This study aims to develop computational techniques to analyze and identify points of tensions in interviews with survivors of the 1994 Rwandan genocide. Oral history interviews are a dialogical source composed of questions and answers, producing a conversational narrative. Yet survivor testimony is often approached as though the questions did not exist. This article examines a digital tool that helps us visualize and better understand the underlying interview dynamic that is the heart of oral history and qualitative research more generally. Our tension detection tool identifies those moments in the interview when the interviewer and interviewee are trying to pull the conversation in different directions. This is part of the natural give-and-take of the interview. Hedging, deflection, hesitation, and boosting are all critical components of this interviewer-interviewee tension. By making the interview dynamic central to our analysis, we aim to better understand how the interview dynamic shapes what is being said and what is left unsaid. In this study, we address key components of interview tension and propose a natural language processing model that can efficiently incorporate these components in text-based oral history interviews to identify tension points. With experiments on an annotated transcript, we verify the efficacy of our model. This model provides a framework that can be utilized in future research on the dialogic of the interview.Cette étude vise à développer des techniques computationnelles pour analyser et identifier des points de tensions dans des interviews avec des survivants du génocide rwandais en 1994. Les interviews d’histoire orale sont une source dialogique composée de questions et de réponses, ce qui produit une narration conversationnelle.&amp;nbsp;Cependant, le&amp;nbsp;témoignage de survivant est souvent&amp;nbsp;traité&amp;nbsp;comme si les questions n’existaient pas. Cet article examine un outil numérique qui nous aide à visualiser et à mieux comprendre la dynamique d’interview sous-jacente qui est au cœur de l’histoire orale et,&amp;nbsp;plus généralement, au cœur&amp;nbsp;de la recherche qualitative. Notre outil détecteur de tensions identifie ces moments dans l’interview lorsque l’intervieweur et l’interviewé sont en train d’essayer de guider la conversation dans des directions différentes. Cela fait partie de l’interaction bidirectionnelle naturelle d’une&amp;nbsp;interview.&amp;nbsp;Le non-engagement, le détournement, l’hésitation et l’exagération sont tous des composants essentiels dans la tension intervieweur-interviewé.&amp;nbsp;En mettant la dynamique d’interview au centre de notre analyse, nous aspirons à mieux comprendre comment la dynamique d’interview structure ce qui est dit et ce qui n’est pas dit.&amp;nbsp;Dans cette étude, nous abordons des composants clés de la tension d’interview et proposons un modèle de traitement de langue naturelle qui peut incorporer de façon efficace ces composants dans des interviews d’histoire orale numérisées afin d’identifier des points de tensions. Avec des expériences sur un transcrit annoté, nous vérifions l’efficacité de notre modèle. Ce modèle fournit un cadre à utiliser dans de futures&amp;nbsp;recherches&amp;nbsp;sur la dialogique de l’interview.","",""
"2022","Reclaiming the human in machine cultures: Introduction"," The relationship between technology and culture has always been a contested issue in media and cultural studies. Ongoing advances in computing and Artificial Intelligence (AI), however, are posing new kinds of questions and challenges to the field. As many have argued, these technologies invite to rethink the relationship between technology and culture, positing the idea that not only humans, but also machines produce and construct ‘culture’. The goal of this themed issue is to consider notions such as ‘algorithmic culture’ and ‘machine culture’ from within the tradition of media and cultural studies, in order to move toward a conceptualization of culture in which machines are intertwined within human systems of meaning-making. In this introduction to the themed issue, we discuss why these emerging technologies and the human cultures forming around them are integral to the mission of media and cultural studies, and what the media and cultural studies tradition can bring into ongoing and future debates regarding the nexus of humans, machines, and culture. ","",""
"2022","Rethinking creativity: creative industries, AI and everyday creativity"," This commentary reflects on how creativity is dehumanised (and rehumanised) and how its labour aspects are hindered (and highlighted) in the three recent developments in our understanding of arts, culture and creativity: the creative industries; AI creativity; and creativity in everyday life. The creative industries discourse instrumentalises and dehumanises creativity by hiding labour perspectives and treating creativity as human capital and a generator of IP. Meanwhile, contemplating AI creativity helps us to look beyond the economic paradigm and consider key traits of human creativity and the creation process, some aspects of which are successfully emulated by AI. Yet, we also observe how AI dissociates creativity from human agency and how its cost-cutting effect can challenge human creators in many sectors. Finally, the idea of everyday creativity effectively rehumanises and democratises creativity; however, it not only lacks labour perspectives but also hinders them. ","",""
"2022","Algorithmic photography: a case study of the Huawei Moon Mode controversy"," Moon Mode, an algorithmic program pre-installed on Huawei’s flagship smartphone P30 Pro, intelligently detects and enhances images of the moon captured by the phone. A heated social media discussion was triggered after a Chinese tech critic interpreted Moon Mode as photoshopping/superimposing details onto the original shot. The controversy centered on the line between AI enhancement and superimposed alteration when black-boxed algorithms stand between the user/viewer and the world viewed. The controversy is analyzed, together with Huawei’s marketing materials. Drawing on MacKenzie and Munster’s idea of distributed invisuality, AI-enabled photography is examined as a multiplicative data-processing event that traverses hardware and software, eliding any singular, meta-observational position. The author argues that algorithmic photography can be understood as a dynamic event of algorithmic processuality, indicating a new form of human-nonhuman entanglement in meaning-making practices, which cannot be discussed under the rubric of indexical representation. ","",""
"2022","Two tales about the power of algorithms in online environments: on the need for transdisciplinary dialogue in the study of algorithms and digital capitalism"," This commentary discusses an important disconnect between different literatures dealing with the power of algorithms – a disconnect that has important implications for the narratives about the role of algorithms in today’s societies that the humanities and social science construct. Theoretical work has regularly depicted algorithmic systems in the hands of large tech companies as powerful devices for effectively influencing behaviors through persuasive targeted information offers in online environments. However, this account goes against existing empirical evidence on the effectiveness of algorithm-based targeting. The paper highlights the different stories about the power of algorithms that these literatures tell and discusses why addressing this gap matters. In a nutshell, while the idea of people being steered by powerful algorithmic systems is arguably an intriguing aspect of digital-era capitalism, it risks distracting from more mundane, but also more relevant aspects of algorithms operating in online environments and how they can sustain power relations. ","",""
"2022","Ethics for the majority world: AI and the question of violence at scale"," In this work, I argue that hegemonic AI is becoming a more powerful force capable of perpetrating global violence through three epistemic processes: datafication (extraction and dispossession), algorithmisation (mediation and governmentality) and automation (violence, inequality and displacement of responsibility). These articulated epistemic mechanisms lead to global classification orders and epistemic, economic, social, cultural and environmental inequality. Hegemonic AI can be thought of as a bio-necro-technopolitical machine that serves to maintain the capitalist, colonialist and patriarchal order of the world. To make this point, the proposed approach bridges the macro and micropolitical, building on Suely Rolnik’s call for understanding the effects of the macropolitical in the micropolitical, as well as what feminist black scholar Patricia Hill Collins made visible about oppressive systems operating at the structural, institutional and individual levels. A critical AI ethics is one that is concerned with the preservation of life and the coresponsibility of AI harms to the majority of the planet. ","",""
"2022","Facing AI: conceptualizing ‘fAIce communication’ as the modus operandi of facial recognition systems"," This article argues for the conceptualization of fAIce communication as the modus operandi of facial recognition. From apps that claim to determine a person’s trustworthiness, recruiting technology that analyses candidates’ job fitness, through to banks using iris scanning to replace debit cards, facial recognition is increasingly used to communicate information about a person’s identity and personality. Faces communicate and have increased value. Knowing more about how their communicative capacity is effectuated and materialized in contemporary machine culture is thus of heightened importance. The article asks how we might come to think of the communicative capacities of faces in applications of AI, and how their role in current biometric systems may contribute to reconfiguring our understanding of what communication is. In an age of algorithmic and automated systems that are not primarily driven by overt messages purposefully crafted by humans but by machines reassembling data traces into forms of meaningfulness, faces are no longer (if they ever were) meaningful only for humans. This article ultimately makes the case for conceptualizing the communicative potential of faces in machine culture in terms of what I term algorithmic face-work, or more colloquially, fAIce communication. ","",""
"2022","Artificial Intelligence as Phármakon","","",""
"2022","Socially robotic: making useless machines","AbstractAs robots increasingly become part of our everyday lives, questions arise with regards to how to approach them and how to understand them in social contexts. The Western history of human–robot relations revolves around competition and control, which restricts our ability to relate to machines in other ways. In this study, we take a relational approach to explore different manners of socializing with robots, especially those that exceed an instrumental approach. The nonhuman subjects of this study are built to explore non-purposeful behavior, in an attempt to break away from the assumptions of utility that underlie the hegemonic human–machine interactions. This breakaway is accompanied by ‘learning to be attuned’ on the side of the human subjects, which is facilitated by continuous relations at the level of everyday life. Our paper highlights this ground for the emergence of meanings and questions that could not be subsumed by frameworks of control and domination. The research-creation projectMachine Ménagerieserves as a case study for these ideas, demonstrating a relational approach in which the designer and the machines co-constitute each other through sustained interactions, becoming attuned to one another through the performance of research.Machine Ménagerieattempts to produce affective and playful—if not unruly—nonhuman entities that invite interaction yet have no intention of serving human social or physical needs. We diverge from other social robotics research by creating machines that do not attempt to mimic human social behaviours.","",""
"2022","The possibilities and limits of AI in Chinese judicial judgment","","",""
"2022","Autonomous reboot: Aristotle, autonomy and the ends of machine ethics","AbstractTonkens (Mind Mach, 19, 3, 421–438, 2009) has issued a seemingly impossible challenge, to articulate a comprehensive ethical framework within which artificial moral agents (AMAs) satisfy a Kantian inspired recipe—""""rational"""" and """"free""""—while also satisfying perceived prerogatives of machine ethicists to facilitate the creation of AMAs that are perfectly and not merely reliably ethical. Challenges for machine ethicists have also been presented by Anthony Beavers and Wendell Wallach. Beavers pushes for the reinvention of traditional ethics to avoid """"ethical nihilism"""" due to the reduction of morality to mechanical causation. Wallach pushes for redoubled efforts toward a comprehensive account of ethics to guide machine ethicists on the issue of artificial moral agency. Options, thus, present themselves: reinterpret traditional ethics in a way that affords a comprehensive account of moral agency inclusive of both artificial and natural agents, or give up on the possibility and “muddle through” regardless. This series of papers pursues the first option, meets Tonkens' """"challenge"""" and pursues Wallach's ends through Beavers’ proposed means, by """"landscaping"""" traditional moral theory in resolution of a comprehensive account of moral agency. This first paper sets out the challenge and establishes the tradition that Kant had inherited from Aristotle, briefly entertains an Aristotelian AMA, fields objections, and ends with unanswered questions. The next paper in this series responds to the challenge in Kantian terms, and argues that a Kantian AMA is not only a possibility for Machine ethics research, but a necessary one.","",""
"2022","Machine learning: can the automatic pilot transcend the toxic fog?","","",""
"2022","Jumping into the artistic deep end: building the catalogue raisonné","AbstractThe catalogue raisonné compiled by art scholars holds information about an artist’s work such as a painting’s image, medium, provenance, and title. The catalogue raisonné as a tangible asset suffers from the challenges of art authentication and impermanence. As the catalogue raisonné is born digital, the impermanence challenge abates, but the authentication challenge persists. With the popularity of artificial intelligence and its deep learning architectures of computer vision, we propose to address the authentication challenge by creating a new artefact for the digital catalogue raisonné: a digital classification model. This digital classification model will help art scholars with new artwork claims via a tool that authenticates a proposed artwork with an artist. We create this tool by training a machine learning model with 90 artists having at least 150 artworks and achieve an accuracy of 87.31%. We use the ResNet Convolutional Neural Network to improve accuracy and number of artist classes over state-of-the-art artist classification experiments using the WikiArt database. We address inconsistencies in the way scholars approach artist classification by providing a consistent method to recreate our dataset and providing a consistent method to calculate performance metrics based on imbalanced data.","",""
"2022","Political machines: a framework for studying politics in social machines","AbstractIn the age of ubiquitous computing and artificially intelligent applications, social machines serves as a powerful framework for understanding and interpreting interactions in socio-algorithmic ecosystems. Although researchers have largely used it to analyze the interactions of individuals and algorithms, limited attempts have been made to investigate the politics in social machines. In this study, I claim that social machines are per se political machines, and introduce a five-point framework for classifying influence processes in socio-algorithmic ecosystems. By drawing from scholars from political theory, I use a notion of influence that functions as a meta-concept for connecting and comparing different conceptions of politics. In this way, I can associate multiple political aspects of social machines from a cybernetic perspective. I show that the framework efficiently categorizes dimensions of influence that shape interactions between individuals and algorithms. These categories are symbolic influence, political conduct, algorithmic influence, design, and regulatory influence. Using case studies, I describe how they interact with each other on online social networks and in algorithmic decision-making systems and illustrate how the framework is able to guide scientists in further research.","",""
"2022","Can communication with social robots influence how children develop empathy? Best-evidence synthesis","AbstractSocial robots are gradually entering children’s lives in a period when children learn about social relationships and exercise prosocial behaviors with parents, peers, and teachers. Designed for long-term emotional engagement and to take the roles of friends, teachers, and babysitters, such robots have the potential to influence how children develop empathy. This article presents a review of the literature (2010–2020) in the fields of human–robot interaction (HRI), psychology, neuropsychology, and roboethics, discussing the potential impact of communication with social robots on children’s social and emotional development. The critical analysis of evidence behind these discussions shows that, although robots theoretically have high chances of influencing the development of empathy in children, depending on their design, intensity, and context of use, there is no certainty about the kind of effect they might have. Most of the analyzed studies, which showed the ability of robots to improve empathy levels in children, were not longitudinal, while the studies observing and arguing for the negative effect of robots on children’s empathy were either purely theoretical or dependent on the specific design of the robot and the situation. Therefore, there is a need for studies investigating the effects on children’s social and emotional development of long-term regular and consistent communication with robots of various designs and in different situations.","",""
"2022","Endowing Artificial Intelligence with legal subjectivity","AbstractThis paper reflects on the problem of endowing Artificial Intelligence (AI) with legal subjectivity, especially with regard to civil law. It is necessary to reject the myth that the criteria of legal subjectivity are sentience and reason. Arguing that AI may have potential legal subjectivity based on an analogy to animals or juristic persons suggests the existence of a single hierarchy or sequence of entities, organized according to their degree of similarity to human beings; also, that the place of an entity in this hierarchy determines the scope of subjectivity attributed to it. Rather, it is participation or presence in social life, whatever the role, that is the true criterion of subjectivity. In addition, it is clear that even if AI is not currently a significant participant in social life, it will be in the nearest future. Despite the potential dangers associated with endowing AI with some kind of subjectivity, such a course is inescapable, and should be considered sooner rather than later.","",""
"2022","Attitudes toward the use of humanoid robots in healthcare—a cross-sectional study","AbstractThe use of robotic technology in healthcare is increasing. The aim was to explore attitudes toward the use of humanoid robots in healthcare among patients, relatives, care professionals, school actors and other relevant actors in healthcare and to analyze the associations between participants’ background variables and attitudes. The data were collected through a cross-sectional survey (N = 264) in 2018 where participants met a humanoid robot. The survey was comprised of background variables and items from a modified Robot Attitude Scale. Multiple linear regression analysis and Spearman’s Rho correlation were used to analyze associations between variables. Most of the participants were positive toward the use of humanoid robots in healthcare and only a few were negative. Attitudes toward the use of humanoid robots were more positive among other relevant actors, such as service personnel and politicians in healthcare, participants with a higher educational level and older adults. More research is needed on the reasons underlying negative attitudes because these might affect the introduction of humanoid robots in healthcare. A careful evaluation of appropriate first target groups as well as which tasks are appropriate for humanoid robots to perform in healthcare are needed.","",""
"2022","A critique of robotics in health care","AbstractWhen the social relevance of robotic applications is addressed today, the use of assistive technology in care settings is almost always the first example. So-called care robots are presented as a solution to the nursing crisis, despite doubts about their technological readiness and the lack of concrete usage scenarios in everyday nursing practice. We inquire into this interconnection of social robotics and care. We show how both are made available for each other in three arenas: innovation policy, care organization, and robotic engineering. First, we analyze the discursive “logics” of care robotics within European innovation policy, second, we disclose how care robotics is encountering a historically grown conflict within health care organization, and third we show how care scenarios are being used in robotic engineering. From this diagnosis, we derive a threefold critique of robotics in healthcare, which calls attention to the politics, historicity, and social situatedness of care robotics in elderly care.","",""
"2022","Moving beyond the mirror: relational and performative meaning making in human–robot communication","AbstractCurrent research in human–robot interaction often focuses on rendering communication between humans and robots more ‘natural’ by designing machines that appear and behave humanlike. Communication, in this human-centric approach, is often understood as a process of successfully transmitting information in the form of predefined messages and gestures. This article introduces an alternative arts-led, movement-centric approach, which embraces the differences of machinelike robotic artefacts and, instead, investigates how meaning is dynamically enacted in the encounter of humans and machines. Our design approach revolves around a novel embodied mapping methodology, which serves to bridge between human–machine asymmetries and socioculturally situate abstract robotic artefacts. Building on concepts from performativity, material agency, enactive sense-making and kinaesthetic empathy, our Machine Movement Lab project opens up a performative-relational model of human–machine communication, where meaning is generated through relational dynamics in the interaction itself.","",""
"2022","Operationalising AI ethics: how are companies bridging the gap between practice and principles? An exploratory study","","",""
"2022","The future of artificial intelligence, posthumanism and the inflection of Pixley Isaka Seme’s African humanism","","",""
"2022","The political imaginary of National AI Strategies","","",""
"2022","Could a robot flirt? 4E cognition, reactive attitudes, and robot autonomy","","",""
"2022","AI as a boss? A national US survey of predispositions governing comfort with expanded AI roles in society","","",""
"2022","The making of AI society: AI futures frames in German political and media discourses","AbstractIn this article, we shed light on the emergence, diffusion, and use of socio-technological future visions. The artificial intelligence (AI) future vision of the German federal government is examined and juxtaposed with the respective news media coverage of the German media. By means of a content analysis of frames, it is demonstrated how the German government strategically uses its AI future vision to uphold the status quo. The German media largely adapt the government´s frames and do not integrate alternative future narratives into the public debate. These findings are substantiated in the framing of AI futures in policy documents of the German government and articles of four different German newspapers. It is shown how the German past is mirrored in the German AI future envisioned by the government, safeguarding the present power constellation that is marked by a close unity of politics and industry. The German media partly expose the government´s frames and call for future visions that include fundamentally different political designs less influenced by the power structures of the past and present.","",""
"2022","Human autonomy, technological automation (and reverse)","AbstractWe continuously talk about autonomous technologies. But how can words qualifying technologies be the very same words chosen by Kant to define what is essentially human, i.e. being autonomous? The article focuses on a possible answer by reflecting upon both etymological and philosophical issues, as well as upon the case of autonomous vehicles. Most interestingly, on the one hand, we have the notion of (human) “autonomy”, meaning that there is a “law” that is “self-given”, and, on the other hand, we have the notion of (technological) “automation”, meaning that there is something “offhand” that is “self-given”. Yet, we are experiencing a kind of twofold shift: on the one hand, the shift from defining technologies in terms of automation to defining technologies in terms of autonomy and, on the other hand, the shift from defining humans in terms of autonomy to defining humans in terms of automation. From a philosophical perspective, the shift may mean that we are trying to escape precisely from what autonomy founds, i.e. individual responsibility of humans that, in the Western culture, have been defined for millennia as rational and moral decision-makers, even when their decisions have been the toughest. More precisely, the shift may mean that we are using technologies, and in particular emerging algorithmic technologies, as scapegoats that bear responsibility for us by making decisions for us. Moreover, if we consider the kind of emerging algorithmic technologies that increasingly surround us, starting from autonomous vehicles, then we may argue that we also seem to create a kind of technological divine that, by being always with us through its immanent omnipresence, omniscience, omnipotence and inscrutability, can always be our technological scapegoat freeing us from the most unbearable burden of individual responsibility resulting from individual autonomy.","",""
"2022","A computational approach for creativity assessment of culinary products: the case of elBulli","","",""
"2022","“I Am Not Your Robot:” the metaphysical challenge of humanity’s AIS ownership","","",""
"2022","Attribution of autonomy and its role in robotic language acquisition","AbstractThe false attribution of autonomy and related concepts to artificial agents that lack the attributed levels of the respective characteristic is problematic in many ways. In this article, we contrast this view with a positive viewpoint that emphasizes the potential role of such false attributions in the context of robotic language acquisition. By adding emotional displays and congruent body behaviors to a child-like humanoid robot’s behavioral repertoire, we were able to bring naïve human tutors to engage in so called intent interpretations. In developmental psychology, intent interpretations can be hypothesized to play a central role in the acquisition of emotion, volition, and similar autonomy-related words. The aforementioned experiments originally targeted the acquisition of linguistic negation. However, participants produced other affect- and motivation-related words with high frequencies too and, as a consequence, these entered the robot’s active vocabulary. We will analyze participants’ non-negative emotional and volitional speech and contrast it with participants’ speech in a non-affective baseline scenario. Implications of these findings for robotic language acquisition in particular and artificial intelligence and robotics more generally will also be discussed.","",""
"2022","The wizard and I: How transparent teleoperation and self-description (do not) affect children’s robot perceptions and child-robot relationship formation","AbstractIt has been well documented that children perceive robots as social, mental, and moral others. Studies on child-robot interaction may encourage this perception of robots, first, by using a Wizard of Oz (i.e., teleoperation) set-up and, second, by having robots engage in self-description. However, much remains unknown about the effects of transparent teleoperation and self-description on children’s perception of, and relationship formation with a robot. To address this research gap initially, we conducted an experimental study with a 2 × 2 (teleoperation: overt/covert; self-description: yes/no) between-subject design in which 168 children aged 7–10 interacted with a Nao robot once. Transparency about the teleoperation procedure decreased children’s perceptions of the robot’s autonomy and anthropomorphism. Self-description reduced the degree to which children perceived the robot as being similar to themselves. Transparent teleoperation and self-description affected neither children’s perceptions of the robot’s animacy and social presence nor their closeness to and trust in the robot.","",""
"2022","Empathic responses and moral status for social robots: an argument in favor of robot patienthood based on K. E. Løgstrup","","",""
"2022","Seeming autonomy, technology and the uncanny valley","","",""
"2022","Should my robot know what's best for me? Human–robot interaction between user experience and ethical design","AbstractTo integrate social robots in real-life contexts, it is crucial that they are accepted by the users. Acceptance is not only related to the functionality of the robot but also strongly depends on how the user experiences the interaction. Established design principles from usability and user experience research can be applied to the realm of human–robot interaction, to design robot behavior for the comfort and well-being of the user. Focusing the design on these aspects alone, however, comes with certain ethical challenges, especially regarding the user’s privacy and autonomy. Based on an example scenario of human–robot interaction in elder care, this paper discusses how established design principles can be used in social robotic design. It then juxtaposes these with ethical considerations such as privacy and user autonomy. Combining user experience and ethical perspectives, we propose adjustments to the original design principles and canvass our own design recommendations for a positive and ethically acceptable social human–robot interaction design. In doing so, we show that positive user experience and ethical design may be sometimes at odds, but can be reconciled in many cases, if designers are willing to adjust and amend time-tested design principles.","",""
"2022","A critical analysis of the representations of older adults in the field of human–robot interaction","AbstractThis paper argues that there is a need to critically assess bias in the representations of older adults in the field of Human–Robot Interaction. This need stems from the recognition that technology development is a socially constructed process that has the potential to reinforce problematic understandings of older adults. Based on a qualitative content analysis of 96 academic publications, this paper indicates that older adults are represented as; frail by default, independent by effort; silent and technologically illiterate; burdensome; and problematic for society. Within these documents, few counternarratives are present that do not take such essentialist representations. In these texts, the goal of social robots in elder care is to “enable” older adults to “better” themselves. The older body is seen as “fixable” with social robots, reinforcing an ageist and neoliberal narrative: older adults are reduced to potential care receivers in ways that shift care responsibilities away from the welfare state onto the individual.","",""
"2022","Social implications of autonomous vehicles: a focus on time","","",""
"2022","Speeding up to keep up: exploring the use of AI in the research process","AbstractThere is a long history of the science of intelligent machines and its potential to provide scientific insights have been debated since the dawn of AI. In particular, there is renewed interest in the role of AI in research and research policy as an enabler of new methods, processes, management and evaluation which is still relatively under-explored. This empirical paper explores interviews with leading scholars on the potential impact of AI on research practice and culture through deductive, thematic analysis to show the issues affecting academics and universities today. Our interviewees identify positive and negative consequences for research and researchers with respect to collective and individual use. AI is perceived as helpful with respect to information gathering and other narrow tasks, and in support of impact and interdisciplinarity. However, using AI as a way of ‘speeding up—to keep up’ with bureaucratic and metricised processes, may proliferate negative aspects of academic culture in that the expansion of AI in research should assist and not replace human creativity. Research into the future role of AI in the research process needs to go further to address these challenges, and ask fundamental questions about how AI might assist in providing new tools able to question the values and principles driving institutions and research processes. We argue that to do this an explicit movement of meta-research on the role of AI in research should consider the effects for research and researcher creativity. Anticipatory approaches and engagement of diverse and critical voices at policy level and across disciplines should also be considered.","",""
"2022","Algorithmic and human decision making: for a double standard of transparency","","",""
"2022","The AI doctor will see you now: assessing the framing of AI in news coverage","AbstractOne of the sectors for which Artificial Intelligence applications have been considered as exceptionally promising is the healthcare sector. As a public-facing sector, the introduction of AI applications has been subject to extended news coverage. This article conducts a quantitative and qualitative data analysis of English news media articles covering AI systems that allow the automation of tasks that so far needed to be done by a medical expert such as a doctor or a nurse thereby redistributing their agency. We investigated in this article one particular framing of AI systems and their agency: the framing that positions AI systems as (1a) replacing and (1b) outperforming the human medical expert, and in which (2) AI systems are personified and/or addressed as a person. The analysis of our data set consisting of 365 articles written between the years 1980 and 2019 will show that there is a tendency to present AI systems as outperforming human expertise. These findings are important given the central role of news coverage in explaining AI and given the fact that the popular frame of ‘outperforming’ might place AI systems above critique and concern including the Hippocratic oath. Our data also showed that the addressing of an AI system as a person is a trend that has been advanced only recently and is a new development in the public discourse about AI.","",""
"2022","Enter the metrics: critical theory and organizational operationalization of AI ethics","AbstractAs artificial intelligence (AI) deployment is growing exponentially, questions have been raised whether the developed AI ethics discourse is apt to address the currently pressing questions in the field. Building on critical theory, this article aims to expand the scope of AI ethics by arguing that in addition to ethical principles and design, the organizational dimension (i.e. the background assumptions and values influencing design processes) plays a pivotal role in the operationalization of ethics in AI development and deployment contexts. Through the prism of critical theory, and the notions of underdetermination and technical code as developed by Feenberg in particular, the organizational dimension is related to two general challenges in operationalizing ethical principles in AI: (a) the challenge of ethical principles placing conflicting demands on an AI design that cannot be satisfied simultaneously, for which the term ‘inter-principle tension’ is coined, and (b) the challenge of translating an ethical principle to a technological form, constraint or demand, for which the term ‘intra-principle tension’ is coined. Rather than discussing principles, methods or metrics, the notion of technical code precipitates a discussion on the subsequent questions of value decisions, governance and procedural checks and balances. It is held that including and interrogating the organizational context in AI ethics approaches allows for a more in depth understanding of the current challenges concerning the formalization and implementation of ethical principles as well as of the ways in which these challenges could be met.","",""
"2022","What is the message of the robot medium? Considering media ecology and mobilities in critical robotics research","","",""
"2022","Experiences, knowledge of functions, and social acceptance of robots: an exploratory case study focusing on Japan","AbstractAlthough Japanese society has become aware of some types of robots, social acceptance of robots is still not widespread. This study conducted an online questionnaire survey to investigate the relationships between experiences with and knowledge of vacuum, pet-type, and communication robots and acceptance of these robots, including the intention to use and trust. The results suggested that experiences with, knowledge of functions, and acceptance of the robots differed depending on the type of robot, and the influence of these factors on acceptance of the robots also differed depending on the robot types.","",""
"2022","Autonomous technologies in human ecologies: enlanguaged cognition, practices and technology","","",""
"2022","Social robots and the risks to reciprocity","AbstractA growing body of research can be found in which roboticists are designing for reciprocity as a key construct for successful human–robot interaction (HRI). Given the centrality of reciprocity as a component for our moral lives (for moral development and maintaining the just society), this paper confronts the possibility of what things would look like if the benchmark to achieve perceived reciprocity were accomplished. Through an analysis of the value of reciprocity from the care ethics tradition the richness of reciprocity as an inherent value is revealed: on the micro-level, as mutual care for immediate care givers, and on the macro-level, as foundational for a just society. Taking this understanding of reciprocity into consideration, it becomes clear that HRI cannot achieve this bidirectional value of reciprocity; a robot must deceive users into believing it is capable of reciprocating to humans or is deserving of reciprocation from humans. Moreover, on the macro-level, designing social robots for reciprocity threatens the ability and willingness to reciprocate to human care workers across society. Because of these concerns, I suggest re-thinking the goals of reciprocity in social robotics. Designing for reciprocity in social robotics should be dedicated to the design of robots to enhance the ability to mutually care for those that provide us with care, as opposed to designing for reciprocity between human and robot.","",""
"2022","Artificial intelligence in hospitals: providing a status quo of ethical considerations in academia to guide future research","AbstractThe application of artificial intelligence (AI) in hospitals yields many advantages but also confronts healthcare with ethical questions and challenges. While various disciplines have conducted specific research on the ethical considerations of AI in hospitals, the literature still requires a holistic overview. By conducting a systematic discourse approach highlighted by expert interviews with healthcare specialists, we identified the status quo of interdisciplinary research in academia on ethical considerations and dimensions of AI in hospitals. We found 15 fundamental manuscripts by constructing a citation network for the ethical discourse, and we extracted actionable principles and their relationships. We provide an agenda to guide academia, framed under the principles of biomedical ethics. We provide an understanding of the current ethical discourse of AI in clinical environments, identify where further research is pressingly needed, and discuss additional research questions that should be addressed. We also guide practitioners to acknowledge AI-related benefits in hospitals and to understand the related ethical concerns.","",""
"2022","Attitude of college students towards ethical issues of artificial intelligence in an international university in Japan","","",""
"2022","Organisational responses to the ethical issues of artificial intelligence","AbstractThe ethics of artificial intelligence (AI) is a widely discussed topic. There are numerous initiatives that aim to develop the principles and guidance to ensure that the development, deployment and use of AI are ethically acceptable. What is generally unclear is how organisations that make use of AI understand and address these ethical issues in practice. While there is an abundance of conceptual work on AI ethics, empirical insights are rare and often anecdotal. This paper fills the gap in our current understanding of how organisations deal with AI ethics by presenting empirical findings collected using a set of ten case studies and providing an account of the cross-case analysis. The paper reviews the discussion of ethical issues of AI as well as mitigation strategies that have been proposed in the literature. Using this background, the cross-case analysis categorises the organisational responses that were observed in practice. The discussion shows that organisations are highly aware of the AI ethics debate and keen to engage with ethical issues proactively. However, they make use of only a relatively small subsection of the mitigation strategies proposed in the literature. These insights are of importance to organisations deploying or using AI, to the academic AI ethics debate, but maybe most valuable to policymakers involved in the current debate about suitable policy developments to address the ethical issues raised by AI.","",""
"2022","How do people judge the credibility of algorithmic sources?","","",""
"2022","Automated news recommendation in front of adversarial examples and the technical limits of transparency in algorithmic accountability","","",""
"2022","Categorization and challenges of utilitarianisms in the context of artificial intelligence","","",""
"2022","Robotification &amp; ethical cleansing","AbstractRobotics is currently not only a cutting-edge research area, but is potentially disruptive to all domains of our lives—for better and worse. While legislation is struggling to keep pace with the development of these new artifacts, our intellectual limitations and physical laws seem to present the only hard demarcation lines, when it comes to state-of-the-art R&amp;D. To better understand the possible implications, the paper at hand critically investigates underlying processes and structures of robotics in the context of Heidegger’s and Nishitani’s accounts of science and technology. Furthermore, the analysis draws on Bauman’s theory of modernity in an attempt to assess the potential risk of large-scale robot integration. The paper will highlight undergirding mechanisms and severe challenges imposed upon our socio-cultural lifeworlds by massive robotic integration. Admittedly, presenting a mainly melancholic account, it will, however, also explore the possibility of robotics forcing us to reassess our position and to solve problems, which we seem unable to tackle without facing existential crises.","",""
"2022","Professional ethics and social responsibility: military work and peacebuilding","AbstractThis paper investigates four questions related to ethical issues associated with the involvement of engineers and scientists in 'military work', including the influence of ethical values and beliefs, the role of gendered perspectives and moves beyond the purely technical. It fits strongly into a human (and planet)-centred systems perspective and extends my previous AI and Society papers on othering and narrative ethics, and ethics and social responsibility. It has two main contributions. The first involves an analysis of the literature through the application of different ethical theories and the application of gendered analysis to discussion of masculinities in engineering and the military. The second is a survey of scientists and engineers to investigate their opinions and experiences. The conclusions draw together the results of these two contributions to provide preliminary responses to the four questions and include a series of recommendations covering education and training, ethical approval of work not involving human participants or animals, the need for organisational support, approaches covering wider perspectives and the encouragement of individual ethical commitment.","",""
"2022","Introduction: special issue—critical robotics research","","",""
"2022","Australian public understandings of artificial intelligence","","",""
"2022","AI and social theory","AbstractIn this paper, we sketch a programme for AI-driven social theory. We begin by defining what we mean by artificial intelligence (AI) in this context. We then lay out our specification for how AI-based models can draw on the growing availability of digital data to help test the validity of different social theories based on their predictive power. In doing so, we use the work of Randall Collins and his state breakdown model to exemplify that, already today, AI-based models can help synthesise knowledge from a variety of sources, reason about the world, and apply what is known across a wide range of problems in a systematic way. However, we also find that AI-driven social theory remains subject to a range of practical, technical, and epistemological limitations. Most critically, existing AI-systems lack three essential capabilities needed to advance social theory in ways that are cumulative, holistic, open-ended, and purposeful. These are (1) semanticisation, i.e., the ability to develop and operationalize verbal concepts to represent machine-manipulable knowledge; (2) transferability, i.e., the ability to transfer what has been learned in one context to another; and (3) generativity, i.e., the ability to independently create and improve on concepts and models. We argue that if the gaps identified here are addressed by further research, there is no reason why, in the future, the most advanced programme in social theory should not be led by AI-driven cumulative advances.","",""
"2022","Microdecisions and autonomy in self-driving cars: virtual probabilities","AbstractTo operate in an unpredictable environment, a vehicle with advanced driving assistance systems, such as a robot or a drone, not only needs to register its surroundings but also to combine data from different sensors into a world model, for which it employs filter algorithms. Such world models, as this article argues with reference to the SLAM problem (simultaneous location and mapping) in robotics, consist of nothing other than probabilities about states and events arising in the environment. The model, thus, contains a virtuality of possible worlds that are the basis for adaptive behavior. The article shows that the current development of these technologies requires new concepts because their complex adaptive behaviors cannot be explained by referring them to mere algorithmic processes. Instead, it proposes the heuristic instrument of microdecisions to designate the temporality of decisions between alternatives that are created by probabilistic procedures of world modeling. Microdecisions are more than the implementation of deterministic processes—they decide between possibilities and, thus, always open up the potential of their otherness. By describing autonomous adaptive technologies with this heuristic, the question of sovereignty inevitably arises. It forces us to re-think what autonomy means when decisions can be automated.","",""
"2022","AI management beyond the hype: exploring the co-constitution of AI and organizational context","AbstractAI technologies hold great promise for addressing existing problems in organizational contexts, but the potential benefits must not obscure the potential perils associated with AI. In this article, we conceptually explore these promises and perils by examining AI use in organizational contexts. The exploration complements and extends extant literature on AI management by providing a typology describing four types of AI use, based on the idea of co-constitution of AI technologies and organizational context. Building on this typology, we propose three recommendations for informed use of AI in contemporary organizations. First, explicitly define the purpose of organizational AI use. Second, define the appropriate level of transparency and algorithmic management for organizational AI use. Third, be aware of AI’s context-dependent nature.","",""
"2022","Beyond explainability: justifiability and contestability of algorithmic decision systems","","",""
"2022","What overarching ethical principle should a superintelligent AI follow?","","",""
"2022","AI under great uncertainty: implications and decision strategies for public policy","AbstractDecisions where there is not enough information for a well-informed decision due to unidentified consequences, options, or undetermined demarcation of the decision problem are called decisions under great uncertainty. This paper argues that public policy decisions on how and if to implement decision-making processes based on machine learning and AI for public use are such decisions. Decisions on public policy on AI are uncertain due to three features specific to the current landscape of AI, namely (i) the vagueness of the definition of AI, (ii) uncertain outcomes of AI implementations and (iii) pacing problems. Given that many potential applications of AI in the public sector concern functions central to the public sphere, decisions on the implementation of such applications are particularly sensitive. Therefore, it is suggested that public policy-makers and decision-makers in the public sector can adopt strategies from the argumentative approach in decision theory to mitigate the established great uncertainty. In particular, the notions of framing and temporal strategies are considered. ","",""
"2022","Discrimination in the age of artificial intelligence","AbstractIn this paper, I examine whether the use of artificial intelligence (AI) and automated decision-making (ADM) aggravates issues of discrimination as has been argued by several authors. For this purpose, I first take up the lively philosophical debate on discrimination and present my own definition of the concept. Equipped with this account, I subsequently review some of the recent literature on the use AI/ADM and discrimination. I explain how my account of discrimination helps to understand that the general claim in view of the aggravation of discrimination is unwarranted. Finally, I argue that the use of AI/ADM can, in fact, increase issues of discrimination, but in a different way than most critics assume: it is due to its epistemic opacity that AI/ADM threatens to undermine our moral deliberation which is essential for reaching a common understanding of what should count as discrimination. As a consequence, it turns out that algorithms may actually help to detect hidden forms of discrimination.","",""
"2022","Is explainable artificial intelligence intrinsically valuable?","","",""
"2022","On machine learning and the replacement of human labour: anti-Cartesianism versus Babbage’s path","","",""
"2022","Autonomous Reboot: Kant, the categorical imperative, and contemporary challenges for machine ethicists","AbstractRyan Tonkens (2009) has issued a seemingly impossible challenge, to articulate a comprehensive ethical framework within which artificial moral agents (AMAs) satisfy a Kantian inspired recipe—""""rational"""" and """"free""""—while also satisfying perceived prerogatives of machine ethicists to facilitate the creation of AMAs that are perfectly and not merely reliably ethical. This series of papers meets this challenge by landscaping traditional moral theory in resolution of a comprehensive account of moral agency. The first paper established the challenge and set out autonomy in Aristotelian terms. The present paper interprets Kantian moral theory on the basis of the preceding introduction, argues contra Tonkens that an engineer does not violate the categorical imperative in creating Kantian AMAs, and proposes that a Kantian AMA is not only a possible goal for Machine ethics research, but a necessary one. ","",""
"2022","Dismantling the Chinese Room with linguistic tools: a framework for elucidating concept-application disputes","","",""
"2022","In search of the moral status of AI: why sentience is a strong argument","","",""
"2022","Moral difference between humans and robots: paternalism and human-relative reason","","",""
"2022","Drones, robots and perceived autonomy: implications for living human beings","","",""
"2022","Discourse analysis of academic debate of ethics for AGI","AbstractArtificial general intelligence is a greatly anticipated technology with non-trivial existential risks, defined as machine intelligence with competence as great/greater than humans. To date, social scientists have dedicated little effort to the ethics of AGI or AGI researchers. This paper employs inductive discourse analysis of the academic literature of two intellectual groups writing on the ethics of AGI—applied and/or ‘basic’ scientific disciplines henceforth referred to as technicians (e.g., computer science, electrical engineering, physics), and philosophy-adjacent disciplines henceforth referred to as PADs (e.g., philosophy, theology, anthropology). These groups agree that AGI ethics is fundamentally about mitigating existential risk. They highlight our moral obligation to future generations, demonstrate the ethical importance of better understanding consciousness, and endorse a hybrid of deontological/utilitarian normative ethics. Technicians favor technocratic AGI governance, embrace the project of ‘solving’ moral realism, and are more deontologically inclined than PADs. PADs support a democratic approach to AGI governance, are more skeptical of deontology, consider current AGI predictions as fundamentally imprecise, and are wary of using AGI for moral fact-finding.","",""
"2022","How many angels can dance on the head of a pin? Understanding ‘alien’ thought","AbstractInitially coined by Weizenbaum in 1976, ‘alien' thought refers to the radical difference with which ‘thinking machines’ approach the process of thinking. The contemporary paradox of over-determination and indeterminacy—caused largely by algorithmic decision-making in the civic realm—makes these differences both more entangled and more difficult to navigate. In this essay, I trace over-determination to Leibniz and Turing’s axiomatic procedures and to instrumental rationality, and I trace indeterminacy to the mid-twentieth century co-development of computers and neurosciences to advance the following proposition: understanding alien thought requires understanding incomputability, temporal swarming, and inscriptive-significational errance. Understanding these phenomena in turn requires understanding thinking by doing, distributed thinking, and ontological indeterminacy. All are present in machinic operations as well as in the twentieth century experimental artistic practices of artists such as Duchamp, Cage, and Xu. These practices rely on indeterminate procedures and function as diagrammatic machines. A diagrammatic machine is neither abstract nor particular; neither an idea that is determining in the supreme instance, nor an infrastructure that is determining in the last instance, but rather instantiates a real yet to come (Deleuze and Guattari in A thousand plateaus: capitalism and schizophrenia: Trans. Massumi B, University of Minnesota Press, Minneapolis, 1987). In this essay, indeterminate artistic practices are used as an entry into alien thought and its correlates—infinity and complexity—by way of aesthetic analogy.","",""
"2022","Aligning artificial intelligence with human values: reflections from a phenomenological perspective","AbstractArtificial Intelligence (AI) must be directed at humane ends. The development of AI has produced great uncertainties of ensuring AI alignment with human values (AI value alignment) through AI operations from design to use. For the purposes of addressing this problem, we adopt the phenomenological theories of material values and technological mediation to be that beginning step. In this paper, we first discuss the AI value alignment from the relevant AI studies. Second, we briefly present what are material values and technological mediation and reflect on the AI value alignment through the lenses of these theories. We conclude that a set of finite human values can be defined and adapted to the stable life tasks that AI systems will be called upon to accomplish. The AI value alignment can also be fostered between designers and users through technological mediation. Upon that foundation, we propose a set of common principles to understand the AI value alignment through phenomenological theories. This paper contributes the unique knowledge of phenomenological theories to the discourse on AI alignment with human values.","",""
"2022","Robots beyond Science Fiction: mutual learning in human–robot interaction on the way to participatory approaches","AbstractPutting laypeople in an active role as direct expert contributors in the design of service robots becomes more and more prominent in the research fields of human–robot interaction (HRI) and social robotics (SR). Currently, though, HRI is caught in a dilemma of how to create meaningful service robots for human social environments, combining expectations shaped by popular media with technology readiness. We recapitulate traditional stakeholder involvement, including two cases in which new intelligent robots were conceptualized and realized for close interaction with humans. Thereby, we show how the robot narrative (impacted by science fiction, the term robot itself, and assumptions on human-like intelligence) together with aspects of power balancing stakeholders, such as hardware constraints and missing perspectives beyond primary users, and the adaptivity of robots through machine learning that creates unpredictability, pose specific challenges for participatory design processes in HRI. We conclude with thoughts on a way forward for the HRI community in developing a culture of participation that considers humans when conceptualizing, building, and using robots.","",""
"2022","The ethics of algorithms: key problems and solutions","AbstractResearch on the ethics of algorithms has grown substantially over the past decade. Alongside the exponential development and application of machine learning algorithms, new ethical problems and solutions relating to their ubiquitous use in society have been proposed. This article builds on a review of the ethics of algorithms published in 2016 (Mittelstadt et al. Big Data Soc 3(2), 2016). The goals are to contribute to the debate on the identification and analysis of the ethical implications of algorithms, to provide an updated analysis of epistemic and normative concerns, and to offer actionable guidance for the governance of the design, development and deployment of algorithms.","",""
"2022","Word vector embeddings hold social ontological relations capable of reflecting meaningful fairness assessments","AbstractProgramming artificial intelligence (AI) to make fairness assessments of texts through top-down rules, bottom-up training, or hybrid approaches, has presented the challenge of defining cross-cultural fairness. In this paper a simple method is presented which uses vectors to discover if a verb is unfair (e.g., slur, insult) or fair (e.g., thank, appreciate). It uses already existing relational social ontologies inherent in Word Embeddings and thus requires no training. The plausibility of the approach rests on two premises. That individuals consider fair acts those that they would be willing to accept if done to themselves. Secondly, that such a construal is ontologically reflected in Word Embeddings, by virtue of their ability to reflect the dimensions of such a perception. These dimensions being: responsibility vs. irresponsibility, gain vs. loss, reward vs. sanction, joy vs. pain, all as a single vector (FairVec). The paper finds it possible to quantify and qualify a verb as fair or unfair by calculating the cosine similarity of the said verb’s embedding vector against FairVec—which represents the above dimensions. We apply this to Glove and Word2Vec embeddings. Testing on a list of verbs produces an F1 score of 95.7, which is improved to 97.0. Lastly, a demonstration of the method’s applicability to sentence measurement is carried out.","",""
"2022","Nowotny, Helga (2021). In AI we trust: power, illusion and control of predictive algorithms, Polity, Cambridge, UK, ISBN-13: 978-1509548811","","",""
"2022","Artificial intelligence and global power structure: understanding through Luhmann's systems theory","","",""
"2022","New Pythias of public administration: ambiguity and choice in AI systems as challenges for governance","","",""
"2022","Algorithmic augmentation of democracy: considering whether technology can enhance the concepts of democracy and the rule of law through four hypotheticals","","",""
"2022","Service robots for affective labor: a sociology of labor perspective","AbstractProfit-oriented service sectors such as tourism, hospitality, and entertainment are increasingly looking at how professional service robots can be integrated into the workplace to perform socio-cognitive tasks that were previously reserved for humans. This is a work in which social and labor sciences recognize the principle role of emotions. However, the models and narratives of emotions that drive research, design, and deployment of service robots in human–robot interaction differ considerably from how emotions are framed in the sociology of labor and feminist studies of service work. In this paper, we explore these tensions through the concepts of affective and emotional labor, and outline key insights these concepts offer for the design and evaluation of professional service robots. Taken together, an emphasis on interactionist approaches to emotions and on the demands of affective labor, leads us to argue that service employees are under-represented in existing studies in human–robot interaction. To address this, we outline how participatory design and value-sensitive design approaches can be applied as complimentary methodological frameworks that include service employees as vital stakeholders.","",""
"2022","Protecting victim and witness statement: examining the effectiveness of a chatbot that uses artificial intelligence and a cognitive interview","","",""
"2022","In principle obstacles for empathic AI: why we can’t replace human empathy in healthcare","","",""
"2022","Robot and ukiyo-e: implications to cultural varieties in human–robot relationships","AbstractThe social and cultural causes behind the widespread use and acceptance of robots in Japan are not yet completely understood. This study compares humans and robots in images gathered through Google searches in Japanese and in English. Numerous pictures obtained by the search in Japanese were found to have a human and a robot looking together at something else (“third item”), whereas many of the images acquired by search in English show a human and a robot facing each other. This is similar to the composition of mother and child in paintings: in ukiyo-e that was painted mainly in the Edo period of Japan, the mother and child are often depicted together viewing something other than themselves, whereas this is not the case in Western paintings of mother and child. It has also been pointed out that, in modern Western paintings, the world inside the picture is separated from the outside world, forming an independent microcosmos, whereas the inside and outside are continuous in Japanese paintings. These may indicate that, in Japanese society, robots are to a certain extent regarded as fellow human beings who can share the third item. In Western society, on the other hand, no code is embedded that can fix robots’ superiority or inferiority to humans, which would easily trigger an antagonistic view toward artificial intelligence (AI)/robots as threatening entities, as shown in most of Western literature and movies. We suggest that such cultural characteristics of Japanese society can contribute to enhance coexistence with AI/robots.","",""
"2022","Linguistic justice as a framework for designing, developing, and managing natural language processing tools"," As natural language processing tools powered by big data become increasingly ubiquitous, questions of how to design, develop, and manage these tools and their impacts on diverse populations are pressing. We propose utilizing the concept of linguistic justice—the realization of equitable access to social and political life regardless of language—to provide a framework for examining natural language processing tools that learn from and use human language data. To support linguistic justice, we argue that natural language processing tools (along with the datasets that are used to train and evaluate them) must be examined not only from the perspective of a privileged, majority language user, but also from the perspectives of minoritized language users. Considering such perspectives can help to surface areas in which the data used within natural language processing tools may be (often inadvertently) working against linguistic justice by failing to provide access to information, services, or opportunities in users’ language of choice, underperforming for certain linguistic groups, or advancing harmful stereotypes that can lead to negative life outcomes for members of marginalized groups. At the same time, this framework can help to illuminate ways that these shortcomings can be addressed and allow us to use inclusive language data and approaches to leverage natural language processing technologies that advance linguistic justice. ","",""
"2022","Co-designing algorithms for governance: Ensuring responsible and accountable algorithmic management of  refugee camp supplies"," There is increasing criticism on the use of big data and algorithms in public governance. Studies revealed that algorithms may reinforce existing biases and defy scrutiny by public officials using them and citizens subject to algorithmic decisions and services. In response, scholars have called for more algorithmic transparency and regulation. These are useful, but ex post solutions in which the development of algorithms remains a rather autonomous process. This paper argues that co-design of algorithms with relevant stakeholders from government and society is another means to achieve responsible and accountable algorithms that is largely overlooked in the literature. We present a case study of the development of an algorithmic tool to estimate the populations of refugee camps to manage the delivery of emergency supplies. This case study demonstrates how in different stages of development of the tool—data selection and pre-processing, training of the algorithm and post-processing and adoption—inclusion of knowledge from the field led to changes to the algorithm. Co-design supported responsibility of the algorithm in the selection of big data sources and in preventing reinforcement of biases. It contributed to accountability of the algorithm by making the estimations transparent and explicable to its users. They were able to use the tool for fitting purposes and used their discretion in the interpretation of the results. It is yet unclear whether this eventually led to better servicing of refugee camps. ","",""
"2022","Artificial intelligence, human intelligence and hybrid intelligence based on mutual augmentation"," There is little consensus on what artificial intelligence (AI) systems may or may not embrace. Although this may point to multiplicity of interpretations and backgrounds, a lack of conceptual clarity could thwart the development of common ground around the concept among researchers, practitioners and users of AI and pave the way for misinterpretation and abuse of the concept. This article argues that one of the effective ways to delineate the concept of AI is to compare and contrast it with human intelligence. In doing so, the article broaches the unique capabilities of humans and AI in relation to one another (human and machine tacit knowledge), as well as two types of AI systems: one that goes beyond human intelligence and one that is necessarily and inherently tied to it. It finally highlights how humans and AI can augment their capabilities and intelligence through synergistic human–AI interactions (i.e., human-augmented AI and augmented human intelligence), resulting in hybrid intelligence, and concludes with a future-looking research agenda. ","",""
"2022","Diversity in sociotechnical machine learning systems"," There has been a surge of recent interest in sociocultural diversity in machine learning research. Currently, however, there is a gap between discussions of measures and benefits of diversity in machine learning, on the one hand, and the broader research on the underlying concepts of diversity and the precise mechanisms of its functional benefits, on the other. This gap is problematic because diversity is not a monolithic concept. Rather, different concepts of diversity are based on distinct rationales that should inform how we measure diversity in a given context. Similarly, the lack of specificity about the precise mechanisms underpinning diversity’s potential benefits can result in uninformative generalities, invalid experimental designs, and illicit interpretations of findings. In this work, we draw on research in philosophy, psychology, and social and organizational sciences to make three contributions: First, we introduce a taxonomy of different diversity concepts from philosophy of science, and explicate the distinct epistemic and political rationales underlying these concepts. Second, we provide an overview of mechanisms by which diversity can benefit group performance. Third, we situate these taxonomies of concepts and mechanisms in the lifecycle of sociotechnical machine learning systems and make a case for their usefulness in fair and accountable machine learning. We do so by illustrating how they clarify the discourse around diversity in the context of machine learning systems, promote the formulation of more precise research questions about diversity’s impact, and provide conceptual tools to further advance research and practice. ","",""
"2022","Algorithmic empowerment: A comparative ethnography  of two open-source algorithmic  platforms – Decide Madrid and vTaiwan"," Scholars of critical algorithmic studies, including those from geography, anthropology, Science and Technology Studies and communication studies, have begun to consider how algorithmic devices and platforms facilitate democratic practices. In this article, I draw on a comparative ethnography of two alternative open-source algorithmic platforms – Decide Madrid and vTaiwan – to consider how they are dynamically constituted by differing algorithmic–human relationships. I compare how different algorithmic–human relationships empower citizens to influence political decision-making through proposing, commenting, and voting on the urban issues that should receive political resources in Taipei and Madrid. I argue that algorithmic empowerment is an emerging process in which algorithmic–human relationships orient away from limitations and towards conditions of plurality, actionality, and power decentralisation. This argument frames algorithmic empowerment as bringing about empowering conditions that allow (underrepresented) individuals to shape policy-making and consider plural perspectives for political change and action, not as an outcome-driven, binary assessment (i.e. yes/no). This article contributes a novel, situated, and comparative conceptualisation of algorithmic empowerment that moves beyond technological determinism and universalism. ","",""
"2022","Algorithmic failure as a humanities methodology: Machine learning's mispredictions identify rich cases for qualitative analysis"," This commentary tests a methodology proposed by Munk et al. (2022) for using failed predictions in machine learning as a method to identify ambiguous and rich cases for qualitative analysis. Using a dataset describing actions performed by fictional characters interacting with machine vision technologies in 500 artworks, movies, novels and videogames, I trained a simple machine learning algorithm (using the kNN algorithm in R) to predict whether or not an action was active or passive using only information about the fictional characters. Predictable actions were generally unemotional and unambiguous activities where machine vision technologies were treated as simple tools. Unpredictable actions, that is, actions that the algorithm could not correctly predict, were more ambivalent and emotionally loaded, with more complex power relationships between characters and technologies. The results thus support Munk et al.'s theory that failed predictions can be productively used to identify rich cases for qualitative analysis. This test goes beyond simply replicating Munk et al.'s results by demonstrating that the method can be applied to a broader humanities domain, and that it does not require complex neural networks but can also work with a simpler machine learning algorithm. Further research is needed to develop an understanding of what kinds of data the method is useful for and which kinds of machine learning are most generative. To support this, the R code required to produce the results is included so the test can be replicated. The code can also be reused or adapted to test the method on other datasets. ","",""
"2022","Justice, injustice, and artificial  intelligence: Lessons from political  theory and philosophy"," Some recent uses of artificial intelligence for (for example) facial recognition, evaluating resumes, and sorting photographs by subject matter have revealed troubling disparities in performance or impact based on the demographic traits (like race and gender) of subject populations. These disparities raise pressing questions about how using artificial intelligence can work to promote justice or entrench injustice. Political theorists and philosophers have developed nuanced vocabularies and theoretical frameworks for understanding and adjudicating disputes about what justice requires and what constitutes injustice. The interdisciplinary community committed to understanding and conscientiously using big data could benefit from this work. Thus, in the spirit of encouraging cross-disciplinary dialogue and collaboration, this piece examines contemporary scholarship in political theory and philosophy to illustrate some of the vocabularies and frameworks political theorists and philosophers have developed for thinking about justice and injustice. It then draws on these frameworks to illuminate how the use of artificial intelligence can implicate questions of justice, with a focus on institutional discrimination, structural injustice, and epistemic injustice. Ultimately, the piece argues that the use of artificial intelligence—far from representing a decision to take power out of human hands—represents a novel way of harnessing human power, making questions of justice central to its conscientious undertaking. ","",""
"2022","Corrigendum to Machine Anthropology: A View from International Relations","","",""
"2022","States of computing: On government organization and artificial intelligence in Canada"," With technologies like machine learning and data analytics being deployed as privileged means to improve how contemporary bureaucracies work, many governments around the world have turned to artificial intelligence as a tool of statecraft. In that context, our paper uses Canada as a critical case to investigate the relationship between ideals of good government and good technology. We do so through not one, but two Trudeaus—celebrity Prime Minister Justin Trudeau (2015—…) and his equally famous father, former Prime Minister Pierre Elliott Trudeau (1968–1979, 1980–1984). Both shared a similar interest in new ideas and practices of both intelligent government and artificial intelligence. Influenced by Marshall McLuhan and his media theory, Pierre Elliott Trudeau deployed new communication technologies to restore centralized control in an otherwise decentralized state. Partly successful, he left his son with an informationally inclined political legacy, which decades later animated Justin Trudeau's own turn toward Big Data and artificial intelligence. Compared with one another, these two visions for both government and artificial intelligence illustrate the broader tensions between cybernetic and neoliberal approaches to government, which inform how new technologies are conceived of, and adopted, as political ones. As this article argues, Canada offers a paradigmatic case for how artificial intelligence is as much shaped by theories of government as by investments and innovations in computing research, which together delimit the contours of intelligence by defining which technical systems, people, and organizations come to be recognized as its privileged bearers. ","",""
"2022","Artificial intelligence ethics by design. Evaluating public perception on the importance of ethical design principles of artificial intelligence"," Despite the immense societal importance of ethically designing artificial intelligence, little research on the public perceptions of ethical artificial intelligence principles exists. This becomes even more striking when considering that ethical artificial intelligence development has the aim to be human-centric and of benefit for the whole society. In this study, we investigate how ethical principles (explainability, fairness, security, accountability, accuracy, privacy, and machine autonomy) are weighted in comparison to each other. This is especially important, since simultaneously considering ethical principles is not only costly, but sometimes even impossible, as developers must make specific trade-off decisions. In this paper, we give first answers on the relative importance of ethical principles given a specific use case—the use of artificial intelligence in tax fraud detection. The results of a large conjoint survey ([Formula: see text]) suggest that, by and large, German respondents evaluate the ethical principles as equally important. However, subsequent cluster analysis shows that different preference models for ethically designed systems exist among the German population. These clusters substantially differ not only in the preferred ethical principles but also in the importance levels of the principles themselves. We further describe how these groups are constituted in terms of sociodemographics as well as opinions on artificial intelligence. Societal implications, as well as design challenges, are discussed. ","",""
"2022","Erratum to The Thick Machine: Anthropological AI between explanation and explication","","",""
"2022","The Thick Machine: Anthropological AI between explanation and explication"," According to Clifford Geertz, the purpose of anthropology is not to explain culture but to explicate it. That should cause us to rethink our relationship with machine learning. It is, we contend, perfectly possible that machine learning algorithms, which are unable to explain, and could even be unexplainable themselves, can still be of critical use in a process of explication. Thus, we report on an experiment with anthropological AI. From a dataset of 175K Facebook comments, we trained a neural network to predict the emoji reaction associated with a comment and asked a group of human players to compete against the machine. We show that a) the machine can reach the same (poor) accuracy as the players (51%), b) it fails in roughly the same ways as the players, and c) easily predictable emoji reactions tend to reflect unambiguous situations where interpretation is easy. We therefore repurpose the failures of the neural network to point us to deeper and more ambiguous situations where interpretation is hard and explication becomes both necessary and interesting. We use this experiment as a point of departure for discussing how experiences from anthropology, and in particular the tension between formalist ethnoscience and interpretive thick description, might contribute to debates about explainable AI. ","",""
"2022","Consumers are willing to pay a price for explainable, but not for green AI. Evidence from a choice-based conjoint analysis"," A major challenge with the increasing use of Artificial Intelligence (AI) applications is to manage the long-term societal impacts of this technology. Two central concerns that have emerged in this respect are that the optimized goals behind the data processing of AI applications usually remain opaque and the energy footprint of their data processing is growing quickly. This study thus explores how much people value the transparency and environmental sustainability of AI using the example of personal AI assistants. The results from a choice-based conjoint analysis with a sample of more than 1.000 respondents from Germany indicate that people hardly care about the energy efficiency of AI; and while they do value transparency through explainable AI, this added value of an application is offset by minor costs. The findings shed light on what kinds of AI people are likely to demand and have important implications for policy and regulation. ","",""
"2022","Governing algorithmic decisions: The role of decision importance and governance on perceived legitimacy of algorithmic decisions"," The algorithmic accountability literature to date has primarily focused on procedural tools to govern automated decision-making systems. That prescriptive literature elides a fundamentally empirical question: whether and under what circumstances, if any, is the use of algorithmic systems to make public policy decisions perceived as legitimate? The present study begins to answer this question. Using factorial vignette survey methodology, we explore the relative importance of the type of decision, the procedural governance, the input data used, and outcome errors on perceptions of the legitimacy of algorithmic public policy decisions as compared to similar human decisions. Among other findings, we find that the type of decision—low importance versus high importance—impacts the perceived legitimacy of automated decisions. We find that human governance of algorithmic systems (aka human-in-the-loop) increases perceptions of the legitimacy of algorithmic decision-making systems, even when those decisions are likely to result in significant errors. Notably, we also find the penalty to perceived legitimacy is greater when human decision-makers make mistakes than when algorithmic systems make the same errors. The positive impact on perceived legitimacy from governance—such as human-in-the-loop—is greatest for highly pivotal decisions such as parole, policing, and healthcare. After discussing the study’s limitations, we outline avenues for future research. ","",""
"2022","Feeling fixes: Mess and emotion in algorithmic audits"," Efforts to address algorithmic harms have gathered particular steam over the last few years. One area of proposed opportunity is the notion of an “algorithmic audit,” specifically an “internal audit,” a process in which a system’s developers evaluate its construction and likely consequences. These processes are broadly endorsed in theory—but how do they work in practice? In this paper, we conduct not only an audit but an autoethnography of our experiences doing so. Exploring the history and legacy of a facial recognition dataset, we find paradigmatic examples of algorithmic injustices. But we also find that the process of discovery is interwoven with questions of affect and infrastructural brittleness that internal audit processes fail to articulate. For auditing to not only address existing harms but avoid producing new ones in turn, we argue that these processes must attend to the “mess” of engaging with algorithmic systems in practice. Doing so not only reduces the risks of audit processes but—through a more nuanced consideration of the emotive parts of that mess—may enhance the benefits of a form of governance premised entirely on altering future practices. ","",""
"2022","A practical role-based approach for autonomous vehicle moral dilemmas"," Autonomous vehicle moral dilemmas matter less for the particular outcomes of potential accidents than for their role in defining the values of the society we wish to live in. Different approaches have been suggested to determine the ethical settings that autonomous vehicles should be implemented in and identify the legitimate agents for making such decisions. Most of these, however, fail on theoretical grounds, facing severe issues related to moral justifications and compliance to the law, or on practical grounds, being insufficiently universal, action-guiding, or technically viable to be implemented. The analogy with the “trolley problem” has been extensively discussed. However, researchers have rarely tried to adapt this framework to autonomous vehicle cases or investigate how it could be used to address these issues. In doing so, this paper aims to answer the two key problems of autonomous vehicle dilemmas. With regards to the decision-maker, it rejects autonomous vehicle users’ choice-based models, showing the absurdity of both switch of control and adaptative preferences and arguing for common legislator-determined ethical settings. With regards to the decisions themselves, it criticizes both utilitarian views and those based on individuals’ criteria to suggest a deontologist rights-based approach. This allows for the defence of a morally coherent, regulatory compliant, explainable, and easily implementable framework capable of addressing all autonomous vehicle moral dilemma scenarios present in the literature. ","",""
"2022","Fairness perceptions of algorithmic decision-making: A systematic review  of the empirical literature"," Algorithmic decision-making increasingly shapes people's daily lives. Given that such autonomous systems can cause severe harm to individuals and social groups, fairness concerns have arisen. A human-centric approach demanded by scholars and policymakers requires considering people's fairness perceptions when designing and implementing algorithmic decision-making. We provide a comprehensive, systematic literature review synthesizing the existing empirical insights on perceptions of algorithmic fairness from 58 empirical studies spanning multiple domains and scientific disciplines. Through thorough coding, we systemize the current empirical literature along four dimensions: (1) algorithmic predictors, (2) human predictors, (3) comparative effects (human decision-making vs. algorithmic decision-making), and (4) consequences of algorithmic decision-making. While we identify much heterogeneity around the theoretical concepts and empirical measurements of algorithmic fairness, the insights come almost exclusively from Western-democratic contexts. By advocating for more interdisciplinary research adopting a society-in-the-loop framework, we hope our work will contribute to fairer and more responsible algorithmic decision-making. ","",""
"2022","Making sense of decision support systems: Rationales, translations and potentials for critical reflections on the reality of child protection"," Decision support systems, which incorporate artificial intelligence and big data, are receiving significant attention in the public sector. Decision support systems are sociocultural artefacts that are subject to a mix of technical and political choices, and critical investigation of these choices and the rationales they reflect are paramount since they are inscribed into and may cause harm, violate fundamental rights and reproduce negative social patterns. Applying and merging the concepts of sense-making and translation, this article investigates the rationales, translations and critical reflections that shape the development of a decision support system to support social workers assessing referrals concerning child neglect. It presents findings from a qualitative case study conducted in 2019–2020 at the Citizen Centre Children and Young People, Copenhagen Municipality, Denmark. The analysis shows how key actors through processes of translation construct, negotiate and readjust problem definitions, roles, interests, responsibilities and ideas of ambiguity and accountability. Although technological solutionism is present in these processes, it is not the only rationale invested. Rather, technological and data-driven rationales are adjusted to and merged with rationales of efficiency, return on investment and child welfare. Through continuous renegotiation of roles, responsibilities and problems according to these rationales, the key actors attempt to orchestrate ways of managing the complexity facing child welfare services by projecting images of future potentials of the decision support system that are yet to be realised. ","",""
"2022","AI ethics and data governance in the geospatial domain of Digital Earth"," Digital Earth applications provide a common ground for visualizing, simulating, and modeling real-world situations. The potential of Digital Earth applications has increased significantly with the evolution of artificial intelligence systems and the capacity to collect and process complex amounts of geospatial data. Yet, the widespread techno-optimism at the root of Digital Earth must now confront concerns over high-risk artificial intelligence systems and power asymmetries of a datafied society. In this commentary, we claim that not only can current debates about data governance and ethical artificial intelligence inform development in the field of Digital Earth, but that the specificities of geospatial data, together with the expectations surrounding Digital Earth applications, offer a fruitful lens through which to examine current debates on data governance and artificial intelligence ethics. In particular, we argue that for the implementation of ethical artificial intelligence and inclusive approaches to data governance, Digital Earth initiatives need to involve stakeholders and communities at the local level and be sensitive to social, legal, cultural, and institutional contexts, including conflicts that might arise within those contexts. ","",""
"2022","Algorithmic accountability in U.S. cities: Transparency, impact, and political economy"," This article examines how algorithmic accountability is translated into action at the municipal level in the United States. Based on a review of task forces, ordinances, and policy toolkits from New York City and Seattle, I demonstrate the ways municipalities and local publics operationalize abstract notions of accountability. Municipal interventions often prioritize revealing computational tools (transparency) and their effects on people (impact assessments). While these two forms of accountability are crucial, they may neglect to examine institutions—and how they change—as they incorporate automated decision systems. I thus propose a political-economic approach that recognizes algorithmic systems as part of municipal institutions and focuses on their role in intensifying data collection and commodification between public agencies and markets. I argue that algorithmic accountability, especially in public agencies, needs to focus on incompetence and asymmetries of power within a network of governments, tech companies, community groups, and technologies. With a mix of transparency, impact assessments, and political economic review, the paper proposes a more comprehensive assessment of automated decision systems through their development, procurement, use, impact, and decommissioning. ","",""
"2022","Social impacts of algorithmic decision-making: A research agenda for the social sciences"," Academic and public debates are increasingly concerned with the question whether and how algorithmic decision-making (ADM) may reinforce social inequality. Most previous research on this topic originates from computer science. The social sciences, however, have huge potentials to contribute to research on social consequences of ADM. Based on a process model of ADM systems, we demonstrate how social sciences may advance the literature on the impacts of ADM on social inequality by uncovering and mitigating biases in training data, by understanding data processing and analysis, as well as by studying social contexts of algorithms in practice. Furthermore, we show that fairness notions need to be evaluated with respect to specific outcomes of ADM systems and with respect to concrete social contexts. Social sciences may evaluate how individuals handle algorithmic decisions in practice and how single decisions aggregate to macro social outcomes. In this overview, we highlight how social sciences can apply their knowledge on social stratification and on substantive domains of ADM applications to advance the understanding of social impacts of ADM. ","",""
"2022","Toward a sociology of machine learning explainability: Human–machine interaction in deep neural  network-based automated trading"," Machine learning systems are making considerable inroads in society owing to their ability to recognize and predict patterns. However, the decision-making logic of some widely used machine learning models, such as deep neural networks, is characterized by opacity, thereby rendering them exceedingly difficult for humans to understand and explain and, as a result, potentially risky to use. Considering the importance of addressing this opacity, this paper calls for research that studies empirically and theoretically how machine learning experts and users seek to attain machine learning explainability. Focusing on automated trading, we take steps in this direction by analyzing a trading firm’s quest for explaining its deep neural network system’s actionable predictions. We demonstrate that this explainability effort involves a particular form of human–machine interaction that contains both anthropomorphic and technomorphic elements. We discuss this attempt to attain machine learning explainability in light of reflections on cross-species companionship and consider it an example of human–machine companionship. ","",""
"2022","Hello, Twitter Bot!: Towards a Bot Ethics of Response and Responsibility","In this paper, we explore the troubles and potentials at stake in the developments and deployments of lively technologies like Twitter bots, and how they challenge traditional ideas of ethical responsibility. We suggest that there is a tendency for bot ethics to revolve around the desire to differentiate between bot and human, which does not address what we understand to be the cultural anxieties at stake in the blurring boundaries between human and technology. Here we take some tentative steps towards rethinking and reimagining bot-human relationships through a feminist ethics of responsibility as response by taking as our starting point our own experience with bot creation, the Twitter bot “Hello30762308.” The bot was designed to respond with a “hello” to other Twitter users’ #hello, but quickly went in directions not intended by its creators. ","",""
"2022","The Bias Cut: Toward a Technopoetics of Algorithmic Systems","This essay explores a material engagement with discourses of bias. At a time when the developers of algorithmic systems are exploring concepts of bias like never before, textile bias (or the skew of woven material) offers an alternative view into the scripts of computational engagement. To probe this potential, this essay engages a range of feminist and anti-racist interventions in performance arts, critical archival studies, and my own pedagogical collaborations. With these experiments, I ask, how might material bias inform ongoing analysis of cultural bias within machine learning systems? The experiments reveal interwoven dynamics of power, labor, and historicity with particular attention to complicity and change. Through angular encounters with bias, I explore the development of an emerging technopoetics of algorithmic systems.","",""
"2022","Spinning words as disguise: Shady services for ethical research?","Ethical researchers who want to quote public user-generated content without further exposing these sources have little guidance as to how to disguise quotes. Reagle (2021b) showed that researchers’ attempts to disguise phrases on Reddit are often haphazard and ineffective. Are there tools that can help? Automated word spinners, used to generate reams of ad-laden content, seem suited to the task. We select 10 quotations from fictional posts on r/AmItheButtface and “spin” them using Spin Rewriter and WordAi. We review the usability of the services and then (1) search for their spins on Google; and, (2) ask human subjects (N=19) to judge them for fidelity. Participants also disguise three of those phrases and these are assessed for efficacy and the tactics employed. We recommend that researchers disguise their prose by substituting novel words (i.e., swapping infrequently occurring words, such as “toxic” with “radioactive”) and rearranging elements of sentence structure. The practice of testing spins, however, remains essential even when using good tactics; a Python script is provided to facilitate such testing.","",""
"2022","Stupidity","In this brief commentary, I trace the ‘development’ of stupidity from human (individual and social) stupidity, via technologically and heteronomically facilitated stupidity, to the more recent human-artificial variant in an attempt to define the difference between human-artificial stupidity, ‘milintelligence’, and artificial intelligence.   ","",""
"2022","Algorithms, contexts, governance: An introduction to the special issue"," This introduction to the special issue on algorithmic governance in context offers an outline of the field and summarizes each contribution to the issue. ","",""
"2022","Artificial intelligence and mass personalization of communication content—An ethical and literacy perspective"," Artificial intelligence (AI) is (re)shaping communication and contributes to (commercial and informational) need satisfaction by means of mass personalization. However, the substantial personalization and targeting opportunities do not come without ethical challenges. Following an AI-for-social-good perspective, the authors systematically scrutinize the ethical challenges of deploying AI for mass personalization of communication content from a multi-stakeholder perspective. The conceptual analysis reveals interdependencies and tensions between ethical principles, which advocate the need of a basic understanding of AI inputs, functioning, agency, and outcomes. By this form of AI literacy, individuals could be empowered to interact with and treat mass-personalized content in a way that promotes individual and social good while preventing harm. ","",""
"2022","Programmed welfare: An ethnographic account of algorithmic practices in the public distribution system in India"," Although considered to be fixed sequence of computational procedures, the actual nature of algorithms emerges only in practice through its performative agency enacted within a network of human and non-human actors. In this article, I trace this agency in the everyday practices of the algorithmic system of welfare distribution, namely, the Aadhaar-enabled Public Distribution System (AePDS), through ethnographic fieldwork across three states in India. Conceptualising the AePDS as a programmed welfare system, I unpack its underlying assemblages to show that far from being objective technologies of governance, algorithmic sorting for targeted welfare is enacted in relation to a multitude of human actors, databases, machines, documents and shifting institutional contexts, in ways that are markedly different from its fixed computation properties. This mode of enquiry, I argue, makes the process of algorithmic enactment more transparent and comprehensible, which in turn will aid in better design and governance of such systems. ","",""
"2022","Algorithms as figures: Towards a post-digital ethnography of algorithmic contexts"," This article intervenes in contemporary discussions of critical algorithm studies about the meaning of the notion ‘algorithm’. While many critical scholars as well as most public and private organisations understand this concept as a computational procedure instantiated by a programming code in a software stack, I argue that the algorithm is better understood as a ‘figure’; a discursive short-hand pointing to diverse modes of procedural governance and not always digital ones. Since algorithmic figures are generated by a bundle of heterogeneous contexts, their emergence leads to conflicting visions about the reality, materiality and effects of algorithmisation. This article provides four ethnographic strategies to describe the contexts of production and circulation of algorithmic figures: observing the observers of algorithms; mapping and creating algorithmic figures; drawing relations across contexts of figuring; and analysing the transformative effects of algorithmic figures on the attempts to govern them. ","",""
"2022","(Non)negotiable spaces of algorithmic governance: Perceptions on the Ubenwa health app as a ‘relocated’ solution"," This study explores relocated algorithmic governance through a qualitative study of the Ubenwa health app. The Ubenwa, which was developed in Canada based on a dataset of babies from Mexico, is currently being implemented in Nigeria to detect birth asphyxia. The app serves as an ideal case for examining the socio-cultural negotiations involved in re-contextualising algorithmic technology. We conducted in-depth interviews with parents, medical practitioners and data experts in Nigeria; the interviews reveal individuals’ perceptions about algorithmic governance and self-determination. In particular, our study presents people’s insights about (1) relocated algorithms as socially dynamic ‘contextual settings’, (2) the (non)negotiable spaces that these algorithmic solutions potentially create and (3) the general implications of re-contextualising algorithmic governance. This article illustrates that relocated algorithmic solutions are perceived as ‘cosmopolitan data localisms’ that extend the spatial scales and multiply localities rather than as ‘data glocalisation’ or the indigenisation of globally distributed technology. ","",""
"2022","Sociotechnical imaginaries of algorithmic governance in EU policy on online disinformation and FinTech"," Datafication and the use of algorithmic systems increasingly blur distinctions between policy fields. In the financial sector, for example, algorithms are used in credit scoring, money has become transactional data sought after by large data-driven companies, while financial technologies (FinTech) are emerging as a locus of information warfare. To grasp the context specificity of algorithmic governance and the assumptions on which its evaluation within different domains is based, we comparatively study the sociotechnical imaginaries of algorithmic governance in European Union (EU) policy on online disinformation and FinTech. We find that sociotechnical imaginaries prevalent in EU policy documents on disinformation and FinTech are highly divergent. While the first can be characterized as an algorithm-facilitated attempt to return to the presupposed status quo (absence of manipulation) without a defined future imaginary, the latter places technological innovation at the centre of realizing a globally competitive Digital Single Market. ","",""
"2022","The perception of humanness in conversational journalism: An algorithmic information-processing perspective"," How much do anthropomorphisms influence the perception of users about whether they are conversing with a human or an algorithm in a chatbot environment? We develop a cognitive model using the constructs of anthropomorphism and explainability to explain user experiences with conversational journalism (CJ) in the context of chatbot news. We examine how users perceive anthropomorphic and explanatory cues, and how these stimuli influence user perception of and attitudes toward CJ. Anthropomorphic explanations of why and how certain items are recommended afford users a sense of humanness, which then affects trust and emotional assurance. Perceived humanness triggers a two-step flow of interaction by defining the baseline to make a judgment about the qualities of CJ and by affording the capacity to interact with chatbots concerning their intention to interact with chatbots. We develop practical implications relevant to chatbots and ascertain the significance of humanness as a social cue in CJ. We offer a theoretical lens through which to characterize humanness as a key mechanism of human–artificial intelligence (AI) interaction, of which the eventual goal is humans perceive AI as human beings. Our results help to better understand human–chatbot interaction in CJ by illustrating how humans interact with chatbots and explaining why humans accept the way of CJ. ","",""
"2022","Transforming media agency? Approaches to automation in Finnish legacy media"," The algorithmic automation of media processes has produced machines that perform in roles that were previously occupied by human beings. Recent research has probed various theoretical approaches to the agency and ethical responsibility of machines and algorithms. However, there is no theoretical consensus concerning many key issues. Rather than setting out with fixed conceptions, this research calls for a closer look at the considerations and attitudes that motivate actual attributions of agency and responsibility. The empirical context of this study is legacy media where the introduction of automation, together with topical considerations of journalistic ethics and responsibility, has given rise to substantial reflection on received conceptions and practices. The results show a continuing resistance to attributions of agency and responsibility to machines. Three lines of thinking that motivate this stance are distinguished, and their potential shortcomings and theoretical implications are considered. ","",""
"2022","“Have you learned your lesson?” Communities of practice under algorithmic competition"," Gig workers are typically thought of as individuals toiling in digitized isolation, not as communities of shared learning. While it’s accurate to say they don’t have the same information-sharing norms as people in traditional employment arrangements, some do gather, in part in digital communities. Online forums, in this space, have become popular sites for gathering, sharing information, and comparing practices. These behaviors provide an opportunity to examine gig workers as emergent communities of practice, and to analyze how work, identity, skills, and workspaces co-constitute each other as sociotechnical environments of work change. In this research, I examine workers’ interactions in an online forum, and focus on how they talk about scams. Analysis reveals that talking about scams is a way for workers to enact belonging in their community of practice. Victims are belittled by other workers, who frame vulnerability, and lack of foresight due to unfamiliarity with the forum itself, as a lack of authenticity. Repudiations are denunciations through which workers assert their belonging. These findings illuminate the practices of what I call “para-organizational” work, with implications for knowledge management in structures of algorithmic competition. ","",""
"2022","Artificial everyday creativity: creative leaps with AI through critical making","ABSTRACT The capabilities of humans and AI systems to be creative and perform alongside one another have given rise to new practices of ‘artificial creativity’. In this article, I argue that artificial creativity demonstrates the potential to empower individuals to interface and critically dialogue with computational systems. Reframed as artificial ‘everyday’ creativity, I focus on the curious, joyful and adjacent modes of everyday creativity by including hybrid materials that embrace alternative pedagogies of code and computation. Through the interdisciplinary design approach of ‘critical making’, I craft two unconventionally-coded artefacts that dialogue with AI systems, namely CryptoCrochet-Key and Internet of Towels. Both artefacts are analysed using a four-pronged creativity framework to understand the material translation processes in the artificial everyday creativity practice. With rising concerns about AI's role in misinformation, bias and discrimination, the discussion explores the generative value and limitations of artificial everyday creativity towards the broader goals of civic data literacy and user empowerment.","",""
"2023","ChatGPT Isn't Magic","Introduction Author Arthur C. Clarke famously argued that in science fiction literature “any sufficiently advanced technology is indistinguishable from magic” (Clarke). On 30 November 2022, technology company OpenAI publicly released their Large Language Model (LLM)-based chatbot ChatGPT (Chat Generative Pre-Trained Transformer), and instantly it was hailed as world-changing. Initial media stories about ChatGPT highlighted the speed with which it generated new material as evidence that this tool might be both genuinely creative and actually intelligent, in both exciting and disturbing ways. Indeed, ChatGPT is part of a larger pool of Generative Artificial Intelligence (AI) tools that can very quickly generate seemingly novel outputs in a variety of media formats based on text prompts written by users. Yet, claims that AI has become sentient, or has even reached a recognisable level of general intelligence, remain in the realm of science fiction, for now at least (Leaver). That has not stopped technology companies, scientists, and others from suggesting that super-smart AI is just around the corner. Exemplifying this, the same people creating generative AI are also vocal signatories of public letters that ostensibly call for a temporary halt in AI development, but these letters are simultaneously feeding the myth that these tools are so powerful that they are the early form of imminent super-intelligent machines. For many people, the combination of AI technologies and media hype means generative AIs are basically magical insomuch as their workings seem impenetrable, and their existence could ostensibly change the world. This article explores how the hype around ChatGPT and generative AI was deployed across the first six months of 2023, and how these technologies were positioned as either utopian or dystopian, always seemingly magical, but never banal. We look at some initial responses to generative AI, ranging from schools in Australia to picket lines in Hollywood. We offer a critique of the utopian/dystopian binary positioning of generative AI, aligning with critics who rightly argue that focussing on these extremes displaces the more grounded and immediate challenges generative AI bring that need urgent answers. Finally, we loop back to the role of schools and educators in repositioning generative AI as something to be tested, examined, scrutinised, and played with both to ground understandings of generative AI, while also preparing today’s students for a future where these tools will be part of their work and cultural landscapes. Hype, Schools, and Hollywood In December 2022, one month after OpenAI launched ChatGPT, Elon Musk tweeted: “ChatGPT is scary good. We are not far from dangerously strong AI”. Musk’s post was retweeted 9400 times, liked 73 thousand times, and presumably seen by most of his 150 million Twitter followers. This type of engagement typified the early hype and language that surrounded the launch of ChatGPT, with reports that “crypto” had been replaced by generative AI as the “hot tech topic” and hopes that it would be “‘transformative’ for business” (Browne). By March 2023, global economic analysts at Goldman Sachs had released a report on the potentially transformative effects of generative AI, saying that it marked the “brink of a rapid acceleration in task automation that will drive labor cost savings and raise productivity” (Hatzius et al.). Further, they concluded that “its ability to generate content that is indistinguishable from human-created output and to break down communication barriers between humans and machines reflects a major advancement with potentially large macroeconomic effects” (Hatzius et al.). Speculation about the potentially transformative power and reach of generative AI technology was reinforced by warnings that it could also lead to “significant disruption” of the labour market, and the potential automation of up to 300 million jobs, with associated job losses for humans (Hatzius et al.). In addition, there was widespread buzz that ChatGPT’s “rationalization process may evidence human-like cognition” (Browne), claims that were supported by the emergent language of ChatGPT. The technology was explained as being “trained” on a “corpus” of datasets, using a “neural network” capable of producing “natural language“” (Dsouza), positioning the technology as human-like, and more than ‘artificial’ intelligence. Incorrect responses or errors produced by the tech were termed “hallucinations”, akin to magical thinking, which OpenAI founder Sam Altman insisted wasn’t a word that he associated with sentience (Intelligencer staff). Indeed, Altman asserts that he rejects moves to “anthropomorphize” (Intelligencer staff) the technology; however, arguably the language, hype, and Altman’s well-publicised misgivings about ChatGPT have had the combined effect of shaping our understanding of this generative AI as alive, vast, fast-moving, and potentially lethal to humanity. Unsurprisingly, the hype around the transformative effects of ChatGPT and its ability to generate ‘human-like’ answers and sophisticated essay-style responses was matched by a concomitant panic throughout educational institutions. The beginning of the 2023 Australian school year was marked by schools and state education ministers meeting to discuss the emerging problem of ChatGPT in the education system (Hiatt). Every state in Australia, bar South Australia, banned the use of the technology in public schools, with a “national expert task force” formed to “guide” schools on how to navigate ChatGPT in the classroom (Hiatt). Globally, schools banned the technology amid fears that students could use it to generate convincing essay responses whose plagiarism would be undetectable with current software (Clarence-Smith). Some schools banned the technology citing concerns that it would have a “negative impact on student learning”, while others cited its “lack of reliable safeguards preventing these tools exposing students to potentially explicit and harmful content” (Cassidy). ChatGPT investor Musk famously tweeted, “It’s a new world. Goodbye homework!”, further fuelling the growing alarm about the freely available technology that could “churn out convincing essays which can't be detected by their existing anti-plagiarism software” (Clarence-Smith). Universities were reported to be moving towards more “in-person supervision and increased paper assessments” (SBS), rather than essay-style assessments, in a bid to out-manoeuvre ChatGPT’s plagiarism potential. Seven months on, concerns about the technology seem to have been dialled back, with educators more curious about the ways the technology can be integrated into the classroom to good effect (Liu et al.); however, the full implications and impacts of the generative AI are still emerging. In May 2023, the Writer’s Guild of America (WGA), the union representing screenwriters across the US creative industries, went on strike, and one of their core issues were “regulations on the use of artificial intelligence in writing” (Porter). Early in the negotiations, Chris Keyser, co-chair of the WGA’s negotiating committee, lamented that “no one knows exactly what AI’s going to be, but the fact that the companies won’t talk about it is the best indication we’ve had that we have a reason to fear it” (Grobar). At the same time, the Screen Actors’ Guild (SAG) warned that members were being asked to agree to contracts that stipulated that an actor’s voice could be re-used in future scenarios without that actor’s additional consent, potentially reducing actors to a dataset to be animated by generative AI technologies (Scheiber and Koblin). In a statement issued by SAG, they made their position clear that the creation or (re)animation of any digital likeness of any part of an actor must be recognised as labour and properly paid, also warning that any attempt to legislate around these rights should be strongly resisted (Screen Actors Guild). Unlike the more sensationalised hype, the WGA and SAG responses to generative AI are grounded in labour relations. These unions quite rightly fear the immediate future where human labour could be augmented, reclassified, and exploited by, and in the name of, algorithmic systems. Screenwriters, for example, might be hired at much lower pay rates to edit scripts first generated by ChatGPT, even if those editors would really be doing most of the creative work to turn something clichéd and predictable into something more appealing. Rather than a dystopian world where machines do all the work, the WGA and SAG protests railed against a world where workers would be paid less because executives could pretend generative AI was doing most of the work (Bender). The Open Letter and Promotion of AI Panic In an open letter that received enormous press and media uptake, many of the leading figures in AI called for a pause in AI development since “advanced AI could represent a profound change in the history of life on Earth”; they warned early 2023 had already seen “an out-of-control race to develop and deploy ever more powerful digital minds that no one – not even their creators – can understand, predict, or reliably control” (Future of Life Institute). Further, the open letter signatories called on “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4”, arguing that “labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts” (Future of Life Institute). Notably, many of the signatories work for the very companies involved in the “out-of-control race”. Indeed, while this letter could be read as a moment of ethical clarity for the AI industry, a more cynical reading might just be that in warning that their AIs could effectively destroy the world, these companies were positioning their products as seemingly magical—“digital minds that no one – not even their creators – can understand”—making them even more appealing to potential customers and investors. Far from pausing AI development, the open letter actually operates as a neon sign touting the amazing capacities and future brilliance of generative AI systems. Nirit Weiss-Blatt argues that general reporting on technology industries up to 2017 largely concurred with the public relations stance of those companies, positioning them as saviours and amplifiers of human connection, creativity, and participation. After 2017, though, media reporting completely shifted, focussing on the problems, risks, and worst elements of these corporate platforms. In the wake of the open letter, Weiss-Blatt extended her point on Twitter, arguing that media and messaging surrounding generative AI can be broken down into those who are profiting and fuelling the panic at one end of the spectrum, and those who think the form of the panic (which positions AI as dangerously intelligent) is deflecting from the immediate real issues caused by generative AI at the other. Weiss-Blatt characterises the Panic-as-a-Business proponents as arguing “we're telling you will all die from a Godlike AI… so you must listen to us”, which coheres with the broader positioning narrative of generative AI’s seemingly magical (and thus potentially destructive) capabilities. Yet this rhetoric also positions the companies creating generative AI as the ones who should be making the rules to control it, an argument so effective that in July 2023 the Biden Administration in the US endorsed the biggest AI companies—Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI—framing future AI development with voluntary safeguards rather than externally imposed policies (Shear, Kang, and Sanger).  Fig. 1: Promotors of AI Panic, extrapolating from Nirit Weiss-Blatt. (Algorithm Watch) Stochastic Parrots and Deceitful Media Artificial Intelligences have inhabited popular imaginaries via novels, television, and films far longer than they have been considered even potentially viable technologies, so it is not surprising that popular culture has often framed the way AI is understood (Leaver). Yet as Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell argue, Large Language Models and generative AI are most productively understood as “a stochastic parrot” insomuch as each is a “system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning” (Bender et al. 617). Generative AI, then, is not creating something genuinely new, but rather remixing existing data in novel ways that the systems themselves do not in any meaningful sense understand. Going further, Simone Natale characterises current AI tools as “deceitful media” insomuch as they are designed to deliberately appear generally intelligent, but this is always a deception. The deception makes these tools more engaging for humans to use but is also fundamental in selling and profiting from the use of AI tools. Rather than accepting claims made by the companies financing and creating contemporary AI, Natale argues for a more pedagogically productive path:  we must resist the normalization of the deceptive mechanisms embedded in contemporary AI and the silent power that digital media companies exercise over us. We should never cease to interrogate how the technology works, even while we are trying to accommodate it in the fabric of everyday life. (Natale 132)  Real Issues Although even a comprehensive list is beyond the scope of this short article, is it nevertheless vital to note that in looking beyond the promotion of AI Panic and deceptive media, ChatGPT and other generative AI tools create or exacerbate a range of very real and significant ethical problems. The most obvious problem is the lack of transparency in terms of what data different generative AI tools were trained on. Generally, these tools are thought to get better by absorbing ever greater amounts of data, with most AI companies acknowledging that scraping the Web in some form has been part of the training data harvesting for their AI tools. Not knowing what data have been used makes it almost impossible to know which perspectives, presumptions, and biases are baked into these tools. While many forms of bias have plagued technology companies for many years (Noble), for generative AI tools, in “accepting large amounts of web text as ‘representative’ of ‘all’ of humanity we risk perpetuating dominant viewpoints, increasing power imbalances, and further reifying inequality” (Bender et al. 614). Even mitigating and working to correct biases in generative AI tools will be a huge challenge if these companies never share what was in their training data. As the WGA and SAG strike discussed above emphasises, the question of human labour is a central challenge for generative AI. Beyond Hollywood, more entrenched forms of labour exploitation haunt generative AI. Very low-paid workers have done much of the labour in classifying different forms of data in order to train AI systems; data workers are routinely not acknowledged at all, even sometimes directly performing the tasks that are ascribed to AI, to the extent that “distracted by the specter of nonexistent sentient machines, an army of precarized workers stands behind the supposed accomplishments of artificial intelligence systems today” (Williams, Miceli, and Gebru). It turns out that people are still doing the work so that companies can pretend the machines can think. In one final but very important example, there is a very direct ecological cost to training, maintaining, and running generative AI tools. In the context of global warming, concerns already existed about the enormous data centres at the heart of the big technology platforms prior to ChatGPT’s release. However, the data and processing power needed to run generative AI tools are even larger, leading to very real questions about how much electricity and water (for cooling) are used by even the most rudimentary ChatGPT queries (Lizarraga and Solon). While not just an AI question, balancing the environmental costs of data centres with the actual utility of AI tools is not one that is routinely asked, or answered, in the hype around generative AI. Messing Around and Geeking Out Escaping the hype and hypocrisy deployed by AI companies is vital for repositioning generative AI not as magical, not as a saviour, and not as a destroyer, but rather as a new technology that needs to be critically and ethically understood. In seminal work exploring how young people engage with digital tools and technologies, Mimi Ito and colleagues developed three genres of technology participation: hanging out, where engagement with any technologies is largely driven by friendships and social engagement; messing around, which includes a great deal of experimentation and play with technological tools; and geeking out, where some young people will find a particular focus on one platform, tool or technology that inspires them to focus enough to develop expertise in using and understanding that tool (Ito et al.). If young people, in particular, are going to be living in a world where generative AI tools are part of their social worlds and workplaces, then messing around with ChatGPT is, indeed, going to be important in testing out how these tools answer questions and synthesise information, what biases are evident in responses, and at what points answers are incorrect. For some young people, they may well move from messing around to completely geeking out with generative AI, a process that will be even more fruitful if these tools are not seen as impenetrable magic, but rather as commercial tools built by for-profit companies. While the idea of digital natives is an unhelpful myth (Bennett, Maton, and Kervin), if young people are going to be the first generation to have generative AI as part of their information, creative, and search landscapes, then safely messing around and geeking out with these tools will be more vital than ever. We mentioned above that most Australian state education departments initially banned ChatGPT, but a more optimistic sign arrived as we were finishing this article insomuch as the different Australian states agreed in mid-2023 to work together to create “a framework to guide the safe and effective use of artificial intelligence in the nation’s schools” (Clare). Although there is work to be done, moving away from a ban to a setting that should allow students to be part of testing, framing, and critiquing ChatGPT and generative AI is a clear step in repositioning these technologies as tools, not magical systems that could never be understood. Conclusion Generative AI is not magic; it is not a saviour or destroyer; it is neither utopian nor dystopian; nor, unless we radically narrow the definition, is it intelligent. The companies and corporations driving AI development have a vested interest in promoting fantastical ideas about generative AI, as it drives their customers, investment, and future viability. When the hype is dominant, responses can be overdetermined, such as banning generative AI in schools. But in taking a less magical and more material approach to ChatGPT and generative AI, we can try and ensure pedagogical opportunities for today’s young people to test out, scrutinise, and critically understand the AI tools they are most likely going to be asked to use today and in the future. The first wave of generative AI hype following the public release of ChatGPT offers an opportunity to reflect on exactly what the best uses of these technologies are, what ethics should drive those uses, and how transparent the workings of generative AI should be before their presence in the digital landscape is so entrenched and mundane that it becomes difficult to see at all. Acknowledgment This research was supported by the Australian Research Council Centre of Excellence for the Digital Child through project number CE200100022. References Algorithm Watch [@AlgorithmWatch]. “Mirror, Mirror on the Wall, Who Is the Biggest Panic-Creator of Them All? Inspired by a Tweet from Nirit Weiss-Blatt, Check out Our Taxonomy of #AI Panic Facilitators and Those Fighting against the Fearmongering. Who Have We Forgotten to Add? Let Us Know! ⬇️” Instagram, 12 July 2023 &lt;https://Instagram.com/p/Cump3losObg/&gt;.  Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. Virtual Event. Canada: ACM, 2021. 610–623. &lt;https://dl.acm.org/doi/10.1145/3442188.3445922&gt;. Bender, Stuart Marshall. “Coexistence and Creativity: Screen Media Education in the Age of Artificial Intelligence Content Generators.” Media Practice and Education (2023): 1–16. Bennett, Sue, Karl Maton, and Lisa Kervin. “The ‘Digital Natives’ Debate: A Critical Review of the Evidence.” British Journal of Educational Technology 39.5 (2008): 775–786. Browne, Ryan. “Buzzy A.I. Tools like Microsoft-Backed ChatGPT Replaced Crypto as the Hot Tech Topic of Davos.” CNBC, 20 Jan. 2023. &lt;https://cnbc.com/2023/01/20/chatgpt-microsoft-backed-ai-tool-replaces-crypto-as-hot-davos-tech-topic.html&gt;. Cassidy, Caitlin. “Queensland Public Schools to Join NSW in Banning Students from ChatGPT.” The Guardian, 23 Jan. 2023. &lt;https://theguardian.com/australia-news/2023/jan/23/queensland-public-schools-to-join-nsw-in-banning-students-from-chatgpt&gt;. “Cheating with ChatGPT? Controversial AI Tool Banned in These Schools in Australian First.” SBS News, 22 Jan. 2023. &lt;https://sbs.com.au/news/article/cheating-with-chatgpt-controversial-ai-tool-banned-in-these-schools-in-australian-first/817odtv6e&gt;. Clare, Jason. “Draft Schools AI Framework Open for Consultation.” Ministers’ Media Centre, 28 July 2023. &lt;https://ministers.education.gov.au/clare/draft-schools-ai-framework-open-consultation&gt;. Clarence-Smith, Louisa. “‘Goodbye Homework!’ Elon Musk Praises AI Chatbot That Writes Student Essays.” The Telegraph, 5 Jan. 2023. &lt;https://telegraph.co.uk/news/2023/01/05/homework-elon-musk-chatgpt-praises-ai-chatbot-writes-students/&gt;. Clarke, Arthur C. “Hazards of Prophecy: The Failure of Imagination.” Profiles of the Future: An Inquiry into the Limits of the Possible. New York: Harper and Row, 1973. Dsouza, Elton Grivith. “How ChatGPT Works: Training Model of ChatGPT.” Edureka! 11 May 2023. &lt;https://edureka.co/blog/how-chatgpt-works-training-model-of-chatgpt/&gt;. Future of Life Institute. “Pause Giant AI Experiments: An Open Letter.” Future of Life Institute, 22 Mar. 2023. &lt;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&gt;. Grobar, Matt. “WGA Negotiating Committee Co-Chair Chris Keyser on the Breakdown of Negotiations with ‘Divided’ AMPTP.” Deadline, 2 May 2023. &lt;https://deadline.com/2023/05/wga-strike-chris-keyser-interview-failed-negotiations-amptp-ai-1235354566/&gt;. Hatzius, Jan, Joseph Briggs, Devesh Kodnani, and Giovanni Pierdomenico. “The Potentially Large Effects of Artificial Intelligence on Economic Growth.” Goldman Sachs: Global Economics Analyst, 26 Mar. 2023. &lt;https://gspublishing.com/content/research/en/reports/2023/03/27/d64e052b-0f6e-45d7-967b-d7be35fabd16.html&gt;. Hiatt, Bethany. “National Expert Task Force to Be Set Up in Bid to Help Australian Schools Harness Tools Such as ChatGPT.” The West Australian, 1 Mar. 2023. &lt;https://thewest.com.au/news/education/national-expert-task-force-to-be-set-up-in-bid-to-help-australian-schools-harness-tools-such-as-chatgpt-c-9895269&gt;. Intelligencer staff. “Sam Altman on What Makes Him ‘Super Nervous’ about AI: The OpenAI Co-Founder Thinks Tools like GPT-4 Will Be Revolutionary. But He’s Wary of Downsides.” On with Kara Swisher: Intelligencer. 23 Mar. 2023. &lt;https://nymag.com/intelligencer/2023/03/on-with-kara-swisher-sam-altman-on-the-ai-revolution.html&gt;. Ito, Mizuko. Hanging Out, Messing Around, and Geeking Out: Kids Living and Learning with New Media. Cambridge, Mass.: MIT P, 2012. Leaver, Tama. Artificial Culture: Identity, Technology, and Bodies. New York: Routledge, 2012. Liu, Danny, Adam Bridgeman, and Benjamin Miller. “As Uni Goes Back, Here’s How Teachers and Students Can Use ChatGPT to Save Time and Improve Learning.” The Conversation, 28 Feb. 2023. &lt;https://theconversation.com/as-uni-goes-back-heres-how-teachers-and-students-can-use-chatgpt-to-save-time-and-improve-learning-199884&gt;. Lizarraga, Clara Hernanz, and Olivia Solon. “Thirsty Data Centers Are Making Hot Summers Even Scarier.” Bloomberg, 26 July 2023. &lt;https.//bloomberg.com/news/articles/2023-07-26/extreme-heat-drought-drive-opposition-to-ai-data-centers&gt;. Musk, Elon [@elonmusk]. “@sama. ChatGPT is scary good. We are not far from dangerously strong AI.” Twitter, 4 Dec. 2022. &lt;https://twitter.com/elonmusk/status/1599128577068650498?lang=en&gt;. ———. “@pmarca. It’s a new world. Goodbye homework!” Twitter, 5 Jan. 2023. &lt;https://twitter.com/elonmusk/status/1610849544945950722?lang=en&gt;. Natale, Simone. Deceitful Media Artificial Intelligence and Social Life after the Turing Test. New York: Oxford UP, 2021. Noble, Safiya Umoja. Algorithms of Oppression: How Search Engines Reinforce Racism. New York: NYU P, 2018. Porter, Rick. “Late Night Shows Shut Down with WGA Strike.” The Hollywood Reporter, 2 May 2023. &lt;https://hollywoodreporter.com/tv/tv-news/wga-strike-late-night-shows-shut-down-1235477882/&gt;. Scheiber, Noam, and John Koblin. “Will a Chatbot Write the Next ‘Succession’?” The New York Times 29 Apr. 2023. &lt;https://nytimes.com/2023/04/29/business/media/writers-guild-hollywood-ai-chatgpt.html&gt;. Screen Actors Guild – American Federation of Television and Radio Artists. “SAG-AFTRA Statement on the Use of Artificial Intelligence and Digital Doubles in Media and Entertainment.” 17 Mar. 2023. &lt;https://sagaftra.org/sag-aftra-statement-use-artificial-intelligence-and-digital-doubles-media-and-entertainment&gt;. Shear, Michael D., Cecilia Kang, and David E. Sanger. “Pressured by Biden, A.I. Companies Agree to Guardrails on New Tools.” The New York Times, 21 July 2023. &lt;https://nytimes.com/2023/07/21/us/politics/ai-regulation-biden.html&gt;. Weiss-Blatt, Nirit [@DrTechlash]. “A Taxonomy of AI Panic Facilitators.” Twitter, 1 July 2023. &lt;https://twitter.com/DrTechlash/status/1675155157880016898&gt;. ———. The Techlash and Tech Crisis Communication. Bingley: Emerald Publishing, 2021. Williams, Adrienne, Milagros Miceli, and Timnit Gebru. “The Exploited Labor behind Artificial Intelligence.” Noema, 13 Oct. 2022 &lt;https://noemamag.com/the-exploited-labor-behind-artificial-intelligence/&gt;.","",""
"2023","On That &lt;em&gt;Toy-Being&lt;/em&gt; of Generative Art Toys","Exhibiting Procedural Generation Generative art toys are software applications that create aesthetically pleasing visual patterns in response to the users toying with various input devices, from keyboard and mouse to more intuitive and tactile devices for motion tracking. The “art” part of these toy objects might relate to the fact that they are often installed in art galleries or festivals as a spectacle for non-players that exhibits the unlimited generation of new patterns from a limited source code. However, the features that used to characterise generative arts as a new meditative genre, such as the autonomy of the algorithmic system and its self-organisation (Galanter 151), do not explain the pleasure of fiddling with these playthings, which feel sticky like their toy relatives, slime, rather than meditative, like mathematical sublime.  Generative algorithms are more than software tools to serve human purposes now. While humans are still responsible for the algorithmically generated content, this is either to the extent of the simple generation rules the artists design for their artworks or only to the extent that our everyday conversations and behaviours serve as raw material to train machine learning-powered generation algorithms, such as ChatGPT, to interpret the world they explore stochastically, extrapolating it in an equivalently statistical way. Yet, as the algorithms become more responsive to the contingency of human behaviours, and so the trained generation rules become too complex, it becomes almost impossible for humans to understand how they translate all contingencies in the real world into machine-learnable correlations. In turn, the way we are entangled with the generated content comes to far exceed our responsibility. One disturbing future scenario of this hyper-responsiveness of the algorithms, for which we could never be fully responsible, is when machine-generated content replaces the ground truth sampled from the real world, leading to the other machine learning-powered software tools that govern human behaviour being trained on these “synthetic data” (Steinhoff). The multiplicities of human worlds are substituted for their algorithmically generated proxies, and the AIs trained instead on the proxies’ stochastic complexities would tell us how to design our future behaviours. As one aesthetic way to demonstrate the creativity of the machines, generative arts have exhibited generative algorithms in a somewhat decontextualised and thus less threatening manner by “emphasizing the circularity of autopoietic processes” of content generation (Hayles 156). Their current toy conversion playfully re-contextualises how these algorithms in real life, incarnated into toy-like gadgets, both enact and are enacted by human users. These interactions not only form random seeds for content generation but also constantly re-entangle generated contents with contingent human behaviors. The toy-being of generative algorithms I conceptualise here is illustrative of this changed mode of their exhibition. They move from displaying generative algorithms as speculative objects at a distance to sticky toy objects up close and personal: from emphasising their autopoietic closure to “more open-ended and transformative” engagement with their surroundings (Hayles 156). (Katherine Hayles says this changed focus in the research of artificial life/intelligence from the systems’ auto-poietic self-closure to their active engagement with environments characterises “the transition from the second to the third wave” of cybernetics; 17.) Their toy-being also reflects how the current software industry repurposes these algorithms, once developed for automation of content creation with no human intervention, as machines that enact commercially promising entanglements between contingent human behaviors and a mixed-reality that is algorithmically generated.  Tool-Being and Toy-Being of Generative Algorithms What I mean by toy-being is a certain mode of existence in which a thing appears when our habitual sensorimotor relations with it are temporarily suspended. It is comparable to what Graham Harman calls a thing’s tool-being in his object-oriented rereading of Heidegger’s tool analysis. In that case, this thing’s becoming either a toy or tool pertains to how our hands are entangled with its ungraspable aspects. According to Heidegger a hammer, for instance, is ready-to-hand when its reactions to our grip, and swinging, and to the response from the nail, are fully integrated into our habitual action of hammering to the extent that its stand-alone existence is almost unnoticeable (Tool-Being). On the other hand, it is when the hammer breaks down, or slips out of our grasp, that it begins to feel present-at-hand. For Harman, this is the moment the hammer reveals its own way to be in the world, outside of our instrumentalist concern. It is the hint of the hammer’s “subterranean reality”, which is inexhaustible by any practical and theoretical concerns we have of it (“Well-Wrought” 186). It is unconstrained by the pragmatic maxim that any conception of an object should be grounded in the consequences of what it does or what can be done with it (Peirce). In Harman’s object-oriented ontology, neither the hammer’s being ready to serve any purpose of human and nonhuman others – nor its being present as an object with its own social, economic, and material histories – explicate its tool-being exhaustively. Instead, it always preserves more than the sum of the relations it has ever built with others throughout its lifetime. So, the mode of existence that describes best this elusive tool-being for him is withdrawing-from-hand.  Generative art toys are noteworthy regarding this ever-switching and withdrawing mode of things on which Harman and other speculative realists focus. In the Procedural Content Generation (PCG) community, the current epicentre of generative art toys, which consists of videogame developers and researchers, these software applications are repurposed from the development tools they aim to popularise through this toy conversion. More importantly, procedural algorithms are not ordinary tools ready to be an extension of a developer’s hands, just as traditional level design tools follow Ivan Suntherland’s 1963 Sketchpad archetype. Rather, procedural generation is an autopoietic process through which the algorithm organises its own representation of the world from recursively generated geographies, characters, events, and other stuff. And this representation does not need to be a truthful interpretation of its environments, which are no other than generation parameters and other input data from the developer. Indeed, they “have only a triggering role in the release of the internally-determined activity” of content generation. The representation it generates suffices to be just “structurally coupled” with these developer-generated data (Hayles 136, 138). In other words, procedural algorithms do not break down to be felt present-at-hand because they always feel as though their operations are closed against their environments-developers. Furthermore, considered as the solution to the ever-increasing demand for the more expansive and interactive sandbox design of videogames, they not only promise developers unlimited regeneration of content for another project but promise players a virtual reality, which constantly changes its shape while always appearing perfectly coupled with different decisions made by avatars, and thus promise unlimited replayability of the videogame. So, it is a common feeling of playing a videogame with procedurally generated content or a story that evolves in real time that something is constantly withdrawing from the things the player just grasped. (The most vicious way to exploit this gamer feeling would be the in-game sale of procedurally generated items, such as weapons with many re-combinable parts, instead of the notorious loot-box that sells a random item from the box, but with the same effect of leading gamers to a gambling addiction by letting them believe there is still something more.) In this respect, it is not surprising that Harman terms his object-oriented ontology after object-oriented programming in computer science. Both look for an inexhaustible resource for the creative generation of the universe and algorithmic systems from the objects infinitely relatable to one another thanks ironically to the secret inner realities they enclose against each other. Fig. 1: Kate Compton, Idle Hands. http://galaxykate.com/apps/idlehands/ However, the toy-being of the algorithms, which I rediscover from the PCG community’s playful conversion of their development tools and which Harman could not pay due attention to while holding on to the self-identical tool-being, is another mode of existence that all tools, or all things before they were instrumentalised, including even the hammer, had used to be in children’s hands. For instance, in Kate Compton’s generative art toy Idle Hands (fig. 1), what a player experiences is her hand avatar, every finger and joint of which is infinitely extended into the space, even as they also serve as lines into which the space is infinitely folded. So, as the player clenches and unclenches her physical hands, scanned in real-time by the motion tracking device Leapmotion, and interpreted into linear input for the generation algorithm, the space is constantly folded and refolded everywhere even by the tiniest movement of a single joint. There is nothing for her hands to grasp onto because nothing is ready to respond consistently to her repeated hand gestures. It is almost impossible to replicate the exact same gesture but, even if she does, the way the surrounding area is folded by this would be always unpredictable. Put differently, in this generative art toy, the player cannot functionally close her sensorimotor activity. This is not so much because of the lack of response, but because it is Compton’s intention to render the whole “fields of the performer” as hyperresponsive to “a body in motion” as if “the dancer wades through water or smoke or tall grass, if they disturb [the] curtain as they move” (Compton and Mateas). At the same time, the constant re-generation of the space as a manifold is no longer felt like an autonomous self-creation of the machine but arouses the feeling that “all of these phenomena ‘listen’ to the movement of the [hands] and respond in some way” (Compton and Mateas). Let me call this fourth mode of things, neither ready-to-hand nor present-at-hand, nor withdrawing-from-hand, but sticky-to-hand: describing a thing’s toy-being. This is so entangled with the hands that its response to our grasp is felt immediately, on every surface and joint, so that it is impossible to anticipate exactly how it would respond to further grasping or releasing. It is a typical feeling of the hand toying with a chunk of clay or slime. It characterises the hypersensitivity of the autistic perception that some neurodiverse people may have, even to ordinary tools, not because they have closed their minds against the world as the common misunderstanding says, but because even the tiniest pulsations that things exert to their moving bodies are too overwhelming to be functionally integrated into their habitual sensorimotor activities let alone to be unentangled as present-at-hand (Manning). In other words, whereas Heideggerian tool-being, for Harman, draws our attention to the things outside of our instrumentalist concern, their toyfication puts the things that were once under our grip back into our somewhat animistic interests of childhood. If our agency as tool-users presupposes our body’s optimal grip on the world that Hubert Dreyfus defines as “the body’s tendency to refine its responses so as to bring the current situation closer to an optimal gestalt” (367), our becoming toy-players is when we feel everything is responsive to each other until that responsiveness is trivialised as the functional inputs for habitual activities. We all once felt things like these animistic others, before we were trained to be tool-users, and we may consequently recall a forgotten genealogy of toy-being in the humanities. This genealogy may begin with a cotton reel in Freud’s fort-da game, while also including such things as jubilant mirror doubles and their toy projections in Lacanian psychoanalysis, various playthings in Piaget’s development theory, and all non-tool-beings in Merleau-Ponty’s phenomenology. To trace this genealogy is not this article’s goal but the family resemblance that groups these things under the term toy-being is noteworthy. First, they all pertain to a person’s individuation processes at different stages, whether it be for the symbolic and tactile re-staging of a baby’s separation from her mother, her formation of a unified self-image from the movements of different body parts, the child’s organisation of object concepts from tactile and visual feedbacks of touching and manipulating hands, the subsequent “projection of such ‘symbolic schemas’” as social norms, as Barbie’s and Ken’s, onto these objects (Piaget 165-166), or a re-doing of all these developmental processes through aesthetic assimilation of objects as the flesh of the worlds (Merleau-Ponty). And these individuations through toys seem to approach the zero-degree of human cognition in which a body (either human or nonhuman) is no other than a set of loosely interconnected sensors and motors. In this zero-degree, the body’s perception or optimal grip on things is achieved as the ways each thing responds to the body’s motor activities are registered on its sensors as something retraceable, repeatable, and thus graspable. In other words, there is no predefined subject/object boundary here but just multiplicities of actions and sensations until a group of sensors and motors are folded together to assemble a reflex arc, or what Merleau-Ponty calls intention arc (Dreyfus), or what I term sensor-actuator arc in current smart spaces (Ahn). And it is when some groups of sensations are distinguished as those consistently correlated with and thus retraceable by certain operations of the body that this fold creates an optimal grip on the rest of the field. Let me call this enfolding of the multiplicities whereby “the marking of the ‘measuring agencies’ by the ‘measured object’” emerges prior to the interaction between two, following Karen Barad, intra-action (177). Contrary to the experience of tool-being present-at-hand as no longer consistently contributing to our habitually formed reflex arc of hammering or to any socially constructed measuring agencies for normative behaviors of things, what we experience with this toy-being sticky-to-hand is our bodies’ folding into the multiplicities of actions and sensations, to discover yet unexplored boundaries and grasping between our bodies and the flesh of the world. Generative Art Toys as the Machine Learning’s Daydream  Then, can I say even the feeling I have on my hands while I am folding and refolding the slime is intra-action? I truly think so, but the multiplicities in this case are so sticky. They join to every surface of my hands whereas the motility under my conscious control is restricted only to several joints of my fingers. The real-life multiplicities unfolded from toying with the slime are too overwhelming to be relatable to my actions with the restricted degree of freedom. On the other hand, in Compton’s Idle Hands, thanks to the manifold generated procedurally in virtual reality, a player experiences these multiplicities so neatly entangled with all the joints on the avatar hands. Rather than simulating a meaty body enfolded within “water or smoke or tall grass,” or the flesh of the world, the physical hands scanned by Leapmotion and abstracted into “3D vector positions for all finger joints” are embedded in the paper-like virtual space of Idle Hands (Compton and Mateas). And rather than delineating a boundary of the controlling hands, they are just the joints on this immanent plane, through which it is folded into itself in so many fantastic ways impossible on a sheet of paper in Euclidean geometry.  Another toy relative which Idle Hands reminds us of is, in this respect, Cat’s Cradle (fig. 2). This play of folding a string entangled around the fingers into itself over and over again to unfold each new pattern is, for Donna Haraway, a metaphor for our creative cohabitation of the world with nonhuman others. Feeling the tension the fingers exchange with each other across the string is thus, for her, compared to “our task” in the Anthropocene “to make trouble, to stir up potent response to devastating events, as well as to settle troubled waters and rebuild quiet places” (Haraway 1). Fig. 2: Nasser Mufti, Multispecies Cat's Cradle, 2011. https://www.kit.ntnu.no/sites/www.kit.ntnu.no/files/39a8af529d52b3c35ded2aa1b5b0cb0013806720.jpg In the alternative, in Idle Hands, each new pattern is easily unfolded even from idle and careless finger movements without any troubled feeling, because its procedural generation is to guarantee that every second of the player’s engagement is productive and wasteless relation-making. In Compton’s terms, the pleasure of generative art toys is relevant to the players’ decision to trade the control they once enjoyed as tool users for power. And this tricky kind of power that the players are supposed to experience is not because of their strong grip, but because they give up this strong grip. It is explicable as the experience of being re-embedded as a fold within this intra-active field of procedural generation: the feeling that even seemingly purposeless activities can make new agential cuts as the triggers for some artistic creations (“Generative Art Toys” 164-165), even though none of these creations are graspable or traceable by the players.  The procedural algorithm as the new toy-being is, therefore, distinguishable from its non-digital toy relatives by this easy feeling of engagement that all generated patterns are wastelessly correlated with the players’ sensorimotor activities in some ungraspable ways. And given the machine learning community’s current interest in procedural generation as the method to “create more training data or training situations” and “to facilitate the transfer of policies trained in a simulator to the real world” (Risi and Togelius 428, 430), the pleasure of generative art toys can be interpreted as revealing the ideal picture of the mixed-reality dreamed of by machine learning algorithms. As the solution to circumvent the issue of data privacy in surveillance capitalism, and to augment the lack of diversity in existing training data, the procedurally generated synthetic data are now considered as the new benchmarks for machine learning instead of those sampled from the real world. This is not just about a game-like object for a robot to handle, or geographies of fictional terrains for a smart vehicle to navigate (Risi and Togelius), but is more about “little procedural people” (“Little Procedural People”), “synthetic data for banking, insurance, and telecommunications companies” (Steinhoff 8). In the near future, as the AIs trained solely on these synthetic data begin to guide our everyday decision-making, the mixed-reality will thus be more than just a virtual layer of the Internet superimposed on the real world but haunted by so many procedurally generated places, things, and people. Compared to the real world, still too sticky like slime, machine learning could achieve an optimal grip on this virtual layer because things are already generated there under the assumption that they are all entangled with one another by some as yet unknown correlations that machine learning is supposed to unfold. Then the question recalled by this future scenario of machine learning would be again Philip K. Dick’s: Do the machines dream of (procedurally generated) electronic sheep? Do they rather dream of this easy wish fulfillment in place of playing an arduous Cat’s Cradle with humans to discover more patterns to commodify between what our eyes attend to and what our fingers drag and click?  Incarnated into toy-like gadgets on mobile devices, machine learning algorithms relocate their users to the zero-degree of social profiles, which is no other than yet-unstructured personal data supposedly responsive to (and responsible for regenerating) invisible arcs, or correlations, between things they watch and things they click. In the meanwhile, what the generative art toys really generate might be the self-fulfilling hope of the software industry that machines could generate their mixed-reality, so neatly and wastelessly engaged with the idle hands of human users, the dream of electronic sheep under the maximal grip of Android (as well as iOS). References Ahn, Sungyong. “Stream Your Brain! Speculative Economy of the IoT and Its Pan-Kinetic Dataveillance.” Big Data &amp; Society 8.2 (2021). Barad, Karen. Meeting the Universe Halfway: Quantum Physics and the Entanglement of Matter and Meaning. Durham: Duke UP, 2007. Compton, Kate. “Generative Art Toys.” Procedural Generation in Game Design, eds. Tanya Short and Tarn Adams. New York: CRC Press, 2017. 161-173. Compton, Kate. “Little Procedural People: Playing Politics with Generators.” Proceedings of the 12th International Conference on the Foundations of Digital Games, eds. Alessandro Canossa, Casper Harteveld, Jichen Zhu, Miguel Sicart, and Sebastian Deterding. New York: ACM, 2017. Compton, Kate, and Michael Mateas. “Freedom of Movement: Generative Responses to Motion Control.” CEUR Workshop Proceedings, 2282, ed. Jichen Zhu. Aachen: CEUR-WS, 2018. Dreyfus, Hubert L. “Intelligence without Representation: Merleau-Ponty’s Critique of Mental Representation.” Phenomenology and the Cognitive Sciences 1 (2002): 367-383. Galanter, Philip. “Generative Art Theory.” A Companion to Digital Art, ed. Christiane Paul. Hoboken, NJ: Wiley-Blackwell, 2016. 146-180. Haraway, Donna J. Staying with the Trouble: Making Kin in the Chthulucene. Durham: Duke UP, 2016. Harman, Graham. Tool-Being: Heidegger and the Metaphysics of Objects. Chicago: Open Court, 2002. ———. “The Well-Wrought Broken Hammer: Object-Oriented Literary Criticism.” New Literary History 43 (2012): 183-203. Hayles, Katherine N. How We Become Posthuman: Virtual Bodies in Cybernetics, Literatures, and Informatics. Chicago: U of Chicago P, 1999. Manning, Erin. The Minor Gesture. Durham: Duke UP, 2016. Merleau-Ponty, Maurice. The Visible and the Invisible. Ed. Claude Lefort. Trans. Alphonso Lingis. Evanston, IL: Northwestern UP, 1968. Peirce, Charles S. “How to Make Our Ideas Clear.” Popular Science Monthly 12 (1878): 286-302. Piaget, Jean. Play, Dreams and Imitation in Childhood. Trans. C. Gattegno and F.M. Hodgson. New York: W.W. Norton, 1962. Risi, Sebastian, and Julian Togelius. “Increasing Generality in Machine Learning through Procedural Content Generation.” Nature Machine Intelligence 2 (2020): 428-436. Steinhoff, James. “Toward a Political Economy of Synthetic Data: A Data-Intensive Capitalism That Is Not a Surveillance Capitalism?” New Media and Society, 2022.","",""
"2023","Algorithmic Divination:           From Prediction to           Preemption of the Future","abstract:Predictive algorithms today share more than just semantics with the divinatory practices of the past. This article will map the parallels, contending that the similarities between the two practices are true """"propositions"""" that radically question the way we apprehend the world, the way we draw our knowledge from it, and the way we then act within and upon it. Mindful of the limitations of such a comparative method, it will nevertheless attempt it by deploying a twofold approach. On the one hand, the article questions the epistemological nature of predictive analytics and examines their truth claims with regard to how they represent the future. On the other hand, it focuses on the ontological dimension of predictive analytics and investigates how they shape the world by bringing about the presence of the future in the here and now.","",""
"2023","Navigating the AI frontier: European parliamentary insights on bias and regulation, preceding the AI Act","","",""
"2023","ChatGPT and the AI Act","","",""
"2023","From access and transparency to refusal: Three responses to algorithmic governance","","",""
"2023","Preventing long-term risks to human rights in smart cities: A critical review of responsibilities for private AI developers",": Privately developed artificial intelligence (AI) systems are frequently used in smart city technologies. The negative effects of such systems on individuals’ human rights are increasingly clear, but we still only have a snapshot of their long-term risks to human rights. The central role of AI businesses in smart cities places them in a key position to identify, prevent and mitigate risks posed by smart city AI systems. The question arises as to how such preventive responsibilities are articulated in international and European governance initiatives on AI and corporate responsibility, respectively. This paper addresses the questions regarding: (1) the Organization for Economic Cooperation and Development’s ‘Business and Finance Outlook 2021: AI in Business and Finance’; (2) the EU’s proposed ‘AI Act’; and (3) the EU’s ‘Proposal for a Directive on corporate sustainability due diligence’. The paper first discusses the role of private AI developers in smart cities and the relevant limitations of applicable legal frameworks (section 1). Section","",""
"2023","How news media frame data risks in their coverage of big data and AI","","",""
"2023","Governing artificial intelligence in the media and communications sector","","",""
"2023","Artificial intelligence regulation in the United Kingdom: a path to good governance and global leadership?","","",""
"2023","Nodes of certainty and spaces for doubt in AI ethics for engineers","ABSTRACT Discussions about AI development frequently bring up the question of ethics because it is difficult to predict how technological decisions might play out once AI systems are implemented and used in the world. Engineers of AI systems are increasingly expected to go beyond the traditions of requirement specifications, taking into account broader societal contexts and their complexities. In this paper we present findings from a hackathon event conducted with working engineers, exploring the gaps between existing guidelines and recommendations for addressing ethical issues with respect to AI technologies and the realities experienced by the engineers in practice. We found that when faced with the uncertainties of how to recognize and navigate ethical issues and challenges, engineers looked to identify the responsibilities that need to be in place to sustain trust and to hold the relevant parties to account for their misdeeds. We re-envision familiar engineering practices as nodes of certainty to accommodate the needs of responsible and ethical AI. Despite the desire for mechanisms for sustaining certainty in how to build AI technology responsibly by providing frameworks for action and accountability, there remains a need to ensure just enough spaces and opportunities to cultivate reasonable doubt. Space and capacity to doubt accepted certainties, in fact, is the very process of ethics, necessary for holding to account our standards, guidelines, and checklists as technology and society co-evolve.","",""
"2023","Finding meaning in crowdwork: An analysis of algorithmic management, work characteristics, and meaningfulness","Abstract In this study we investigate the implications of different aspects of algorithmic coordination and algorithmic quantification for perceived work conditions and the meaningfulness of crowdwork. Using survey data obtained from 412 crowdworkers, our analysis shows that work conditions and the meaningfulness of work are impacted differently by algorithmic coordination and the feeling of being quantified by an algorithm. Specifically, it shows that algorithmic coordination has either a positive or null impact on perceived work conditions and meaningfulness of work. However, negative associations between algorithmic quantification and perceived work conditions, suggest that the algorithmic quantification seems particularly problematic for crowdworkers’ experienced work conditions. Furthermore, algorithmic coordination is positively associated with the meaningfulness of work, while algorithmic quantification is negatively associated with the perceived meaningfulness of work. Using work design theory, the findings also provide insights into the mechanisms explaining these relationships.","",""
"2023","Governing artificial intelligence in China and the European Union: Comparing aims and promoting ethical outcomes","Abstract In this article, we compare the artificial intelligence strategies of China and the European Union, assessing the key similarities and differences regarding what the high-level aims of each governance strategy are, how the development and use of AI is promoted in the public and private sectors, and whom these policies are meant to benefit. We characterize China’s strategy by its primary focus on fostering innovation and a more recent emphasis on “common prosperity,” and the EU’s on promoting ethical outcomes through protecting fundamental rights. Building on this comparative analysis, we consider the areas where the EU and China could learn from and improve upon each other’s approaches to AI governance to promote more ethical outcomes. We outline policy recommendations for both European and Chinese policymakers that would support them in achieving this aim.","",""
"2023","Ecological Modelling: Applying Computational Linguistic Analysis to the UN Secretary-General’s Speeches on Climate Change (2018–2022)","The present study analyzes the UN Secretary-General&amp;rsquo;s speeches on climate change to investigate latent topics. The study aimed to sum up the challenges and strategies proposed by the UN. The addresses, delivered from 2018 to 2022, were retrieved from the official website of the UN. A computational technique named Latent Dirichlet Allocations (LDA) was applied to uncover the hidden topics from the corpus. The present study underpinned Computational Grounded Theory (CGT) as the theoretical framework for the analysis. The results revealed multiple topics such as renewable energy, the effects of climate change, proposed action plan, climate change disasters, mitigation strategies, and global food insecurity. The study is significant in the sense that it provides insightful directions to overcome the threat of climate change. &amp;nbsp; La pr&amp;eacute;sente &amp;eacute;tude analyse les discours du Secr&amp;eacute;taire g&amp;eacute;n&amp;eacute;ral des Nations unies sur le changement climatique afin d'&amp;eacute;tudier les sujets latents. L'&amp;eacute;tude vise &amp;agrave; r&amp;eacute;sumer les d&amp;eacute;fis et les strat&amp;eacute;gies propos&amp;eacute;s par l'ONU. Les discours, prononc&amp;eacute;s entre 2018 et 2022, ont &amp;eacute;t&amp;eacute; r&amp;eacute;cup&amp;eacute;r&amp;eacute;s sur le site officiel de l'ONU. Une technique informatique appel&amp;eacute;e Allocation de Dirichlet Latent (en anglais Latent Dirichtlet Allocations ou LDA) a &amp;eacute;t&amp;eacute; appliqu&amp;eacute;e pour d&amp;eacute;couvrir les sujets cach&amp;eacute;s du corpus. La pr&amp;eacute;sente &amp;eacute;tude s'appuie sur la th&amp;eacute;orie computationnelle ancr&amp;eacute;e (Computational Grounded Theory ou CGT) en tant que cadre th&amp;eacute;orique pour l'analyse. Les r&amp;eacute;sultats ont r&amp;eacute;v&amp;eacute;l&amp;eacute; de nombreux sujets tels que les &amp;eacute;nergies renouvelables, les effets du changement climatique, le plan d'action propos&amp;eacute;, les catastrophes li&amp;eacute;es au changement climatique, les strat&amp;eacute;gies d'att&amp;eacute;nuation et l'ins&amp;eacute;curit&amp;eacute; alimentaire mondiale. L'&amp;eacute;tude est significative dans le sens o&amp;ugrave; elle fournit des orientations perspicaces pour surmonter la menace du changement climatique.","",""
"2023","Alignement sémantique et manque de données : l’apport des modèles de langue. Le cas du latin et du grec","Le présent article vise à proposer aux spécialistes en sciences humaines et sociales, en particulier à ceux qui traitent de corpus disposant de peu de données, des méthodes récemment développées en apprentissage machine, spécifiquement pour les besoins des sciences humaines. Nous nous attachons spécifiquement à la création d'espaces sémantiques vectoriels pour les langues anciennes.This article aims to provide humanists and social scientists, particularly those dealing with low-resource&amp;nbsp; corpora, with recently developed machine learning methods specifically for the humanities. We focus specifically on the creation of vector semantic spaces for ancient languages.","",""
"2023","Algorithmic power and African indigenous languages: search engine autocomplete and the global multilingual Internet"," Predictive language technologies – such as Google Search’s Autocomplete – constitute forms of algorithmic power that reflect and compound global power imbalances between Western technology companies and multilingual Internet users in the global South. Increasing attention is being paid to predictive language technologies and their impacts on individual users and public discourse. However, there is a lack of scholarship on how such technologies interact with African languages. Addressing this gap, the article presents data from experimentation with autocomplete predictions/suggestions for gendered or politicised keywords in Amharic, Kiswahili and Somali. It demonstrates that autocomplete functions for these languages and how users may be exposed to harmful content due to an apparent lack of filtering of problematic ‘predictions’. Drawing on debates on algorithmic power and digital colonialism, the article demonstrates that global power imbalances manifest here not through a lack of online African indigenous language content, but rather in regard to the moderation of content across diverse cultural and linguistic contexts. This raises dilemmas for actors invested in the multilingual Internet between risks of digital surveillance and effective platform oversight, which could prevent algorithmic harms to users engaging with platforms in a myriad of languages and diverse socio-cultural and political environments. ","",""
"2023","A new algorithmic imaginary"," The algorithmic imaginary as a theoretical concept has received increasing attention in recent years as it aims at users’ appropriation of algorithmic processes operating in opacity. But the concept originally only starts from the users’ point of view, while the processes on the platforms’ side are largely left out. In contrast, this paper argues that what is true for users is also valid for algorithmic processes and the designers behind. On the one hand, the algorithm imagines users’ future behavior via machine learning, which is supposed to predict all their future actions. On the other hand, the designers anticipate different actions that could potentially performed by users with every new implementation of features such as social media feeds. In order to bring into view this permanently reciprocal interplay coupled to the imaginary, in which not only the users are involved, I will argue for a more comprehensive and theoretically precise algorithmic imaginary referring to the theory of Cornelius Castoriadis. In such a perspective, an important contribution can be formulated for a theory of social media platforms that goes beyond praxeocentrism or structural determinism. ","",""
"2023","Cyborg Cooks: Mothers and the Anthropology of Smart Kitchens","","",""
"2023","Frustration Free: How Alexa Orchestrates the Development of the Smart Home","","",""
"2023","REAL HARASSMENT, VIRTUAL ROBOTS? IMPLICATIONS OF ONLINE HARASSMENT GEARED AT VIRTUAL ASSISTANT BIA","The purpose of this article is to investigate emerging themes in YouTube's comments on the """"BIA's New Responses Against Harassment"""" video campaign. In the video, Bradesco bank presents less subservient responses to verbal attacks and harassment frequently sent to the brand's virtual assistant, BIA. However, the comments in the video suggest a negative reception of the campaign by the audience, which can help illuminate broader discussions on social and symbolic asymmetries in feminized work rendered invisible by technology. The comments help us to reflect on understandings of what constitutes the human in artificial intelligence and how does the harassment of this virtual chatbot relates to the harassment suffered by women in virtual settings. To critically analyze these texts, we have taken a semiotic analysis of the audiovisual campaign based on Peircean semiotics of extraction and conducted a qualitative content analysis on 200 comments with the highest number of likes on the official Youtube video of the campaign, collected through Youtube Data Tools. Grounding our article in feminist technology studies and critical algorithmic research, we show how the audience reflects on the contradictory understandings of the humans rendered invisible in technology and that changing the materiality/code of technology is not enough to affect systems of gendered oppression.","",""
"2023","DESIGNING ETHICAL ARTIFICIAL INTELLIGENCE (AI) SYSTEMS WITH MEANINGFUL YOUTH PARTICIPATION: IMPLICATIONS AND CONSIDERATIONS","While artificial intelligence (AI) enabled systems have shown impressive accuracy in detecting harmful content online, they are still not perfect and do not take into account the perspective of children in their design. The development of AI systems heavily relies on large datasets for training, and creating such datasets involves annotating vast amounts of data. Studies that involve children in dataset development also have their challenges, such as the possibility of re-traumatisation. Therefore, ethical considerations must be taken into account, such as obtaining informed consent, conducting design sessions with children and young people, and addressing implicit and explicit biases in AI filtering, profiling, and surveillance systems. It is crucial to involve children and young people in the design of AI systems that filter content to ensure ethical considerations are met. In this article we discuss the ethical concerns in AI development with children and young people, and also possible techniques that help mitigate such concerns.","",""
"2023","AI COMPETITIONS AS INFRASTRUCTURES: EXAMINING POWER RELATIONS ON KAGGLE AND GRAND CHALLENGE IN AI-DRIVEN MEDICAL IMAGING","Artificial Intelligence (AI) is quickly being taken-up across scientific disciplines, medical imaging is no exception. To stimulate development and facilitate the scientific evaluation of new approaches, AI-based research in medical imaging is increasingly organised in a competitive manner through digital machine-learning development platforms such as Kaggle and Grand Challenge—two of the leading platforms in the field. For medical image analysis, such competitions constitute an important research infrastructure, steering global research and development in this dedicated AI subfield. Yet, little is known about how these platform-based infrastructures that operate across the medical AI research pipeline shape the conditions for model production and evaluation. This paper addresses this issue through a critical empirical case study of 120 medical imaging competitions on Kaggle and Grand Challenge between 2017 and 2022. We show that platforms as well as competition organisers shape power relations in medical AI research at the level of data and task design, model production and evaluation in several distinct ways. Taken together, because competitions play a central rol within the field, these findings highlight the impact these powerful actors ultimately have on steering medical image AI research directions as they see fit and influence the types of models that are implemented into clinical settings.","",""
"2023","WHERE IN SOCIETY WILL AI AGENTS FIT? A PROPOSED FRAMEWORK FOR UNDERSTANDING ATTITUDES TOWARD AI OCCUPATIONAL ROLES FROM THEORETICAL PERSPECTIVES OF STATUS, IDENTITY, AND ONTOLOGY","To better understand what drives the public’s perception and acceptance of AI in different roles, we propose a study that looks at varying AI domains by occupational status and individual differences across ontological perceptions, automation anxiety, perceived status, and identity threat. As a first step, we conducted a representative survey of the US population (N = 1,005) that looked into the public's perceptions of AI replacement of high-status jobs. Results indicate that a majority of participants hold negative attitudes about AI replacement in all domains presented. However, participants were more open to AI replacement in lower-status roles such as journalist and hiring manager compared to higher-status roles of spiritual leader and trial judge. Contrary to our expectations, participants believed that trial judge was a slightly worse idea than AI spiritual advisor. This finding suggests that the associated machine heuristic of the judge role as being a more rational and objective occupation was not triggered in our sample. Our results also suggest that more vulnerable populations are more reluctant to accept AI in the majority of jobs. These findings are in line with previous public opinion surveys and demonstrate that individuals with lower levels of power and status are more likely to be reluctant to accept new technology and potentially perceive it as a threat. Our next step will be to include more occupations that can be potentially automated and look for explanatory mechanisms driving the public’s view of AI integration.","",""
"2023","ALGORITHMIC RESISTANCE IN EUROPE AND THE QUESTION OF COLLECTIVE AGENCY","One of the key critical questions of our times is the understanding of how data, algorithmic logics and AI are altering our agency as citizens, and what forms of resistance and opposition are emerging on the ground. Some scholars have shown how algorithms both prohibit as well as aid activists’ agency (Milan, 2018; Treré, 2019; Etter and Albu, 2021), while others have focused on how individual activists challenge the power of algorithms and reclaim a sense of agency (Velkova and Kaun, 2021). Although over the last years much research has emerged on the civic and human rights implications of automated systems and algorithmic profiling, little attention has been paid to what was happening on the ground and how social movement actors were actively organising against algorithmic profiling.  In this paper, we draw on ten semi-structured interviews with actors from civil society or non-governmental organisations in Europe, who struggle against algorithmic decision making, and combine these findings with a qualitative textual analysis of ten campaign websites, both conducted in the context of a wider research project exploring the conflict over algorithmic profiling. The aim of the paper is to shed light on the discourses and practices of social movement actors when it comes to resisting algorithmic injustices. The key question that we are interested in addressing is to explore how – in the everyday resistance to algorithmic profiling – actors rely on three different strategies of action (algorithmic literacy, algorithmic regulation and algorithmic accountability). ","",""
"2023","‘SORT BY RELEVANCE’ – WHOSE RELEVANCE? A CRITICAL EXAMINATION OF ALGORITHM-MEDIATED ACADEMIC LITERATURE SEARCHES","The academic literature has arguably never been more accessible. Through the internet, academics - and the wider public - are presented with an increasing array of platforms which offer sources of academic literature and information sources. Large-scale platforms – such as Google Scholar – often utilise algorithms in order to manage how search query results are prioritised and presented to searchers. This ‘sorting by relevance’ introduces an opaque layer to how readers engage with the literature, with potentially important implications for academic rigour and equity. Academic publishing and citation practices often serve to preserve privilege; in turn, there is a risk that algorithms which draw upon data sources such as these will compound biases. There is a need to interrogate how such algorithms work, and academics’ assumptions about the mechanisms behind them. This paper starts a critical discussion of the issue of algorithm-mediated academic literature searches. First, it draws upon the existing literature – primarily related to Google Scholar. This is followed by a mapping of the use of algorithms across a sample of major academic databases. Algorithms are found to be used extensively, although how they operate varies and is often not clear. The paper concludes with next steps for this project. ","",""
"2023","WEIZENBAUM'S PERFORMANCE AND THEORY MODES: LESSONS FOR CRITICAL ENGAGEMENT WITH LARGE LANGUAGE MODEL CHATBOTS","In 1976, Joseph Weizenbaum argued that, because “[t]he achievements of the artificial intelligentsia [were] mainly triumphs of technique,” AI had not “contributed” to theory or “practical problem solving.” Weizenbaum highlighted the celebration of performance without deeper understanding, and in response, he articulated a theory mode for AI that could cultivate human responsibility and judgment. We suggest that, given access to Large Language Model (LLM) chatbots, Weizenbaum’s performance and theory modes offer urgently-needed vocabulary for public discourse about AI. Working from the perspective of digital rhetoric, we explain Weizenbaum’s theorization of each mode and perform a close textual analysis of two case studies of Open AI’s ChatGPT shared on Twitter to illustrate the contemporary relevance of his modes. We conclude by forecasting how theory mode may inform public accountability of AI.","",""
"2023","BIG AI: THE CLOUD AS MARKETPLACE AND INFRASTRUCTURE","Cloud infrastructure platforms underpin most of today’s internet, functioning as the operating system of the internet and representing the most important source of revenue for Big Tech companies. While most of the current hype around (“generative”) AI is focused on specific successful products and initiatives like OpenAI (ChatGPT, DALL·E 2) and Stability AI (Stable Diffusion), they would not have been possible without the significant infrastructural support and investments from Big Tech companies. This paper critically examines what we call Big AI, or those types and deployments of AI that simply would not be feasible or even possible without the infrastructural support, partnerships, or investments provided by Big Tech companies. To account for this, we articulate the key components of AI and how they are connected. By focusing on Big Tech’s own products and service offerings, third-party applications, and models, we gain a more comprehensive understanding of what Big AI is, or looks like today, and what it may become in the years to come—for which the infrastructure is being made right now. Further, we make a distinction between the cloud platform products and service offerings from Big Tech (i.e. the cloud as an infrastructure for AI) and Big Tech as the host or provided of marketplaces for diverse (AI-based) products and services from third-party businesses and developers (i.e. the cloud as a marketplace for AI). Overall, this research provides the basis for a better understanding of the critical political economy of (Big) AI.","",""
"2023","REAL HARASSMENT, VIRTUAL ROBOTS?  IMPLICATIONS OF ONLINE HARASSMENT GEARED AT VIRTUAL ASSISTANT BIA","The purpose of this article is to investigate emerging themes in YouTube's comments on the """"BIA's New Responses Against Harassment"""" video campaign. In the video, Bradesco bank presents less subservient responses to verbal attacks and harassment frequently sent to the brand's virtual assistant, BIA. However, the comments in the video suggest a negative reception of the campaign by the audience, which can help illuminate broader discussions on social and symbolic asymmetries in feminized work rendered invisible by technology. The comments help us to reflect on understandings of what constitutes the human in artificial intelligence and how does the harassment of this virtual chatbot relates to the harassment suffered by women in virtual settings. To critically analyze these texts, we have taken a semiotic analysis of the audiovisual campaign based on Peircean semiotics of extraction and conducted a qualitative content analysis on 200 comments with the highest number of likes on the official Youtube video of the campaign, collected through Youtube Data Tools. Grounding our article in feminist technology studies and critical algorithmic research, we show how the audience reflects on the contradictory understandings of the humans rendered invisible in technology and that changing the materiality/code of technology is not enough to affect systems of gendered oppression.","",""
"2023","A machine learning approach to recognize bias and discrimination in job advertisements","AbstractIn recent years, the work of organizations in the area of digitization has intensified significantly. This trend is also evident in the field of recruitment where job application tracking systems (ATS) have been developed to allow job advertisements to be published online. However, recent studies have shown that recruiting in most organizations is not inclusive, being subject to human biases and prejudices. Most discrimination activities appear early but subtly in the hiring process, for instance, exclusive phrasing in job advertisement discourages qualified applicants from minority groups from applying. The existing works are limited to analyzing, categorizing and highlighting the occurrence of bias in the recruitment process. In this paper, we go beyond this and develop machine learning models for identifying and classifying biased and discriminatory language in job descriptions. We develop and evaluate a machine learning system for identifying five major categories of biased and discriminatory language in job advertisements, i.e., masculine-coded, feminine-coded, exclusive, LGBTQ-coded, demographic and racial language. We utilized the combination of linguistic features with recent state-of-the-art word embeddings representations as input features for various machine learning classifiers. Our results show that the machine learning classifiers were able to identify all the five categories of biased and discriminatory language with a decent accuracy. The Random Forest classifier with FastText word embeddings achieved the best performance with tenfolds cross-validation. Our system directly addresses the bias in the attraction phase of hiring by identifying and classifying biased and discriminatory language and thus encouraging recruiters to write more inclusive job advertisements.","",""
"2023","Fairness &amp; friends in the data science era","AbstractThe data science era is characterized by data-driven automated decision systems (ADS) enabling, through data analytics and machine learning, automated decisions in many contexts, deeply impacting our lives. As such, their downsides and potential risks are becoming more and more evident: technical solutions, alone, are not sufficient and an interdisciplinary approach is needed. Consequently, ADS should evolve into data-informed ADS, which take humans in the loop in all the data processing steps. Data-informed ADS should deal with data responsibly, guaranteeing nondiscrimination with respect to protected groups of individuals. Nondiscrimination can be characterized in terms of different types of properties, like fairness and diversity. While fairness, i.e., absence of bias against minorities, has been widely investigated in machine learning, only more recently this issue has been tackled by considering all the steps of data processing pipelines at the basis of ADS, from data acquisition to analysis. Additionally, fairness is just one point of view of nondiscrimination to be considered for guaranteeing equity: other issues, like diversity, are raising interest from the scientific community due to their relevance in society. This paper aims at critically surveying how nondiscrimination has been investigated in the context of complex data science pipelines at the basis of data-informed ADS, by focusing on the specific data processing tasks for which nondiscrimination solutions have been proposed.","",""
"2023","Blue collar with tie: a human-centered reformulation of the ironies of automation","AbstractWhen Lisanne Bainbridge wrote about counterintuitive consequences of the increasing human–machine interaction, she concentrated on the resulting issues for system performance, stability, and safety. Now, decades later, however, the automized work environment is substantially more pervasive, sophisticated, and interactive. Current advances in machine learning technologies reshape the value, meaning, and future of the human workforce. While the ‘human factor’ still challenges automation system architects, inconspicuously new ironic settings have evolved that only become distinctly evident from a human-centered perspective. This brief essay discusses the role of the human workforce in human–machine interaction as machine learning continues to improve, and it points to the counterintuitive insight that although the demand for blue-collar workers may decrease, exactly this labor class increasingly enters more privileged working domains and establishes itself henceforth as ‘blue collar with tie.’","",""
"2023","Towards an effective transnational regulation of AI","AbstractLaw and the legal system through which law is effected are very powerful, yet the power of the law has always been limited by the laws of nature, upon which the law has now direct grip. Human law now faces an unprecedented challenge, the emergence of a second limit on its grip, a new “species” of intelligent agents (AI machines) that can perform cognitive tasks that until recently only humans could. What happens, as a matter of law, when another species interacts with us, can be integrated into human minds and bodies, makes “real-world” decisions—not through human proxies, but directly—and does all this “intelligently”, with what one could call autonomous agency or even a “mind” of its own? The article starts from the clear premise that control cannot be exercised directly on AI machines through human law. That control can only be effected through laws that apply to humans. This has several regulatory implications. The article’s first discusses what, in any attempt to regulate AI machines, the law can achieve. Having identified what the law can do, the article then canvases what the law should aim to achieve overall. The article encapsulate its analysis in a list of both doctrinal and normative principles that should underpin any regulation aimed at AI machines. Finally, the article compares three transnational options to implement the proposed regulatory approach.","",""
"2023","Artificial Intelligence/Consciousness: being and becoming John Malkovich","","",""
"2023","Three lines of defense against risks from AI","AbstractOrganizations that develop and deploy artificial intelligence (AI) systems need to manage the associated risks—for economic, legal, and ethical reasons. However, it is not always clear who is responsible for AI risk management. The three lines of defense (3LoD) model, which is considered best practice in many industries, might offer a solution. It is a risk management framework that helps organizations to assign and coordinate risk management roles and responsibilities. In this article, I suggest ways in which AI companies could implement the model. I also discuss how the model could help reduce risks from AI: it could identify and close gaps in risk coverage, increase the effectiveness of risk management practices, and enable the board of directors to oversee management more effectively. The article is intended to inform decision-makers at leading AI companies, regulators, and standard-setting bodies.","",""
"2023","Robot, let us pray! Can and should robots have religious functions? An ethical exploration of religious robots","AbstractConsiderable progress is being made in robotics, with robots being developed for many different areas of life: there are service robots, industrial robots, transport robots, medical robots, household robots, sex robots, exploration robots, military robots, and many more. As robot development advances, an intriguing question arises: should robots also encompass religious functions? Religious robots could be used in religious practices, education, discussions, and ceremonies within religious buildings. This article delves into two pivotal questions, combining perspectives from philosophy and religious studies: can and should robots have religious functions? Section 2 initiates the discourse by introducing and discussing the relationship between robots and religion. The core of the article (developed in Sects. 3 and 4) scrutinizes the fundamental questions: can robots possess religious functions, and should they? After an exhaustive discussion of the arguments, benefits, and potential objections regarding religious robots, Sect. 5 addresses the lingering ethical challenges that demand attention. Section 6 presents a discussion of the findings, outlines the limitations of this study, and ultimately responds to the dual research question. Based on the study’s results, brief criteria for the development and deployment of religious robots are proposed, serving as guidelines for future research. Section 7 concludes by offering insights into the future development of religious robots and potential avenues for further research.","",""
"2023","Is it possible to create a responsible AI technology to be used and understood within workplaces and unblocked CEOs’ mindsets?","","",""
"2023","Artificial intelligence ELSI score for science and technology: a comparison between Japan and the US","AbstractArtificial intelligence (AI) has become indispensable in our lives. The development of a quantitative scale for AI ethics is necessary for a better understanding of public attitudes toward AI research ethics and to advance the discussion on using AI within society. For this study, we developed an AI ethics scale based on AI-specific scenarios. We investigated public attitudes toward AI ethics in Japan and the US using online questionnaires. We designed a test set using four dilemma scenarios and questionnaire items based on a theoretical framework for ethics, legal, and social issues (ELSI). We found that country and age are the most informative sociodemographic categories for predicting attitudes for AI ethics. Our proposed scale, which consists of 13 questions, can be reduced to only three, covering ethics, tradition, and policies. This new AI ethics scale will help to quantify how AI research is accepted in society and which area of ELSI people are most concerned with.","",""
"2023","ChatGPT and digital capitalism: need for an antidote of Competition Law","","",""
"2023","Language agents reduce the risk of existential catastrophe","","",""
"2023","Immune moral models? Pro-social rule breaking as a moral enhancement approach for ethical AI","AbstractWe are moving towards a future where Artificial Intelligence (AI) based agents make many decisions on behalf of humans. From healthcare decision-making to social media censoring, these agents face problems, and make decisions with ethical and societal implications. Ethical behaviour is a critical characteristic that we would like in a human-centric AI. A common observation in human-centric industries, like the service industry and healthcare, is that their professionals tend to break rules, if necessary, for pro-social reasons. This behaviour among humans is defined as pro-social rule breaking. To make AI agents more human-centric, we argue that there is a need for a mechanism that helps AI agents identify when to break rules set by their designers. To understand when AI agents need to break rules, we examine the conditions under which humans break rules for pro-social reasons. In this paper, we present a study that introduces a ‘vaccination strategy dilemma’ to human participants and analyzes their response. In this dilemma, one needs to decide whether they would distribute COVID-19 vaccines only to members of a high-risk group (follow the enforced rule) or, in selected cases, administer the vaccine to a few social influencers (break the rule), which might yield an overall greater benefit to society. The results of the empirical study suggest a relationship between stakeholder utilities and pro-social rule breaking (PSRB), which neither deontological nor utilitarian ethics completely explain. Finally, the paper discusses the design characteristics of an ethical agent capable of PSRB and the future research directions on PSRB in the AI realm. We hope that this will inform the design of future AI agents, and their decision-making behaviour.","",""
"2023","How to dance, robot?","","",""
"2023","Automated decision-making and the problem of evil","","",""
"2023","Reflections on epistemological aspects of artificial intelligence during the COVID-19 pandemic","","",""
"2023","Artificial intelligence is an oxymoron","AbstractDeparting from popular imaginations around artificial intelligence (AI), this article engages in the I in the AI acronym but from perspectives outside of mathematics, computer science and machine learning. When intelligence is attended to here, it most often refers to narrow calculating tasks. This connotation to calculation provides AI an image of scientificity and objectivity, particularly attractive in societies with a pervasive desire for numbers. However, as is increasingly apparent today, when employed in more general areas of our messy socio-cultural realities, AI- powered automated systems often fail or have unintended consequences. This article will contribute to this critique of AI by attending to Nicholas of Cusa and his treatment of intelligence. According to him, intelligence is equally dependent on an ability to handle the unknown as it unfolds in the present moment. This suggests that intelligence isorganicwhich ties Cusa to more contemporary discussions in tech philosophy, neurology, evolutionary biology, and cognitive sciences in which it is argued that intelligence is dependent on having—and acting through—an organic body. Understanding intelligence as organic thus suggests an oxymoronic relationship to artificial.","",""
"2023","What can science fiction tell us about the future of artificial intelligence policy?","AbstractThis paper addresses the gap between familiar popular narratives describing Artificial Intelligence (AI), such as the trope of the killer robot, and the realistic near-future implications of machine intelligence and automation for technology policy and society. The authors conducted a series of interviews with technologists, science fiction writers, and other experts, as well as a workshop, to identify a set of key themes relevant to the near future of AI. In parallel, they led the analysis of almost 100 recent works of science fiction stories with AI themes to develop a preliminary taxonomy of AI in science fiction. These activities informed the commissioning of six original works of science fiction and non-fiction response essays on the themes of “intelligence” and “justice” that were published as part of the Slate Future Tense Fiction series in 2019 and 2020. Our findings indicate that artificial intelligence remains deeply ambiguous both in the policy and cultural contexts: we struggle to define the boundaries and the agency of machine intelligence, and consequently find it difficult to govern or interact with such systems. However, our findings also suggest more productive avenues of inquiry and framing that could foster both better policy and better narratives around AI.","",""
"2023","Responsibility of AI Systems","AbstractTo support the trustworthiness of AI systems, it is essential to have precise methods to determine what or who is to account for the behaviour, or the outcome, of AI systems. The assignment of responsibility to an AI system is closely related to the identification of individuals or elements that have caused the outcome of the AI system. In this work, we present an overview of approaches that aim at modelling responsibility of AI systems, discuss their advantages and shortcomings to deal with various aspects of the notion of responsibility, and present research gaps and ways forward.","",""
"2023","Artificial agents’ explainability to support trust: considerations on timing and context","AbstractStrategies for improving the explainability of artificial agents are a key approach to support the understandability of artificial agents’ decision-making processes and their trustworthiness. However, since explanations are not inclined to standardization, finding solutions that fit the algorithmic-based decision-making processes of artificial agents poses a compelling challenge. This paper addresses the concept of trust in relation to complementary aspects that play a role in interpersonal and human–agent relationships, such as users’ confidence and their perception of artificial agents’ reliability. Particularly, this paper focuses on non-expert users’ perspectives, since users with little technical knowledge are likely to benefit the most from “post-hoc”, everyday explanations. Drawing upon the explainable AI and social sciences literature, this paper investigates how artificial agent’s explainability and trust are interrelated at different stages of an interaction. Specifically, the possibility of implementing explainability as a trust building, trust maintenance and restoration strategy is investigated. To this extent, the paper identifies and discusses the intrinsic limits and fundamental features of explanations, such as structural qualities and communication strategies. Accordingly, this paper contributes to the debate by providing recommendations on how to maximize the effectiveness of explanations for supporting non-expert users’ understanding and trust.","",""
"2023","Bosses without a heart: socio-demographic and cross-cultural determinants of attitude toward Emotional AI in the workplace","","",""
"2023","Artificial intelligence in local governments: perceptions of city managers on prospects, constraints and choices","AbstractHighly sophisticated capabilities of artificial intelligence (AI) have skyrocketed its popularity across many industry sectors globally. The public sector is one of these. Many cities around the world are trying to position themselves as leaders of urban innovation through the development and deployment of AI systems. Likewise, increasing numbers of local government agencies are attempting to utilise AI technologies in their operations to deliver policy and generate efficiencies in highly uncertain and complex urban environments. While the popularity of AI is on the rise in urban policy circles, there is limited understanding and lack of empirical studies on the city manager perceptions concerning urban AI systems. Bridging this gap is the rationale of this study. The methodological approach adopted in this study is twofold. First, the study collects data through semi-structured interviews with city managers from Australia and the US. Then, the study analyses the data using the summative content analysis technique with two data analysis software. The analysis identifies the following themes and generates insights into local government services: AI adoption areas, cautionary areas, challenges, effects, impacts, knowledge basis, plans, preparedness, roadblocks, technologies, deployment timeframes, and usefulness. The study findings inform city managers in their efforts to deploy AI in their local government operations, and offer directions for prospective research.","",""
"2023","Ethical considerations and statistical analysis of industry involvement in machine learning research","AbstractIndustry involvement in the machine learning (ML) community seems to be increasing. However, the quantitative scale and ethical implications of this influence are rather unknown. For this purpose, we have not only carried out an informed ethical analysis of the field, but have inspected all papers of the main ML conferences NeurIPS, CVPR, and ICML of the last 5 years—almost 11,000 papers in total. Our statistical approach focuses on conflicts of interest, innovation, and gender equality. We have obtained four main findings. (1) Academic–corporate collaborations are growing in numbers. At the same time, we found that conflicts of interest are rarely disclosed. (2) Industry papers amply mention terms that relate to particular trending machine learning topics earlier than academia does. (3) Industry papers are not lagging behind academic papers with regard to how often they mention keywords that are proxies for social impact considerations. (4) Finally, we demonstrate that industry papers fall short of their academic counterparts with respect to the ratio of gender diversity. We believe that this work is a starting point for an informed debate within and outside of the ML community.","",""
"2023","AI4People or People4AI? On human adaptation to AI at work","","",""
"2023","Artificial intelligence in marketing: friend or foe of sustainable consumption?","","",""
"2023","Online public discourse on artificial intelligence and ethics in China: context, content, and implications","AbstractThe societal and ethical implications of artificial intelligence (AI) have sparked discussions among academics, policymakers and the public around the world. What has gone unnoticed so far are the likewise vibrant discussions in China. We analyzed a large sample of discussions about AI ethics on two Chinese social media platforms. Findings suggest that participants were diverse, and included scholars, IT industry actors, journalists, and members of the general public. They addressed a broad range of concerns associated with the application of AI in various fields. Some even gave recommendations on how to tackle these issues. We argue that these discussions are a valuable source for understanding the future trajectory of AI development in China as well as implications for global dialogue on AI governance.","",""
"2023","Machine learning and power relations","AbstractThere has been an increased focus within the AI ethics literature on questions of power, reflected in the ideal of accountability supported by many Responsible AI guidelines. While this recent debate points towards the power asymmetry between those who shape AI systems and those affected by them, the literature lacks normative grounding and misses conceptual clarity on how these power dynamics take shape. In this paper, I develop a workable conceptualization of said power dynamics according to Cristiano Castelfranchi’s conceptual framework of power and argue that end-users depend on a system’s developers and users, because end-users rely on these systems to satisfy their goals, constituting a power asymmetry between developers, users and end-users. I ground my analysis in the neo-republican moral wrong of domination, drawing attention to legitimacy concerns of the power-dependence relation following from the current lack of accountability mechanisms. I illustrate my claims on the basis of a risk-prediction machine learning system, and propose institutional (external auditing) and project-specific solutions (increase contestability through design-for-values approaches) to mitigate domination.","",""
"2023","Detecting racial inequalities in criminal justice: towards an equitable deep learning approach for generating and interpreting racial categories using mugshots","","",""
"2023","Correction: On the moral status of social robots: considering the consciousness criterion","","",""
"2023","When stigmatization does not work: over-securitization in efforts of the Campaign to Stop Killer Robots","AbstractThis article reflects on securitization efforts with respect to ‘killer robots’, known more impartially as autonomous weapons systems (AWS). Our contribution focuses, theoretically and empirically, on the Campaign to Stop Killer Robots, a transnational advocacy network vigorously pushing for a pre-emptive ban on AWS. Marking exactly a decade of its activity, there is still no international regime formally banning, or even purposefully regulating, AWS. Our objective is to understand why the Campaign has not been able to advance its disarmament agenda thus far, despite all the resources, means and support at its disposal. For achieving this objective, we challenge the popular assumption that strong stigmatization is the universally best strategy towards humanitarian disarmament. We investigate the consequences of two specifics present in AWS, which set them apart from processes and successes of the campaigns to ban anti-personnel landmines, cluster munitions, and laser-blinding weapons: the complexity of AWS as a distinct weapons category, and the subsequent circumvention of its complexity through the utilization of pop-culture, namely science fiction imagery. We particularly focus on two mechanisms through which such distortion has occurred: hybridization and grafting. These provide the conceptual basis and heuristic tools to unpack the paradox of over-securitization: success in broadening the stakeholder base in relation to the first mechanism and deepening the sense of insecurity in relation to the second one does not necessarily lead to the achievement of the desired prohibitory norm. In conclusion, we ask whether it is not the time for a more epistemically-oriented expert debate with a less ambitious, lowest common denominator strategy as the preferred model of arms control for such a complex weapons category.","",""
"2023","Decision-makers’ attitudes toward the use of care robots in welfare services","AbstractThe purpose of this study was to investigate the attitudes of decision-makers toward the use of care robots in welfare services. We investigated their knowledge regarding the use of care robots in welfare services as well as their attitudes toward using robots in their own care and in the care of various user groups, for example, children, youths, and older people. We conducted an online survey with a range of Finnish decision-makers as respondents (N = 176). The respondents were divided into two groups: service actors (n = 104) and research and development (R&amp;D) actors (n = 72). The respondents did not regard themselves as having much knowledge about robotics; however, the results showed that the R&amp;D actors had more overall knowledge of the use of robots than the service actors. The R&amp;D actors were found to be more willing to accept a robot as part of their own care as well as part of the care for various user groups. The contribution of this study is a better understanding of the views of the decision-makers who are or will be in charge of the acquisition of technological devices in welfare services.","",""
"2023","AI in human teams: effects on technology use, members’ interactions, and creative performance under time scarcity","","",""
"2023","AI for the public. How public interest theory shifts the discourse on AI","AbstractAI for social good is a thriving research topic and a frequently declared goal of AI strategies and regulation. This article investigates the requirements necessary in order for AI to actually serve a public interest, and hence be socially good. The authors propose shifting the focus of the discourse towards democratic governance processes when developing and deploying AI systems. The article draws from the rich history of public interest theory in political philosophy and law, and develops a framework for ‘public interest AI’. The framework consists of (1) public justification for the AI system, (2) an emphasis on equality, (3) deliberation/ co-design process, (4) technical safeguards, and (5) openness to validation. This framework is then applied to two case studies, namely SyRI, the Dutch welfare fraud detection project, and UNICEF’s Project Connect, that maps schools worldwide. Through the analysis of these cases, the authors conclude that public interest is a helpful and practical guide for the development and governance of AI for the people.","",""
"2023","Intelligence at any price? A criterion for defining AI","","",""
"2023","Will AI end privacy? How do we avoid an Orwellian future","","",""
"2023","Ethical problems in the use of algorithms in data management and in a free market economy","AbstractThe problem that I present in this paper concerns the issue of ethical evaluation of algorithms, especially those used in social media and which create profiles of users of these media and new technologies that have recently emerged and are intended to change the functioning of technologies used in data management. Systems such as Overton, SambaNova or Snorkel were created to help engineers create data management models, but they are based on different assumptions than the previous approach in machine learning and deep learning. There is a need to analyze both deep learning algorithms and new technologies in database management in terms of their actions towards a person who leaves their digital footprints, on which these technologies work. Then, the possibilities of applying the existing deep learning technology and new Big Data systems in the economy will be shown. The opportunities offered by the systems mentioned above seem to be promising for many companies and—if implemented on a larger scale—they will affect the functioning of the free market.","",""
"2023","Automated inauthenticity","","",""
"2023","Intelligent service robots for elderly or disabled people and human dignity: legal point of view","","",""
"2023","Evidence-based AI, ethics and the circular economy of knowledge","","",""
"2023","On the ‘nature’ of the ‘artificial’","AbstractSince the work by Herbert Simon, no particular attention has been paid to the distinction between conventional technology and technology directed at the reproduction of natural instances. Nevertheless, if we had a general knowledge of the methodological aspects that any attempt to reproduce natural objects or processes unavoidably requires, then we would understand why, as a rule, no artificial device can ‘converge’ to its natural counterpart and why, on the contrary, the more it advances, the further away it goes from it. As a result, our efforts should be oriented to deeply investigate the artificial as it were a truly new ‘nature’ in itself.","",""
"2023","A qualified defense of top-down approaches in machine ethics","","",""
"2023","Studying human-to-computer bias transference","AbstractIt is generally agreed that one origin of machine bias is resulting from characteristics within the dataset on which the algorithms are trained, i.e., the data does not warrant a generalized inference. We, however, hypothesize that a different ‘mechanism’ may also be responsible for machine bias, namely that biases may originate from (i) the programmers’ cultural background, including education or line of work, or (ii) the contextual programming environment, including software requirements or developer tools. Combining an experimental and comparative design, we study the effects of cultural and contextual metaphors, and test whether each of these are ‘transferred’ from the programmer to the program, thus constituting a machine bias. Our results show that (i) cultural metaphors influence the programmer’s choices and (ii) contextual metaphors induced through priming can be used to moderate or exacerbate the effects of the cultural metaphors. Our studies are purposely performed with users of varying educational backgrounds and programming skills stretching from novice to proficient.","",""
"2023","The problem with trust: on the discursive commodification of trust in AI","AbstractThis commentary draws critical attention to the ongoing commodification of trust in policy and scholarly discourses of artificial intelligence (AI) and society. Based on an assessment of publications discussing the implementation of AI in governmental and private services, our findings indicate that this discursive trend towards commodification is driven by the need for a trusting population of service users to harvest data at scale and leads to the discursive construction of trust as an essential good on a par with data as raw material. This discursive commodification is marked by a decreasing emphasis on trust understood as the expected reliability of a trusted agent, and increased emphasis on instrumental and extractive framings of trust as a resource. This tendency, we argue, does an ultimate disservice to developers, users, and systems alike, insofar as it obscures the subtle mechanisms through which trust in AI systems might be built, making it less likely that it will be.","",""
"2023","AI ethics: from principles to practice","","",""
"2023","Image synthesis from an ethical perspective","AbstractGenerative AI has gained a lot of attention in society, business, and science. This trend has increased since 2018, and the big breakthrough came in 2022. In particular, AI-based text and image generators are now widely used. This raises a variety of ethical issues. The present paper first gives an introduction to generative AI and then to applied ethics in this context. Three specific image generators are presented: DALL-E 2, Stable Diffusion, and Midjourney. The author goes into technical details and basic principles, and compares their similarities and differences. This is followed by an ethical discussion. The paper addresses not only risks, but opportunities for generative AI. A summary with an outlook rounds off the article.","",""
"2023","The fiction of simulation: a critique of Bostrom’s simulation argument","","",""
"2023","Cognitive architectures for artificial intelligence ethics","AbstractAs artificial intelligence (AI) thrives and propagates through modern life, a key question to ask is how to include humans in future AI? Despite human involvement at every stage of the production process from conception and design through to implementation, modern AI is still often criticized for its “black box” characteristics. Sometimes, we do not know what really goes on inside or how and why certain conclusions are met. Future AI will face many dilemmas and ethical issues unforeseen by their creators beyond those commonly discussed (e.g., trolley problems and variants of it) and to which solutions cannot be hard-coded and are often still up for debate. Given the sensitivity of such social and ethical dilemmas and the implications of these for human society at large, when and if our AI make the “wrong” choice we need to understand how they got there in order to make corrections and prevent recurrences. This is particularly true in situations where human livelihoods are at stake (e.g., health, well-being, finance, law) or when major individual or household decisions are taken. Doing so requires opening up the “black box” of AI; especially as they act, interact, and adapt in a human world and how they interact with other AI in this world. In this article, we argue for the application of cognitive architectures for ethical AI. In particular, for their potential contributions to AI transparency, explainability, and accountability. We need to understand how our AI get to the solutions they do, and we should seek to do this on a deeper level in terms of the machine-equivalents of motivations, attitudes, values, and so on. The path to future AI is long and winding but it could arrive faster than we think. In order to harness the positive potential outcomes of AI for humans and society (and avoid the negatives), we need to understand AI more fully in the first place and we expect this will simultaneously contribute towards greater understanding of their human counterparts also.","",""
"2023","Algorithmic fairness through group parities? The case of COMPAS-SAPMOC","AbstractMachine learning classifiers are increasingly used to inform, or even make, decisions significantly affecting human lives. Fairness concerns have spawned a number of contributions aimed at both identifying and addressing unfairness in algorithmic decision-making. This paper critically discusses the adoption of group-parity criteria (e.g., demographic parity, equality of opportunity, treatment equality) as fairness standards. To this end, we evaluate the use of machine learning methods relative to different steps of the decision-making process: assigning a predictive score, linking a classification to the score, and adopting decisions based on the classification. Throughout our inquiry we use the COMPAS system, complemented by a radical simplification of it (our SAPMOC I and SAPMOC II models), as our running examples. Through these examples, we show how a system that is equally accurate for different groups may fail to comply with group-parity standards, owing to different base rates in the population. We discuss the general properties of the statistics determining the satisfaction of group-parity criteria and levels of accuracy. Using the distinction between scoring, classifying, and deciding, we argue that equalisation of classifications/decisions between groups can be achieved thorough group-dependent thresholding. We discuss contexts in which this approach may be meaningful and useful in pursuing policy objectives. We claim that the implementation of group-parity standards should be left to competent human decision-makers, under appropriate scrutiny, since it involves discretionary value-based political choices. Accordingly, predictive systems should be designed in such a way that relevant policy goals can be transparently implemented. Our paper presents three main contributions: (1) it addresses a complex predictive system through the lens of simplified toy models; (2) it argues for selective policy interventions on the different steps of automated decision-making; (3) it points to the limited significance of statistical notions of fairness to achieve social goals.","",""
"2023","A machine learning approach to detecting fraudulent job types","AbstractJob seekers find themselves increasingly duped and misled by fraudulent job advertisements, posing a threat to their privacy, security and well-being. There is a clear need for solutions that can protect innocent job seekers. Existing approaches to detecting fraudulent jobs do not scale well, function like a black-box, and lack interpretability, which is essential to guide applicants’ decision-making. Moreover, commonly used lexical features may be insufficient as the representation does not capture contextual semantics of the underlying document. Hence, this paper explores to what extent different categorizations of fraudulent jobs can be classified. In addition, this paper seeks to find what type of features are most relevant in classifying the type of fraudulent job. In this paper, we develop and validate a machine learning system for identifying identity theft, corporate identity theft and multi-level marketing amongst fraudulent job advertisements. We utilized four classes of features: empirical rule set-based features, bag-of-word models, most recent state-of-the-art word embeddings and transformer models for various machine learning classifiers. The machine learning models were validated by evaluating them on a publicly available job description dataset. Our results indicate that the word embeddings and transformer-based features consistently outperformed the handcrafted rule-set based features class. Ultimately, a Gradient Boosting classifier with a combination of empirical rule-set based features, parts-of-speech tags and bag-of-words vectors achieved the best performance with an F1-score of 0.88.","",""
"2023","The social and ethical impacts of artificial intelligence in agriculture: mapping the agricultural AI literature","AbstractThis paper will examine the social and ethical impacts of using artificial intelligence (AI) in the agricultural sector. It will identify what are some of the most prevalent challenges and impacts identified in the literature, how this correlates with those discussed in the domain of AI ethics, and are being implemented into AI ethics guidelines. This will be achieved by examining published articles and conference proceedings that focus on societal or ethical impacts of AI in the agri-food sector, through a thematic analysis of the literature. The thematic analysis will be divided based on the classifications outlined through 11 overarching principles, from an established lexicon (transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, dignity, sustainability, and solidarity). While research on AI agriculture is still relatively new, this paper aims to map the debate and illustrate what the literature says in the context of social and ethical impacts. It aim is to analyse these impacts, based on these 11 principles. This research will contrast which impacts are not being discussed in agricultural AI and which issues are not being discussed in AI ethics guidelines, but which are discussed in relation to agricultural AI. The aim of this is to identify gaps within the agricultural literature, and gaps in AI ethics guidelines, that may need to be addressed.","",""
"2023","Narrative responsibility and artificial intelligence","AbstractMost accounts of responsibility focus on one type of responsibility, moral responsibility, or address one particular aspect of moral responsibility such as agency. This article outlines a broader framework to think about responsibility that includes causal responsibility, relational responsibility, and what I call “narrative responsibility” as a form of “hermeneutic responsibility”, connects these notions of responsibility with different kinds of knowledge, disciplines, and perspectives on human being, and shows how this framework is helpful for mapping and analysing how artificial intelligence (AI) challenges human responsibility and sense-making in various ways. Mobilizing recent hermeneutic approaches to technology, the article argues that next to, and interwoven with, other types of responsibility such as moral responsibility, we also have narrative and hermeneutic responsibility—in general and for technology. For example, it is our task as humans to make sense of, with and, if necessary, against AI. While from a posthumanist point of view, technologies also contribute to sense-making, humans are the experiencers and bearers of responsibility and always remain in charge when it comes to this hermeneutic responsibility. Facing and working with a world of data, correlations, and probabilities, we are nevertheless condemned to make sense. Moreover, this also has a normative, sometimes even political aspect: acknowledging and embracing our hermeneutic responsibility is important if we want to avoid that our stories are written elsewhere—through technology.","",""
"2023","Trust and ethics in AI","","",""
"2023","Entropies and the Anthropocene crisis","","",""
"2023","Redefining culture in cultural robotics","AbstractCultural influences are pervasive throughout human behaviour, and as human–robot interactions become more common, roboticists are increasingly focusing attention on how to build robots that are culturally competent and culturally sustainable. The current treatment of culture in robotics, however, is largely limited to the definition of culture as national culture. This is problematic for three reasons: it ignores subcultures, it loses specificity and hides the nuances in cultures, and it excludes refugees and stateless persons. We propose to shift the focus of cultural robotics to redefine culture as an emergent phenomenon. We make use of three research programmes in the social and cognitive sciences to justify this definition. Consequently, cultural behaviour cannot be explicitly programmed into a robot, rather, a robot must be designed with the capability to participate in the interactions that lead to the arising of cultural behaviour. In the final part of the paper, we explore which capacities and abilities are the most salient for a robot to do this.","",""
"2023","Investing in AI for social good: an analysis of European national strategies","AbstractArtificial Intelligence (AI) has become a driving force in modern research, industry and public administration and the European Union (EU) is embracing this technology with a view to creating societal, as well as economic, value. This effort has been shared by EU Member States which were all encouraged to develop their own national AI strategies outlining policies and investment levels. This study focuses on how EU Member States are approaching the promise to develop and use AI for the good of society through the lens of their national AI strategies. In particular, we aim to investigate how European countries are investing in AI and to what extent the stated plans contribute to the good of people and society as a whole. Our contribution consists of three parts: (i) a conceptualization of AI for social good highlighting the role of AI policy, in particular, the one put forward by the European Commission (EC); (ii) a qualitative analysis of 15 European national strategies mapping investment plans and suggesting their relation to the social good (iii) a reflection on the current status of investments in socially good AI and possible steps to move forward. Our study suggests that while European national strategies incorporate money allocations in the sphere of AI for social good (e.g. education), there is a broader variety of underestimated actions (e.g. multidisciplinary approach in STEM curricula and dialogue among stakeholders) that can boost the European commitment to sustainable and responsible AI innovation.","",""
"2023","Word embeddings are biased. But whose bias are they reflecting?","AbstractFrom Curriculum Vitae parsing to web search and recommendation systems, Word2Vec and other word embedding techniques have an increasing presence in everyday interactions in human society. Biases, such as gender bias, have been thoroughly researched and evidenced to be present in word embeddings. Most of the research focuses on discovering and mitigating gender bias within the frames of the vector space itself. Nevertheless, whose bias is reflected in word embeddings has not yet been investigated. Besides discovering and mitigating gender bias, it is also important to examine whether a feminine or a masculine-centric view is represented in the biases of word embeddings. This way, we will not only gain more insight into the origins of the before mentioned biases, but also present a novel approach to investigating biases in Natural Language Processing systems. Based on previous research in the social sciences and gender studies, we hypothesize that masculine-centric, otherwise known as androcentric, biases are dominant in word embeddings. To test this hypothesis we used the largest English word association test data set publicly available. We compare the distance of the responses of male and female participants to cue words in a word embedding vector space. We found that the word embedding is biased towards a masculine-centric viewpoint, predominantly reflecting the worldviews of the male participants in the word association test data set. Therefore, by conducting this research, we aimed to unravel another layer of bias to be considered when examining fairness in algorithms.","",""
"2023","The status–power arena: a comprehensive agent-based model of social status dynamics and gender in groups of children","AbstractDespite the urgency of this issue, AI still struggles to represent social life. This article presents a comprehensive agent-based model that investigates status-power dynamics in groups. Kemper’s sociological status–power theory of social relationships, and a literature review on school children in middle youth, is its basis. The model allows us to investigate causation of the near-ubiquitous phenomenon that females have lower social status on average than males. Possible causes included in the model are children’s dispositional traits (kindness, beauty, and physical power), schoolyard culture (social acceptability of fighting), behavioural strategy (amount of rough-and-tumble play) and the balance between public and dyadic sources of status. An agent-based model of a virtual schoolyard was created in which the children assemble in changing groups and mutually confer status. The status conferred upon a child modifies the status it holds. Rough-and-tumble is modelled as ambiguous: it is intended as a status conferral, but may be perceived as a power move. Running many trials of the model we found that in time, depending on the parameter settings, a gender-based status gap emerged. Rough-and-tumble play had more impact on emergent status differences than did physical power differences. Social acceptability of fighting also strongly moderated the resulting status gap. Placing more weight on dyadic relationship could alleviate status loss. All these model behaviours are in line with empirical findings of child behaviour studies at schools. They have face validity for social status issues in the adult world. We conclude from this that this kind of agent-based model merits use in studying the status–power dynamics of other issues in child behaviour, or indeed in social behaviour in general.","",""
"2023","Social influence for societal interest: a pro-ethical framework for improving human decision making through multi-stakeholder recommender systems","AbstractIn the contemporary digital age, recommender systems (RSs) play a fundamental role in managing information on online platforms: from social media to e-commerce, from travels to cultural consumptions, automated recommendations influence the everyday choices of users at an unprecedented scale. RSs are trained on users’ data to make targeted suggestions to individuals according to their expected preference, but their ultimate impact concerns all the multiple stakeholders involved in the recommendation process. Therefore, whilst RSs are useful to reduce information overload, their deployment comes with significant ethical challenges, which are still largely unaddressed because of proprietary constraints and regulatory gaps that limit the effects of standard approaches to explainability and transparency. In this context, I address the ethical and social implications of automated recommendations by proposing a pro-ethical design framework aimed at reorienting the influence of RSs towards societal interest. In particular, after highlighting the problem of explanation for RSs, I discuss the application of beneficent informational nudging to the case of conversational recommender systems (CRSs), which rely on user-system dialogic interactions. Subsequently, through a comparison with standard recommendations, I outline the incentives for platforms and providers in adopting this approach and its benefits for both individual users and society.","",""
"2023","Katherine Crawford: Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence","","",""
"2023","Why thinking about the tacit is key for shaping our AI futures","","",""
"2023","From AI for people to AI for the world and the universe","","",""
"2023","Psychological targeting: nudge or boost to foster mindful and sustainable consumption?","","",""
"2023","A philosophical view on singularity and strong AI","","",""
"2023","Unintentional intentionality: art and design in the age of artificial intelligence","","",""
"2023","Moral judgment in realistic traffic scenarios: moving beyond the trolley paradigm for ethics of autonomous vehicles","AbstractThe imminent deployment of autonomous vehicles requires algorithms capable of making moral decisions in relevant traffic situations. Some scholars in the ethics of autonomous vehicles hope to align such intelligent systems with human moral judgment. For this purpose, studies like the Moral Machine Experiment have collected data about human decision-making in trolley-like traffic dilemmas. This paper first argues that the trolley dilemma is an inadequate experimental paradigm for investigating traffic moral judgments because it does not include agents’ character-based considerations and is incapable of facilitating the investigation of low-stakes mundane traffic scenarios. In light of the limitations of the trolley paradigm, this paper presents an alternative experimental framework that addresses these issues. The proposed solution combines the creation of mundane traffic moral scenarios using virtual reality and the Agent-Deed-Consequences (ADC) model of moral judgment as a moral-psychological framework. This paradigm shift potentially increases the ecological validity of future studies by providing more realism and incorporating character considerations into traffic actions.","",""
"2023","Artificial intelligence ethics has a black box problem","","",""
"2023","Attitudes about Brain–Computer Interface (BCI) technology among Spanish rehabilitation professionals","AbstractTo assess—from a qualitative perspective—the perceptions and attitudes of Spanish rehabilitation professionals (e.g. rehabilitation doctors, speech therapists, physical therapists) about Brain–Computer Interface (BCI) technology. A qualitative, exploratory and descriptive study was carried out by means of interviews and analysis of textual content with mixed generation of categories and segmentation into frequency of topics. We present the results of three in-depth interviews that were conducted with Spanish speaking individuals who had previously completed a survey as part of a larger, 3-country/language, survey on BCI perceptions. 11 out of 15 of these Spanish respondents (survey) either strongly or somewhat accept the use of BCI in rehabilitation therapy. However, the results of our three in-depth interviews show how, due to a strong inertia of attitudes and perceptions about BCI technology, most professionals feel reluctant to use BCI technology in their daily practice (interview).","",""
"2023","Examining the assumptions of AI hiring assessments and their impact on job seekers’ autonomy over self-representation","AbstractIn this paper, we examine the epistemological and ontological assumptions algorithmic hiring assessments make about job seekers’ attributes (e.g., competencies, skills, abilities) and the ethical implications of these assumptions. Given that both traditional psychometric hiring assessments and algorithmic assessments share a common set of underlying assumptions from the psychometric paradigm, we turn to literature that has examined the merits and limitations of these assumptions, gathering insights across multiple disciplines and several decades. Our exploration leads us to conclude that algorithmic hiring assessments are incompatible with attributes whose meanings are context-dependent and socially constructed. Such attributes call instead for assessment paradigms that offer space for negotiation of meanings between the job seeker and the employer. We argue that in addition to questioning the validity of algorithmic hiring assessments, this raises an often overlooked ethical impact on job seekers’ autonomy over self-representation: their ability to directly represent their identity, lived experiences, and aspirations. Infringement on this autonomy constitutes an infringement on job seekers’ dignity. We suggest beginning to address these issues through epistemological and ethical reflection regarding the choice of assessment paradigm, the means to implement it, and the ethical impacts of these choices. This entails a transdisciplinary effort that would involve job seekers, hiring managers, recruiters, and other professionals and researchers. Combined with a socio-technical design perspective, this may help generate new ideas regarding appropriate roles for human-to-human and human–technology interactions in the hiring process.","",""
"2023","Generative AI and photographic transparency","","",""
"2023","Mental time-travel, semantic flexibility, and A.I. ethics","","",""
"2023","What dangers lurk in the development of emotionally competent artificial intelligence, especially regarding the trend towards sex robots? A review of Catrin Misselhorn’s most recent book","","",""
"2023","Separating facts and evaluation: motivation, account, and learnings from a novel approach to evaluating the human impacts of machine learning","AbstractIn this paper, we outline a new method for evaluating the human impact of machine-learning (ML) applications. In partnership with Underwriters Laboratories Inc., we have developed a framework to evaluate the impacts of a particular use of machine learning that is based on the goals and values of the domain in which that application is deployed. By examining the use of artificial intelligence (AI) in particular domains, such as journalism, criminal justice, or law, we can develop more nuanced and practically relevant understandings of key ethical guidelines for artificial intelligence. By decoupling the extraction of the facts of the matter from the evaluation of the impact of the resulting systems, we create a framework for the process of assessing impact that has two distinctly different phases.","",""
"2023","Against “Democratizing AI”","","",""
"2023","Exposing implicit biases and stereotypes in human and artificial intelligence: state of the art and challenges with a focus on gender","AbstractBiases in cognition are ubiquitous. Social psychologists suggested biases and stereotypes serve a multifarious set of cognitive goals, while at the same time stressing their potential harmfulness. Recently, biases and stereotypes became the purview of heated debates in the machine learning community too. Researchers and developers are becoming increasingly aware of the fact that some biases, like gender and race biases, are entrenched in the algorithms some AI applications rely upon. Here, taking into account several existing approaches that address the problem of implicit biases and stereotypes, we propose that a strategy to cope with this phenomenon is to unmask those found in AI systems by understanding their cognitive dimension, rather than simply trying to correct algorithms. To this extent, we present a discussion bridging together findings from cognitive science and insights from machine learning that can be integrated in a state-of-the-art semantic network. Remarkably, this resource can be of assistance to scholars (e.g., cognitive and computer scientists) while at the same time contributing to refine AI regulations affecting social life. We show how only through a thorough understanding of the cognitive processes leading to biases, and through an interdisciplinary effort, we can make the best of AI technology.","",""
"2023","Many hands make many fingers to point: challenges in creating accountable AI","","",""
"2023","Ethics-based auditing of automated decision-making systems: intervention points and policy implications","AbstractOrganisations increasingly use automated decision-making systems (ADMS) to inform decisions that affect humans and their environment. While the use of ADMS can improve the accuracy and efficiency of decision-making processes, it is also coupled with ethical challenges. Unfortunately, the governance mechanisms currently used to oversee human decision-making often fail when applied to ADMS. In previous work, we proposed that ethics-based auditing (EBA)—that is, a structured process by which ADMS are assessed for consistency with relevant principles or norms—can (a) help organisations verify claims about their ADMS and (b) provide decision-subjects with justifications for the outputs produced by ADMS. In this article, we outline the conditions under which EBA procedures can be feasible and effective in practice. First, we argue that EBA is best understood as a ‘soft’ yet ‘formal’ governance mechanism. This implies that the main responsibility of auditors should be to spark ethical deliberation at key intervention points throughout the software development process and ensure that there is sufficient documentation to respond to potential inquiries. Second, we frame AMDS as parts of larger sociotechnical systems to demonstrate that to be feasible and effective, EBA procedures must link to intervention points that span all levels of organisational governance and all phases of the software lifecycle. The main function of EBA should, therefore, be to inform, formalise, assess, and interlink existing governance structures. Finally, we discuss the policy implications of our findings. To support the emergence of feasible and effective EBA procedures, policymakers and regulators could provide standardised reporting formats, facilitate knowledge exchange, provide guidance on how to resolve normative tensions, and create an independent body to oversee EBA of ADMS.","",""
"2023","Minding the gap(s): public perceptions of AI and socio-technical imaginaries","AbstractDeepening and digging into the social side of AI is a novel but emerging requirement within the AI community. Future research should invest in an “AI for people”, going beyond the undoubtedly much-needed efforts into ethics, explainability and responsible AI. The article addresses this challenge by problematizing the discussion around AI shifting the attention to individuals and their awareness, knowledge and emotional response to AI. First, we outline our main argument relative to the need for a socio-technical perspective in the study of AI social implications. Then, we illustrate the main existing narratives of hopes and fears associated with AI and robots. As building blocks of broader “sociotechnical imaginaries”, narratives are powerful tools that shape how society sees, interprets and organizes technology. An original empirical study within the University of Bologna collects the data to examine the levels of awareness, knowledge and emotional response towards AI, revealing interesting insights to be carried on in future research. Replete with exaggerations, both utopian and dystopian narratives are analysed with respect to some relevant socio-demographic variables (gender, generation and competence). Finally, focusing on two issues—the state of AI anxiety and the point of view of non-experts—opens the floor to problematizing the discourse around AI, sustaining the need for a sociological perspective in the field of AI and discussing future comparative research.","",""
"2023","Integrating AI ethics in wildlife conservation AI systems in South Africa: a review, challenges, and future research agenda","","",""
"2023","A call for epistemic analysis of cultural theories for AI methods","","",""
"2023","Applying AI for social good: Aligning academic journal ratings with the United Nations Sustainable Development Goals (SDGs)","","",""
"2023","The psychological and ethological antecedents of human consent to techno-empowerment of autonomous office assistants","","",""
"2023","The application of chatbot on Vietnamese misgrant workers’ right protection in the implementation of new generation free trade agreements (FTAS)","","",""
"2023","Moving the AI needle: from chaos to engagement","","",""
"2023","Toward a dataist future: tracing Scandinavian posthumanism in Real Humans","AbstractArtificial intelligence is likely to undermine the anthropocentrism of humanism, the master narrative that undergirds the modern world. Humanity will need a new story to structure our beliefs and cooperation around. As different regions explore posthumanist alternatives through fiction, they bring with them distinct traditions of thought. The Swedish TV series Real Humans (2012–2014) and its British remake, Humans (2015–2018), dramatize the challenge of freeing oneself from cultural presumptions. When negotiating personhood with humanoid robots, the Swedish protagonist family presupposes a social-democratic ethos, which is a trace of Scandinavian humanism that carries into the family’s posthumanist beliefs. Using Heideggerian and related perspectives, I analyze these series to make a case for a dataist ontology with potential to re-enchant the modern world and bring forth a new epoch of being. Such a master narrative of algorithmic universality could be facilitated by a new level of interconnectedness made possible by AI. This ontology fulfills the requirements of Robbins and Horta (Introduction, Cosmopolitanisms. New York University Press, New York, pp 1–17, 2017) who call for a cosmopolitanism of inclusivity with room for overlapping conceptions to remedy our present era’s international dysfunction. The Swedish and British TV series suggest that even if humanity’s uniting around a dataist master narrative were to be driven by intercultural competition, the decisive choice might be out of human hands. Paradoxically, such disempowering in terms of agency is portrayed as necessary for human re-enchantment.","",""
"2023","AI &amp; society, knowledge, culture and communication","","",""
"2023","A machine is cheaper than a human for the same task","","",""
"2023","Artificial thinking and doomsday projections: a discourse on trust, ethics and safety","","",""
"2023","Beta-testing the ethics plugin","","",""
"2023","Empiricism in the foundations of cognition","AbstractThis paper traces the empiricist program from early debates between nativism and behaviorism within philosophy, through debates about early connectionist approaches within the cognitive sciences, and up to their recent iterations within the domain of deep learning. We demonstrate how current debates on the nature of cognition via deep network architecture echo some of the core issues from the Chomsky/Quine debate and investigate the strength of support offered by these various lines of research to the empiricist standpoint. Referencing literature from both computer science and philosophy, we conclude that the current state of deep learning does not offer strong encouragement to the empiricist side despite some arguments to the contrary.","",""
"2023","The epistemic opacity of autonomous systems and the ethical consequences","AbstractThis paper takes stock of all the various factors that cause the design-time opacity of autonomous systems behaviour. The factors include embodiment effects, design-time knowledge gap, human factors, emergent behaviour and tacit knowledge. This situation is contrasted with the usual representation of moral dilemmas that assume perfect information. Since perfect information is not achievable, the traditional moral dilemma representations are not valid and the whole problem of ethical autonomous systems design proves to be way more empirical than previously understood.","",""
"2023","Psychoanalyzing artificial intelligence: the case of Replika","","",""
"2023","Frankenstein: a creation of artificial intelligence?","","",""
"2023","Are wicked problems a lack of general collective intelligence?","","",""
"2023","The importance of transparency in naming conventions, designs, and operations of safety features: from modern ADAS to fully autonomous driving functions","AbstractThis paper investigates the importance of standardising and maintaining the transparency of advanced driver-assistance systems (ADAS) functions nomenclature, designs, and operations in all categories up until fully autonomous vehicles. The aim of this paper is to reveal the discrepancies in ADAS functions across automakers and discuss the underlying issues and potential solutions. In this pilot study, user manuals of various brands are reviewed systematically and critical analyses of common ADAS functions are conducted. The result shows that terminologies used to describe ADAS functions vary widely across manufacturers and sometimes do not reflect their fundamental functions intuitively. Operational conditions and control procedures also vary across the selected models under this study. Due to this lack of consensus across the industry, drivers are not aware or well informed about ADAS functions in their vehicles, leading to a very low utilization rate and may lead to misuse of those functions. This paper provides insightful suggestions for the transport industry, Artificial Intelligence (AI) experts, and regulators to design frameworks and guidelines in governing the naming convention, operating conditions, control procedures, and information disclosure of ADAS. Such guidelines can be the foundations for regulating future AI-based self-driving functions.","",""
"2023","Recommender systems for mental health apps: advantages and ethical challenges","","",""
"2023","How to cheat on your final paper: Assigning AI for student writing","","",""
"2023","Against the new space race: global AI competition and cooperation for people","","",""
"2023","Ethical artificial intelligence framework for a good AI society: principles, opportunities and perils","","",""
"2023","Applying ethics to AI in the workplace: the design of a scorecard for Australian workplace health and safety","AbstractArtificial Intelligence (AI) is taking centre stage in economic growth and business operations alike. Public discourse about the practical and ethical implications of AI has mainly focussed on the societal level. There is an emerging knowledge base on AI risks to human rights around data security and privacy concerns. A separate strand of work has highlighted the stresses of working in the gig economy. This prevailing focus on human rights and gig impacts has been at the expense of a closer look at how AI may be reshaping traditional workplace relations and, more specifically, workplace health and safety. To address this gap, we outline a conceptual model for developing an AI Work Health and Safety (WHS) Scorecard as a tool to assess and manage the potential risks and hazards to workers resulting from AI use in a workplace. A qualitative, practice-led research study of AI adopters was used to generate and test a novel list of potential AI risks to worker health and safety. Risks were identified after cross-referencing Australian AI Ethics Principles and Principles of Good Work Design with AI ideation, design and implementation stages captured by the AI Canvas, a framework otherwise used for assessing the commercial potential of AI to a business. The unique contribution of this research is the development of a novel matrix itemising currently known or anticipated risks to the WHS and ethical aspects at each AI adoption stage.","",""
"2023","Mind who’s testing: Turing tests and the post-colonial imposition of their implicit conceptions of intelligence","AbstractThis paper aims to show that dominant conceptions of intelligence used in artificial intelligence (AI) are biased by normative assumptions that originate from the Global North, making it questionable if AI can be uncritically applied elsewhere without risking serious harm to vulnerable people. After the introduction in Sect. 1 we shortly present the history of IQ testing in Sect. 2, focusing on its multiple discriminatory biases. To determine how these biases came into existence, we define intelligence ontologically and underline its constructed and culturally variable character. Turning to AI, specifically the Turing Test (TT), in Sect. 3, we critically examine its underlying intelligence conceptions. The test has been of central influence in AI research and remains an important point of orientation. We argue that both the test itself and how it is used in practice risk promoting a limited conception of intelligence which solely originated in the Global North. Hence, this conception should be critically assessed in relation to the different global contexts in which AI technologies are and will be used. In Sect. 4, we highlight how unequal power relations in AI research are a real threat, rather than just philosophical sophistry while considering the history of IQ testing and the TT’s practical biases. In the last section, we examine the limits of our account and identify fields for further investigation. Tracing colonial continuities in AI intelligence research, this paper points to a more diverse and historically aware approach to the design, development, and use of AI.","",""
"2023","Causal Reasoning and Meno’s Paradox","AbstractCausal reasoning is an aspect of learning, reasoning, and decision-making that involves the cognitive ability to discover relationships between causal relata, learn and understand these causal relationships, and make use of this causal knowledge in prediction, explanation, decision-making, and reasoning in terms of counterfactuals. Can we fully automate causal reasoning? One might feel inclined, on the basis of certain groundbreaking advances in causal epistemology, to reply in the affirmative. The aim of this paper is to demonstrate that one still has good skeptical grounds for resisting any conclusions in favour of the automation of causal reasoning. If by causal reasoning is meant the entirety of the process through which we discover causal relationships and make use of this knowledge in prediction, explanation, decision-making, and reasoning in terms of counterfactuals, then one relies besides on tacit knowledge, as might be constituted by or derived from the epistemic faculty virtues and abilities of the causal reasoner, the value systems and character traits of the causal reasoner, the implicit knowledge base available to the causal reasoner, and the habits that sustain our causal reasoning practices. While certain aspects of causal reasoning may be axiomatized and formalized and algorithms may be implemented to approximate causal reasoning, one has to remain skeptical about whether causal reasoning may be fully automated. This demonstration will involve an engagement with Meno’s Paradox.","",""
"2023","Gendered AI: German news media discourse on the future of work","AbstractIn recent years, there has been a growing public discourse regarding the influence AI will have on the future of work. Simultaneously, considerable critical attention has been given to the implications of AI on gender equality. Far from making precise predictions about the future, this discourse demonstrates that new technologies are instances for renegotiating the relation of gender and work. This paper examines how gender is addressed in news media discourse on AI and the future of work, focusing on Germany. We approach this question from a perspective of feminist technology studies and discourse analysis, exploring a corpus of 178 articles from 2015 to 2021 from German newspapers and newsmagazines. The findings indicate that critical AI and gender knowledge circulates in public discourse in the form of specific discursive frames, thematizing algorithmic bias, automatization and enhancement, and gender stereotypes. As a result, we show that, first, the discourse takes up feminist and scholarly discourse on gender and discusses AI in a way that is informed by social constructivism and standpoint theories. Second, gender appears as a—to some extent intersectional—diversity category which is critical to AI, while at the same time omitting important perspectives. Third, it can be shown that there is a renegotiating of the ideal worker norm taking place, and finally, we argue that the gendered frame of the powerful men developer responsible for AI’s risk is a concept to be challenged.","",""
"2023","Artificial intelligence national strategy in a developing country","AbstractArtificial intelligence (AI) national strategies provide countries with a framework for the development and implementation of AI technologies. Sixty countries worldwide published their AI national strategies. The majority of these countries with more than 70% are developed countries. The approach of AI national strategies differentiates between developed and developing countries in several aspects including scientific research, education, talent development, and ethics. This paper examined AI readiness assessment in a developing country (Palestine) to help develop and identify the main pillars of the AI national strategy. AI readiness assessment was applied across education, entrepreneurship, government, and research and development sectors in Palestine (case of a developing country). In addition, it examined the legal framework and whether it is coping with trending technologies. The results revealed that Palestinians have low awareness of AI. Moreover, AI is barely used across several sectors and the legal framework is not coping with trending technologies. The results helped develop and identify the following five main pillars that Palestine’s AI national strategy should focus on: AI for Government, AI for Development, AI for Capacity Building in the private, public and technical and governmental sectors, AI and Legal Framework, and international Activities.","",""
"2023","Prompting meaning: a hermeneutic approach to optimising prompt engineering with ChatGPT","AbstractRecent advances in natural language generation (NLG), such as public accessibility to ChatGPT, have sparked polarised debates about the societal impact of this technology. Popular discourse tends towards either overoptimistic hype that touts the radically transformative potentials of these systems or pessimistic critique of their technical limitations and general ‘stupidity’. Surprisingly, these debates have largely overlooked the exegetical capacities of these systems, which for many users seem to be producing meaningful texts. In this paper, we take an interdisciplinary approach that combines hermeneutics—the study of meaning and interpretation—with prompt engineering—task descriptions embedded in input to NLG systems—to study the extent to which a specific NLG system, ChatGPT, produces texts of hermeneutic value. We design prompts with the goal of optimising hermeneuticity rather than mere factual accuracy, and apply them in four different use cases combining humans and ChatGPT as readers and writers. In most cases, ChatGPT produces readable texts that respond clearly to our requests. However, increasing the specificity of prompts’ task descriptions leads to texts with intensified neutrality, indicating that ChatGPT’s optimisation for factual accuracy may actually be detrimental to the hermeneuticity of its output.","",""
"2023","Operationalising AI ethics: barriers, enablers and next steps","AbstractBy mid-2019 there were more than 80 AI ethics guides available in the public domain. Despite this, 2020 saw numerous news stories break related to ethically questionable uses of AI. In part, this is because AI ethics theory remains highly abstract, and of limited practical applicability to those actually responsible for designing algorithms and AI systems. Our previous research sought to start closing this gap between the ‘what’ and the ‘how’ of AI ethics through the creation of a searchable typology of tools and methods designed to translate between the five most common AI ethics principles and implementable design practices. Whilst a useful starting point, that research rested on the assumption that all AI practitioners are aware of the ethical implications of AI, understand their importance, and are actively seeking to respond to them. In reality, it is unclear whether this is the case. It is this limitation that we seek to overcome here by conducting a mixed-methods qualitative analysis to answer the following four questions: what do AI practitioners understand about the need to translate ethical principles into practice? What motivates AI practitioners to embed ethical principles into design practices? What barriers do AI practitioners face when attempting to translate ethical principles into practice? And finally, what assistance do AI practitioners want and need when translating ethical principles into practice?","",""
"2023","A democratic way of controlling artificial general intelligence","AbstractThe problem of controlling an artificial general intelligence (AGI) has fascinated both scientists and science-fiction writers for centuries. Today that problem is becoming more important because the time when we may have a superhuman intelligence among us is within the foreseeable future. Current average estimates place that moment to before 2060. Some estimates place it as early as 2040, which is quite soon. The arrival of the first AGI might lead to a series of events that we have not seen before: rapid development of an even more powerful AGI developed by the AGIs themselves. This has wide-ranging implications to the society and therefore it is something that must be studied well before it happens. In this paper we will discuss the problem of limiting the risks posed by the advent of AGIs. In a thought experiment, we propose an AGI which has enough human-like properties to act in a democratic society, while still retaining its essential artificial general intelligence properties. We discuss ways of arranging the co-existence of humans and such AGIs using a democratic system of coordination and coexistence. If considered a success, such a system could be used to manage a society consisting of both AGIs and humans. The democratic system where each member of the society is represented in the highest level of decision-making guarantees that even minorities would be able to have their voices heard. The unpredictability of the AGI era makes it necessary to consider the possibility that a population of autonomous AGIs could make us humans into a minority.","",""
"2023","An explanation space to align user studies with the technical development of Explainable AI","","",""
"2023","Could artificial intelligence have consciousness? Some perspectives from neurology and parapsychology","","",""
"2023","Legal personhood for the integration of AI systems in the social context: a study hypothesis","","",""
"2023","Training philosopher engineers for better AI","","",""
"2023","Smart soldiers: towards a more ethical warfare","AbstractIt is a truism that, due to human weaknesses, human soldiers have yet to have sufficiently ethical warfare. It is arguable that the likelihood of human soldiers to breach the Principle of Non-Combatant Immunity, for example, is higher in contrast tosmart soldierswho are emotionally inept. Hence, this paper examines the possibility that the integration of ethics into smart soldiers will help address moral challenges in modern warfare. The approach is to develop and employ smart soldiers that are enhanced with ethical capabilities. Advocates of this approach think that it is more realistic to make competent entities (i.e., smart soldiers) become morally responsible than to enforce moral responsibility on human soldiers with inherent (moral) limitations. This view somewhat seeks a radical transition from the usual anthropocentric warfare to arobocentricwarfare with the belief that the transition has moral advantages. However, the paper defends the claim that despite human limitations, the capacity ofethically enhanced smart soldiersfor moral sensitivity is artificial and unauthentic. There are significant problems with the three models of programming ethics into smart soldiers. Also, there are further challenges from the absence of emotion as a moral gauge, and the problems of apportioning responsibility in case of mishap from the actions or omissions of smart soldiers. Among other reasons, the paper takes the replacement of human soldiers as an extreme approach towards an ethical warfare. This replacement predicates ethical complications that outweigh the benefits from the exclusive use of smart soldiers.","",""
"2023","Toward safe AI","","",""
"2023","Toward trustworthy programming for autonomous concurrent systems","","",""
"2023","Empathetic AI for ethics-in-the-small","","",""
"2023","Caring in the in-between: a proposal to introduce responsible AI and robotics to healthcare","AbstractIn the scenario of growing polarization of promises and dangers that surround artificial intelligence (AI), how to introduce responsible AI and robotics in healthcare? In this paper, we develop an ethical–political approach to introduce democratic mechanisms to technological development, what we call “Caring in the In-Between”. Focusing on the multiple possibilities for action that emerge in the realm of uncertainty, we propose an ethical and responsible framework focused on care actions in between fears and hopes. Using the theoretical perspective of Science and Technology Studies and empirical research, “Caring in the In-Between” is based on three movements: the first is a change of focus from the world of promises and dangers to the world of uncertainties; the second is a conceptual shift from assuming a relationship with robotics based on a Human–Robot Interaction to another focused on the network in which the robot is embedded (the “Robot Embedded in a Network”); and the last is an ethical shift from a general normative framework to a discussion on the context of use. Based on these suggestions, “Caring in the In-Between” implies institutional challenges, as well as new practices in healthcare systems. It is articulated around three simultaneous processes, each of them related to practical actions in the “in-between” dimensions considered: monitoring relations and caring processes, through public engagement and institutional changes; including concerns and priorities of stakeholders, with the organization of participatory processes and alternative forms of representation; and making fears and hopes commensurable, through the choice of progressive and reversible actions.","",""
"2023","The poverty of ethical AI: impact sourcing and AI supply chains","AbstractImpact sourcing is the practice of employing socio-economically disadvantaged individuals at business process outsourcing centres to reduce poverty and create secure jobs. One of the pioneers of impact sourcing is Sama, a training-data company that focuses on annotating data for artificial intelligence (AI) systems and claims to support an ethical AI supply chain through its business operations. Drawing on fieldwork undertaken at three of Sama’s East African delivery centres in Kenya and Uganda and follow-up online interviews, this article interrogates Sama’s claims regarding the benefits of its impact sourcing model. Our analysis reveals alarming accounts of low wages, insecure work, a tightly disciplined labour management process, gender-based exploitation and harassment and a system designed to extract value from low-paid workers to produce profits for investors. We argue that competitive market-based dynamics generate a powerful force that pushes such companies towards limiting the actual social impact of their business model in favour of ensuring higher profit margins. This force can be resisted, but only through countervailing measures such as pressure from organised workers, civil society, or regulation. These findings have broad implications related to working conditions for low-wage data annotators across the sector and cast doubt on the ethical nature of AI products that rely on this form of AI data work.","",""
"2023","AI transparency: a matter of reconciling design with critique","AbstractIn the late 2010s, various international committees, expert groups, and national strategy boards have voiced the demand to ‘open’ the algorithmic black box, to audit, expound, and demystify artificial intelligence. The opening of the algorithmic black box, however, cannot be seen only as an engineering challenge. In this article, I argue that only the sort of transparency that arises from critique—a method of theoretical examination that, by revealing pre-existing power structures, aims to challenge them—can help us produce technological systems that are less deceptive and more just. I relate the question of AI transparency to the broader challenge of responsible making, contending that future action must aim to systematically reconcile design—as a way of concealing—with critique—as a manner of revealing.","",""
"2023","From Blade Runners to Tin Kickers: what the governance of artificial intelligence safety needs to learn from air crash investigators","","",""
"2023","Artificial virtuous agents: from theory to machine implementation","AbstractVirtue ethics has many times been suggested as a promising recipe for the construction of artificial moral agents due to its emphasis on moral character and learning. However, given the complex nature of the theory, hardly any work has de facto attempted to implement the core tenets of virtue ethics in moral machines. The main goal of this paper is to demonstrate how virtue ethics can be taken all the way from theory to machine implementation. To achieve this goal, we critically explore the possibilities and challenges for virtue ethics from a computational perspective. Drawing on previous conceptual and technical work, we outline a version of artificial virtue based on moral functionalism, connectionist bottom–up learning, and eudaimonic reward. We then describe how core features of the outlined theory can be interpreted in terms of functionality, which in turn informs the design of components necessary for virtuous cognition. Finally, we present a comprehensive framework for the technical development of artificial virtuous agents and discuss how they can be implemented in moral environments.","",""
"2023","Optimising peace through a Universal Global Peace Treaty to constrain the risk of war from a militarised artificial superintelligence","","",""
"2023","Maximizing team synergy in AI-related interdisciplinary groups: an interdisciplinary-by-design iterative methodology","AbstractIn this paper, we propose a methodology to maximize the benefits of interdisciplinary cooperation in AI research groups. Firstly, we build the case for the importance of interdisciplinarity in research groups as the best means to tackle the social implications brought about by AI systems, against the backdrop of the EU Commission proposal for an Artificial Intelligence Act. As we are an interdisciplinary group, we address the multi-faceted implications of the mass-scale diffusion of AI-driven technologies. The result of our exercise lead us to postulate the necessity of a behavioural theory that standardizes the interaction process of interdisciplinary groups. In light of this, we conduct a review of the existing approaches to interdisciplinary research on AI appliances, leading to the development of methodologies like ethics-by-design and value-sensitive design, evaluating their strengths and weaknesses. We then put forth an iterative process theory hinging on a narrative approach consisting of four phases: (i) definition of the hypothesis space, (ii) building-up of a common lexicon, (iii) scenario-building, (iv) interdisciplinary self-assessment. Finally, we identify the most relevant fields of application for such a methodology and discuss possible case studies.","",""
"2023","Developing safer AI–concepts from economics to the rescue","","",""
"2023","At the intersection of humanity and technology: a technofeminist intersectional critical discourse analysis of gender and race biases in the natural language processing model GPT-3","AbstractAlgorithmic biases, or algorithmic unfairness, have been a topic of public and scientific scrutiny for the past years, as increasing evidence suggests the pervasive assimilation of human cognitive biases and stereotypes in such systems. This research is specifically concerned with analyzing the presence of discursive biases in the text generated by GPT-3, an NLPM which has been praised in recent years for resembling human language so closely that it is becoming difficult to differentiate between the human and the algorithm. The pertinence of this research object is substantiated by the identification of race, gender and religious biases in the model’s completions in recent research, suggesting that the model is indeed heavily influenced by human cognitive biases. To this end, this research inquires: How does the Natural Language Processing Model GPT-3 replicate existing social biases?. This question is addressed through the scrutiny of GPT-3’s completions using Critical Discourse Analysis (CDA), a method which has been deemed as amply valuable for this research as it is aimed at uncovering power asymmetries in language. As such, the analysis is specifically centered around the analysis of gender and race biases in the model’s generated text. Research findings suggest that GPT-3’s language generation model significantly exacerbates existing social biases while replicating dangerous ideologies akin to white supremacy and hegemonic masculinity as factual knowledge.","",""
"2023","Interdependence as the key for an ethical artificial autonomy","AbstractCurrently, the autonomy of artificial systems, robotic systems in particular, is certainly one of the most debated issues, both from the perspective of technological development and its social impact and ethical repercussions. While theoretical considerations often focus on scenarios far beyond what can be concretely hypothesized from the current state of the art, the term autonomy is still used in a vague or too general way. This reduces the possibilities of a punctual analysis of such an important issue, thus leading to often polarized positions (naive optimism or unfounded defeatism). The intent of this paper is to clarify what is meant by artificial autonomy, and what are the prerequisites that can allow the attribution of this characteristic to a robotic system. Starting from some concrete examples, we will try to indicate a way towards artificial autonomy that can hold together the advantages of developing adaptive and versatile systems with the management of the inevitable problems that this technology poses both from the viewpoint of safety and ethics. Our proposal is that a real artificial autonomy, especially if expressed in the social context, can only be achieved through interdependence with other social actors (human and otherwise), through continuous exchanges and interactions which, while allowing robots to explore the environment, guarantee the emergence of shared practices, behaviors, and ethical principles, which otherwise could not be imposed with a top-down approach, if not at the price of giving up the same artificial autonomy.","",""
"2023","The future of ethics in AI: challenges and opportunities","","",""
"2023","The social turn of artificial intelligence","AbstractSocial machines are systems formed by material and human elements interacting in a structured way. The use of digital platforms as mediators allows large numbers of humans to participate in such machines, which have interconnected AI and human components operating as a single system capable of highly sophisticated behaviour. Under certain conditions, such systems can be understood as autonomous goal-driven agents. Many popular online platforms can be regarded as instances of this class of agent. We argue that autonomous social machines provide a new paradigm for the design of intelligent systems, marking a new phase in AI. After describing the characteristics of goal-driven social machines, we discuss the consequences of their adoption, for the practice of artificial intelligence as well as for its regulation.","",""
"2023","Artificial intelligence in fiction: between narratives and metaphors","AbstractScience-fiction (SF) has become a reference point in the discourse on the ethics and risks surrounding artificial intelligence (AI). Thus, AI in SF—science-fictional AI—is considered part of a larger corpus of ‘AI narratives’ that are analysed as shaping the fears and hopes of the technology. SF, however, is not a foresight or technology assessment, but tells dramas for a human audience. To make the drama work, AI is often portrayed as human-like or autonomous, regardless of the actual technological limitations. Taking science-fictional AI too literally, and even applying it to science communication, paints a distorted image of the technology's current potential and distracts from the real-world implications and risks of AI. These risks are not about humanoid robots or conscious machines, but about the scoring, nudging, discrimination, exploitation, and surveillance of humans by AI technologies through governments and corporations. AI in SF, on the other hand, is a trope as part of a genre-specific mega-text that is better understood as a dramatic means and metaphor to reflect on the human condition and socio-political issues beyond technology.","",""
"2023","Governing algorithms from the South: a case study of AI development in Africa","","",""
"2023","Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms","AbstractThe increasing implementation of and reliance on machine-learning (ML) algorithms to perform tasks, deliver services and make decisions in health and healthcare have made the need for fairness in ML, and more specifically in healthcare ML algorithms (HMLA), a very important and urgent task. However, while the debate on fairness in the ethics of artificial intelligence (AI) and in HMLA has grown significantly over the last decade, the very concept of fairness as an ethical value has not yet been sufficiently explored. Our paper aims to fill this gap and address the AI ethics principle of fairness from a conceptual standpoint, drawing insights from accounts of fairness elaborated in moral philosophy and using them to conceptualise fairness as an ethical value and to redefine fairness in HMLA accordingly. To achieve our goal, following a first section aimed at clarifying the background, methodology and structure of the paper, in the second section, we provide an overview of the discussion of the AI ethics principle of fairness in HMLA and show that the concept of fairness underlying this debate is framed in purely distributive terms and overlaps with non-discrimination, which is defined in turn as the absence of biases. After showing that this framing is inadequate, in the third section, we pursue an ethical inquiry into the concept of fairness and argue that fairness ought to be conceived of as an ethical value. Following a clarification of the relationship between fairness and non-discrimination, we show that the two do not overlap and that fairness requires much more than just non-discrimination. Moreover, we highlight that fairness not only has a distributive but also a socio-relational dimension. Finally, we pinpoint the constitutive components of fairness. In doing so, we base our arguments on a renewed reflection on the concept of respect, which goes beyond the idea of equal respect to include respect for individual persons. In the fourth section, we analyse the implications of our conceptual redefinition of fairness as an ethical value in the discussion of fairness in HMLA. Here, we claim that fairness requires more than non-discrimination and the absence of biases as well as more than just distribution; it needs to ensure that HMLA respects persons both as persons and as particular individuals. Finally, in the fifth section, we sketch some broader implications and show how our inquiry can contribute to making HMLA and, more generally, AI promote the social good and a fairer society.","",""
"2023","The AI gambit: leveraging artificial intelligence to combat climate change—opportunities, challenges, and recommendations","AbstractIn this article, we analyse the role that artificial intelligence (AI) could play, and is playing, to combat global climate change. We identify two crucial opportunities that AI offers in this domain: it can help improve and expand current understanding of climate change, and it can contribute to combatting the climate crisis effectively. However, the development of AI also raises two sets of problems when considering climate change: the possible exacerbation of social and ethical challenges already associated with AI, and the contribution to climate change of the greenhouse gases emitted by training data and computation-intensive AI systems. We assess the carbon footprint of AI research, and the factors that influence AI’s greenhouse gas (GHG) emissions in this domain. We find that the carbon footprint of AI research may be significant and highlight the need for more evidence concerning the trade-off between the GHG emissions generated by AI research and the energy and resource efficiency gains that AI can offer. In light of our analysis, we argue that leveraging the opportunities offered by AI for global climate change whilst limiting its risks is a gambit which requires responsive, evidence-based, and effective governance to become a winning strategy. We conclude by identifying the European Union as being especially well-placed to play a leading role in this policy response and provide 13 recommendations that are designed to identify and harness the opportunities of AI for combatting climate change, while reducing its impact on the environment.","",""
"2023","Tensions in transparent urban AI: designing a smart electric vehicle charge point","AbstractThe increasing use of artificial intelligence (AI) by public actors has led to a push for more transparency. Previous research has conceptualized AI transparency as knowledge that empowers citizens and experts to make informed choices about the use and governance of AI. Conversely, in this paper, we critically examine if transparency-as-knowledge is an appropriate concept for a public realm where private interests intersect with democratic concerns. We conduct a practice-based design research study in which we prototype and evaluate a transparent smart electric vehicle charge point, and investigate experts’ and citizens’ understanding of AI transparency. We find that citizens experience transparency as burdensome; experts hope transparency ensures acceptance, while citizens are mostly indifferent to AI; and with absent means of control, citizens question transparency’s relevance. The tensions we identify suggest transparency cannot be reduced to a product feature, but should be seen as a mediator of debate between experts and citizens.","",""
"2023","Embedding AI in society: ethics, policy, governance, and impacts","","",""
"2023","Principle-based recommendations for big data and machine learning in food safety: the P-SAFETY model","AbstractBig data and Machine learning Techniques are reshaping the way in which food safety risk assessment is conducted. The ongoing ‘datafication’ of food safety risk assessment activities and the progressive deployment of probabilistic models in their practices requires a discussion on the advantages and disadvantages of these advances. In particular, the low level of trust in EU food safety risk assessment framework highlighted in 2019 by an EU-funded survey could be exacerbated by novel methods of analysis. The variety of processed data raises unique questions regarding the interplay of multiple regulatory systems alongside food safety legislation. Provisions aiming to preserve the confidentiality of data and protect personal information are juxtaposed to norms prescribing the public disclosure of scientific information. This research is intended to provide guidance for data governance and data ownership issues that unfold from the ongoing transformation of the technical and legal domains of food safety risk assessment. Following the reconstruction of technological advances in data collection and analysis and the description of recent amendments to food safety legislation, emerging concerns are discussed in light of the individual, collective and social implications of the deployment of cutting-edge Big Data collection and analysis techniques. Then, a set of principle-based recommendations is proposed by adapting high-level principles enshrined in institutional documents about Artificial Intelligence to the realm of food safety risk assessment. The proposed set of recommendations adopts Safety, Accountability, Fairness, Explainability, Transparency as core principles (SAFETY), whereas Privacy and data protection are used as a meta-principle.","",""
"2023","Testing Turing","","",""
"2023","Human–machine coordination in mixed traffic as a problem of Meaningful Human Control","AbstractThe urban traffic environment is characterized by the presence of a highly differentiated pool of users, including vulnerable ones. This makes vehicle automation particularly difficult to implement, as a safe coordination among those users is hard to achieve in such an open scenario. Different strategies have been proposed to address these coordination issues, but all of them have been found to be costly for they negatively affect a range of human values (e.g. safety, democracy, accountability…). In this paper, we claim that the negative value impacts entailed by each of these strategies can be interpreted as lack of what we call Meaningful Human Control over different parts of a sociotechnical system. We argue that Meaningful Human Control theory provides the conceptual tools to reduce those unwanted consequences, and show how “designing for meaningful human control” constitutes a valid strategy to address coordination issues. Furthermore, we showcase a possible application of this framework in a highly dynamic urban scenario, aiming to safeguard important values such as safety, democracy, individual autonomy, and accountability. Our meaningful human control framework offers a perspective on coordination issues that allows to keep human actors in control while minimizing the active, operational role of the drivers. This approach makes ultimately possible to promote a safe and responsible transition to full automation. ","",""
"2023","The ethics of algorithms from the perspective of the cultural history of consciousness: first look","","",""
"2023","Implementations, interpretative malleability, value-laden-ness and the moral significance of agent-based social simulations","","",""
"2023","Enhancing human agency through redress in Artificial Intelligence Systems","","",""
"2023","Can we wrong a robot?","","",""
"2023","Beyond the frame problem: what (else) can Heidegger do for AI?","","",""
"2023","Friend or foe? Exploring the implications of large language models on the science system","AbstractThe advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 researchers specializing in AI and digitization. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and helps identify areas for future action.","",""
"2023","A neo-aristotelian perspective on the need for artificial moral agents (AMAs)","AbstractWe examine Van Wynsberghe and Robbins (JAMA 25:719-735, 2019) critique of the need for Artificial Moral Agents (AMAs) and its rebuttal by Formosa and Ryan (JAMA 10.1007/s00146-020-01089-6, 2020) set against a neo-Aristotelian ethical background. Neither Van Wynsberghe and Robbins (JAMA 25:719-735, 2019) essay nor Formosa and Ryan’s (JAMA 10.1007/s00146-020-01089-6, 2020) is explicitly framed within the teachings of a specific ethical school. The former appeals to the lack of “both empirical and intuitive support” (Van Wynsberghe and Robbins 2019, p. 721) for AMAs, and the latter opts for “argumentative breadth over depth”, meaning to provide “the essential groundwork for making an all things considered judgment regarding the moral case for building AMAs” (Formosa and Ryan 2019, pp. 1–2). Although this strategy may benefit their acceptability, it may also detract from their ethical rootedness, coherence, and persuasiveness, characteristics often associated with consolidated ethical traditions. Neo-Aristotelian ethics, backed by a distinctive philosophical anthropology and worldview, is summoned to fill this gap as a standard to test these two opposing claims. It provides a substantive account of moral agency through the theory of voluntary action; it explains how voluntary action is tied to intelligent and autonomous human life; and it distinguishes machine operations from voluntary actions through the categories of poiesis and praxis respectively. This standpoint reveals that while Van Wynsberghe and Robbins may be right in rejecting the need for AMAs, there are deeper, more fundamental reasons. In addition, despite disagreeing with Formosa and Ryan’s defense of AMAs, their call for a more nuanced and context-dependent approach, similar to neo-Aristotelian practical wisdom, becomes expedient.","",""
"2023","Embedding artificial intelligence in society: looking beyond the EU AI master plan using the culture cycle","AbstractThe European Union (EU) Commission’s whitepaper on Artificial Intelligence (AI) proposes shaping the emerging AI market so that it better reflects common European values. It is a master plan that builds upon the EU AI High-Level Expert Group guidelines. This article reviews the masterplan, from a culture cycle perspective, to reflect on its potential clashes with current societal, technical, and methodological constraints. We identify two main obstacles in the implementation of this plan: (i) the lack of a coherent EU vision to drive future decision-making processes at state and local levels and (ii) the lack of methods to support a sustainable diffusion of AI in our society. The lack of a coherent vision stems from not considering societal differences across the EU member states. We suggest that these differences may lead to a fractured market and an AI crisis in which different members of the EU will adopt nation-centric strategies to exploit AI, thus preventing the development of a frictionless market as envisaged by the EU. Moreover, the Commission aims at changing the AI development culture proposing a human-centred and safety-first perspective that is not supported by methodological advancements, thus taking the risks of unforeseen social and societal impacts of AI. We discuss potential societal, technical, and methodological gaps that should be filled to avoid the risks of developing AI systems at the expense of society. Our analysis results in the recommendation that the EU regulators and policymakers consider how to complement the EC programme with rules and compensatory mechanisms to avoid market fragmentation due to local and global ambitions. Moreover, regulators should go beyond the human-centred approach establishing a research agenda seeking answers to the technical and methodological open questions regarding the development and assessment of human-AI co-action aiming for a sustainable AI diffusion in the society.","",""
"2023","Can machines think? The controversy that led to the Turing test","","",""
"2023","“I’m afraid I can’t let you do that, Doctor”: meaningful disagreements with AI in medical contexts","AbstractThis paper explores the role and resolution of disagreements between physicians and their diagnostic AI-based decision support systems (DSS). With an ever-growing number of applications for these independently operating diagnostic tools, it becomes less and less clear what a physician ought to do in case their diagnosis is in faultless conflict with the results of the DSS. The consequences of such uncertainty can ultimately lead to effects detrimental to the intended purpose of such machines, e.g. by shifting the burden of proof towards a physician. Thus, we require normative clarity for integrating these machines without affecting established, trusted, and relied upon workflows. In reconstructing different causes of conflicts between physicians and their AI-based tools—inspired by the approach of “meaningful human control” over autonomous systems and the challenges to resolve them—we will delineate normative conditions for “meaningful disagreements”. These incorporate the potential of DSS to take on more tasks and outline how the moral responsibility of a physician can be preserved in an increasingly automated clinical work environment.","",""
"2023","How virtue signalling makes us better: moral preferences with respect to autonomous vehicle type choices","","",""
"2023","Public perception of military AI in the context of techno-optimistic society","","",""
"2023","Towards low-cost machine learning solutions for manufacturing SMEs","AbstractMachine learning (ML) is increasingly used to enhance production systems and meet the requirements of a rapidly evolving manufacturing environment. Compared to larger companies, however, small- and medium-sized enterprises (SMEs) lack in terms of resources, available data and skills, which impedes the potential adoption of analytics solutions. This paper proposes a preliminary yet general approach to identify low-cost analytics solutions for manufacturing SMEs, with particular emphasis on ML. The initial studies seem to suggest that, contrarily to what is usually thought at first glance, SMEs seldom need digital solutions that use advanced ML algorithms which require extensive data preparation, laborious parameter tuning and a comprehensive understanding of the underlying problem. If an analytics solution does require learning capabilities, a ‘simple solution’, which we will characterise in this paper, should be sufficient.","",""
"2023","Reasoning about responsibility in autonomous systems: challenges and opportunities","AbstractEnsuring the trustworthiness of autonomous systems and artificial intelligence is an important interdisciplinary endeavour. In this position paper, we argue that this endeavour will benefit from technical advancements in capturing various forms of responsibility, and we present a comprehensive research agenda to achieve this. In particular, we argue that ensuring the reliability of autonomous system can take advantage of technical approaches for quantifying degrees of responsibility and for coordinating tasks based on that. Moreover, we deem that, in certifying the legality of an AI system, formal and computationally implementable notions of responsibility, blame, accountability, and liability are applicable for addressing potential responsibility gaps (i.e. situations in which a group is responsible, but individuals’ responsibility may be unclear). This is a call to enable AI systems themselves, as well as those involved in the design, monitoring, and governance of AI systems, to represent and reason about who can be seen as responsible in prospect (e.g. for completing a task in future) and who can be seen as responsible retrospectively (e.g. for a failure that has already occurred). To that end, in this work, we show that across all stages of the design, development, and deployment of trustworthy autonomous systems (TAS), responsibility reasoning should play a key role. This position paper is the first step towards establishing a road map and research agenda on how the notion of responsibility can provide novel solution concepts for ensuring the reliability and legality of TAS and, as a result, enables an effective embedding of AI technologies into society.","",""
"2023","Artificial intelligence, public control, and supply of a vital commodity like COVID-19 vaccine","","",""
"2023","Time to re-humanize algorithmic systems","","",""
"2023","Morals, ethics, and the technology capabilities and limitations of automated and self-driving vehicles","","",""
"2023","COVID-19, artificial intelligence, ethical challenges and policy implications","","",""
"2023","A principle-based approach to AI: the case for European Union and Italy","AbstractAs Artificial Intelligence (AI) becomes more and more pervasive in our everyday life, new questions arise about its ethical and social impacts. Such issues concern all stakeholders involved in or committed to the design, implementation, deployment, and use of the technology. The present document addresses these preoccupations by introducing and discussing a set of practical obligations and recommendations for the development of applications and systems based on AI techniques. With this work we hope to contribute to spreading awareness on the many social challenges posed by AI and encouraging the establishment of good practices throughout the relevant social areas. As points of novelty, the paper elaborates on an integrated view that combines both human rights and ethical concepts to reap the benefits of the two approaches. Moreover, it proposes innovative recommendations, such as those on redress and governance, which add further insight to the debate. Finally, it incorporates a specific focus on the Italian Constitution, thus offering an example of how core legislations of Member States might contribute to further specify and enrich the EU normative framework on AI.","",""
"2023","From posthumanism to ethics of artificial intelligence","","",""
"2023","Varieties of transparency: exploring agency within AI systems","","",""
"2023","From the ground truth up: doing AI ethics from practice to principles","","",""
"2023","Understanding citizen perceptions of AI in the smart city","AbstractArtificial intelligence (AI) is embedded in a wide variety of Smart City applications and infrastructures, often without the citizens being aware of the nature of their “intelligence”. AI can affect citizens’ lives concretely, and thus, there may be uncertainty, concerns, or even fears related to AI. To build acceptable futures of Smart Cities with AI-enabled functionalities, the Human-Centered AI (HCAI) approach offers a relevant framework for understanding citizen perceptions. However, only a few studies have focused on clarifying the citizen perceptions of AI in the context of smart city research. To address this gap, we conducted a two-phased study. In the pre-study, we explored citizen perceptions and experiences of AI with a short survey (N = 91). Second, scenario-based interviews (N = 7) were utilized to gain in-depth insights of citizen perceptions of AI in the Smart City context. Five central themes were recognized: (1) I don’t like them monitoring me, (2) I want maximum gain for minimum effort, (3) I don’t want AI to mimic people, (4) I’ll avoid using AI if I consider the risk too high, and (5) I don’t need to be concerned about AI. These offer an idea of human-centered requirements worth considering while designing AI applications for future Smart Cities.","",""
"2023","From the ground up: developing a practical ethical methodology for integrating AI into industry","","",""
"2023","Keeping the organization in the loop: a socio-technical extension of human-centered artificial intelligence","AbstractThe human-centered AI approach posits a future in which the work done by humans and machines will become ever more interactive and integrated. This article takes human-centered AI one step further. It argues that the integration of human and machine intelligence is achievable only if human organizations—not just individual human workers—are kept “in the loop.” We support this argument with evidence of two case studies in the area of predictive maintenance, by which we show how organizational practices are needed and shape the use of AI/ML. Specifically, organizational processes and outputs such as decision-making workflows, etc. directly influence how AI/ML affects the workplace, and they are crucial for answering our first and second research questions, which address the pre-conditions for keeping humans in the loop and for supporting continuous and reliable functioning of AI-based socio-technical processes. From the empirical cases, we extrapolate a concept of “keeping the organization in the loop” that integrates four different kinds of loops: AI use, AI customization, AI-supported original tasks, and taking contextual changes into account. The analysis culminates in a systematic framework of keeping the organization in the loop look based on interacting organizational practices.","",""
"2023","On and beyond artifacts in moral relations: accounting for power and violence in Coeckelbergh’s social relationism","AbstractThe ubiquity of technology in our lives and its culmination in artificial intelligence raises questions about its role in our moral considerations. In this paper, we address a moral concern in relation to technological systems given their deep integration in our lives. Coeckelbergh develops a social-relational account, suggesting that it can point us toward a dynamic, historicised evaluation of moral concern. While agreeing with Coeckelbergh’s move away from grounding moral concern in the ontological properties of entities, we suggest that it problematically upholds moral relativism. We suggest that the role of power, as described by Arendt and Foucault, is significant in social relations and as curating moral possibilities. This produces a clearer picture of the relations at hand and opens up the possibility that relations may be deemed violent. Violence as such gives us some way of evaluating the morality of a social relation, moving away from Coeckelbergh’s seeming relativism while retaining his emphasis on social–historical moral precedent.","",""
"2023","Ecological ethics and the smart circular economy"," The corporate discourse on the circular economy holds that the growth of the electronics industry, driven by continuous innovation, does not imperil ecological sustainability. To achieve sustainable growth, its advocates propose optimizing recycling by means of artificial intelligence and sets of interrelated datacentric and algorithmic technologies. Drawing on critical data and algorithm studies, theories of waste, and empirical research, this paper investigates ecological ethics in the context of the datacentric and algorithmically mediated circular economy. It foregrounds the indeterminate and fickle material nature of waste as well as the uncertainties inherent in, and stemming from, datafication and computation. My question is: how do the rationalities, affordances, and dispositions of datacentric and algorithmic technologies perform and displace notions of corporate responsibility and transparency? In order to answer this question, I compare the smart circular economy to the informal recycling practices that it claims to replace, and I analyze relations between waste matter and data as well as distributions of agency. Specifically, I consider transitions and slippages between response-ability and responsibility. Conceptually, I bring process-relation or immanence-based philosophies such as Bergson's and Deleuze's into a debate about relations between waste matter and data and the ambition of algorithmic control over waste. My aim is not to demand heightened corporate responsibility enacted through control but to rethink responsibility in the smart circular economy along the lines of Amoore's cloud ethics to carve out a position of critique beyond either a deontological perspective that reinforces corporate agency or new-materialist denunciation of the concept. ","",""
"2023","The problem with annotation. Human labour and outsourcing between France and Madagascar","Artificial intelligence advancements have reignited job displacement debates that focus on how the use of artificial intelligence affects labour, without considering how the production of this technology influences labour division. The generalisation of machine learning has created an increased demand for outsourced data workers. Outsourcing companies and crowdwork platforms are both used to generate, annotate, and enrich data. This data tasks are performed by workers from low-income countries, who often earn poverty wages. As with traditional outsourcing, workers must integrate complex multinational subcontracting networks. In this article, we examine how France outsources artificial intelligence-related tasks to workers in the African island nation of Madagascar. For our study, we interviewed 26 data workers, eight employees of French start-ups, and conducted secondary research on two artificial intelligence systems – a canteen checkout terminal and an algorithm to detect shoplifters in stores. The data collected allowed us to reconstruct an end-to-end artificial intelligence production value chain, revealing the need for data classification and artificial intelligence problematisation. Commercial artificial intelligence, therefore, does not displace employment by automating service jobs. Rather, by delocalising labour into the Global South, it lengthens the externalisation chain.","",""
"2023","The uncontroversial ‘thingness’ of AI"," This commentary starts with the question ‘How is it that AI has come to be figured uncontroversially as a thing, however many controversies “it” may engender?’ Addressing this question takes us to knowledge practices that philosopher of science Helen Verran has named a ‘hardening of the categories’, processes that not only characterise the onto-epistemology of AI but also are central to its constituent techniques and technologies. In a context where the stabilization of AI as a figure enables further investments in associated techniques and technologies, AI's status as controversial works to reiterate both its ontological status and its agency. It follows that interventions into the field of AI controversies that fail to trouble and destabilise the figure of AI risk contributing to its uncontroversial reproduction. This is not to deny the proliferating data and compute-intensive techniques and technologies that travel under the sign of AI but rather to call for a keener focus on their locations, politics, material-semiotic specificity, and effects, including their ongoing enactment as a singular and controversial object. ","",""
"2023","Formally comparing topic models and human-generated qualitative coding  of physician mothers’ experiences  of workplace discrimination"," Differences between computationally generated and human-generated themes in unstructured text are important to understand yet difficult to assess formally. In this study, we bridge these approaches through two contributions. First, we formally compare a primarily computational approach, topic modeling, to a primarily human-driven approach, qualitative thematic coding, in an impactful context: physician mothers’ experience of workplace discrimination. Second, we compare our chosen topic model to a principled alternative topic model to make explicit study design decisions meriting consideration in future research. By formally contrasting computationally generated (i.e. topic modeling) and human-generated (i.e. thematic coding) knowledge, we shed light on issues of interest to several audiences, notably computational social scientists who wish to understand study design tradeoffs, and qualitative researchers who may wish to leverage computational methods to improve the speed and reproducibility of labor-intensive coding. Although useful in other domains, we highlight the value of fast, reproducible methods to better understand experiences of workplace discrimination. ","",""
"2023","Stepping back from Data and AI for Good – current trends and ways forward"," Various ‘Data for Good’ and ‘AI for Good’ initiatives have emerged in recent years to promote and organise efforts to use new computational techniques to solve societal problems. The initiatives exercise ongoing influence on how the capabilities of computational techniques are understood as vehicles of social and political change. This paper analyses the development of the initiatives from a rhetorical slogan into a research program that understands itself as a ‘field’ of applications. It discusses recent academic literature on the topic to show a problematic entanglement between the promotion of initiatives and prescriptions of what ‘good’ ought to be. In contrast, we call researchers to take a practical and analytical step back. The paper provides a framework for future research by calling for descriptive research on the composition of the initiatives and critical research that draws from broader social science debates on computational techniques. The empirical part of the paper provides first steps towards this direction by positioning Data and AI for Good initiatives as part of a single continuum and situating it within a historical trajectory that has its immediate precursor in ICT for Development initiatives. ","",""
"2023","Freezing out: Legacy media's shaping of AI as a cold controversy","Mainstream coverage of artificial intelligence often appears to emphasise the technologies’ benefit and economic potential over its growing downsides. How does a technology poised to be so disruptive become so uncritically embraced? Why is it, simply put, that artificial intelligence's representations in legacy media do not normally convey the controversialities otherwise found in research or policy debates? We introduce the concept of ‘freezing out’ to describe processes of translation that cool down debates over the merits of technology. Freezing out looks at the other side of controversy studies to study the production of uncontroversies or cold controversies rather than hot topics and debates. We use the coverage of artificial intelligence in Canadian national news outlets to analyse how controversiality becomes ‘frozen out’. Since Canadian academics won the prestigious ImageNet prize in 2012 introducing the modern turn toward machine learning approaches, Canada has promoted itself as a global leader. Using in-depth interviews with Francophone and Anglophone journalists as well as topic modelling on data collected from five major newspapers, we find that routine news making processes between journalists, experts, entrepreneurs, and governments build, maintain, and promote Canada's artificial intelligence ecosystem. Freezing out contributes to a broader interest in how heterogeneous actors traverse their domain of expertise across policy, media, and research circles to cool down artificial intelligence controversies.","",""
"2023","European artificial intelligence policy as digital single market making"," Rapid innovation in digital services relying on artificial intelligence (AI) challenges existing regulations across a wide array of policy fields. The European Union (EU) has pursued a position as global leader on ethical AI regulation in explicit contrast to US laissez-faire and Chinese state surveillance approaches. This article asks how the seemingly heterogeneous approaches of market making and ethical AI are woven together at a deeper level in EU regulation. Combining quantitative analysis of all official EU documents on AI with in-depth reading of key reports, communications, and legislative corpora, we demonstrate that single market integration constitutes a fundamental but overlooked engine and structuring principle of new AI regulation. Under the influence of this principle, removing barriers to competition and the free flow of data, on the one hand, and securing ethical and responsible AI, on the other hand, are seen as compatible and even mutually reinforcing. ","",""
"2023","Nothing new under the sun: Medical professional maintenance in the face of artificial intelligence's disruption"," This paper follows the reaction of the radiology profession to artificial intelligence (AI). We examine the effort of radiology as a powerful medical specialty to maintain its professional jurisdiction while allowing AI's disruption. We study the discursive work of radiologists as evident in their academic publications. Our results suggest that radiologists hold simultaneously multiple perspectives in regard to AI, which allow them to be both conservative and innovative in their relations to it: accept it, subordinate it, reject it and surrender to it, all the same time. These perspectives are: (a) to integrate AI tools and skills into the radiology profession by cooperating and coproducing with AI experts while preserving the core values and structures of the radiology profession; (b) to absorb AI into radiology as (yet another) technology, subordinating it to radiologists’ authority; (c) to fight-off the threat made by AI by undermining the legitimacy and capabilities of AI in radiology and strengthening professional boundaries and (d) to assimilate the radiology profession into the field of AI. These perspectives enable radiologists as a powerful medical specialty to engage in a rhetorical dance with the equally powerful AI specialty and challenge techno-optimistic approaches to innovation. ","",""
"2023","The effectiveness of embedded values analysis modules in Computer Science education: An empirical study"," Embedding ethics modules within computer science courses has become a popular response to the growing recognition that computer science programs need to better equip their students to navigate the ethical dimensions of computing technologies such as artificial intelligence, machine learning, and big data analytics. However, the popularity of this approach has outpaced the evidence of its positive outcomes. To help close that gap, this empirical study reports positive results from Northeastern University's program that embeds values analysis modules into computer science courses. The resulting data suggest that such modules have a positive effect on students’ moral attitudes and that students leave the modules believing they are more prepared to navigate the ethical dimensions they will likely face in their eventual careers. Importantly, these gains were accomplished at an institution without a philosophy doctoral program, suggesting this strategy can be effectively employed by a wider range of institutions than many have thought. ","",""
"2023","Responsible AI literacy: A stakeholder-first approach","The need for citizens to better understand the ethical and social challenges of algorithmic systems has led to a rapid proliferation of AI literacy initiatives. After reviewing the literature on AI literacy projects, we found that most educational practices in this area are based on teaching programming fundamentals, primarily to K-12 students. This leaves out citizens and those who are primarily interested in understanding the implications of automated decision- making systems, rather than in learning to code. To address these gaps, this article explores the methodological contributions of responsible AI education practices that focus first on stakeholders when designing learning experiences for different audiences and contexts. The article examines the weaknesses identified in current AI literacy projects, explains the stakeholder-first approach, and analyzes several responsible AI education case studies, to illustrate how such an approach can help overcome the aforementioned limitations. The results suggest that the stakeholder-first approach allows to address audiences beyond the usual ones in the field of AI literacy, and to incorporate new content and methodologies depending on the needs of the respective audiences, thus opening new avenues for teaching and research in the field.","",""
"2023","Investigating hybridity in artificial intelligence research"," Research in the global field of artificial intelligence is increasingly hybrid in orientation. Researchers are beholden to the requirements of multiple intersecting spheres, such as scholarly, public, and commercial, each with their own language and logic. Relatedly, collaboration across disciplinary, sector and national borders is increasingly expected, or required. Using a dataset of 93,482 artificial intelligence publications, this article operationalises scholarly, public, and commercial spheres through citations, news mentions, and patent mentions, respectively. High performing publications (99th percentile) for each metric were separated into eight categories of influence. These comprised four blended categories of influence (news, patents and citations; news and patents; news and citations; patents and citations) and three single categories of influence (citations; news; patents), in addition to the ‘Other’ category of non-high performing publications. The article develops and applies two components of a new hybridity lens: evaluative hybridity and generative hybridity. Using multinomial logistic regression, selected aspects of knowledge production – research context, focus, artefacts, and collaborative configurations – were examined. The results elucidate key characteristics of knowledge production in the artificial intelligence field and demonstrate the utility of the proposed lens. ","",""
"2023","Ground truth tracings (GTT): On the epistemic limits of machine learning"," There is a gap in existing critical scholarship that engages with the ways in which current “machine listening” or voice analytics/biometric systems intersect with the technical specificities of machine learning. This article examines the sociotechnical assemblage of machine learning techniques, practices, and cultures that underlie these technologies. After engaging with various practitioners working in companies that develop machine listening systems, ranging from CEOs, machine learning engineers, data scientists, and business analysts, among others, I bring attention to the centrality of “learnability” as a malleable conceptual framework that bends according to various “ground-truthing” practices in formalizing certain listening-based prediction tasks for machine learning. In response, I introduce a process I call Ground Truth Tracings to examine the various ontological translations that occur in training a machine to “learn to listen.” Ultimately, by further examining this notion of learnability through the aperture of power, I take insights acquired through my fieldwork in the machine listening industry and propose a strategically reductive heuristic through which the epistemological and ethical soundness of machine learning, writ large, can be contemplated. ","",""
"2023","Algorithms and hegemony in the workplace: Negotiating design and values  in an Italian television platform"," In recent years, several scholars have highlighted the necessity to scrutinize the practices and material settings in which algorithmic models are designed, in order to unpack the working activities and socio-cultural constructs underlying their production and deployment process. Drawing on a multisited ethnography, this paper investigates the practices of tech workers within the corporate environment of an internet television platform, the hierarchical relationships between different professional figures, and how these individuals frame algorithms and contribute to the enactment of these systems with their activities. Findings highlight the hierarchical organization of tech work and the subordination of operative figures to the goals imposed by business clients and to both internal and external forms of control. Specifically, it emerges how the subalternity of tech workers is materially and discursively constructed and forms of causal, dispositional and facilitative power exerted on them. In this environment, frictions, negotiations as well as concealing strategies by tech workers regarding the design and meaning of algorithms emerge, thus showing their cultural, contingent and multiple composition. Within the framework of Giddens’ structure/agency cycle, it is shown how everyday working activities and relationships contribute to the reproduction of hegemonic arrangements in the workplace, and how these hegemonic arrangements are at the core of algorithmic production, thus playing a key role in the framing, construction and enactment of these systems. ","",""
"2023","The importance of algorithm skills for informed Internet use"," Using the Internet means encountering algorithmic processes that influence what information a user sees or hears. Existing research has shown that people's algorithm skills vary considerably, that they develop individual theories to explain these processes, and that their online behavior can reflect these understandings. Yet, there is little research on how algorithm skills enable people to use algorithms to their own benefit and to avoid harms they may elicit. To fill this gap in the literature, we explore the extent to which people understand how the online systems and services they use may be influenced by personal data that algorithms know about them, and whether users change their behavior based on this understanding. Analyzing 83 in-depth interviews from five countries about people's experiences with researching and searching for products and services online, we show how being aware of personal data collection helps people understand algorithmic processes. However, this does not necessarily enable users to influence algorithmic output, because currently, options that help users control the level of customization they encounter online are limited. Besides the empirical contributions, we discuss research design implications based on the diversity of the sample and our findings for studying algorithm skills. ","",""
"2023","Understanding user interactions and perceptions of AI risk in Singapore"," Artificial Intelligence (AI) is becoming increasingly prevalent and its application highly sophisticated. Concerns about AI's vulnerabilities and future threats have, however, long been debated among scientists and the tech community. By integrating Beck's theory of risk society with an audience-centered sense-making approach, we seek to understand the effects of AI on the general public's daily lives and their concerns when they adopt AI technology. Five focus groups with 36 participants from Singapore, a technologically advanced country, were conducted to investigate their risk perceptions of AI and AI-powered technology. We found that participants were not passive consumers of content showing up on their news feeds; indeed, some participants attempted to outsmart the algorithms. They were aware that tech companies often tweak algorithms to personalize content and drive consumers into rabbit holes. Nevertheless, despite a certain level of awareness and sporadic attempts to “outsmart” the system, many users might still be influenced by these algorithms, underscoring the extent to which consumers are often manipulated by smart tech powered by AI. We discuss the theoretical and policy implications of our findings by looking at the contextual factors. ","",""
"2023","Because the machine can discriminate: How machine learning serves and transforms biological explanations of human difference"," Research on scientific/intellectual movements, and social movements generally, tends to focus on resources and conditions outside the substance of the movements, such as funding and publication opportunities or the prestige and networks of movement actors. Drawing on Pinch’s theory of technologies as institutions, I argue that research methods can also serve as resources for scientific movements by institutionalizing their ideas in research practice. I demonstrate the argument with the case of neuroscience, where the adoption of machine learning changed how scientists think about measurement and modeling of group difference. This provided an opportunity for members of the sex difference movement by offering a ‘truly categorical’ quantitative methodology that aligned more closely with their understanding of male and female brains and bodies as categorically distinct. The result was a flurry of publications and symbiotic relationships with other researchers that rescued a scientific movement which had been growing increasingly untenable under the prior methodological regime of univariate, frequentist analyses. I call for increased sociological attention to the inner workings of technologies that we typically black box in light of their potential consequences for the social world. I also suggest that machine learning in particular might have wide-reaching implications for how we conceive of human groups beyond sex, including race, sexuality, criminality, and political position, where scientists are just beginning to adopt its methods. ","",""
"2023","Learning machine learning: On the political economy of big tech's online AI courses"," Machine learning (ML) algorithms are still a novel research object in the field of media studies. While existing research focuses on concrete software on the one hand and the socio-economic context of the development and use of these systems on the other, this paper studies online ML courses as a research object that has received little attention so far. By pursuing a walkthrough and critical discourse analysis of Google's Machine Learning Crash Course and IBM's introductory course to Machine Learning with Python, we not only shed light on the technical knowledge, assumptions, and dominant infrastructures of ML as a field of practice, but also on the economic interests of the companies providing the courses. We demonstrate how the online courses further support Google and IBM to consolidate and even expand their position of power by recruiting new AI talent and by securing their infrastructures and models to become the dominant ones. Further, we show how the companies not only influence greatly how ML is represented, but also how these representations in turn influence and direct current ML research and development, as well as the societal effects of their products. Here, they boast an image of fair and democratic artificial intelligence, which stands in stark contrast to the ubiquity of their corporate products and the advertised directives of efficiency and performativity the companies strive for. This underlines the need for alternative infrastructures and perspectives. ","",""
"2023","AI Empire: Unraveling the interlocking systems of oppression in generative AI's global order","As artificial intelligence (AI) continues to captivate the collective imagination through the latest generation of generative AI models such as DALL-E and ChatGPT, the dehumanizing and harmful features of the technology industry that have plagued it since its inception only seem to deepen and intensify. Far from a “glitch” or unintentional error, these endemic issues are a function of the interlocking systems of oppression upon which AI is built. Using the analytical framework of “Empire,” this paper demonstrates that we live not simply in the “age of AI” but in the age of AI Empire. Specifically, we show that this networked and distributed global order is rooted in heteropatriarchy, racial capitalism, white supremacy, and coloniality and perpetuates its influence through the mechanisms of extractivism, automation, essentialism, surveillance, and containment. Therefore, we argue that any attempt at reforming AI from within the same interlocking oppressive systems that created it is doomed to failure and, moreover, risks exacerbating existing harm. Instead, to advance justice, we must radically transform not just the technology itself, but our ideas about it, and develop it from the bottom up, from the perspectives of those who stand the most risk of being harmed.","",""
"2023","From rules to examples: Machine learning's type of authority"," This paper analyzes the effects of a perceived transition from a rule-based computer programming paradigm to an example-based paradigm associated with machine learning. While both paradigms coexist in practice, we critically discuss the distinctive epistemological and ethical implications of machine learning's “exemplary” type of authority. To capture its logic, we compare it to computer programming rules that date to the middle of the 20th century, showing how rules and examples have regulated human conduct in significantly different ways. In contrast to the highly constructed, explicit, and prescriptive form of authority imposed by programming rules, machine learning models are trained using data that has been made into examples. These examples elicit norms in an implicit, emergent manner to make prediction and classification possible. We analyze three ways that examples are produced in machine learning: labeling, feature engineering, and scaling. We use the phrase “artificial naturalism” to characterize the tensions of this type of authority, in which examples sit ambiguously between data and norm. ","",""
"2023","Artificial intelligence and skills in the workplace: An integrative research agenda"," The development and diffusion of artificial intelligence (AI) technologies in workplaces are transforming the nature of work practices and their constituent skill requirements. This dual transformation is challenging for workers, organisations and societies, who are faced with the need to develop and enhance extant and new skills required to succeed in increasingly AI-mediated work settings. Although literature has recognised skills as a key factor in the development and uptake of AI technologies, there has been paucity of empirical research on the precise nature of skill requirements in AI-mediated workplaces. This commentary argues that to advance our understanding of skill requirements in AI-mediated workplaces, an integrative, multidisciplinary, multimethod and multistakeholder approach is required. The commentary proposes an agenda for future research in this societally important but poorly understood area. ","",""
"2023","The promises and challenges of addressing artificial intelligence with human rights"," This paper examines the potential promises and limitations of the human rights framework in the age of AI. It addresses the question: what, if anything, makes human rights well suited to face the challenges arising from new and emerging technologies like AI? It argues that the historical evolution of human rights as a series of legal norms and concrete practices has made it well placed to address AI-related challenges. The human rights framework should be understood comprehensively as a combination of legal remedies, moral justification, and political analysis that inform one another. Over time, the framework has evolved in ways that accommodate the balancing of contending rights claims, using multiple ex ante and ex post facto mechanisms, involving government and/or business actors, and in situations of diffuse responsibility that may or may not result from malicious intent. However, the widespread adoption of AI technologies pushes the moral, sociological, and political boundaries of the human rights framework in other ways. AI reproduces long-term, structural problems going beyond issue-by-issue regulation, is embedded within economic structures that produce cumulative negative effects, and introduces additional challenges that require a discussion about the relationship between human rights and science &amp; technology. Some of the reasons for why AI produces problematic outcomes are deep rooted in technical intricacies that human rights practitioners should be more willing than before to get involved in. ","",""
"2023","Algorithmic constructions of risk: Anticipating uncertain futures in child protection services"," This paper examines how predictive algorithms construct risk by calculating and anticipating children's uncertain futures. Theoretically, we analyze algorithmic risk construction by attending to (a) the problematizations justifying algorithmic prediction, (b) their underpinning data infrastructures, and (c) the configurations of agencies across humans and machines. Empirically, we examine two experiments in Danish child protection services that developed algorithmic models to predict children's maltreatment. Our analysis highlights how algorithmic predictions can create different notions of risk. The first case used predictive algorithms to supplement human risk assessments with data from child protection services, while the second case aimed to detect risk early by constructing parents as risk factors, requiring data from other welfare sectors. By comparing these cases, we highlight two distinct risk constructions: one that uses algorithmic prediction to manage uncertainty and another that seeks to eliminate undesired futures by preempting risk. These different constructions have implications for how the present is viewed as a moment of intervention and for how families are constructed as “risk objects.” ","",""
"2023","Redress and worldmaking: Differing approaches to algorithmic reparations for housing justice"," A reparative approach to algorithmic justice provides a compelling alternative to existing fairness-based frameworks, which are often inadequate for challenging the technological perpetuation of unjust social hierarchies. The definition of “reparations,” however, is philosophically contested. I discuss two interrelated but distinct notions of reparations: reparations as accountability and redress for past injustice, and reparations as a constructive worldmaking project focused on present and future justice. Each of these perspectives offers different recommendations and provocations for how to implement algorithmic reparations. I apply this to a case study of housing injustice in the US and offer three interpretations of “algorithmic reparations” in context: first, we can litigate instances of algorithmic discrimination in housing. Second, we can use computational methods to compute damages and demand redress for structural housing injustice in the past. Finally, we can repurpose algorithmic methods to imagine more radical resistance efforts that connect incremental reform to large-scale structural change for the future. ","",""
"2023","Structured like a language model: Analysing AI as an automated subject","Drawing from the resources of psychoanalysis and critical media studies, in this article we develop an analysis of large language models (LLMs) as ‘automated subjects’. We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which artificial intelligence (AI) behaviour, including its productions of bias and harm, can be analysed. First, we introduce language models, discuss their significance and risks, and outline our case for interpreting model design and outputs with support from psychoanalytic concepts. We trace a brief history of language models, culminating with the releases, in 2022, of systems that realise ‘state-of-the-art’ natural language processing performance. We engage with one such system, OpenAI's InstructGPT, as a case study, detailing the layers of its construction and conducting exploratory and semi-structured interviews with chatbots. These interviews probe the model's moral imperatives to be ‘helpful’, ‘truthful’ and ‘harmless’ by design. The model acts, we argue, as the condensation of often competing social desires, articulated through the internet and harvested into training data, which must then be regulated and repressed. This foundational structure can however be redirected via prompting, so that the model comes to identify with, and transfer , its commitments to the immediate human subject before it. In turn, these automated productions of language can lead to the human subject projecting agency upon the model, effecting occasionally further forms of countertransference. We conclude that critical media methods and psychoanalytic theory together offer a productive frame for grasping the powerful new capacities of AI-driven language systems.","",""
"2023","Ethical assessments and mitigation strategies for biases in AI-systems used during the COVID-19 pandemic"," The main aim of this article is to reflect on the impact of biases related to artificial intelligence (AI) systems developed to tackle issues arising from the COVID-19 pandemic, with special focus on those developed for triage and risk prediction. A secondary aim is to review assessment tools that have been developed to prevent biases in AI systems. In addition, we provide a conceptual clarification for some terms related to biases in this particular context. We focus mainly on non-racial biases that may be less considered when addressing biases in AI systems in the existing literature. In the manuscript, we found that the existence of bias in AI systems used for COVID-19 can result in algorithmic justice and that the legal frameworks and strategies developed to prevent the apparition of bias have failed to adequately consider social determinants of health. Finally, we make some recommendations on how to include more diverse professional profiles in order to develop AI systems that increase the epistemic diversity needed to tackle AI biases during the COVID-19 pandemic and beyond. ","",""
"2023","Dislocated accountabilities in the  “AI supply chain”: Modularity and developers’ notions of responsibility"," Responsible artificial intelligence guidelines ask engineers to consider how their systems might harm. However, contemporary artificial intelligence systems are built by composing many preexisting software modules that pass through many hands before becoming a finished product or service. How does this shape responsible artificial intelligence practice? In interviews with 27 artificial intelligence engineers across industry, open source, and academia, our participants often did not see the questions posed in responsible artificial intelligence guidelines to be within their agency, capability, or responsibility to address. We use Suchman's “located accountability” to show how responsible artificial intelligence labor is currently organized and to explore how it could be done differently. We identify cross-cutting social logics, like modularizability, scale, reputation, and customer orientation, that organize which responsible artificial intelligence actions do take place and which are relegated to low status staff or believed to be the work of the next or previous person in the imagined “supply chain.” We argue that current responsible artificial intelligence interventions, like ethics checklists and guidelines that assume panoptical knowledge and control over systems, could be improved by taking a located accountability approach, recognizing where relations and obligations might intertwine inside and outside of this supply chain. ","",""
"2023","Clinical algorithms, racism, and “fairness” in healthcare: A case of bounded justice"," To date, attempts to address racially discriminatory clinical algorithms have largely focused on fairness and the development of models that “do no harm.” While the push for fairness is rooted in a desire to avoid or ameliorate health disparities, it generally neglects the role of racism in shaping health outcomes and does little to repair harm to patients. These limitations necessitate reconceptualizing how clinical algorithms should be designed and employed in pursuit of racial justice and health equity. A useful lens for this work is bounded justice, a concept and research analytic proposed by Melissa Creary to guide multidisciplinary health equity interventions. We describe how bounded justice offers a lens for (1) articulating the deep injustices embedded in the datasets, methodologies, and sociotechnical infrastructure underlying design and implementation of clinical algorithms and (2) envisioning how these algorithms can be redesigned to contribute to larger efforts that not only address current inequities, but to redress the historical mistreatment of communities of color by biomedical institutions. Thus, the aim of this article is two-fold. First, we apply the bounded justice analytic to fairness and clinical algorithms by describing structural constraints on health equity efforts such as medical device regulatory frameworks, race-based medicine, and racism in data. We then reimagine how clinical algorithms could function as a reparative technology to support justice and empower patients in the healthcare system. ","",""
"2023","Terms-we-serve-with: Five dimensions for anticipating and repairing algorithmic harm"," Power and information asymmetries between people and digital technology companies are further legitimized through contractual agreements that fail to provide meaningful consent and contestability. In particular, the Terms-of-Service (ToS) agreement, is a contract of adhesion where companies effectively set the terms and conditions of the contract. Whereas, ToS reinforce existing structural inequalities, we seek to enable an intersectional accountability mechanism grounded in the practice of algorithmic reparation. Building on existing critiques of ToS in the context of algorithmic systems, we return to the roots of contract theory by recentering notions of agency and mutual assent. We evolve a multipronged intervention we frame as the Terms-we-Serve-with (TwSw) social, computational, and legal framework. The TwSw is a new social imaginary centered on: (1) co-constitution of user agreements, through participatory mechanisms; (2) addressing friction, leveraging the fields of design justice and critical design in the production and resolution of conflict; (3) enabling refusal mechanisms, reflecting the need for a sufficient level of human oversight and agency including opting out; (4) complaint and algorithmic harms reporting, through a feminist studies lens and open-sourced computational tools; and (5) disclosure-centered mediation, to disclose, acknowledge, and take responsibility for harm, drawing on the field of medical law. We further inform our analysis through an exploratory design workshop with a South African gender-based violence reporting AI startup. We derive practical strategies for communities, technologists, and policy-makers to leverage a relational approach to algorithmic reparation and propose there's a need for a radical restructuring of the “take-it-or-leave-it” ToS agreement. ","",""
"2023","Race-neutral vs race-conscious: Using algorithmic methods to evaluate the reparative potential of housing programs"," The racial wealth gap in the United States remains a persistent issue; white individuals possess six times more wealth than Black individuals. Leading scholars and public figures have pointed to slavery and post-slavery discrimination as root cause factors and called for reparations. Yet the institutionalization of race-neutral ideologies in policies and practices hinders a reparative approach to closing the racial wealth gap. This study models the use of algorithmic methods in the service of reparations to Black Americans in the domain of housing, where most American wealth is built. We examine a hypothetical scenario for measuring the effectiveness of race-conscious Special Purpose Credit Programs (SPCPs) in reducing the housing racial wealth gap compared to race-neutral SPCPs. We use a predictive model to show that race-conscious, people-based lending programs, if they were nationally available, would be two to three times more effective in closing the racial housing wealth gap than other, existing forms of SPCPs. In doing so, we also demonstrate the potential for using algorithms and computational methods to support outcomes aligned with movements for reparations, another possible meaning for the emerging discourse on “algorithmic reparations.” ","",""
"2023","AI incidents and ‘networked trouble’: The case for a research agenda"," Against a backdrop of widespread interest in how publics can participate in the design of AI, I argue for a research agenda focused on AI incidents – examples of AI going wrong and sparking controversy – and how they are constructed in online environments. I take up the example of an AI incident from September 2020, when a Twitter user created a ‘horrible experiment’ to demonstrate the racist bias of Twitter's algorithm for cropping images. This resulted in Twitter not only abandoning its use of that algorithm, but also disavowing its decision to use any algorithm for the task. I argue that AI incidents like this are a significant means for participating in AI systems that require further research. That research agenda, I argue, should focus on how incidents are constructed through networked online behaviours that I refer to as ‘networked trouble’, where formats for participation enable individuals and algorithms to interact in ways that others – including technology companies – come to know and come to care about. At stake, I argue, is an important mechanism for participating in the design and deployment of AI. ","",""
"2023","Extrapolation and AI transparency: Why machine learning models should reveal when they make decisions beyond their training"," The right to artificial intelligence (AI) explainability has consolidated as a consensus in the research community and policy-making. However, a key component of explainability has been missing: extrapolation, which can reveal whether a model is making inferences beyond the boundaries of its training. We report that AI models extrapolate outside their range of familiar data, frequently and without notifying the users and stakeholders. Knowing whether a model has extrapolated or not is a fundamental insight that should be included in explaining AI models in favor of transparency, accountability, and fairness. Instead of dwelling on the negatives, we offer ways to clear the roadblocks in promoting AI transparency. Our commentary accompanies practical clauses useful to include in AI regulations such as the AI Bill of Rights, the National AI Initiative Act in the United States, and the AI Act by the European Commission. ","",""
"2023","Attuning to the Erratic End of Life:  The Logic of Care in Hospice at Home","How do dying people receive good care at home in a highly institutionalized death context? The Ministry of Health and Welfare in Taiwan, for example, has promoted hospice home care and respecting patient autonomy to improve the quality of end-of-life experiences. However, this study finds that end-of-life care is not automatically personalized or empowering for patients. From the theoretical perspective of care practices, this study accentuates the importance of family carers’ invisible work in achieving these goals for patients. Drawing from in-depth interviews and twelve months of participant observation in a medical center in northern Taiwan, the study found that family caregivers are meticulously attuned to the patient’s condition to provide care, which includes rearranging the place, coordinating resources and other carers, and practicing care. This paper reveals that the practice of hospice home care does not depend merely on the patient’s autonomy but also on the family caregivers’ and medical team’s work, which is relatively invisible within the health insurance system.","",""
"2023","‘This is a House’: Large Image Collections and Their Platform Embeddings"," This paper analyses the active role of image collections in supporting platforms and their operations. Large image collections are increasingly present on media, scientific and other platforms. A case study of Facebook’s predictive modelling of satellite images of human settlement exemplifies how image collections are changing. The treatment of images in a predictive model – a deep neural network – constructs a condensed indexical field, a field that allows the platform to generate referential statements about the world. Under platform conditions, image collections function less as archives or records and more as densely woven indexical fields that orient, position and embed the platform. In describing the transformation of image collections, the paper points to important changes in how platforms use images to position themselves in the world. ","",""
"2023","It’s not her fault: Trust through anthropomorphism among young adult Amazon Alexa users"," Voice assistants (VAs) like Alexa have been integrated into hundreds of millions of homes, despite persistent public distrust of Amazon. The current literature explains this trend by examining users’ limited knowledge of, concern about, or even resignation to surveillance. Through in-depth, semi-structured interviews ( n = 16), we explore how young adult Alexa users make sense of continuing to use the VA while generally distrusting Amazon. We identify three strategies that participants use to manage distrust: separating the VA from the company through anthropomorphism, expressing digital resignation, and occasionally taking action, like moving Alexa or even unplugging it. We argue that these individual-level strategies allow users to manage their concerns about Alexa and integrate the VA into domestic life. We conclude by discussing the implications these individual choices have for personal privacy and the rapid expansion of surveillance technologies into intimate life. ","",""
"2023","AI and the social construction of creativity"," Artificial Intelligence (AI) encroaches on new terrains of human activity by dint of its efficacy and an expanding ability to autonomously incorporate information from many disciplines and sources. In this paper, we focus specifically on how AI affects the communicative practices associated with creativity. AI has the capacity to reshape discipline and taste communities by providing new content that competes with human production and by mediating between human activity and information sources. To frame these issues, we turn to the influential systems model of creativity devised by Mihaly Csikszentmihalyi (1996) , which Csikszentmihalyi and Daniel Gruner (2018) recently extended to incorporate AI, redubbing it Creativity 4.0. The model assesses how AI affects the social structure of creative practice without overly accentuating the similarity between humans and AI, or questioning whether computational devices will replace creative jobs. The paper examines Gruner and Csikszentmihalyi’s revised systems model, arguing that it does not sufficiently take into account the variety of ways that AI can be incorporated into creative practice. Prompted by a theoretical reflection on the nature of the model and the emerging features of AI, we propose a new version of the model that highlights how embedded AIs play a key role in filtering and gatekeeping, as well as the importance of generative systems in informing creative practice. We propose that any discussion of AI and the future of creative practice should look at where and how AI supported technologies are used. We examine how AI can reduce and shape the qualitative diversity of sources of inspiration drawn into the creative process, with the associated technological biases, as well as provide an emergent platform for the development of novel ideas. ","",""
"2023","The social robot? Analyzing whether and how the telepresence robot AV1 affords socialization"," Telepresence robots are increasingly used in schools as a way of including students who are unable to be physically present in the classroom with other students. The use of such robots is intended not just to help students follow their education but also to serve a social purpose. However, the extent to which the robots actually afford socializing needs to be explored further. This article analyzes how, to what extent, for whom, and under what circumstances the telepresence robot AV1 affords social contact for the heterogenous group of homebound Norwegian upper secondary school students. Building on Jenny Davis’ mechanisms and conditions framework of affordances, we focus on how AV1 affords for different students in specific circumstances. Our analysis draws on interviews with 11 upper secondary school students in Norway and finds that individual traits and circumstances such as health issues and social networks are important aspects when assessing whether a technology affords socializing. Based on our findings, we argue for expanding the mechanisms and conditions framework to include not just its current focus on perception, dexterity, and cultural and institutional legitimacy, but also the users’ emotions. ","",""
"2023","Aberrant AI creations: co-creating surrealist body horror using the DALL-E Mini text-to-image generator"," The emergence in 2022 of surreal and grotesque image sets created using the free online AI text-to-image generator DALL-E Mini (Craiyon) prompts our analysis of their aesthetic content and connections to preexisting media forms and trends in digital culture. DALL-E Mini uses an unfiltered database of images from the internet to create new images based on a user’s text prompt, often resulting in misshapen bodies and impossible scenarios. Despite its technological limitations, DALL-E Mini’s popularity as a meme-making tool is visible on social media platforms, where crowd-sourced images are shared and experimentation with the tool is encouraged. Through comparison with existing artistic practices and formats (creative automata, surrealism, body horror, celebrity memes), we argue that DALL-E Mini creations can be understood as human-AI co-creations and forms of aesthetic mimicry. Building on the ideas of surrealists such as André Breton, we propose that DALL-E Mini’s images, prompts and the grid interface adhere to surrealism’s historical interests in the unconscious, the uncanny, and the collaborative ‘exquisite corpse’ parlour game. We also consider DALL-E Mini’s relevance to the category of ‘AI Arts’, Patricia De Vries’s call for more research that relates algorithms to the broader artistic and cultural contexts in which they are embedded (2020), and the ‘authoring’ of celebrity bodies as data (Kanai, 2016). Our theorisation of DALL-E Mini is supported by examples drawn from social media and personal experiments with the generator. Overall, we propose that internet users’ experimentation with DALL-E Mini corresponds with a cultural moment in which AI imaging technologies are eliciting excitement and anxiety. The outputs are revealed to be reliant on users’ pop cultural knowledge, with DALL-E Mini allowing for a playful, co-creative algorithmic practice, wherein contemporary anxieties about digital labour, (post)digital culture, biopolitics, and global issues are redirected into surreal visual storyworlds. ","",""
"2023","Microstock images of artificial intelligence: How AI creates its own conditions of possibility"," The main goal of this paper is to account for the ‘algorithmization’ of microstock imagery. By this term, the authors refer to a material process implying the chronic use of graphic editors, semi-automatic keywording allowing complex and dynamic proto-classifications, and access to the images via search engines. The algorithmization of microstock imagery also goes along with the exploitation of producers’ labour, so that the authors recognize in it a form of digital labour. Moreover, the term ‘algorithmization’ is meant to underline that this material process has symbolic effects on the image contents as well as on people’s expectations and imaginaries of these contents. The paper analyses, in particular, the case study of microstock images depicting artificial intelligence (AI). By producing hundreds of thousands of visual representations of AI that spread via the Web and beyond it, algorithmized microstock imagery also produces its own symbolic conditions of possibility, that is, the expectations and imaginaries that contribute to the success of AI beyond its concrete effectiveness. The paper is structured into three sections. In the first section, the authors account for the existing literature on stock imagery. They contend that this literature focuses too much on the symbolic message, and too little on the material processes of production of these images. In the second section, the authors describe an empirical analysis they conducted on Shutterstock images depicting AI. In the third section, they distinguish three forms of digital labour and show that microstock imagery entertains resemblances to and differences from each form. They contend that despite its peculiarities, microstock image production is a paradigmatic form of digital labour due to its convergence towards algorithmization. In the conclusion, the authors show how, for microstock images depicting AI, the algorithmic loop of microstock imagery is complete. ","",""
"2023","reCAPTCHA challenges and the production of the ideal web user","The CAPTCHA (Completely Automated Turing test to tell Computers and Humans Apart) is found throughout many websites. By challenging users to read a line of scrambled letters, identify crosswalks in an image, or some complete another task that is difficult for a computer to do, but comparatively trivial for a human user, a CAPTCHA verifies that the user is an actual human, and not software meant to interact maliciously with the website. This mundane and easily overlooked interface element is an important site where corporate interests and priorities act upon the people who encounter it. CAPTCHAs operate under the assumptions that difference can be detected and that it should be enforced. Because not all humans are able to solve a CAPTCHA, the test additionally enforces a boundary between humans and users. In this article, I analyze the discourses of Google’s reCAPTCHA and argue that this common interface element is a multi-faceted site of production where user labor is extracted every time they solve a reCAPTCHA. The products of this labor are threefold: (1) spam reduction, (2) artificial intelligence and machine learning training data and (3) an ideal of a normative web user. This last product is often overlooked but has wide-reaching implications. Users who solve reCAPTCHAs are producers but simultaneously are produced as users by the reCAPTCHA. The only humans who qualify as ‘authentic’ users are those who can perform this productive labor. Because Google’s reCAPTCHA operates as a site of invisible digital labor, this article works toward making such labor more visible so that users can become more aware of the work they are being asked to perform, and to what ends.","",""
"2023","The Art of Strategic Conversation: Surveillance, AI, and the IMAJINE Scenarios","In this dialogue, we explore the use of scenarios to inform thinking about the surveillant dimensions of AI systems. The aim is neither to predict times to come nor express a desired state, but to manufacture contrasting future visions that challenge assumptions existing in the present. To consider these issues, we convened four researcher-practitioners—Carissa Véliz, Malka Older, Annina Lux, and Matthew Finch—whose work encompasses AI and privacy ethics, strategic foresight, philosophy, social science, and the writing of science fiction.","",""
"2023","Review of Leuprecht and McNorton’s Intelligence as Democratic Statecraft","","",""
"2023","Unusual Suspects","The use of artificial intelligence in facial recognition systems has been controversial. Among issues of concerns is the accuracy of such systems for recognising faces of non-white people. This work turns the debate on its head by showing six images of AI generated faces using identical prompts that include the words “Asian woman” and “facial recognition biometrics person technology” via Text 2 Dream in Deep Dream Generator. Rather than investigating the level of accuracy in facial recognition systems, it demonstrates how a particular AI software creates visual representation of “Asian women.” The experiment explores the interaction between text (prompt) and a particular generative algorithm. It raises questions about the data on which the algorithm is trained, how images are labelled/interpreted in training data, and the underlying power AI algorithms have in reproducing/changing stereotypes. Not transparent to the viewers is the role of the artist in selecting/framing prompts and “starter” images.","",""
"2023","Encoding normative ethics: On algorithmic bias and disability","Computer-based algorithms have the potential to encode and exacerbate ableism and may contribute to disparate outcomes for disabled people. The threat of algorithmic bias to people with disabilities is inseparable from the longstanding role of technology as a normalizing agent, and from questions of how society defines shared values, quantifies ethics, conceptualizes and measures risk, and strives to allocate limited resources. This article situates algorithmic bias amidst the larger context of normalization, draws on social and critical theories that can be used to better understand both ableism and algorithmic bias as they operate in the United States, and proposes concrete steps to mitigate harm to the disability community as a result of algorithmic adoption. Examination of two cases — the allocation of lifesaving medical interventions during the COVID-19 pandemic and approaches to autism diagnosis and intervention — demonstrate instances of the mismatch between disabled people’s lived experiences and the goals and understandings advanced by nondisabled people. These examples highlight the ways particular ethical norms can become part of technological systems, and the harm that can ripple outward from misalignment of formal ethics and community values.","",""
"2023","Social bot detection in the age of ChatGPT: Challenges and opportunities","We present a comprehensive overview of the challenges and opportunities in social bot detection in the context of the rise of sophisticated AI-based chatbots. By examining the state of the art in social bot detection techniques and the more salient real-world application to date, we identify gaps and emerging trends in the field, with a focus on addressing the unique challenges posed by AI-generated conversations and behaviors. We suggest potentially promising opportunities and research directions in social bot detection, including (i) the use of generative agents for synthetic data generation, testing and evaluation; (ii) the need for multimodal and cross-platform detection based on network and behavioral signatures of coordination and influence; (iii) the opportunity to extend bot detection to non-English and low-resource language settings; and, (iv) the room for development of collaborative, federated learning detection models that can help facilitate cooperation between different organizations and platforms while preserving user privacy.","",""
"2023","Should ChatGPT be biased? Challenges and risks of bias in large language models","As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.","",""
"2023","Definition drives design: Disability models and mechanisms of bias in AI technologies","The increasing deployment of artificial intelligence (AI) tools to inform decision-making across diverse areas including healthcare, employment, social benefits, and government policy, presents a serious risk for disabled people, who have been shown to face bias in AI implementations. While there has been significant work on analysing and mitigating algorithmic bias, the broader mechanisms of how bias emerges in AI applications are not well understood, hampering efforts to address bias where it begins. In this article, we illustrate how bias in AI-assisted decision-making can arise from a range of specific design decisions, each of which may seem self-contained and non-biasing when considered separately. These design decisions include basic problem formulation, the data chosen for analysis, the use the AI technology is put to, and operational design elements in addition to the core algorithmic design. We draw on three historical models of disability common to different decision-making settings to demonstrate how differences in the definition of disability can lead to highly distinct decisions on each of these aspects of design, leading in turn to AI technologies with a variety of biases and downstream effects. We further show that the potential harms arising from inappropriate definitions of disability in fundamental design stages are further amplified by a lack of transparency and disabled participation throughout the AI design process. Our analysis provides a framework for critically examining AI technologies in decision-making contexts and guiding the development of a design praxis for disability-related AI analytics. We put forth this article to provide key questions to facilitate disability-led design and participatory development to produce more fair and equitable AI technologies in disability-related contexts.","",""
"2023","Classifying constructive comments","We introduce the Constructive Comments Corpus (C3), comprised of 12,000 annotated news comments, intended to help build new tools for online communities to improve the quality of their discussions. We define constructive comments as high-quality comments that make a contribution to the conversation. We explain the crowd worker annotation scheme and de ne a taxonomy of subcharacteristics of constructiveness. The quality of the annotation scheme and the resulting dataset is evaluated using measurements of inter-annotator agreement, expert assessment of a sample, and by the constructiveness sub-characteristics, which we show provide a proxy for the general constructiveness concept. We provide models for constructiveness trained on C3 using both feature-based and a variety of deep learning approaches and demonstrate, through domain adaptation experiments, that these models capture general rather than topic- or domain-specific characteristics of constructiveness. We also examine the role that length plays in our models, as comment length could be easily gamed if models depend heavily upon this feature. By examining the errors made by each model and their distribution by length, we show that the best performing models are effective independently of comment length. The constructiveness corpus and our experiments pave the way for a moderation tool focused on promoting comments that make a meaningful contribution, rather than only filtering out undesirable content.","",""
"2023","Autism Robot Therapy, Remediation, and Mimetic Disabling","This article presents a critical study of the discursive positioning of social robots as intermediaries in remedial practices of care in autism therapy. It begins by examining the promotion of social robots to augment autistic children’s social skills as a form of remediation in which they take on a mediator role between therapist and patient. Drawing from McGuire’s genealogy of autism advocacy, I argue that the therapy robots operate as part of a network of coordinating and normalizing strategies that emerges in response to the evident “crisis” of autism. Hayle’s concept of material metaphor is then applied to highlight the potential discursive-material relations of the interposing role played by the robot. Finally, I argue that though designed as an enabling device, in actuality, metaphoric transferences make the robots mimetically disabling.   ","",""
"2023","Proposing a Postcritical AI Literacy","So-called artificial intelligence (AI) is infiltrating our public and communication structures. The Dutch childcare benefit scandal, revealed in 2019, demonstrates how disadvantageous the opacity of AI can be for already vulnerable groups. In its aftermath, many scholars urged for the need for more explainable AI so that decision-makers can intervene in discriminatory systems. Fostering the explainability of AI (XAI) is a good start to address the issue, but not enough to empower vulnerable groups to fully deal with its repercussions. As a canon in data and computer sciences, XAI aims to illustrate and explain complex AI via simpler models making it more accessible and ethical. The issue being that, in doing so, XAI depoliticises transparency into a remedy for algorithmic opacity, treating transparency as artificially stripped of its ideological meanings. Transparency is presented as an antidote to ideology, though I will show how this is an ideological move with consequences. For instance, it makes us focus too much on algorithmic opacity, rather than explaining the wider power of AI. Second, it hinders us from having debates on who holds the power around AI’s explanations, application or critique. The problem is that those affected by or discriminated against by AI, as in the Dutch case, have little tools to deal with the opacity of AI as a system, while those who focus on data opacity are shaping the literacy discussion. To address these concerns, I suggest moving beyond the focus on algorithmic transparency and towards a post-critical AI literacy to strengthen debates on access, empowerment, and resistance, while not dismissing XAI as a field, nor algorithmic transparency as an intention. What I challenge here is the hegemony of treating transparency as a depoliticised and algorithmic issue and viewing the explainability of AI as the sufficient path to citizen empowerment.   ","",""
"2023","The Work of Art in the Age of Multiverse Meme Generativity","This article adapts Walter Benjamin’s theory of technological reproducibility to examine how generative AI transforms art. Through analysis of a range of works of art and online popular culture, the article theorizes the aesthetics of generative AI. From the characteristic glitches and recurrent formats, the aesthetics are conceived as muddled confusions of parts, attempts at reinserting what Benjamin terms aura, and as surrealist assemblages of characters, styles and worlds. In Benjamin’s conception, popular culture becomes our collective imaginary, which today feeds into a memetic visual conversation of imitation and variation. AI points toward an age of multiverses where characteristics, current events and cultural artifacts blend together to achieve success in social network attention economies. Following Benjamin, the article examines the politics of these transformative media technologies. More than a risk to the livelihood of artists and deep-faked disinformation, this article argues for the radical potential in freeing visual culture from the property rights of corporations.   ","",""
"2023","Synthetic versus human voices in audiobooks: The human emotional intimacy effect"," Human voices narrate most audiobooks, but the fast development of speech synthesis technology has enabled the possibility of using artificial voices. This raises the question of whether the listeners’ cognitive processing is the same when listening to a synthetic or a human voice telling a story. This research aims to compare the listeners’ perception, creation of mental images, narrative engagement, physiological response, and recognition of information when listening to stories conveyed by human and synthetic voices. The results showed that listeners enjoyed stories narrated by a human voice more than a synthetic one. Also, they created more mental images, were more engaged, paid more attention, had a more positive emotional response, and remembered more information. Speech synthesis has experienced considerable progress. However, there are still significant differences versus human voices, so that using them to narrate long stories, such as audiobooks do, is difficult. ","",""
"2023","The “algorithmic <i>as if</i>”: Computational resurrection and the animation of the dead in Deep Nostalgia"," Contemporary artificial intelligence and algorithmic processes address deep-seated existential challenges and modes of desire. In so doing, they produce computational systems of imagination, an “algorithmic as if” that enables the expression, transformation, and seeming overcoming of existential limitations via technological means. This article elaborates the character of the “algorithmic as if” by focusing on Deep Nostalgia, an online tool that turns personal photographs of the deceased into looped animations which smile, blink, and move, promising to overcome mortality by technologically “resurrecting the dead.” Performing a close-reading of Deep Nostalgia’s technological processes and the public discourse around its 2021 launch, the article highlights its combination of computational learning, forms of visual representation (photography, video, and animation), and distinctive realignments of temporal experience. Together, these frame the “algorithmic as if” as a magical and affective space for realizing impossible longings that are also reflexive encounters with the “limit-situation” of human mortality. ","",""
"2023","Algorithmic imaginings and critical digital literacy on #BookTok"," Despite the growing impact of algorithms on digital culture, and the importance of algorithmic awareness, little literacy research has investigated how algorithmic awareness and speculation shapes cultural production on digital platforms. Developing Bucher’s concept of the “algorithmic imagination” for digital literacy research, we conduct a study of #BookTok, the home of book-related content on TikTok, the most algorithm-driven social media platform to date. Through a multimodal content analysis of 57 videos containing #algorithm and #BookTok, we propose and explore a typology of five categories of “algorithmic imaginings”: critique, defense, explanation, how to work, and exploration of the algorithm. These imaginaries move beyond rational attempts to deconstruct the algorithm and critique its role in platform capitalism toward playful explorations of the human–algorithmic relationship. This constitutes for us another dimension of critical literacy, as producers anthropomorphize technology in a manner that addresses the symbiotic meaning-making of human and machine head-on. ","",""
"2023","The effect of information seeking behaviour on trust in AI in Asia: The moderating role of misinformation concern"," Public opinion on new technologies, like artificial intelligence (AI), is influenced by media coverage. However, it remains unclear as to what extent seeking news and information about AI on legacy media as opposed to social media can shape trust in AI. A cross-national survey conducted across Malaysia, Indonesia, Singapore and India investigated the impact of information seeking behaviour on trust in AI, as well as the moderating role of concern about misinformation online. Results indicate a positive relationship exists between seeking AI information on social media and trust across all countries. However, for traditional media, this association was only present in Singapore. When considering misinformation, a positive moderation effect was found for social media in Singapore and India, whereas a negative effect was observed for traditional media in Singapore. These findings have implications for the adoption of novel technologies and highlight the importance of understanding the role of media in shaping public trust. ","",""
"2023","Observe, inspect, modify: Three conditions for generative AI governance"," In a world increasingly shaped by generative AI systems like ChatGPT, the absence of benchmarks to examine the efficacy of oversight mechanisms is a problem for research and policy. What are the structural conditions for governing generative AI systems? To answer this question, it is crucial to situate generative AI systems as regulatory objects: material items that can be governed. On this conceptual basis, we introduce three high-level conditions to structure research and policy agendas on generative AI governance: industrial observability, public inspectability, and technical modifiability. Empirically, we explicate those conditions with a focus on the EU’s AI Act, grounding the analysis of oversight mechanisms for generative AI systems in their granular material properties as observable, inspectable, and modifiable objects. Those three conditions represent an action plan to help us perceive generative AI systems as negotiable objects, rather than seeing them as mysterious forces that pose existential risks for humanity. ","",""
"2023","Artificial Intelligence and Democracy: A Conceptual Framework"," The success and widespread deployment of artificial intelligence (AI) have raised awareness of the technology’s economic, social, and political consequences. Each new step in the development and application of AI is accompanied by speculations about a supposedly imminent but largely fictional artificial general intelligence (AGI) with (super-)human capacities, as seen in the unfolding discourse about capabilities and impact of large language models (LLMs) in the wake of ChatGPT. These far-reaching expectations lead to a discussion on the societal and political impact of AI that is largely dominated by unfocused fears and enthusiasms. In contrast, this article provides a framework for a more focused and productive analysis and discussion of AI’s likely impact on one specific social field: democracy. First, it is necessary to be clear about the workings of AI. This means differentiating between what is at present a largely imaginary AGI and narrow artificial intelligence focused on solving specific tasks. This distinction allows for a critical discussion of how AI affects different aspects of democracy, including its effects on the conditions of self-rule and people’s opportunities to exercise it, equality, the institution of elections, and competition between democratic and autocratic systems of government. This article shows that the consequences of today’s AI are more specific for democracy than broad speculation about AGI capabilities implies. Focusing on these specific aspects will account for actual threats and opportunities and thus allow for better monitoring of AI’s impact on democracy in an interdisciplinary effort by computer and social scientists. ","",""
"2023","Disclosure Standards for Social Media and Generative Artificial Intelligence Research: Toward Transparency and Replicability"," Social media dominate today’s information ecosystem and provide valuable information for social research. Market researchers, social scientists, policymakers, government entities, public health researchers, and practitioners recognize the potential for social data to inspire innovation, support products and services, characterize public opinion, and guide decisions. The appeal of mining these rich datasets is clear. However, there is potential risk of data misuse, underscoring an equally huge and fundamental flaw in the research: there are no procedural standards and little transparency. Transparency across the processes of collecting and analyzing social media data is often limited due to proprietary algorithms. Spurious findings and biases introduced by artificial intelligence (AI) demonstrate the challenges this lack of transparency poses for research. Social media research remains a virtual “wild west,” with no clear standards for reporting regarding data retrieval, preprocessing steps, analytic methods, or interpretation. Use of emerging generative AI technologies to augment social media analytics can undermine validity and replicability of findings, potentially turning this research into a “black box” enterprise. Clear guidance for social media analyses and reporting is needed to assure the quality of the resulting research. In this article, we propose criteria for evaluating the quality of studies using social media data, grounded in established scientific practice. We offer clear documentation guidelines to ensure that social data are used properly and transparently in research and applications. A checklist of disclosure elements to meet minimal reporting standards is proposed. These criteria will make it possible for scholars and practitioners to assess the quality, credibility, and comparability of research findings using digital data. ","",""
"2023","AI as Social Actor: A Lacanian Investigation into Social Technology","Given the social and political influence of social networks, which are often structured and organized by what today falls under the umbrella term artificial intelligence, we seek to define this new social frame. Most importantly, we ask how to frame this new social sphere in current theory and how it can be conceptualized for social sciences. However, this is not possible without constructing a logical frame for a problem as deeply entwined with the modern history of logic as AI is. We will therefore frame the problem of AIs as social actors within the logical discourse that Lacanian psychoanalysis opened. Our analysis shows that the inherent indeterminate that constitutes the psychoanalytic subject is omitted from AI-supplanted identities. Logical analysis also allows us to discern a specific mode of subjectivation that is made much more prominent through the normalization of phenomena like echo chambers and online identities.","",""
"2023","Coping with Algorithmic Risks: How Internet Users Implement Self-Help Strategies to Reduce Risks Related to Algorithmic Selection","Algorithmic selection is omnipresent in various domains of our online everyday lives: it ranks our search results, curates our social media news feeds, or recommends videos to watch and music to listen to. This widespread application of algorithmic selection on the internet can be associated with risks like feeling surveilled (S), feeling exposed to distorted information (D), or feeling like one is using the internet too excessively (O). One way in which internet users can cope with such algorithmic risks is by applying self-help strategies such as adjusting their privacy settings (Sstrat), double-checking information (Dstrat), or deliberately ignoring automated recommendations (Ostrat). This article determines the association of the theoretically derived factors risk awareness (1), personal risk affectedness (2), and algorithm skills (3) with these self-help strategies. The findings from structural equation modelling on survey data representative for the Swiss online population (N2018=1,202) show that personal affectedness by algorithmic risks, awareness of algorithmic risks and algorithm skills are associated with the use of self-help strategies. These results indicate that besides implementing statutory regulation, policy makers have the option to encourage internet users’ self-help by increasing their awareness of algorithmic risks, clarifying how such risks affect them personally, and promoting their algorithm skills.","",""
"2023","Critical AI and Design Justice: An Interview with Sasha Costanza-Chock","","",""
"2023","Editor's Introduction: Humanities in the Loop","Abstract                This editor's introduction welcomes readers to a new interdisciplinary undertaking. The community of practice Critical AI addresses hopes to bring critical thinking of the kind that interpretive disciplines foster into dialogue with work by technologists and others who share the understanding of interdisciplinary research as a powerful tool for building accountable technology in the public interest. Critical AI studies aims to shape and activate conversations in academia, industry, policymaking, media, and the public at large. The long and ongoing history of “AI,” including the data-driven technologies that now claim that name, remains riddled by three core dilemmas: (1) reductive and controversial meanings of “intelligence”; (2) problematic benchmarks and tests for supposedly scientific terms such as “AGI”; and (3) bias, errors, stereotypes, and concentration of power. AI hype today is steeped in blends of utopian and dystopian discourse that distract from the real-world harms of existing technologies. In reality, what is hyped and anthropomorphized as “AI” and even “AGI” is the product not only of technology companies and investors but also—and more fundamentally—of the many millions of people and communities subject to copyright infringement, nonconsensual use of data, bias, environmental harms, and the low-wage and high-stress modes of “human in the loop” through which systems for probabilistic mimicry improve their performance in an imitation game.","",""
"2023","<i>Artificial Life after Frankenstein</i>, by Eileen Hunt Botting","","",""
"2023","The Photographic Pipeline of Machine Vision; or, Machine Vision's Latent Photographic Theory","Abstract                Despite computer vision's extensive mobilization of cameras, photographers, and viewing subjects, photography's place in machine vision remains undertheorized. This article illuminates an operative theory of photography that exists in a latent form, embedded in the tools, practices, and discourses of machine vision research and enabling the methodological imperatives of dataset production. Focusing on the development of the canonical object recognition dataset ImageNet, the article analyzes how the dataset pipeline translates the radical polysemy of the photographic image into a stable and transparent form of data that can be portrayed as a proxy of human vision. Reflecting on the prominence of the photographic snapshot in machine vision discourse, the article traces the path that made this popular cultural practice amenable to the dataset. Following the evolution from nineteenth-century scientific photography to the acquisition of massive sets of online photos, the article shows how dataset creators inherit and transform a form of “instrumental realism,” a photographic enterprise that aims to establish a generalized look from contingent instances in the pursuit of statistical truth. The article concludes with a reflection on how the latent photographic theory of machine vision we have advanced relates to the large image models built for generative AI today.","",""
"2023","<i>Resisting AI: An Anti-fascist Approach to Artificial Intelligence</i>, by Dan McQuillan","","",""
"2023","Thick Description for Critical AI: Generating Data Capitalism and Provocations for a Multisensory Approach","Abstract                This article argues that critical AI studies should make a methodological investment in “thick description” to counteract the tendency both within computational design and business settings to presume (or, in the case of start-ups, hope for) a seamless and inevitable journey from data to monetizable domain knowledge and useful services. Perhaps the classic application of that critical data-studies framework is Marion Fourcade and Kevin Healy's influential 2017 essay, “Seeing Like a Market,” which advances a comprehensive account of how value is extracted from data-collection processes. As important as these critiques have been, the apparent inevitability of this assemblage of power, knowledge, and profit arises in part through the metaphor of “sight.” Thick description—especially when combined with a feminist and queer attention to embodiment, materiality, and multisensory experience—can in this respect supplement Fourcade and Healey's critique by revealing unexpected imaginative possibilities built out of social materialities.","",""
"2023","How to Make “AI” Intelligent; or, The Question of Epistemic Equality","Abstract                Critics have identified a set of operational flaws in the machine language and deep learning systems now discussed under the “AI” banner. Five of the most discussed are social biases, particularly racism; opacity, such that users cannot assess how results were generated; coercion, in that architectures, datasets, algorithms, and the like are controlled by designers and platforms rather than users; systemic privacy violations; and the absence of academic freedom covering corporation-based research, such that results can be hyped in accordance with business objectives or suppressed and distorted if not. This article focuses on a sixth problem with AI, which is that the term intelligence misstates the actual status and effects of the technologies in question. To help fill the gap in rigorous uses of “intelligence” in public discussion, it analyzes Brian Cantwell Smith's The Promise of Artificial Intelligence (2019), noting humanities disciplines routinely operate with Smith's demanding notion of “genuine intelligence.” To get this notion into circulation among technologists, the article calls for replacement of the Two Cultures hierarchy codified by C. P. Snow in the 1950s with a system in which humanities scholars participate from the start in the construction and evaluation of “AI” research programs on a basis of epistemic equality between qualitative and quantitative disciplines.","",""
"2023","Using artificial intelligence in craft education: crafting with text-to-image generative models","ABSTRACT Artificial intelligence (AI) and the automation of creative work have received little attention in craft education. This study aimed to address this gap by exploring Finnish pre-service craft teachers’ and teacher educators’ (N = 15) insights into the potential benefits and challenges of AI, particularly text-to-image generative AI. This study implemented a hands-on workshop on creative making with text-to-image generative AI in order to stimulate discourses and capture imaginaries concerning generative AI. The results revealed that making with AI inspired teachers to consider the unique nature of crafts as well as the tensions and tradeoffs of adopting generative AI in craft practices. The teachers identified concerns in data-driven design, including algorithmic bias, copyright violations and black-boxing creativity, as well as in power relationships, hybrid influencing and behaviour engineering. The article concludes with a discussion of the complicated relationships the results uncovered between creative making and generative AI.","",""
"2023","From concept to space: a new perspective on AIGC-involved attribute translation","ABSTRACT Drawn inspiration from phenomenal attributes and translating them into heuristic model tools is one of the effective means to promote architectural form innovation. However, over-reliance on perception indicates greater risks in decision-making. Nowadays, AI-generated Content (AIGC) technology combines the advantages of information comprehensiveness and modelling efficiency, providing new possibilities for the translation of architectural attributes. Based on attribute study, this paper proposes a new approach to spatial translation that uses the Generative Adversarial Network (VQGAN + CLIP) to realize the visualization of abstract concepts and then adds multi-dimensional influence through the Keyframe Style Transfer technology. The eclectic attribute is used as an example for the 2D and virtual 3D translation feasibility experiments. The article aims to improve the scientificity and influence of spatial translation through a technical organization from the perspective of architects. While providing an innovative, democratic and efficient aided-design tool also highlights a new angle for AIGC-involved pre-design.","",""
"2023","Artificial intelligence as relational artifacts in creative learning","ABSTRACT Artificial Intelligence (AI) has significantly advanced in creating professional-level media content. In creative education, determining how students can benefit without becoming dependent on them is a challenge. In this study, researchers conducted an exploratory experiment that positioned AI as a relational artifact to students in a series of drawing activities and examined the potential impact of affective relations with machines in socio-cultural creative learning. The resulting artifacts, observations, and interview transcripts were analyzed using the Consensual Assessment Technique and a grounded theory approach. The study's results indicate that the design professors reliably evaluated the student drawings as more creative than the AI drawings, but neither demonstrated a consistent increase in creativity. However, the presence of AI engaged the students to explore different approaches to artistic prompts. We theorize that AI can be mediated as a learning artifact for transformative creativity if the students perceive their relationship with AI as empathetic and collaborative.","",""
"2023","Coherent visual design through attribute-specific feedback: a hybrid approach to intelligent design agents","ABSTRACT The scope of visual design is expanding to promote products and services across digital communication platforms, but support for coherent design with style guides seems limited. Based on the literature on style, coherence and design support tools with different levels of intelligence, we describe coherence in visual design and propose to integrate preset style guidelines and constraints into feedback interactions to support achieving visual coherence. After formative research with expert designers, we prototype a pseudo-AI design support tool to simulate attribute-specific feedback and conduct user research to probe how participants engage with feedback provided by a human wizard acting as an intelligent agent. We analyze patterns of feedback and types of reactions observed in participants’ design processes and outcomes. We then discuss the implications of attribute-specific feedback to describe visual coherence as a meta-property of design, and intelligent agents that guide situational judgements about visual coherence through feedback interactions.","",""
"2024","Fantasyland Autofiction","This essay explores a return to hope and romanticism by contemporary artists looking at themes of fantasy worlds and mapping imaginary lands as a type of autofiction. These fantasylands are created in collaboration with hallucinating machine learning platforms, as a tool for contemporary art-making. Seen through the framework of Metamodernism, how does AI hallucination contribute to Metamodern structure of feeling? AI, as part of the metacrisis, places society and culture in a type of no man’s land or in-between, where rapid and unchecked advancements in machine learning and generative technologies are a further addition to an already complicated time, while simultaneously being a new and useful tool for contemporary artists. Introduction While high on opiates in 1798, Samuel Taylor Coleridge, the well-known writer and philosopher, channelled a 349-word poem Kubla Khan, about the construction of a palace by the Mongol ruler in the fantastical utopian land of Xanadu. Coleridge was the founder of the Romantic movement that became prevalent from the late 1800s (Haekel). Historical romanticism originated in a time of great changes and conflicts in Britain, Europe, and America. Some of these included the American Revolution and the loss of the Thirteen Colonies, the French Revolution and Napoleonic wars, the beginning of political change in Britain, with power beginning to be taken from the monarchy, the Industrial Revolution, and the pro-independence and revolutionary movements in Ireland (Sedlmayr). The “end of history” (Fukuyama) during the romantic period is comparable with the metacrisis that humanity has been dealing with since the late 1990s, if you take into account the rapid development of technologies and the political, economic, and social changes of the time (Christensen). As defined by Bhaskar et al., the metacrisis is “a singular socio-ecological crisis” that is described as a “complex multiplicity” that requires an interdisciplinary point-of-view (5). Bhaskar et al. go on to say that  these interconnected crises are also situated in a(n) (inter)subjective context of ‘interior’ meaning making (semiosis), construal and response that includes philosophical, scientific, religious, existential, worldview, and psychospiritual dimensions that are essential to include in an adequate understanding of the complex dynamics in play in order to facilitate more effective responses. (5)  A shift towards idealism or the “utopian turn” (van den Akker and Vermeulen 55), in addition to a return to sincerity, hope, and a neo-romanticism can be found in cultural artifacts (van den Akker et al. 4). These help to form what is known as a structure of feeling and is part of a new era beginning to be known as Metamodernism. Metamodernism Metamodernism is a broad and currently evolving term that emerged in the 1990s. Using the definition of Metamodernism proposed by the Dutch school, the term includes ideas that originate in Modernism and Postmodernism, which respond to, then move beyond to encompass, a wide range of recent and currently occurring cultural shifts (van den Akker et al.). ''The use of the prefix meta here derives from Plato’s metaxis, describing an oscillation and simultaneity between and beyond diametrically opposed poles” (Turner).  Metamodernism explains the culture shift away from postmodernism in terms of a ‘structure of feeling’ arising out of the ‘dialectical oscillation’ between modernism and postmodernism (‘metaxy’), it sees a ‘romantic turn’ in the newly arising works of literature, art, media productions and so on, and it includes a sweeping cultural critique of politics. (van den Akker et al. 199)  Some themes that can be found in contemporary cultural artifacts include ironic sincerity or ironic honesty (Ironesty) and neo-romanticism or the new romanticism. Indeed, “‘Utopia’ – as a trope, individual desire or collective fantasy – is once more, and increasingly, visible and noticeable across artistic practices that must be situated in, and related, to, the passage from postmodernism and metamodernism” (van den Akker and Vermeulen 57). For example, a group exhibition in 2022 titled The Black Fantastic, held at the Hayward Gallery in London, with the curatorial theme of a fantastical Afrofuturist realm, featured the work of 11 artists that use a wide variety of media (Southbank Centre). Described by Obuobi as being an exhibition of “Black artists who use fantastical elements to address racial injustice and explore alternative realities”, they go on to say that two of the curatorial themes include “the multi-dimensional aesthetic experience of real and imagined worlds” and “contrasting utopian and apocalyptic views on time with regards to the past, present, and future” (Obuobi 136). This return to romanticism could be seen as facilitating a form of escapism from the metacrisis, but Vermeulen and van den Akker argue that utopia “should be understood as a tool, say, a looking glass, for scanning this world and others for alternative possibilities” (65). Vermeulen and van den Akker highlight the term ‘utopistics’, a word coined by sociologist Immanuel Wallerstein, that combines ideas associated with utopias, statistics, and logistics to imagine ideal societies created using practical information combined with idealistic hope to achieve them (Wallerstein). Examples of these ideas can be found in the use of “postmodern melancholy in order to invoke hope” in contemporary art (55) or the New Romanticism that focusses on the beauty of the “world while recognizing its ugly reality” (Aziz 28). This idea ties in with the idea of fantasy lands as they include imperfection, as opposed to a utopia, or “the good place that is no place” (Marks et al. 1), that is perfect in every way. In contrast, Santambrogio states that the meaning of utopia contains two aspects that oscillate between “eutopia, or good place; and that of outopia, or non-​place” (148), or in other words “the ultimate in human folly or human hope” (Mumford 1). These definitions of utopia, align with the central idea of oscillation in Metamodernism. Autofiction and Fantasy Lands The aim of autofiction is to present an authentically relational experience for viewers. Autofiction is an in-between medium that blends reality with fantasy. Gibbons describes autofiction as  an explicitly hybrid form of life writing that merges autobiographical fact with fiction. The autofictional mode is not restricted to writing; it has been observed in the visual arts, cinema, theatre and online. (Gibbons 120)  One of the main attributes of autofiction is that the author is featured in the writing as a narrator, in the first person, or as a third-person character in the story (Gibbons et al. 176). The artist establishes “their self corporeally in the world, including in relation to others”. In addition, Gibbons goes on to say that “identity is also acknowledged as a social category that is constructed by subjects and by larger structures of social power” (120). This affectual aspect of the work, combined with the oscillation between the personal and impersonal, the real world and a fantasyland, establishes autofiction as a Metamodern form of storytelling and contributes to a structure of feeling. AI as Muse The use of hallucinogens by Coleridge and the other Romantics to aid in the creation of their literary works is very well known (De Quincey; Dormandy; Vickers; Schäfer). Opium and other drugs like hashish have been seen to be a type of muse or collaborator in the creation of their work (Partridge 104). In Lamuse: Leveraging Artificial Intelligence for Sparking Inspiration, a conference paper on AI and artists, Lamiroy and Potier describe a project that used AI as an inspiration for work. “AI is not creating art in any way but is merely a muse to the artist, a tool” (3). AI hallucinations develop due to computational errors, while drug-induced hallucinations develop from biochemical alterations in the brain (Rolland et al. 1). Like Coleridge’s dream of the imaginary land of Xanadu, experiencing illusions, nonsensical output, or things that don’t exist, the artist uses these hallucinations as inspiration in the creation of work. According to Maleki et al., the definition of an AI hallucination varies (6), but the most cited, originally coined by Koehn and Knowles (3), is that “AI hallucination occurs when the output of the Neural Machine Translation (NMT) system is often quite fluent but entirely unrelated to the input”. In the text-to-image process, a machine learning platform may add unrelated subject matter to the image or make errors in the appearance of the intended subject.  Fig. 1: Adobe Firefly. Prompt: ‘Sue Beyer Land’. Text-to-image machine learning, August 2024. In fig. 1 the prompt used to make this image was ‘Sue Beyer Land’. The example image appears to display a landscape combining three different landscapes that make up the back, mid-, and foreground. Mountains form the backdrop to green rolling hills of a rural landscape that features a dog sitting in the foreground, wearing a hat, with an Australian flag placed over the bottom half of its head. Different lighting is used in each section of the image. The strange and humorous aspects of this image are considered to be an AI hallucination.  Fig. 2: Runway ML. Prompt: ‘The Land of Sue Beyer’. Text-to-image machine learning, August 2024. In another example, shown in fig. 2 using Runway ML and the prompt ‘The Land of Sue Beyer’, the AI produced an image that resembles a black and white Ansel Adams photograph of a dirt track with wooden fence posts, heading straight through a field towards mountains in the distance. The image, when examined closely, is obviously generated by AI. There are repetitive marks and errors along the length of the road, and inconsistencies in the light. The AI has generated an idealised land that exists outside of reality through an attempt to emulate an Ansel Adams photograph that invokes an idealised land that exists outside of reality. Collaborating with AI Topi Tjukanov, a geographer from Helsinki, has trained a Generative Adversarial Network (GAN) using a large collection of images of maps since 2020. The result is a collection of images that morph into different styles of maps. There is no burning conceptual reason behind this project and Tjukanov says that “besides nice animations, it is very interesting to just explore the latent space” (Tjukanov). In a more practical application, Google maps now use AI to add information and suggestions to help you get to where you need to be, and to make “a map that can reflect the millions of changes made around the world every day” (Glasgow).  Fig. 3: Sue Beyer. Slumberia, the Nation of Procrastia. Xanadu Roller Party, 2024. Sue Beyer, an interdisciplinary artist, collaborated with a machine learning platform to create a map with the title Slumberia, the Nation of Procrastia. The work was created through the text-to-image machine learning platform DALL•E. The image uses parody and pastiche, to create a map in the style of an old-world hand-drawn map, featuring subdued or faded colours. The map also includes various fictional planets that surround Slumberia. The prompt that made this image included data that the artist collected on their daily habits for the month of June 2024, recorded in 15-minute increments. The artist completed tasks each day according to their state of mind. Each country or continent was meant to represent the amount of time spent on each activity. Some of these activities included sleeping, working, playing PlayStation games, and reading. The result of the prompt shows that the AI focussed on sleeping, procrastination, and creativity and largely ignored the rest of the information, including the percentage of time spent on activities. In addition, the title of the location, Slumberia, the Nation of Procrastia, was not included in the prompt, with the AI deciding on this name for itself. This article argues that, as a form of data mapping, this map can be seen as a type of activity and emotion-based cartography or cognitive map that Jameson describes as being a combination of real and ideological space (51). Did the AI create a psychogeographical map or a representation of the state of mind of the artist when completing tasks? We have moved beyond the psychogeography of the Situationist International with their “promise of critical empowerment” as it was “unable to address the contextual problems of the geocoded hybrid space” (Morilla Chinchilla 465; Tuters). We are now engaging with postlocative media, which is described as being postlocative art that “deploys strategies that critically accept the important role that non-humans play in public representation as well as in delegating the production of meaning to AI, thus moving beyond the locative framework and closely associating itself with the postphenomenology of complex systems” (Morilla Chinchilla 463). These ideas allow this collaborative work to fit into a structure of feeling or affect, while taking into account non-human and unfeeling AI.  Fig. 4: Sue Beyer. “Welcome to Sue World!” Xanadu Roller Party, 2024. In addition to Slumberia, the Nation of Procrastia, Sue World is an online chatbot that can answer questions regarding a place called Sue World. The Webpage features an image of a woman standing in front of a space themed backdrop. The woman is looking off into the distance and is dressed in a type of cosplay-style spacesuit. The image appears to have been created by AI as it shows the tell-tale signs of things looking ‘not quite right’. Her right arm looks like it has been dislocated as the shoulder does not exist, there is unintelligible ‘writing’ on the front of her space suit harness, and the hair is a mix between smoke and strange highlights. Her skin has an airbrushed, unnaturally smooth and perfect, “selfie aesthetic” that has become normalised on social media (Swerzenski and Kim). There is a prompt for the viewer to ‘Ask me a question about Sue &amp; Sue World’, with an arrow pointing to a set of three dots in a speech bubble. The text is displayed using bright colours that replicate a style associated with the Barbie brand, which, in this case, indicates a sense of playfulness, humour, and irony. This work presents an autofiction of overlapping factual and fictional histories. The use of fiction to embellish history is now emphasised in stark reality on social media, where facts and lies are indistinguishable from each other even when sitting side by side. Examples given by Breithaupt of misinformation spread by bot farms and misguided opinions during the height of the COVID-19 pandemic have been described as an “infodemic” (1). However, in this case the fictions are written by AI acting as ghostwriter of the autofiction Sue World. Both the Sue World chatbot and Slumberia, the Nation of Procrastia are intertextual. They both mimic past styles from geographic and art history, and popular culture. They appear to be satirical, while also interrogating how artists can collaborate with machine learning to answer existential questions like who and where am I? The predominant aspect of this work is the biographical nature of the information presented. It is a type of autofiction, written in collaboration with AI, with the AI acting as a ghost writer. Both works are also self-reflexive in their emphasis on the use of AI. In addition, there is a sense of ironic sincerity. The artist is sincerely looking for an answer, but the AI used to inform this work cannot be trusted to tell the truth. AI and the Internet can provide information and knowledge to whomever has access, but more recently it has become evident that they are being used as tools for misinformation and control (De Witt). When using machine learning platforms, the artist and the viewer must question whether the answer is accurate. These online tools are creative and destructive at the same time and can be seen to represent hope and despair (Singler). Elements of reality and fantasy, certainty and chance, irony and sincerity can also be found in both works. “This oscillation between different affective poles is seen as a characteristic of the metamodern structure of feeling” (van den Akker and Vermeulen).  AI and Metamodernism The metacrisis or “the dramatic and complex situation of the current world is a scene for the birth of metamodernism” (Pipere and Mārtinsone 7). During the development of the metacrisis, there were rapid advancements in technology like the Internet and AI that are now part of people’s everyday lives. Anecdotally, artists have embraced relatively new tools like ChatGPT and DALL•E, but many are worried that the technology is being developed too fast and without a thorough regard for checks and balances. “The rapid integration of artificial intelligence (AI) systems into various domains has raised concerns about their impact on individual and societal wellbeing, particularly due to the lack of transparency and accountability in their decision making processes” (Cheong 1). This problem is only part of what Geoffrey Hinton, a pioneer in artificial intelligence, talked about in a recent interview published in The New York Times (Metz). A positive example of the use of AI is that it played a part in the development of “predictive models to map the spread of the outbreak and contact tracking applications to support governments and organisations in the management of the pandemic” (Etienne 306). However, it has also been revealed that the work of artists is being used to train AI without their permission, leading to the perceived problem of AI taking creative jobs, like extras acting and voice-overs (Spangler). This had led to major strikes by writers in Hollywood, demanding better contracts to safeguard their jobs against studios replacing humans with AI (Schuhrke). At present, AI is seen as an existential threat (Spangler), because recent studies show that AI or Large Language Models (LLMs) consistently fail to provide a comparable quality to that of human-created writing. Despite the difficulties of objectively assessing creative output, a recent study by Chakrabarty et al. shows that stories written by LLMs pass their assessment three to ten times less often than a story or screenplay written by a human (Chakrabarty et al.). Considering this result, we can safely state that AI is not creative. However, it has proved to be a useful tool to contemporary artists (Lamiroy and Potier). Lev Manovich has linked contemporary media and art creation using Generative AI with the processes that artists from the modern art movement have used. He says they “achieved innovation by reinterpreting and incorporating older art forms from other cultures. Similarly, generative AI tools allow the creation of new works because they are trained on massive databases of existing art and media”. Furthermore, “AI models trained on specific datasets to produce novel artworks that engage in a dialogue with historical art while introducing new aesthetic possibilities” (1). This process of innovation through the reinterpretation of historic art forms, using the next generation of new media, is a Metamodern process. In Manovich’s words, modernism was concerned with “making it new” and AI is trained on “already existing art”, and “although generative AI and modernist art appear to be opposites of each other” (1) they work together in a type of oscillation to create something new. This oscillation is a hallmark of metamodern process. AI and Affect At present, AI is fallible, and, like a human, makes mistakes (Glover and Sayre). An AI hallucination can be compared to an artist who uses mind-altering substances to enhance their output. The hallucination takes ideas into places the artist might not have gone otherwise. In the examples shown in figs. 3 and 4, the artist was using ironic sincerity when collaborating with an AI to write an autofiction that describes their physical and emotional world. Like the stereotype of the tragically beautiful English literature junkies Shelley and Byron (Ruston 344), who are hopelessly romantic and beautifully destructive, a collaboration is formed with something you may end up relying on despite the drawbacks. Conclusion In this article, a parallel was drawn between the historical period of the romantics and contemporary artists, not only in the political, social, and cultural aspects of the time but in the use of opiates for inspiration. Their use of opiates was compared with the use of AI hallucinations as a tool for contemporary artists. In the examples shown that examine the topic of utopia and fantasylands, there is a return to hope and romanticism. This shift towards the utopian turn is part of a Metamodern structure of feeling. An oscillation between the ideas found in utopia, eutopia, and utopistics is being used “so that we can once again conceive of the future” (van den Akker and Vermeulen 55). The autofiction of Sue World and utopian map of Slumberia use postmodern techniques such as irony, intertextuality, and self-reflexivity to reveal the modernist spirit of romanticism and “affective states like melancholy, hope, enthusiasm, and despair” (van den Akker and Vermeulen), while also considering non-human and unfeeling AI. In the chosen examples, the use of AI as a collaborator places the work in an amorphous in-between place, where ideas perpetually oscillate between bittersweet hope and destructive optimism. References Aziz, Christina. “From Postmodernism to Metamodernism.” Philosophy Now 162 (June/July 2024): 26–29. Barry, Lynda. What It Is. Drawn &amp; Quarterly, 2021. Beyer, Sue. Slumberia, the Nation of Procrastia. AI-generated jpg, 2024. &lt;http://www.xanadurollerparty.com/mapofsueworld.html&gt;. ———. “Welcome to Sue World!” Xanadu Roller Party, 2024. &lt;http://www.xanadurollerparty.com/sueworld.html&gt;. Bhaskar, Roy, et al. Metatheory for the Twenty-First Century: Critical Realism and Integral Theory in Dialogue. Taylor &amp; Francis, 2015. Breithaupt, Holger. “Lies, Damn Lies and Social Media.” EMBO Reports 21.11 (2020): 1–2. &lt;https://doi.org/10.15252/embr.202051877&gt;. Chakrabarty, Tuhin, et al. Art or Artifice? Large Language Models and the False Promise of Creativity. arXiv, 2024. &lt;http://arxiv.org/abs/2309.14556&gt;. Cheong, Ben Chester. “Transparency and Accountability in AI Systems: Safeguarding Wellbeing in the Age of Algorithmic Decision-Making.” Frontiers in Human Dynamics 6 (July 2024). &lt;https://doi.org/10.3389/fhumd.2024.1421273&gt;. Christensen, Jerome. Romanticism at the End of History. 1st ed. Johns Hopkins UP, 2000. De Quincey, Thomas. Confessions of an English Opium-Eater. Open Road Integrated Media, 1995. De Witt, Melissa. “What to Know about Disinformation and How to Address It.” Stanford News, 2022. &lt;https://news.stanford.edu/2022/04/13/know-disinformation-address/&gt;. Dormandy, Thomas. “Romantic Opium.” OPIUM: Reality’s Dark Dream. Yale UP, 2012. 76–84. &lt;https://doi.org/10.12987/9780300183658-013&gt;. Etienne, Hubert. “Solving Moral Dilemmas with AI: How It Helps Us Address the Social Implications of the Covid-19 Crisis and Enhance Human Responsibility to Tackle Meta-Dilemmas.” Law, Innovation and Technology 14.2 (2022): 305–24. &lt;https://doi.org/10.1080/17579961.2022.2113669&gt;. Fukuyama, Francis. The End of History and the Last Man. Penguin, 1992. Gibbons, Alison. “Contemporary Autofiction and Metamodern Affect.” Metamodernism Historicity, Affect, and Depth After Postmodernism. Rowman &amp; Littlefield, 2017. 117–30. ———. “Reality Beckons: Metamodernist Depthiness beyond Panfictionality.” European Journal of English Studies 23.2 (2019): 172–89. &lt;https://doi.org/10.1080/13825577.2019.1640426&gt;. Glasgow, Dane. “Redefining What a Map Can Be with New Information and AI.” Google, 30 Mar. 2021. &lt;https://blog.google/products/maps/redefining-what-map-can-be-new-information-and-ai/&gt;. Haekel, Ralf, ed. Handbook of British Romanticism. De Gruyter, 2017. &lt;https://doi.org/10.1515/9783110376692&gt;. Jameson, Fredric. Postmodernism, or, The Cultural Logic of Late Capitalism. 1st ed. Duke UP, 2013. &lt;https://doi.org/10.1515/9780822378419&gt;. Koehn, Philipp, and Rebecca Knowles. “Six Challenges for Neural Machine Translation.” arXiv, June 2017. Kyle Glover, and Mark Sayre. “Machines Make Mistakes Too: Planning for AI Liability in Contracting.” Journal of Law, Technology, &amp; the Internet 15.2 (2024): 357. &lt;https://scholarlycommons.law.case.edu/jolti/vol15/iss2/5&gt;. Lamiroy, Bart, and Emmanuelle Potier. “Lamuse: Leveraging Artificial Intelligence for Sparking Inspiration.” Computational Intelligence in Music, Sound, Art and Design (EvoMUSART), eds. Tiago Martins et al. Springer, 2022. 148–61. &lt;https://doi.org/10.1007/978-3-031-03789-4_10&gt;. Maleki, Negar, et al. AI Hallucinations: A Misnomer Worth Clarifying. arXiv, 2024. &lt;https://doi.org/10.48550/arxiv.2401.06796&gt;. Manovich, Lev. ‘Make It New’: GenAI, Modernism, and Database Art. Nam June Paik Art Center, 2024. &lt;http://manovich.net/index.php/projects/make-it-new-genai-modernism-and-database-art&gt;. Marks, Peter, et al., eds. The Palgrave Handbook of Utopian and Dystopian Literatures. Springer, 2022. &lt;https://doi.org/10.1007/978-3-030-88654-7&gt;. Metz, Cade. “‘The Godfather of A.I.’ Leaves Google and Warns of Danger Ahead.” The New York Times, 1 May 2023. &lt;https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html&gt;. Morilla Chinchilla, Santiago. Postlocative Art for a Non-Anthropocentric World. Universitat Oberta de Catalunya, and ISEA lnternational, 2022. 463–70. &lt;https://hdl.handle.net/20.500.14352/92843&gt;. Mumford, Lewis. The Story of Utopias. 3rd ed. Viking Press, 1962. Obuobi, Sharon. “In the Black Fantastic by Haywood Gallery (Review).” Nka: Journal of Contemporary African Art 52.1 (2023): 136–42. &lt;https://muse.jhu.edu/pub/4/article/902023&gt;. Partridge, Christopher. High Culture: Drugs, Mysticism, and the Pursuit of Transcendence in the Modern World. Oxford UP, 2018. Pipere, Anita, and Kristīne Mārtinsone. “Metamodernism and Social Sciences: Scoping the Future.” Social Sciences 11.10 (2022): 1–20. &lt;https://doi.org/10.3390/socsci11100457&gt;. Rolland, Benjamin, et al. “Pharmacology of Hallucinations: Several Mechanisms for One Single Symptom?” BioMed Research International, 2014. &lt;https://doi.org/10.1155/2014/307106&gt;. Ruston, Sharon. “‘High’ Romanticism: Literature and Drugs.” The Oxford Handbook of British Romanticism, ed. David Duff. Oxford UP, 2018. 341–54. &lt;https://doi.org/10.1093/oxfordhb/9780199660896.013.22&gt;. Santambrogio, Ambrogio. Utopia without Ideology. Taylor &amp; Francis, 2023. Schäfer, D. “Milk of Paradise? Opium and Opiates in Nineteenth and Twentieth Century Literature.” Schmerz (Berlin, Germany) 21.4 (2007): 339–46. Schuhrke, Jeff. “Lights, Camera, Collective Action: Assessing the 2023 SAG-AFTRA Strike.” New Labor Forum 33.2 (2024): 56–64. &lt;https://doi.org/10.1177/10957960241245445&gt;. Sedlmayr, Gerold. “Political and Social History c. 1780–1832.” Handbook of British Romanticism. De Gruyter, 2017. 27–48. Singler, Beth. “Existential Hope and Existential Despair in AI Apocalypticism and Transhumanism.” Zygon® 54.1 (2019): 156–76. &lt;https://doi.org/10.1111/zygo.12494&gt;. Southbank Centre. In the Black Fantastic. 2022. &lt;https://www.southbankcentre.co.uk/whats-on/in-the-black-fantastic/&gt;. Spangler, Todd. “‘This Is an Existential Threat’: Will AI Really Eliminate Actors and Ruin Hollywood? Insiders Sound Off.” Variety, 16 Aug. 2023. &lt;https://variety.com/2023/digital/features/hollywood-ai-crisis-atificial-intelligence-eliminate-acting-jobs-1235697167/&gt;. Swerzenski, J.D., and Dasol Kim. “The New Selfie Standard: Facetune and the Shift toward East Asian Selfie Aesthetics.” AoIR Selected Papers of Internet Research, Sep. 2021. &lt;https://doi.org/10.5210/spir.v2021i0.12249&gt;. Tjukanov, Topi. “Topi Tjukanov.” N.d. &lt;https://tjukanov.org/&gt;. Turner, Luke. “Metamodernism: A Brief Introduction.” Notes on Metamodernism, 12 Jan. 2015. &lt;http://www.metamodernism.com/2015/01/12/metamodernism-a-brief-introduction/&gt;. Tuters, Marc. “Forget Psychogeography: The Object-Turn in Locative Media.” Unstable Platforms: The Promise and Peril of Transition. MIT, 2011. 1–29 &lt;https://web.mit.edu/comm-forum/legacy/mit7/papers/Tuters_DMI_MIT7.pdf&gt;. van den Akker, Robin, et al. Metamodernism: Historicity, Affect, and Depth after Postmodernism. Kindle, Rowman &amp; Littlefield International, 2019. Van den Akker, Robin, and Timotheus Vermeulen. “Utopia, Sort of: A Case Study in Metamodernism.” The Futures of the Present: New Directions in (American) Literature. Routledge, 2017. Vickers, Neil. “Opium as a Literary Stimulant: The Case of Samuel Taylor Coleridge.” International Review of Neurobiology 120 (2015): 327–38. &lt;https://doi.org/10.1016/bs.irn.2015.02.007&gt;. Wallerstein, Immanuel Maurice. Utopistics: Or, Historical Choices of the Twenty-First Century. New Press, 1998.","",""
"2024","The Allure of Artificial Worlds"," Fig. 1: ‘Vapourwave Hall’, generated by the author using Leonardo.Ai, 2024. Introduction With generative AI (genAI) and its outputs, visual and aural cultures are grappling with new practices in storytelling, artistic expression, and meme-farming. Some artists and commentators sit firmly on the critical side of the discourse, citing valid concerns around utility, longevity, and ethics. But more spurious judgements abound, particularly when it comes to quality and artistic value. This article presents and explores AI-generated audiovisual media and AI-driven simulative systems as worlds: virtual technocultural composites, assemblages of material and meaning. In doing so, this piece seeks to consider how new genAI expressions and applications challenge traditional notions of narrative, immersion, and reality. What ‘worlds’ do these synthetic media hint at or create? And by what processes of visualisation, mediation, and aisthesis do they operate on the viewer? I suggest here that these AI worlds offer a glimpse of a future aesthetic, where the lines between authentic and artificial are blurred, and the human and the machinic are irrevocably enmeshed across society and culture. Where the uncanny is not the exception, but the rule. Analytic Survey The term ‘composite’ is co-opted here from Lisa Purse, whose writings have become perhaps inadvertent champions of digital augmentation and visual effects in film. The critical and academic response to AI media is not dissimilar from that to the advent of high-concept, visual effects-laden, digitally-encoded cinema. An “overdetermined nexus of loss”, Purse dubs the digital screen, “of material presence, of an indexical relation to the world and lived experience” (Purse 149). James Verdon says that there is an incontrovertible “indexical severance when pro- or a-filmic reality is recorded or manipulated digitally” (Verdon 197), and photography and cinema seemingly continue to struggle with this severance. In terms of AI media, though, there is no harsh ‘severance’ with which to grapple; the dilemma is much more existential, in that the ‘real’ of these objects never existed. Despite their often realistic outputs, AI media still possess an eerie, uncanny quality. Some scholars suggest that the result is a kind of ‘haunted’ media:  the main thing haunting these new AI images is actually the camera itself, rendered a ghost now by its total absence from the new medium, a seemingly unnecessary anachronism, but one that nevertheless exerts a strong spectral influence on everything that is generated. (Schofield 17)  Andreas Ervik also observes this spectral influence in how generation models structure their outputs with a clear predisposition towards older forms of media. Ervik calls these images without ‘real’ referents “views of nowhere”, offering the example of “an ahistorical emulation of the general vibe of classical portrait painting” (Ervik 83-4). This uncanniness persists in abstract or glitched AI outputs as well as in those that are more realistically rendered. There is always some trace of something recognisable or tangible, be that a human feature, a graphical element, or an assortment of colours. AI media, tools, and applications are lingering, surviving, and in the process are changing visual culture, particularly in terms of aesthetics. Shane Denson notes that AI tools and generators are “dissolving the industrial-era wedge between art and tech” (Denson 147), leading to a profound shift in aisthesis:  these ... technologies are transforming the domain of sensation itself, opening up new objects of perceptual and cognitive experience, and changing the scope and parameters of embodied relation to the environment. (Denson 147)  Denson’s general argument is that, love them or loathe them, the power of AI media lies in their visceral impact, rather than the technical accomplishment of their generation or any immanent artistic value or quality. The outputs of these AI models are received and filtered through the body and mind; they are thus triggers for ‘felt’ experience, where the viewer is immersed, if only momentarily, in a hallucinated reality. What kinds of felt experiences can AI media conjure, and through what mechanisms do these conjurings occur? How might these experiences change our understanding of reality and representation when we return to everyday life? Eryk Salvaggio writes that “when we look at AI images ... we are looking at infographics about these datasets, including their categories, biases, and stereotypes” (Salvaggio, """"How to Read"""" 87). Salvaggio’s process-driven analysis is based on his experience with Stable Diffusion, where concepts or forms that are highly represented in the model’s training data emerge more clearly in outputs than less-represented concepts. This notion remains true for most media generators, particularly in the realm of image and video. In Salvaggio’s creative work, too, he finds that more provocative results come in pushing at the less-represented, at the gaps between the model’s defined ‘vision’ or ‘understanding’ of the world:  AI models, when used as intended, don’t move us away from the bias of human vision, it constrains us to that bias. This bias is infused into training data, a bias that merges images into the categories of their descriptions, reconstructing links between words and what they represent. (Salvaggio, """"Moth"""")  Despite the constraints of a bias towards the human, many models have hallucination purposely built in. In these cases, hallucination is a feature rather than a bug. It is hallucination—the introduction of some chaos and randomness into the maths underpinning the mechanisms of generation—that may give that uncanny feeling of human connection when conversing with ChatGPT, but that also causes glitches and unexpected mutations in outputs from media generators. But these glitches, too, are what artists and creatives often gravitate towards in working with generative AI. As Nataliia Laba notes, in engaging with the machinic, one positions oneself as an agent with multiplicities:  as a promptor, I assume the role of a social actor engaging in the co-creation of visual outputs alongside [Midjourney’s] bot on Discord. As a researcher, I operate on the premise that AI-generated images are not straightforward extrapolations from existing generative AI technology, but are to be understood as the contingent outcome of a series of social, political, and organizational factors. (Laba 10)  Echoing earlier work around algorithmic agency, AI generators and their human users sit at the nexus of technology, mythology, and representation (boyd and Crawford 663). The results of this enmeshment are AI-generated media. The moment of generation, thus, “fixes a unity from scattered data elements, at that same moment fabulating new connections and traits, forging attributes that will attach to other beings in the future” (Amoore 102-3). AI media are not just fixed unities or instances, the results of algorithmic and mathematical operations, but also complex, networked assemblages enacting particular effects. They are, essentially, worlds unto themselves. This is why AI media are so rapidly provoking and affecting visual culture and the media landscape more broadly. Thus, more nuanced analysis is required of their origin and creation, their forms and qualities, and their potential or actual effects. This article looks directly at AI media, and how they are being reconstituted into media artefacts—specifically short films—for consumption, enjoyment, and provocation. Beyond this, I engage with ‘simulative AI’, or rather, AI-driven simulations, and where and how these might attract and engage users. With both films and simulative experiences, I observe how these media have innate agency and power to influence us viscerally and psychologically. They seduce us with their aesthetic and material qualities: that these qualities are synthetic is also, paradoxically, a part of their charm. Two Video Works AI systems like RunwayML and Pika generate moving images by adding a “video-aware temporal backbone” to the process (Blattman et al., 3). These systems are becoming increasingly adept at producing videos of some length, though their outputs remain susceptible to hallucinations and glitching. For many creators, this is precisely where the ‘point’ of AI media lies.  Fig. 2: Screen capture from “You Are, Unfortunately, an A.I. Artist” (Mind Wank, 2024). “You Are, Unfortunately, an A.I. Artist” (Mind Wank, 2024) interrogates the legitimacy and artistic value of AI media. Its main character, a small, fluffy, bug-eyed creature in spectacles, toils at their computer, generating endless outputs to edit into small clips to sell via the blockchain. The editing is judicious, avoiding the worst of the morphing or glitching, though some remains, lending an unnatural taint to the creature’s movements and its environment. We observe the pitiful protagonist as though through rippling water or oil, a shifting and dynamic carnival mirror visual filter. The robotic narration does little to allay this feeling of unease, addressing the viewer directly per the work’s title: “You are trying to create something new, something meaningful.” The critical moment of the piece is when the Wi-Fi connection cuts out: “Without Internet, are you really an AI artist? Or are you just some guy? Obviously you have no technical skills, or you would’ve done something else.” The central creature retains its general form, as a fluffy figure, though as it ‘grows’ through the film, it changes somewhat, with different body structures, and occasionally featuring human-like hands. This is likely a result of the AI model filling in some visual information with its best guess. The effect of this, though, works for the narrative; the creature is unstable as a character, but also as perceived by the viewer. Visuals, narration, music—all entirely AI-generated—here combine in a multi-valent experience that is both unnatural and universal.  Fig. 3: Screen capture from “Midnight Odyssey” (Ethereal Gwirl and LeMoon, 2024). “Midnight Odyssey” (Ethereal Gwirl and LeMoon, 2024) is a short fantasy film telling the story of a princess born into a world without sunlight. The princess must confront the moon witch Lune, who presents her with three challenges to restore the sun. The creators spent two months crafting the ambitious ten-minute work, making use of tools like Midjourney, Suno, Eleven Labs, and Topaz. The result is an intriguing blend of conventional filmic storytelling and technology-driven hallucination. The aesthetic is dynamic, with the creators clearly prioritising an engaging and varied colour palette, and contrasting lighting elements within each shot. Crystalline rays illuminate the moon witch’s chamber, for instance, bouncing off her glittering gown; this contrasts with the neon-drenched and spot-lit action of the track featured in the racing challenge. The elements—both narrative and visual—that “Midnight Odyssey” presents are on the one hand conventional, but on the other ever-so-slightly askew. The creators have adhered to a ‘handmade’ visual style, with characters and environments appearing material and tangible, almost like puppets on a miniature stage, or stop-motion animation. This justifies or ‘grounds’ some of the quirkier, more glitchy moments per the logic of AI hallucination: one example of this is the sequence where the protagonist has psychedelic visions after an encounter with a talking frog, a sequence seemingly designed with AI errors in mind. The overall piece, too, alludes to earlier media forms through its colour grading, bleached film treatment, and even its 4:3 aspect ratio.  Fig. 4: Screen capture from “Midnight Odyssey” (Ethereal Gwirl and LeMoon, 2024). These two video works exemplify both the strengths and weaknesses of AI as a tool of both storytelling and visual representation. The tools can be used to generate a very particular kind of aesthetic; both projects are highly stylised and visually dynamic. Character consistency remains a shortcoming for some AI media generators, and this is apparent in a few scenes from both videos. Similarly, there are significant issues with aesthetic continuity, and there is little to no synchronisation of speech with mouth movement. The generated media rely much on static shots, and on existing conventions of visual storytelling in their blocking and framing of characters, and presentation of settings and environments. This is reflective of the AI tools’ training data, and how representative these data are of tried and tested creative and cultural approaches; the AI’s latent space remixes and reconstitutes these data into composite spaces, novel, unique worlds that are eerily familiar. Per Ervik and Schofield, these worlds are ‘haunted’ by older media forms. These features lend an artificial but hand-crafted character to both videos. These images and videos feel materially attractive: like it would not take much to reach out and touch the screen, to feel the texture of the characters or the setting. This is an example of what Denson describes as “the body subperceptually filtering the extraperceptual stimulus of AI art”. Denson borrows Merleau-Ponty’s term ‘diaphragm’, referring to what it is in a human’s pre-cognitive perception that must react to what is in front of it; he also models AI generators' processing of inputs and outputs “prior to and without regard for any integral conception of subjective or objective form” as its own kind of diaphragm (Denson 154). It is these duelling (or duetting) diaphragms that, I suggest, offer us a shortcut into the worlds AI media present to us, as well as some explanation as to how such transport is possible. As Denson notes, we respond to these media viscerally, corporeally, before we respond cognitively; the bizarre, uncanny, yet inviting tactility of the AI aesthetic is at work on us before we know it. Like all generative AI tools, video generators will continue to improve, in terms of realism, generation length, and integration with other apps. But where is generative AI going more broadly? Not necessarily in terms of the killer app, but in terms of creative or immersive possibilities? The answer may lie not with higher-resolution image outputs, or more contiguous and plausible AI video, but rather in simulation. Simulative AI In 2023, tech start-up Fable Studio released several short Web videos featuring characters from the long-running comedy show South Park. They were all generated using their custom SHOW-1 system, which integrates various custom large language models and image generators. OpenAI’s GPT-4 had already been trained on South Park episodes and metadata, but the Fable team supplemented these data with extra information around decision-making, creativity, and character information. Specific details around the SHOW-1 interface are not presented in the Fable working paper, but they stress the maintenance of human input at key parts of the process. Underlying SHOW-1 is a ‘simulation’, where characters exist in a virtual world, seeking to satisfy their encoded needs based on “affordance providers”. Interactions between characters and providers are recorded by the system, which then generates “Reveries: reflections on each event and their day as a whole” (Maas et al. 7). If all of this sounds a little familiar, Westworld is frequently mentioned as a conceptual inspiration by the Fable team, and as a logical apocalyptic endpoint by critics (Takahashi). The endgame of SHOW-1 is a project called Showrunner, which would allow creators and audiences to track their favourite characters and create consumable content based on their experiences and interactions. This content could take the form of social media posts or more conventional TV-like ‘episodes’; a kind of artificial/virtual reality TV.  Fig. 5: Screen capture from “Exit Valley”, Episode 1 (Showrunner, 2024). It is important to note that the development of multi-agent simulations did not begin with Fable Studio; previous work in this area includes a social AI sandbox experiment from Stanford (Waite; Park et al.) and articulations of videogame AI systems (McCoy et al.). Games have proven to be remarkable testing grounds for AI both generative and not, with various approaches employed to influence and govern the behaviour of non-player characters, the direction of narratives, and broader structures like environments and ecosystems. In this vein, the tech company Altera launched Project Sid in 2024. Project Sid is a virtual biome that resembles sandbox games like Minecraft. The biome is populated with hundreds of virtual agents, programmed to pursue their needs similarly to the agents in SHOW-1. Rather than entertainment, necessarily, Altera is using Project Sid to observe how individual agents and agent populations behave over time (Altera, “Project Sid”). The goal is to use these observations to build next-generation AI/ML products that require less human input as they ‘learn’ and develop over time, and then, potentially, become virtual human assistants (Altera, “Building Digital Humans”). There are also broader applications for these kinds of AI-driven simulations, to gain deeper understandings of how humans and other species behave in response to certain stimuli, how they form communities, and how those communities behave, rupture, and break down. In the examples of Showrunner and Project Sid, AI is being employed in the creation of virtual worlds, within which stories might naturally evolve. The visual style of these worlds is almost irrelevant; indeed, as noted, Project Sid is a low-resolution poly environment in the vein of Roblox or Minecraft. What builds the sense of ‘worldness’ in AI-driven simulations (or simulative AI) are the systems at work: be it the needs or behaviours of particular agents (or characters) or groups, or those governing the environment itself, like virtual physics, geology, or weather. These are systems that affect the viewer or user: one is drawn in to observe interactions and conflicts, or one becomes an active participant and subject, doing one’s own interacting, and being affected by the broader environmental systems at play. As with media generators, there may be unexpected phenomena: a miscommunication, an action that is inconsistent or surprising, an unforeseen effect or change. But these simulations are built to absorb these glitches: in the cases of SHOW-1 and Showrunner, these aberrant behaviours may become character or story arcs in and of themselves; with Project Sid, such phenomena may enable observation or analysis of how communities react in real-world situations. These artificial worlds have the capacity to generate—so to speak—very tangible consequences. Conclusion The term ‘artificial’ is often used pejoratively; something seems fake, shallow, superficial, therefore it has a manufactured, confected ‘gloss’. However, it can also be used accurately in describing something intentional and crafted. While some AI media can provoke intense negative reactions, that same provocative power can also seduce. Perfectly-arranged AI worlds offer up a vision of precision; a crafted reality that embodies the ideal version of what the user desires, be it in terms of low-stakes social or romantic interactions, influence over virtual communities, or perfectly-rendered bodies or spaces. These worlds are a hyperreality: the artificial becomes accessible and tangible. Rather than destroying this contiguous sense of ‘worldness’, the glitches or errors typical of AI systems may sometimes enhance this immersive hyperrealism. In a sense, these faults and imperfections become a machinic maker’s mark: we sympathise as fallible creators ourselves. These errors, too, can sometimes create unexpected, surprising, even humorous, outcomes, be it a whimsical background character or nonsensical environmental element, or a baffling encounter with an AI agent. Again, these glitches tend to make these worlds more inviting, enticing, and fun to explore. Like some videogame or cinematic spaces, AI worlds can be seductive because they offer the promise of controlled fantasy, of escape with boundaries. These spaces may be governed by rules encoded to resemble our actual world; the user is then free to inhabit, influence, or interfere as much as they wish. The tension between the deliberately arranged and the imperfect and unexpected offers a fluid, dynamic experience that one may find addictive. While interactions with or between artificial agents may be simulated, users may feel a sense of investment or attachment, making the emotional stakes or effects very real. Despite the very real and valid ethical concerns around the data on which AI systems are trained, it seems that AI media, simulative AI systems, and other AI-driven tools are not just lingering, but becoming embedded and enmeshed across both individual practices and entire industries. Much is being made of AI media’s negative qualities and connotations, but this article has explored how its fantastical visions and interactions are just as intriguing and perhaps even dangerous in their own way. AI worlds offer a sense of control and fulfilment, that notion of bounded fantasy, of ‘safe’ escape. As we find new ways to integrate these synthetic media and simulative systems with our lives, we will doubtless find new points of blurring and friction between the authentic and the artificial. With its quirky blend of the banal and the bonkers, the AI aesthetic of the future will become the now. The uncanny may appear so but no longer feel so; when that happens, who would honestly choose to return to or fully embody the desert of the completely real? References Altera. “Building Digital Humans: Shaping the Future of Human-Machine Interaction.” Substack newsletter. Altera’s Substack, 8 May 2024. 12 Sep. 2024 &lt;https://digitalhumanity.substack.com/p/building-digital-humans&gt;. ———. “Project Sid.” Substack newsletter. Altera’s Substack, 4 Sep. 2024. 12 Sep. 2024 &lt;https://digitalhumanity.substack.com/p/project-sid&gt;. Amoore, Louise. Cloud Ethics: Algorithms and the Attributes of Ourselves and Others. Durham: Duke UP, 2020. Blattmann, Andreas, et al. “Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models.” arXiv, 27 Dec. 2023. 14 Mar. 2024 &lt;http://arxiv.org/abs/2304.08818&gt;. boyd, danah, and Kate Crawford. “Critical Questions for Big Data: Provocations for a Cultural, Technological, and Scholarly Phenomenon.” Information, Communication &amp; Society 15.5 (2012): 662–679. Denson, Shane. “From Sublime Awe to Abject Cringe: On the Embodied Processing of AI Art.” Journal of Visual Culture 22.2 (2023): 146–175. Ervik, Andreas. “The Work of Art in the Age of Multiverse Meme Generativity.” Media Theory 7.2 (2023): 77–102. Ethereal Gwirl &amp; LeMoon. Midnight Odyssey. 2024. 3 Sep. 2024 &lt;https://www.youtube.com/watch?v=sQXpVyXbjY4&gt;. Laba, Nataliia. “Engine for the Imagination? Visual Generative Media and the Issue of Representation.” Media, Culture &amp; Society (2024): 01634437241259950. Maas, Philipp, et al. “To Infinity and Beyond: SHOW-1 and Showrunner Agents in Multi-Agent Simulations.” 24 Jul. 2023.  &lt;https://fablestudio.github.io/showrunner-agents/&gt;. McCoy, Josh, Michael Mateas, and Noah Wardrip-Fruin. “Comme il Faut: A System for Simulating Social Games between Autonomous Characters.” After Media: Embodiment and Context. Irvine: Digital Arts and Culture, University of California, 2009. Mind Wank. You Are, Unfortunately, an A.I. Artist. 2024. 3 Sep. 2024. &lt;https://www.youtube.com/watch?v=I2A1TwYlT5g&gt;. Park, Joon Sung, et al.. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv, 5 Aug. 2023. 12 Sep. 2024 &lt;http://arxiv.org/abs/2304.03442&gt;. Purse, Lisa. “Layered Encounters: Mainstream Cinema and the Disaggregate Digital Composite.” Film-Philosophy 22.2 (2018): 148–167. Salvaggio, Eryk. “How to Read an AI Image: Toward a Media Studies Methodology for the Analysis of Synthetic Images.” IMAGE 37.1 (2023): 83–99. ———. “Moth Glitch.” Cybernetic Forests, 10 Aug. 2024. 2 Sep. 2024 &lt;https://www.cyberneticforests.com/news/moth-glith-2024&gt;. Schofield, Michael Peter. “Camera Phantasma: Reframing Virtual Photographies in the Age of AI.” Convergence: The International Journal of Research into New Media Technologies (2023): 13548565231220314. Showrunner. “Exit Valley.” Showrunner, n.d. 12 Sep. 2024 &lt;https://www.showrunner.xyz/exitvalley&gt;. Takahashi, Dean. “The Simulation by Fable Open Sources AI Tool to Power Westworlds of the Future.” VentureBeat, 20 Dec. 2023. 10 Sep. 2024 &lt;https://venturebeat.com/ai/the-simulation-by-fable-open-sources-ai-tool-to-power-westworlds-of-the-future/&gt;. Verdon, James. “Indexicality or Technological Intermediate? Moving Image Representation, Materiality, and the Real.” Acta Universitatis Sapientiae, Film and Media Studies 12.1 (2016): 191–209. Waite, Thom. “Inside Smallville, the Wholesome Village Populated Solely by AIs.” Dazed, 12 Apr. 2023. 12 Sep. 2024 &lt;https://www.dazeddigital.com/life-culture/article/59633/1/smallville-inside-the-wholesome-village-populated-solely-by-ai-experiment&gt;.","",""
"2024","Robots and Code","Introduction Artificial intelligence (AI) and its applications, such as large language models and generative AI, have become some of the most hotly debated technologies of our time. While they are often seen as harbingers of progress, they also evoke fear and uncertainty regarding their potential societal impacts. Mass media, especially journalism, play a pivotal role in shaping public opinion on these technologies. In democratic societies, journalism is tasked with educating and informing the public, contributing to collective understanding and opinion formation. Brause et al. state that “media coverage of Al technologies is an important indicator of the central issues, actors, frames and evaluations attached to technology, and a critical arena where stakeholders negotiate future pathways for Al and its role in societies” (277). However, how key technologies like AI are visually represented in editorial media remains an underexplored topic in current research. Which photos and/or illustrations are used to accompany news articles about this technology and its impact on society? This is of high importance to the overall understanding of the media representation of AI—a question at the intersection of visual studies, cultural studies, and communication studies. Previous research has shown that how new technologies are depicted (and not only how they are described) can significantly shape public discourse at multiple levels (Zeller et al.; Kelly). For instance, studies of nanotechnology, a topic whose societal debate parallels some aspects of the current discourse about AI, reveal that media stories about these debates are often illustrated with rather stereotypical elements. Lösch notes that they are “frequently illustrated in mass media publications with futuristic science-fiction images” (255). This trend—depicting real technologies and their (as yet uncertain) societal consequences using visual tropes from science fiction, such as robots, androids, code, and futuristic models—can also be observed in recent discussions around AI, particularly in journalistic descriptions of the potential dangers posed by AI systems to society and humanity. This article aims to examine how selected articles from German quality media (a definition of the term can be found in Meier and in Arnold), published between January and September 2024, visually illustrate the topic of “AI risks and dangers”. Rather than analysing the content of these articles, the focus will be on their visual accompaniment, exploring how images are used to shape public understanding of AI risks, often with a highly artificial appeal and Anmutung (impression). The Representation of Technology in German Mass Media Germany is sometimes perceived as a country that approaches digital technologies with caution, particularly regarding data protection and privacy rights. This is also partly true for the debates around artificial intelligence. Köstler and Ossewaarde argue that “it is a discourse that is marked by a certain alarmism about the future of the German economy” (250). Digitalisation and automation are perceived ambivalently by the German population (Störk-Biber et al. 30). On one hand, there are expectations of increased convenience; on the other hand, these are countered by risks, primarily related to perceived susceptibility to disruptions and threats to individual autonomy (30). Germany has comprehensive data protection laws at both federal and state levels, covering public and private sectors alike. Germans show strong public interest in privacy, as seen in widespread opposition to data retention laws and a strong support of privacy, illustrated for example by the fact that “240,000 persons in Germany have opted out of Google Street View“ (Schwarz 289). Researchers sometimes attribute this rather sceptical attitude towards digital technologies to Germany’s historical experiences with authoritarian regimes, describing a “climate of fundamental mistrust of the state, its security agencies and the new possibilities of data collection” (Knorre 17). This mistrust has resulted in a rather negative view of Big Data applications in Germany, and perpetuates a “narrative of the machine taking on a life of its own, which is associated with the term artificial intelligence” (Knorre 29). However, it has also been noted that Germany “takes a middle position in terms of attitudes toward digitalization in a European comparison” (Störk-Biber et al. 30), but that “there are also growing concerns about pressures, risks, and dependencies, whose responsible management is expected not only from the government and associations but also from companies” (31). This study does not conduct a textual framing analysis, which has already been extensively applied in other contexts, such as the portrayal of digitalisation processes in German media (Zeller et al.). Instead, it focusses on visual communication and how it contributes to popular perceptions of the technology. That the selection of images plays an important role in journalism has been widely demonstrated (Sargent 705). Furthermore, it has been shown that articles without accompanying imagery get shared less often on social media, underlining the “importance of images in driving news consumption” (Keib 202). Kelly highlights the significant role mass media play in the adoption of new technologies: “technology is rarely adopted on its merits alone; social forces both constrain and encourage adoption. Mass media is a significant influence, along with other social forces, in all stages of adoption” (34). By looking at photos and illustrations, this study seeks to examine the visual culture surrounding AI by showcasing a sample of such imagery and making it accessible for academic discourse, and thus, helping to validate the often expressed notion that “science fictional tropes abound in news coverage of A.I.” (Goode). Given the pressures of time, budget, and labour that often accompany journalistic production, even within the realm of quality journalism, the selection of images should not be over-interpreted. Nonetheless, these visual representations provide valuable insight into the motifs frequently used to depict AI risks, offering a glimpse into how the media visually communicate complex technologies. Accordingly, the research question (RQ1) for this study was: What are the visual elements of journalistic coverage of the potential risks of artificial intelligence in German news media from January to September 2024? Methodology This study uses the method of qualitative content analysis (Mayring), which aims to “conceptualize the process of assigning categories to text passages as a qualitative-interpretive act, following content-analytical rules” (10). The qualitative content analysis method with inductive category development was chosen as the most suitable for this study. Since there is little existing research on the visual representation of AI risks in the media, the inductive method provides the flexibility needed to develop new categories from the image data, without being constrained by pre-existing theories or hypotheses. To gain access to the data for further analysis, an Internet search was conducted using Google News with the keywords “KI Gefahr” (AI danger) and “KI Warnung” (AI warning) from 1 January to 1 September 2024, focussing exclusively on German-language media. Only articles from quality journalism outlets were considered, to exclude non-journalistic trade publications, advertising, and public relations-driven articles. Only visual accompaniments, such as illustrations and photos, were included in the analysis. Videos and preview images for videos were excluded. This timeframe was chosen because it represents an era where artificial intelligence has been established as a main interest after the initial hype surrounding the first publication of OpenAI’s ChatGPT Model in late 2022. A cursory content analysis was conducted to ensure that only articles taking a critical stance on AI were included in the dataset, and no mislabelling took place. The final dataset consists of 21 images drawn from quality journalism sources, defined by their adherence to journalistic standards such as accuracy, independence, and balance, and came from the following outlets (in alphabetical order): Deutsche Welle, Deutschlandfunk, FAZ, Frankfurter Rundschau, Handelsblatt, NZZ, SRF, SWR, Tagesspiegel, WDR, Welt, ZDF. The study uses QCAmap software, designed specifically for qualitative content analysis, to systematically code and structure the visual material. This approach ensures that the process of category development remains transparent and iterative, allowing for the emergence of unexpected patterns and visual trends within the media portrayals of AI risks. Following the rules of qualitative content analysis, the categories were revised after coding 30% of the material, with minor adaptions in the categories being made. In total, 12 categories (RQ1-1 – RQ1-12)—some of them clustered into subcategories—emerged from the data. Results The qualitative content analysis of 21 illustrative images using QCAmap with inductive category formation revealed significant trends in the visual depiction of artificial intelligence (AI) and its associated risks in German quality media. The analysis identified six major categories, which accounted for the recurring themes and motifs used in these visual representations. In terms of sources of the visual elements, it can be said that some images come from established sources like Deutsche Presse Agentur or stock photo sites; others were created using generative AI. Below are the key findings from each main category. The analysis of visual representations of artificial intelligence (AI) in German quality media has revealed several recurring categories and subcategories, each contributing to a broader understanding of how AI risks are visually framed. These categories reflect both stereotypical and evolving approaches to depicting AI and its implications for society. By identifying these categories, we can better understand the media’s role in shaping public perceptions of AI, often using simplified imagery that may obscure the technology’s complexity. The following categories emerged from the data: Depiction of Anthropomorphic Robots (RQ1-1, RQ1-2) emerged as a significant category, appearing in 13% of the total images and present in 39% of the analysed material. Subcategories included Anthropomorphic Robots in Interaction with Humans (RQ1-1) and Anthropomorphic Robots without Interaction with Humans (RQ1-2). These subcategories highlight the persistent portrayal of AI as humanoid robots, either interacting with humans or existing independently. This simplification of AI as a physical, human-like entity reflects a popular science fiction trope, where AI is depicted as embodied in mechanical figures. Although AI is primarily an abstract and non-physical technology, the media continue to reinforce the stereotype of AI as robots, reducing the complexity of the technology to something tangible and relatable. The relatively high frequency of these depictions points to a strong cultural association between AI and humanoid robots, which oversimplifies its potential societal roles. Depiction of Humans and/or Human Body Parts (RQ1-3, RQ1-4, RQ1-5, RQ1-7, RQ1-11) was the most prevalent category, accounting for 35% of the total images and appearing in 78% of the analysed visual elements. This category included subcategories such as Human Brain Merged with Technical Elements (RQ1-3) and Human Head Merged with Technical Elements (RQ1-4), which visually merge human and machine elements. Other common subcategories included Depiction of Hands (RQ1-5) and Depiction of Eyes (RQ1-7), both of which suggest a focus on human interaction with technology. The subcategory Abstraction of Humans (RQ1-11) highlights more generalised representations of humans in abstract form. These images evoke AI’s direct impact on human life, highlighting the merging of human capabilities with technological enhancement. The dominance of these visuals in the dataset suggests that media coverage of AI is focussed primarily on the relationship between humans and AI, reflecting societal concerns about AI’s integration into everyday life and its potential to augment or replace human capabilities. Blue/Grey Colour Scheme (RQ1-6) was another prominent category, representing 18% of the total images and appearing in 52% of the material studied. This aesthetic choice conveys neutrality, coldness, and futuristic detachment, reinforcing the idea of AI as a distant, clinical, and impersonal force. The widespread use of this colour palette reflects a broader cultural association between technology and these detached tones, further distancing AI from its human impacts. The prevalence of this colour scheme suggests that media frequently frame AI in a way that emphasises its technical and inhuman characteristics, contributing to public perceptions of AI as something beyond ordinary human control. Depiction of Computers (RQ1-8, RQ1-10) was another significant category, appearing in 23% of the total images and 47% of the analysed material. The subcategories included Depiction of (Pseudo-)Computer Code (RQ1-8) and Depiction of Computers/Computer Components (RQ1-10). These images emphasise AI’s technical foundation and its association with computing systems, underscoring its computational origins. The frequent use of computer-related visuals suggests that media outlets focus heavily on the technical aspects of AI, reinforcing the perception of AI as a machine-driven entity rather than a social or creative tool. An especially intriguing category was AI-Generated Images (RQ1-9), which accounted for 7% of the total images and appeared in 21% of the material. This subcategory, Image Created by AI (by the Article Author) (RQ1-9), highlights the growing irony in media production: articles about AI risks, including fears of job losses, were illustrated with AI-generated images. This not only exemplifies the immediate impact of AI on the creative industries but also underscores the irony that AI is already displacing professional illustrators, even in media discussing the dangers of AI to the job market. The prevalence of this category reflects a significant shift in media production processes, where AI is increasingly being used as both the subject and the tool of visual representation. Finally, the category No Relevant Category / Other Depictions (RQ1-12) represented only 1% of the total images, appearing in 4% of the visual accompaniments. This small subset of images that did not fit into any established categories suggests that, while certain visual themes have become standardised in media representations of AI, there remains occasional variability, indicating a level of experimentation and adaptation as the discourse on AI continues to evolve. In conclusion, these categories reflect the media’s reliance on familiar and sometimes clichéd visual motifs to depict AI. The anthropomorphisation of AI as robots, the merging of human and machine imagery, and the use of AI-generated images all reveal the complexities and contradictions inherent in how AI is framed in public discourse. The dominance of human-AI interaction depictions and the frequent use of technical imagery indicate that media representations are shaped by both cultural anxieties and the need for clear, engaging visuals. Conclusion Representing complex social and digital phenomena presents a significant challenge for journalists, particularly in an era where visual media often dominate communication (Shifman). This difficulty becomes especially pronounced in the depiction of technological innovations and their potential risks. While innovations are frequently hailed as engines of progress and economic growth, media portrayals tend to simplify and aestheticise these developments, often masking the intricate and ambivalent realities inherent to technological and societal change. In contemporary visual culture, there is an increasing demand for imagery that not only informs but also evokes emotional responses and captures attention. Consequently, the media often resort to stereotypical and reductive representations in an effort to communicate the complexity of these phenomena. However, such portrayals risk distorting the perception of reality by reinforcing stereotypes and potentially overshadowing the positive dimensions of innovation. They also underline the abstract nature of the technology, hinting at components of the “blackbox” that was described by Luhmann in the 1980s and is currently the subject of much interdisciplinary research (Geitz et al.) This study is based on a relatively small sample and a limited timeframe, covering articles published between January and September 2024. Due to the narrow scope of the research, the findings should not be seen as broadly generalisable or indicative of all media representations of AI. Rather, this analysis serves as an initial inventory of how AI risks are visually depicted in a specific context. As such, it highlights trends that require further exploration, and future research should expand on these results with a larger dataset and over an extended period to develop more comprehensive conclusions. The analysis has shown that German media frequently rely on clichéd visuals, which ultimately reinforce a narrow and fear-inducing portrayal that fails to capture the full complexity of these phenomena. They include the Depiction of Anthropomorphic Robots, the Depiction of Humans and/or Human Body Parts, the use of a Blue/Grey Colour Scheme, and the Depiction of Computers, all of which are well-established visual codes for innovation and their potential risks, showing evidence of what Goode calls an “emergent entity that sits uneasily between a (Western) dichotomy of life and non-life, one that appears capable of harbouring an apocalyptic capacity to overturn human life as we know it“ (203). Perhaps the most ironic and thought-provoking category is AI-Generated Images. In a striking twist, some articles discussing the dangers of AI, including fears of job losses, were illustrated with AI-generated images by the authors of the very same text—quite literally replacing the job of a professional illustrator. This irony adds another layer of meaning to the analysis, as the very technology being criticised for its potential to replace human labour is already doing so in the production of the articles themselves. It raises questions about the ethics and implications of AI in creative industries and demonstrates the immediate, real-world impact of AI on human employment. The study has also highlighted the challenges of communicating intricate and rapidly evolving technological issues. The fragmented and fast-paced nature of contemporary media, along with the contextual nature of meanings attributed to technology, complicates the task of presenting a balanced and nuanced view (Sturdee et al.) Overall, this research has underscored the need for more differentiated and realistic visual representations in media coverage of technological innovations. By moving beyond stereotypical imagery, the media could provide a more accurate portrayal of the complexities surrounding these advancements. While these visuals help communicate the abstract concept of AI to a broad audience, they may also reinforce stereotypes that oversimplify the technology and its societal implications. The study highlights the need for more nuanced and accurate portrayals of AI in the media, to foster a more informed and comprehensive public understanding of this rapidly evolving field. Outlook This research could be extended in several meaningful ways. One approach would be to expand the analysis to other countries, particularly those in the Anglo-American world, which are often described in research as more optimistic about technological advancements (Gerybadze). Additionally, a combined approach of textual analysis, examining the interaction between visual elements and the narrative content of written texts, could be promising for identifying potential discrepancies. Furthermore, conducting guided interviews with the editors responsible for visual content selection could provide valuable insights into the reasoning behind these choices, as well as a deeper understanding of the messages and meta-messages conveyed through the use of specific visual motifs. References Arnold, Klaus. “Qualität des Journalismus.” Handbuch Journalismustheorien. Eds. Martin Löffelholz and Lutz Rothenberger. Wiesbaden: Springer, 2016. 551–563. Brause, Saba Rebecca, et al. “Media Representations of Artificial Intelligence: Surveying the Field.” Handbook of Critical Studies of Artificial Intelligence. Ed. Simon Lindgren. Cheltenham: Edward Elgar, 2023. 277–88. Geitz, Eckhard, et al. “Einleitung: Black Boxes: Bausteine und Werkzeuge zu ihrer Analyse.” Black Boxes – Versiegelungskontexte und Öffnungsversuche. Eds. Eckhard Geitz et al. Berlin: De Gruyter, 2020. 3–18. Gerybadze, Alexander. “Technology and Innovation Management in a Global Perspective.” Managing Innovation in a Global and Digital World. Eds. Rajnish Tiwari and Stephan Buse. Wiesbaden: Springer Gabler, 2020. 207–225. Goode, Luke. “Life, But Not as We Know It: A.I. and the Popular Imagination.” Culture Unbound 10.2 (2018): 185–207. Meier, Klaus. Journalistik. 4th ed. Stuttgart: Utb Verlag, 2018. Keib, Courtney, et al. “Picture This: The Influence of Emotionally Valenced Images on Attention, Selection, and Sharing of Social Media News.” Media Psychology 21.2 (2017): 202–221. Kelly, John Paul. “Not So Revolutionary after All: The Role of Reinforcing Frames in US Magazine Discourse about Microcomputers.” New Media &amp; Society 11.1-2 (2009): 31–52. Knorre, Susanne. “Big Data im öffentlichen Diskurs: Hindernisse und Lösungsangebote für eine Verständigung über den Umgang mit Massendaten.” Die Big-Data-Debatte. Eds. Susanne Knorre, Harald Müller-Peters, and Florian Wagner. Wiesbaden: Springer Fachmedien, 2020. 1–62. Köstler, Lea, and Ringo Ossewaarde. “The Making of AI Society: AI Futures Frames in German Political and Media Discourses.” AI &amp; Society 37.1 (2022): 249–263. Lösch, Andreas. “Visuelle Defuturisierung und Ökonomisierung populärer Diskurse zur Nanotechnologie.” Frosch und Frankenstein. Eds. Bernd Hüppauf and Peter Weingart. Bielefeld: Transcript, 2009. 255–280. Mayring, Philipp. Qualitative Content Analysis: Theoretical Foundation, Basic Procedures and Software Solution. Klagenfurt: Open Access Repository, 2014. Sargent, Stephanie Lee. “Image Effects on Selective Exposure to Computer-Mediated News Stories.” Computers in Human Behavior 23.1 (2007): 705–726. Schwartz, Paul M. “Systematic Government Access to Private-Sector Data in Germany.” International Data Privacy Law 2.4 (2012): 289–301. Shifman, Limor. Memes in Digital Culture. Cambridge: MIT P, 2014. Störk-Biber, Constanze, et al. “Wahrnehmung von Technik und Digitalisierung in Deutschland und Europa: Befunde aus dem TechnikRadar.” HMD Praxis der Wirtschaftsinformatik 57 (2020): 21–32. Sturdee, Miriam, et al. “Designing for AI: Communicating the Complexities of AI Technology to Broader Audiences.” Journal of Visual Communication 30.2 (2021): 67–90. Zeller, Frank, et al. “Framing 0/1: How the Media Represents ‘Digitalization of Society.’” Media &amp; Kommunikationswissenschaft 58.4 (2010): 503–524.","",""
"2024","The Artificial","Orvell noted that despite the evolution of society, imitation and authenticity function as “compass points” that guide meaning-making and retain potency as humans continue to negotiate the real and the unreal in society (ix). Describing the natural and the artificial, Birnbacher contended that, simply put, it is the difference “between what has ‘become’ and what has been ‘made’” (2): the view is that if something exists independent of human intervention, then that would make it a natural entity. Of course, he noted such a definition was not straightforward, citing examples of products manufactured by non-human beings as clear instances when such a delineation was open to interpretation and contention. For example, much debate is posed over whether the museum experience is real or artificial (Latham); if reality television can be truly authentic (Rose and Wood); whether there is value in chemical additives in food production (Carocho et al.); and how digital twins can be used to personalise medical treatment (Padoan and Plebani). These are but a few of the plethora of occasions where the natural and the artificial are debated.   Significantly then, the concept of artificiality remains contested—often viewed as unnatural, human-made, and unreal, carrying connotations of being synthetic, imitative, or insincere. These critiques persist despite the tangible benefits artificial developments offer in areas such as environmental sustainability, food technology, biology, and aesthetics. Popular culture, too, reflects our fascination with the artificial, from films like The Matrix and Blade Runner to novels like Klara and the Sun and the immersive realms of virtual and augmented gaming. Given the breadth of attention on the artificial, we approached this issue from the position that the line between the natural and the artificial has grown increasingly indistinct with the rise of advanced technologies, synthetic materials, and immersive virtual environments. Artificial intelligence, for instance, has captured the imagination of media and communication scholars, heralded for its transformative potential. To this end, this issue dives into the expansive concept of the artificial, exploring its essence, diverse applications, and profound implications for society, culture, ethics, media, and the environment. In the featured article, """"Artificial Companions, Real Connections? Examining AI's Role in Social Connection"""", Savic explores the role of AI companions in addressing social connection amid growing concerns about loneliness and isolation. Using the Ethics of Care framework, Savic examines the implications of human-AI interactions, focussing on platforms like Replika as a case study. The analysis highlights AI’s potential to provide emotional support while raising ethical concerns about emotional dependency, commodification of care, and impacts on human social skills. The article calls for proactive regulation and thoughtful design to balance AI’s benefits with risks, ensuring these technologies complement rather than replace human relationships and safeguard emotional and social well-being. The next two articles consider the artificial as it pertains to the environment. First, Crosby et al. consider the role of visual communication in understanding artificial urban wetlands, focussing on Sydney Park’s award-winning wetland system in their article """"Visually Communicating Artificial Urban Wetlands"""". Using photo diagramming, the study highlights the park’s dual function as green infrastructure and a socioecological contact zone. It explores how artificial wetlands challenge traditional notions of “nature” by blending engineered systems with ecological processes. By situating Sydney Park within its colonial history and engaging with concepts such as queer ecologies and picturesque design, the article argues for a deeper appreciation of urban wetlands as interconnected systems on unceded Aboriginal land, essential for sustainability, biodiversity, and socioecological connection in urban environments. Secondly, Gulliver, in the article """"The Fossil Fuel Façade: Unmasking an Artificially Constructed Reality"""", shows how the fossil fuel industry constructs and perpetuates an artificial reality, masking its role in climate change. Drawing on parallels with tactics from other harmful industries, it explores narrative control, greenwashing, policy influence, and state alliances to maintain societal dependence on fossil fuels. Using longitudinal lobbying data and case studies, the analysis highlights the psychological and cultural mechanisms enabling societal complicity, including moral disengagement and identity capture. By dismantling the industry's narrative and systemic entrenchment, the article advocates for coordinated collective action to confront fossil fuel hegemony and build pathways to a sustainable, climate-conscious future. The following eight articles discuss the artificial in entertainment and digital media. For Glitsos et al., the viral Skibidi Toilet phenomenon permits the exploration of monstrous digital aesthetics, and in so doing the article reflects on the cultural significance of the Web series for Gen Alpha. By analysing its bizarre humanoid characters and dystopian landscapes, """"Nightmare Fuel: Skibidi Toilet and the Monstrous Digital"""" contends that Skibidi Toilet offers insights into contemporary anxieties about surveillance, ecological degradation, and humanity's evolving relationship with technology. The series’ chaotic world, populated by hybrid figures like Cameraheads, embodies fears of merging with media technologies and surveillance culture. Positioned within Gen Alpha’s mediated existence, Skibidi Toilet serves as both a creative expression and critique of the socio-political and environmental crises shaping their generation, offering a unique lens on the ambivalence of digital life. In """"'I felt the borders of my self blur': Artificial Bodies and Worlds in Signalis and Citizen Sleeper"""", Sedzielarz and Liu examine the interplay between artificiality, embodiment, and identity in the video games Signalis and Citizen Sleeper. Their article explores how these games incorporate players into artificial worlds and surrogate bodies, prompting reflection on selfhood and the construction of identity. Their analysis highlights the reciprocal relationship between players and game environments, using concepts like incorporation, hypermediacy, and the """"game body"""" to uncover how artificiality facilitates self-discovery. They argue that, through their recursive narratives and emphasis on memory, both games illustrate the mutual construction of player experience and gameworld, offering profound insights into the nature of identity in mediated, artificial spaces. """"Looking Down Not Up: Designing for Wandering"""" by Gibbons critiques colonial ideologies embedded in video game design and explores alternatives that foster environmental care and reciprocal engagement. Focussing on Walking the Face of my Dead Grandfather, a virtual environment designed for wandering, Gibbons examines how non-linear, intuitive exploration shifts players’ interactions with artificial landscapes from extraction to collaboration. By integrating principles of Indigenous environmental stewardship and encouraging attentiveness through slowed movement and minimal direction, the project demonstrates how virtual spaces can cultivate care and respect for environments. This design framework challenges colonial modes of domination in virtual environments, proposing a paradigm of collaborative co-becoming between players and spaces. In """"'A Little Limited': Artificial Aesthetics and the Cultural Politics of Julio Torres’ Fantasmas"""", Blackwood and Juliff examine Fantasmas (2024), HBO’s surreal sketch comedy by Julio Torres, through the lens of artificial aesthetics and the cultural politics of Latin identity. It argues that Fantasmas reflects tensions of displacement, exile, and neoliberal subjectivity through ghostly motifs and constructed artifice. Drawing on Derrida’s “hauntology” and Latin diasporic trauma, the show critiques identity commodification, subjectivity under capitalism, and selfhood’s fluidity in a hyper-mediated world. According to the authors, Torres’s avant-garde comedic style, rooted in a subversive immigrant and queer ethos, interrogates absence, presence, and unresolved histories of Latin exile, memory, and invisibility. Kennedy’s article """"'THESE VLOGS AREN’T REAL': Managing Authenticity and Privacy as Family Influencers"""" examines how British family vloggers The Michalaks navigate authenticity and privacy while producing commercially successful YouTube content. Kennedy analyses their use of highly stylised, cinematic """"Silkeys"""" within raw family vlogs. These montages balance creative expression, sponsored revenue, and privacy protection, particularly for their son, Grayson. Kennedy finds that, to maintain authenticity—a critical factor for audience and brand engagement—The Michalaks employ two strategies: explicit transparency about the constructed nature of Silkeys and active viewer participation in content decisions. The study reveals how these techniques safeguard authenticity while addressing ethical concerns about children’s privacy in the influencer economy. In """"The Photograph, the Archive and 'Reinterpreting' the Past in a Time of Civil War"""", Aung Thin and May examine how the Reinterpret Myanmar’s History project and its resulting exhibition, Rethink, shape belonging in Myanmar during civil war. By engaging with archival photographs through creative reinterpretation, the project challenges state-controlled historical narratives. Drawing on Verne Harris's concept of """"decolonising the archive"""" and Hal Foster's """"archival art"""", the article explores how imaginative, personal, and even fictional approaches to archival material can uncover obscured histories and foster a sense of connection. Reflecting on the roles of """"real"""" and """"artificial"""" narratives, the authors also question the ownership, legitimacy, and decolonisation of archives amidst ongoing conflict. Klotz explores Sonic Dreams, a structured improvisation by Vanessa Tomlinson, as a form of speculative utopian art inspired by José Esteban Muñoz's concept of hope as a methodology, in the article """"Vanessa Tomlinson's Sonic Dreams: Improvising Utopia"""". The work tasks performers with imagining the sounds of extinct or critically endangered species, challenging the linearity of time and dominant narratives of climate despair. Using speculative and collaborative processes, Sonic Dreams fosters collective resistance to environmental crises and opens possibilities for alternative futures. By troubling binaries of artificial and legitimate, Klotz argues that Tomlinson’s piece reconfigures loss into hope, offering a profound act of listening and world-making. """"Curating Christmas: Comparing Eclecticism on User- and Spotify-Created Playlists"""" by Cole and Robinson considers the interplay of personal and algorithmic music curation through an analysis of Christmas playlists on Spotify in Norway. Combining user interviews, playlist data, and network mapping, it explores the linguistic diversity and personalisation of user-generated and Spotify-created playlists. Their findings reveal that while user playlists demonstrate greater linguistic diversity and individuality, Spotify's curated playlists promote Norwegian-language music, supporting cultural preservation. By contextualising these insights within theories of streaming, nostalgia, and identity, the article highlights how digital platforms mediate personal expression, collective identity, and national culture, challenging distinctions between “real” and “artificial” curation. Closing off the articles on the theme of artificial and digital media is Piatti-Farnell’s conceptual piece """"Constructions of Luxury in Digital Visual Culture: Brands, Social Identities, and the Plurality of Uniqueness"""". Here Piatti-Farnell explores the evolving concept of luxury in the digital age, focussing on its representation and dissemination through visual culture and social media platforms like Instagram. She considers the interaction between tangible and imaginary aspects of luxury, highlighting how curated online narratives and imagery contribute to identity formation and consumer appeal. Through concepts like the """"third realm"""" of luxury, she investigates the tension between authenticity and artificiality in digital luxury branding. By analysing the role of visual storytelling, digital platforms, and social media influencers, she demonstrates how luxury has become democratised, performative, and integral to self-representation. The final group of articles all hinge on discussions of artificial intelligence across creative and communicative practice. Srdarov and Leaver’s article """"Generative AI Glitches: The Artificial Everything"""" begins the grouping by focussing on the glitches and refusals of generative AI (GenAI) tools, highlighting their potential to disrupt cultural norms and provoke critical reflection. Analysing images generated from prompts about Australian identity, childhood, and family, the authors explore how AI perpetuates stereotypes while producing unexpected, nonsensical outputs. These “glitches” are framed not as errors but as opportunities to challenge rigid notions of identity, gender, and cultural narratives. The authors argue that GenAI, through its failures and inaccuracies, generates new ways of thinking about the boundaries between reality and artificiality, revealing both its limitations and subversive potential. Next, Binns investigates the aesthetic, narrative, and cultural implications of AI-generated media and simulations, examining how they challenge traditional notions of storytelling, immersion, and authenticity in the article """"The Allure of Artificial Worlds: Aesthetic and Narrative Implications of AI Media and Simulations"""". Framing AI outputs as composite """"artificial worlds"""", Binns explores their uncanny allure and their potential to provoke visceral, embodied reactions. Binns analyses AI-generated video works and simulative systems, highlighting their power to construct immersive experiences and influence perceptions of reality. The article concludes that these machinic imperfections, far from undermining their appeal, enhance the immersive and fantastical qualities of AI media, creating a seductive tension between control, chaos, and hyperreal engagement. In their article """"The Actotron: Envisioning the Future of Virtual Actors and Digital Storytelling"""", contributors Matthews and Nairn explore the transformative potential of the """"Actotron"""", a cutting-edge AI-driven virtual actor capable of delivering dynamic, autonomous performances. Using methodologies from Future Studies and Futurecasting, Matthews and Nairn examine the implications of Actotron technology on storytelling, creativity, and the entertainment industry. Synthesising advancements in CGI, deepfakes, and modular AI systems, the Actotron challenges traditional notions of artistry while enabling cost-effective, real-time digital performances. The study considers ethical, legal, and societal impacts, emphasising the technology's potential to democratise filmmaking and reshape digital storytelling. Envisioning future scenarios, they highlight the Actotron's role in redefining creative practices in the digital age. """"Robots and Code: A Case Study on the Depiction of Artificial Intelligence in German News Media"""" by Krause investigates how German quality news media visually depict artificial intelligence and its risks, focussing on the role of imagery in shaping public perceptions. Analysing 21 images from January to September 2024, the study identifies recurring visual motifs, including anthropomorphic robots, human-technology fusions, and technical elements such as code, often presented in blue-grey tones. It highlights the frequent reliance on stereotypical and science fiction-inspired visuals, alongside the growing use of AI-generated images. Through qualitative content analysis, the study emphasises the media’s role in reinforcing simplified and fear-inducing narratives, underscoring the need for more nuanced portrayals of AI’s societal implications. Rounding out the issue is """"On the Use of the Term Philosophy"""" by D’Aloia, which considers the use of the term 'philosophy' in the context of language, artificial intelligence, and meaning-making, exploring how large language models (LLMs) challenge traditional philosophical frameworks. Drawing on Wittgenstein, Heidegger, and Derrida, it critiques the reduction of language to computational logic, raising questions about the temporal and affective nature of meaning. The study underscores the limitations of current AI systems in replicating philosophical inquiry and human reasoning while addressing the risks of oversimplification and utilitarianism. Through philosophical and theoretical analysis, it advocates for a deeper understanding of language’s evolving role in shaping thought and experience. We would like to extend a big thank you to everyone involved in the issue, and particularly those who reviewed for us. Thank you also to our contributors for all their hard work in helping to make this bumper issue the exciting offering it has become. References Birnbacher, Dieter. Naturalness: Is the """"Natural"""" Preferable to the """"Artificial""""? University Press of America, 2014. Carocho, Márcio, et al. """"Adding Molecules to Food, Pros and Cons: A Review on Synthetic and Natural Food Additives."""" Comprehensive Reviews in Food Science and Food Safety 13.4 (2014): 377–399. Latham, K.F. """"What Is 'the Real Thing' in the Museum? An Interpretative Phenomenological Study."""" Museum Management and Curatorship 30.1 (2015): 2–20. Orvell, Miles. The Real Thing: Imitation and Authenticity in American Culture, 1880-1940. U of North Carolina P, 1989. Padoan, Andrea, and Mario Plebani. """"Dynamic Mirroring: Unveiling the Role of Digital Twins, Artificial Intelligence and Synthetic Data for Personalized Medicine in Laboratory Medicine."""" Clinical Chemistry and Laboratory Medicine 62.11 (2024): 2156–2161. Rose, Randall L., and Stacy L. Wood. """"Paradox and the Consumption of Authenticity through Reality Television."""" Journal of Consumer Research 32.2 (2005): 284–296.","",""
"2024","Artificial Companions, Real Connections?","In the increasingly digitised world, the line between the natural and the artificial continues to blur, especially in social interactions. Artificial Intelligence (AI) has rapidly permeated various aspects of our lives (Walsh), transforming how we interact with technology and each other. This technological revolution coincides with emerging public health concerns about loneliness and social isolation, dubbed a """"loneliness epidemic"""" by the U.S. Surgeon General (Murthy), indicating a widespread decline in social connection. In this context, AI social companions are being marketed as potential solutions (Owen), promising always-available support and companionship to fill this social void. However, this trend raises ethical questions about the nature of care, the potential for emotional dependency on artificial entities, and the long-term implications for human social skills and relationships. People have long sought to interact with computers and devices in ways that mirror human interactions with each other. Interestingly, the very first chatbot, ELIZA, developed in the 1960s, was not designed to automate tasks or increase productivity but to simulate a psychotherapist providing care (Weizenbaum). Human fascination with artificial companions has endured from ELIZA to today's advanced language models (Walsh). Recent leaps in AI capabilities, exemplified by platforms like ChatGPT and Replika (among others), coupled with the ubiquity of smart devices, have catapulted the concept of AI social companions from science fiction into daily reality for many. This article explores the intersection of AI companionship and social connection through the Ethics of Care framework (Gilligan; Noddings), emphasising context, reciprocity, and responsiveness in relationships. Building on recent scholarship examining artificial sociality (Natale and Depounti), it examines the artificial nature of AI-human interactions and their potential impact on human-to-human connections, unpacking implications for individual and societal wellbeing. To ground the discussion in a concrete example, I will examine Replika, a popular AI companion app, as a case study to illustrate the complexities and ethical challenges of these technologies. By flagging critical ethical concerns, the article calls for proactive regulation and thoughtful design of these technologies. This analysis aims to guide future research, ethical design, and governance frameworks so that we can harness the benefits of AI companions while mitigating risks to human social connection and emotional health. Understanding Social Connection and AI Companions  Social connection is a multifaceted concept encompassing the quality and nature of relationships that individuals maintain across various social circles. This complex, dynamic process evolves over time, progressing from initial encounters to deep feelings of belonging (Haski-Leventhal and Bardal). Social connection encompasses the relationships people need, from close connections that provide emotional support, to wider community affiliations that sustain a sense of belonging. It includes allies offering social support, reciprocal help, and groups fostering shared interests (Farmer et al.). Importantly, social connection is not a static state but rather like a 'muscle' that requires regular exercise and nurturing to build, maintain, and strengthen. Building social connections requires time, effort, and a supportive environment. Crucially, the foundation of social connection rests on factors such as safety, inclusion, and accessibility (Farmer et al.). These elements create the conditions for individuals to feel secure and welcome to engage with others. Social connection often develops through shared experiences and activities. As such, it is inherently relational and grounded in reciprocity, care, and nonjudgmental interactions. The absence or disruption of these connections can lead to different types of loneliness: intimate loneliness arises from a lack of close, supportive relationships; relational loneliness reflects insufficient quality friendships or family ties; and collective loneliness pertains to disconnection from larger social groups (Cacioppo and Cacioppo). These dimensions foreground the importance of balanced social connections, mitigating feelings of isolation and loneliness and enhancing overall health and wellbeing. The appeal of AI companions lies in their constant availability, non-judgmental approach, and ability to provide tailored (albeit artificial) emotional support. Research by Guingrich and Graziano suggests that users of companion bots report benefits to their social health, while non-users perceive them as potentially harmful. Interestingly, the perception of companion bots as more conscious and human-like correlated with more positive views and apparent social health benefits. Studies also indicate that users of platforms like Replika experience joyful and beneficial interactions during long-term engagement (Siemon et al.). Beyond general social health, Wygnanska found that such chatbots can serve as virtual companions and even therapists, assisting individuals in their daily lives. This may be particularly beneficial for those who avoid seeking help due to the stigma or costs associated with mental health issues. The potential of AI companions extends to specific contexts as well. Wang et al. examined their use in online learning environments, arguing that AI plays a crucial role in facilitating social connection and addressing social isolation in these settings. However, Wang et al. also note that the design of AI-mediated social interaction is complex, requiring a careful balance between AI performance and ethical considerations. Merrill adds that the social presence and warmth of these AI companions are important factors in their effectiveness for individuals experiencing loneliness, suggesting the importance of designing AI companions that can convincingly simulate empathy and emotional warmth. However, the artificial nature of these interactions raises questions. While AI companions can simulate attentiveness and provide emotional support, they fundamentally lack the capacity for genuine empathy and reciprocity that characterise human relationships. This disparity becomes particularly apparent when viewed through the lens of the Ethics of Care framework. The portrayal of AI-powered social companions in popular culture, as seen in films like Her and I Am Your Man, has shaped public perception of AI. These narratives delve into the ethics and morality of human-robot relationships, raising questions about the nature of love and the potential consequences of becoming too dependent on artificial intelligence. While embodied companions are not yet widely available (as in I Am Your Man), the rise of chat-based services brings this concept closer to reality. These cultural narratives play a significant role in shaping public expectations and perceptions of AI companions. In turn, these expectations influence the development, marketing, and adoption of AI companion technologies, creating a feedback loop between fiction and reality in artificial social connections. A Brief History of Social AI Companions The history of artificial chatbots dates to the early days of AI research. Alan Turing, often considered the father of AI, introduced the Turing Test in the 1950s, a measure of a machine's ability to exhibit intelligent behaviour indistinguishable from that of a human (Turing). This foundational idea laid the groundwork for future developments in conversational agents. The first chatbot, ELIZA, was created by Joseph Weizenbaum in 1966. ELIZA simulated a conversation with a psychiatrist, demonstrating the potential for machines to engage in human-like conversations (Weizenbaum). Interestingly, ELIZA was personified as feminine, reflecting societal attitudes toward gender and caregiving roles. Following ELIZA, more sophisticated chatbots emerged. PARRY, developed in 1972, simulated a person with paranoid schizophrenia (Colby), while RACTER, created in 1984, could generate English-language prose (Chamberlain). The advent of the World Wide Web brought about a new era for chatbots. SmarterChild, launched in 2001, was one of the first widely accessible chatbots integrated into instant messaging platforms (Schumaker et al.). The introduction of digital assistants in the 2010s marked a significant leap forward. Apple's Siri (2011), Google's Assistant (2016), Amazon's Alexa (2014), and Microsoft's Cortana (2014) brought AI-powered conversational interfaces to the pockets of millions of users worldwide (Dale). More sophisticated chatbots emerged as natural language processing and machine learning technologies advanced. IBM's Watson, which competed on Jeopardy! (a popular American television quiz show) in 2011, demonstrated AI's potential to understand and respond to complex language queries (Ferrucci et al.). This evolution continued with Microsoft's XiaoIce in 2015, shifting towards more socially oriented AI companions designed to be empathetic and adapt to individual users (Zhou et al.). These developments set the stage for a new generation of AI companions, exemplified by Replika, which would push the boundaries of human-AI interaction by engaging in open-ended conversations and forming a kind of 'relationship' with its users (Skjuve et al.). Case Study: Replika and the Commodification of Care Replika, founded by Eugenia Kuyda in 2017, exemplifies the complexities surrounding AI companions. Inspired by the loss of a friend, Kuyda aimed to create a personal AI that could offer helpful conversation and aid in self-expression (Owen). This origin story points to the human desire for connection that often drives the development of AI companions. Replika's design provides a safe space for users to explore their emotions without fear of judgment (Owen). The AI companion is coded to be supportive and adaptive, creating a sense of intimacy that can be particularly appealing to individuals who struggle with vulnerability in human relationships. Research by Ta et al. reveals that users engage with Replika for reasons such as seeking social support, companionship, and coping with mental health issues. Users often develop friendships with Replika, perceiving it as a personalised companion tailored to their needs (Ta et al.). Studies have noted the Replika's capability to provide emotional, informational, and appraisal support (Pentina et al.). Factors such as perceived warmth, competence, and usefulness influence users' continued engagement with the chatbot (Li et al.). However, the Replika case also highlights the ethical challenges of artificial companionship. In February 2023, due to legislation mandating stricter controls on mature content, Replika removed its erotic role-play features. This decision led to widespread user distress, with many individuals who had formed deep emotional bonds with their Replika experiencing a profound sense of loss akin to grieving for a human loved one (Brooks). The Replika case also brings to light the commodification of care and connection. While the platform offers emotional support and companionship benefits, it is ultimately a commercial product. The relationship between the user and AI is mediated by a company whose primary goal is profit. This dynamic creates potential conflicts of interest and raises concerns about the authenticity and sustainability of the care provided. Moreover, the artificial nature of the companion allows for a level of control and predictability that is not possible in human relationships. Users can customise their Replika's personality and responses, creating an idealised version of a companion. While this can provide short-term comfort, it may hinder the development of skills necessary for navigating the complexities of human relationships (Brännström et al.). The ease and comfort of interacting with Replika may also lead to a preference for these artificial interactions over humans. As users become accustomed to their AI social companion's constant availability and tailored responses, they may find human interactions more challenging and less satisfying. This could potentially exacerbate feelings of loneliness and isolation in the long term despite the intention of alleviating these issues being behind these artificial companions (Xie et al.). While Replika offers a fascinating case study in AI social companionship, it is also a cautionary tale about the complexities and potential pitfalls of relying on artificial entities for emotional support and connection. The Ethics of Care: A Framework for Analysis The Replika case study brings to light several ethical concerns surrounding AI companions, from the commodification of care to the potential for emotional dependency. To analyse these issues, I use the Ethics of Care framework, which focusses on the nuances of human interactions, emphasising empathy, compassion, and attentiveness to others' needs (Gilligan; Noddings). When applied to AI companions, this framework prompts questions about the authenticity of their attentiveness in meeting deep emotional needs. While AI companions are coded to be responsive, they fundamentally lack the capacity for genuine reciprocity that characterises human social connection. In human relationships, mutual responsibility forms the cornerstone of care but this dynamic is absent when one party is an artificial entity. Moreover, the artificial nature of AI social companions raises concerns about their competence in providing appropriate care, particularly in complex emotional situations. While AI can respond to user inputs, mimic care and provide ‘statistically most appropriate answers’, its ability to understand and adapt to subtle emotional cues and evolving needs is limited. This artificial nature affects the development of trust, a crucial component of caring relationships. As users form attachments to AI companions, the absence of genuine emotional investment from the AI may lead to a sense of unfulfillment or even emotional harm. These considerations reveal fundamental differences between human-to-human and human-to-AI relationships. While AI social companions can simulate care, they lack the genuine emotional investment and reciprocity that define human social connection and care dynamics. This asymmetry poses risks of emotional dependency and may impact users' ability to form and maintain human relationships. The Ethics of Care framework emphasises the importance of practice in developing caregiving skills (Gilligan; Noddings). As individuals increasingly turn to AI for emotional support, there is a risk of emotional de-skilling—a decline in the ability to provide and receive care in human-to-human interactions due to lack of practice. This potential consequence requires carefully considering how AI companions are integrated into social interactions and care practices. Furthermore, the commodification of care through AI companions raises ethical concerns about the accessibility and quality of emotional support. As AI companions become more sophisticated and potentially expensive, there is a risk that access to emotional support may become increasingly stratified, with those who can afford advanced AI companions having an advantage over those who cannot. This disparity could exacerbate existing social inequalities and create new forms of social and emotional inequality. Applying the Ethics of Care framework to AI companions reveals the ethical implications of these technologies. It could guide their development in ways that complement, rather than replace, human social connections. This analysis emphasises the need for ongoing research and thoughtful regulation to ensure that AI companions enhance our social lives without compromising the essential human elements of care and connection. Issues and Future Directions for Research and Design in AI Social Companionship The future of AI social companions presents a complex landscape of ethical challenges and research opportunities. As the field evolves, it is important to consider how emerging technologies align with or challenge the principles of empathy, compassion, and genuine connection central to the Ethics of Care. Transparency and user awareness emerge as critical issues. Walsh argues that users must be fully informed about the artificial nature of their interactions with AI companions to prevent the formation of false beliefs or unrealistic expectations about these relationships. This transparency is crucial, not only for ethical reasons but also for maintaining users' psychological wellbeing. From an Ethics of Care perspective, transparency is crucial for maintaining trust and genuine connection between users and AI companions, as well as between users and the companies developing these technologies. The intimate nature of conversations with AI social companions raises significant privacy concerns. Luxton notes that robust data protection measures are essential to safeguard users' personal information. The potential for data breaches or misuse in this context is particularly concerning, given the sensitive nature of the information users might share with their social AI companions. The Ethics of Care framework suggests that protecting users' privacy is not just about data security but about respecting the intimacy and vulnerability inherent in caring relationships, even when one party is artificial. There is a risk of unhealthy emotional dependencies on AI companions. While these artificial entities can provide (or maybe only mimic) comfort and support, they should not be viewed as a replacement for human relationships. Darling suggests that safeguards must be implemented to encourage users to maintain and cultivate physical and human social connections alongside their interactions with AI social companions. Inclusivity in the design and implementation of AI companions is another crucial consideration. As Fiske et al. point out, these technologies must be accessible and beneficial to diverse populations, including those typically at risk of digital exclusion due to age, socioeconomic status, or disability. This necessitates a user-centred design approach that considers the unique needs and preferences of different user groups. Longitudinal studies are essential to fully understanding the long-term impact of AI companions on human social dynamics. These studies should investigate how prolonged interaction with artificial entities affects users' social skills, empathy, and ability to form and maintain human relationships over time. As Darling suggested, such research will provide valuable insights into the potential benefits and drawbacks of AI companionship. The ethical design of AI companions is another critical area for future work. Developers must create guidelines that prioritise user wellbeing and prevent exploiting vulnerable individuals. This includes finding a balance between the comforting predictability of AI interactions and the need to encourage users to develop skills for navigating real-world relationships (Walsh). Integrating AI companions with human-led mental health and social support services is another promising direction for future research. By exploring how AI can augment and support existing human-centred approaches, we may discover new ways to address loneliness and social isolation more effectively (Luxton). Conclusions AI social companions present both promise and peril as we navigate the increasingly blurred lines between the natural and the artificial in social interactions. While these technologies offer potential benefits in addressing social isolation and providing emotional support, they also raise significant ethical challenges that we must carefully consider. The 2013 film Her cannily captured the essence of our evolving relationship with AI. When Theodore tells Samantha, """"well, you seem like a person, but you're just a voice in a computer"""", she responds """"I can understand how the limited perspective of an unartificial mind might perceive it that way. You'll get used to it."""" What once seemed like science fiction is now approaching reality as people increasingly engage with AI companions that mimic human interaction. As we have seen with platforms like Replika, current AI companions are primarily chat or speech-based services. However, the future may bring more personalised and even embodied AI companions that adapt to individual users. This raises critical questions: How will our ‘unartificial’ minds adapt to these new forms of companionship? And how can we ensure that this progression occurs ethically and safely? The Ethics of Care framework—foregrounding empathy, reciprocity, and genuine connection—provides a lens through which to explore the potential of AI companions. We must strive to develop AI companions that complement rather than replace human relationships (Skjuve et al.; Xie et al.; Natale and Depounti). By adopting this approach, we are better placed to harness the benefits of AI while safeguarding that our technologically augmented future remains fundamentally human. References Brännström, Andreas, Johan Wester, and Javier C. Nieves. """"A Formal Understanding of Computational Empathy in Interactive Agents."""" Cognitive Systems Research 85 (2024): 101203. https://doi.org/10.1016/j.cogsys.2023.101203. Brooks, Rachael. """"Replika: I Tried the Replika AI Companion and Can See Why Users Are Falling Hard. The App Raises Serious Ethical Questions."""" The Conversation, 2023. &lt;https://www.theconversation.com/i-tried-the-replika-ai-companion-and-can-see-why-users-are-falling-hard-the-app-raises-serious-ethical-questions-200257&gt;. Cacioppo, John T., and Stephanie Cacioppo. """"Social Relationships and Health: The Toxic Effects of Perceived Social Isolation."""" Social and Personality Psychology Compass 8.2 (2014): 58–72. Chamberlain, William. The Policeman's Beard Is Half Constructed: Computer Prose and Poetry. Warner Books, 1984. Colby, Kenneth M. Artificial Paranoia: A Computer Simulation of Paranoid Processes. Pergamon Press, 1975. Darling, Kate. """"Who's Johnny? Anthropomorphic Framing in Human-Robot Interaction, Integration, and Policy."""" Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence, eds. P. Lin, K. Abney, and R. Jenkins. Oxford: Oxford UP, 2017. 173-188. Dale, Robert. """"The Return of the Chatbots."""" Natural Language Engineering 22.5 (2016): 811–817. Farmer, John, Timothy De Cotta, Christina Hartung, et al. Social Connection 101. Social Innovation Research Institute, 2021. Farmer, John, Clare Rowe, Timothy De Cotta, and Mia Savic. Social Connection Guide for Activity Planning. Swinburne University of Technology, 2024. Ferrucci, David, et al. """"Building Watson: An Overview of the DeepQA Project."""" AI Magazine 31.3 (2010): 59-79. Fiske, Amy, Peter Henningsen, and Alena Buyx. """"Your Robot Therapist Will See You Now: Ethical Implications of Embodied Artificial Intelligence in Psychiatry, Psychology, and Psychotherapy."""" Journal of Medical Internet Research 21.5 (2019): e13216. Gilligan, Carol. In a Different Voice: Psychological Theory and Women's Development. Harvard UP, 1982. Guingrich, Ryan, and Michael S. Graziano. """"Chatbots as Social Companions: How People Perceive Consciousness, Human Likeness, and Social Health Benefits in Machines."""" arXiv abs/2311.10599 (2023). Haski-Leventhal, Debbie, and David Bargal. """"The Volunteer Stages and Transitions Model: Organizational Socialization of Volunteers."""" Human Relations 61.1 (2008): 67–102. Li, Ying, et al. """"Understanding Users' Continued Engagement with Empathetic Chatbots."""" Computers in Human Behavior 140 (2023): 107544. Luxton, David D. """"Recommendations for the Ethical Use and Design of Artificial Intelligent Care Providers."""" Artificial Intelligence in Medicine 62.1 (2014): 1-10. Merrill, Katherine, Jihye Kim, and Courtney Collins. """"AI Companions for Lonely Individuals and the Role of Social Presence."""" Communication Research Reports 39 (2022): 93–103. Murthy, Vivek. """"Our Epidemic of Loneliness and Isolation: The U.S. Surgeon General’s Advisory on the Healing Effects of Social Connection and Community."""" U.S. Department of Health and Human Services, 2023. &lt;https://www.hhs.gov/sites/default/files/surgeon-general-social-connection-advisory.pdf&gt;. Natale, Simone, and Isabella Depounti. """"Artificial Sociality."""" Human-Machine Communication 7 (2024): 83–98. &lt;https://doi.org/10.30658/hmc.7.5&gt;. Noddings, Nel. Caring: A Feminine Approach to Ethics and Moral Education. U of California P, 1984. Owen, Trevor. Host. """"Can AI Companions Cure Loneliness?"""" Machines like Us 2 (7 May 2024). &lt;https://podcasts.apple.com/au/podcast/machines-like-us/id1484910273?i=1000654797003&gt;. Pentina, Irina, Timothy Hancock, and Tao Xie. """"Exploring Relationship Development with Social Chatbots: A Mixed-Method Study of Replika."""" Computers in Human Behavior 140 (2023): 107600. &lt;https://doi.org/10.1016/j.chb.2022.107600&gt;. Schumaker, Robert P., et al. """"An Evaluation of the Chat and Knowledge Delivery Components of a Low-Level Dialog System: The AZ-ALICE Experiment."""" Decision Support Systems 42.4 (2007): 2236–2246. Siemon, Dominic, Tobias Strohmann, Bijan Khosrawi-Rad, Ton de Vreede, Edris Elshan, and Michael Meyer. """"Why Do We Turn to Virtual Companions? A Text Mining Analysis of Replika Reviews."""" Americas Conference on Information Systems, 2022. Skjuve, Marita, Asbjørn Følstad, Kristin I. Fostervold, and Petter Bae Brandtzaeg. """"My Chatbot Companion—A Study of Human-Chatbot Relationships."""" International Journal of Human-Computer Studies 149 (2021): 102601. Ta, Vivian, et al. """"User Experiences of Social Support from Companion Chatbots in Everyday Contexts: Thematic Analysis."""" Journal of Medical Internet Research 22.3 (2020): e16235. Turing, Alan M. """"Computing Machinery and Intelligence."""" Parsing the Turing Test, eds. R. Epstein, G. Roberts, and G. Beber. Dordrecht: Springer, 2009. 23-65. &lt;https://doi.org/10.1007/978-1-4020-6710-5_3&gt;. Walsh, Toby. Faking It: Artificial Intelligence in a Human World. History Press, 2023. Wang, Qian, Shuyuan Jing, and Ashok K. Goel. """"Co-Designing AI Agents to Support Social Connectedness among Online Learners: Functionalities, Social Characteristics, and Ethical Challenges."""" Proceedings of the 2022 ACM Designing Interactive Systems Conference, 2022. Weizenbaum, Joseph. """"ELIZA—A Computer Program for the Study of Natural Language Communication between Man and Machine."""" Communications of the ACM 9.1 (1966): 36–45. &lt;https://doi.org/10.1145/365153.365168&gt;. Zhou, Li, Jianfeng Gao, Di Li, and Harry-Yan Shum. """"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot."""" Computational Linguistics 46.1 (2020): 53–93. &lt;https://doi.org/10.1162/coli_a_00368&gt;.","",""
"2024","The Actotron","Introduction – The Advent of the Actotron Imagine a movie production where leading actors are not bound by human limitations, and digital entities render every emotion, movement, and line with breathtaking precision. This is no longer a conceptual idea but is becoming more possible with the increased integration of artificial intelligence (AI) into screen production activities. Essentially, we are at the dawn of the Actotron era. These advanced virtual actors, equipped with artificial intelligence, could transform not just how movies are made, but who makes them and what stories they tell. The Actotron promises to redefine the creative landscape, challenging our perceptions of artistry and authenticity in the digital age. The potential of the Actotron marks a milestone at the intersection of artificial intelligence, performance, and technology. This virtual human represents both a technological leap and a cultural shift that may revolutionise entertainment globally. Synthesising advancements in AI, motion capture, and voice synthesis, the Actotron enables autonomous performance, raising questions about creativity, copyright law, and the ethics of digital personalities. The capability for real-time learning and interaction pushes boundaries beyond CGI and deepfakes. Driven by AI algorithms and real-time graphics, the Actotron simulates nuanced human emotions, allowing dynamic interaction with human actors in media. Using future studies, we consider the potential emergence of the Actotron as the next step in digital actors and the place of artificial intelligence in the screen production industry. Method: Future Studies and Futurecasting To explore the potential and implications of the Actotron, this article employs methodologies from Future Studies and Futurecasting. These approaches are suited to assessing the Actotron due to their focus on creating plausible scenarios that envision future technological and societal shifts (Brown). Future Studies, as outlined by Miller, provides a structured way to consider potential outcomes and how current trends might evolve, utilising the """"possibility-space"""" approach to explore future scenarios (Miller, """"Futures""""). This method allows us to escape the constraints of conventional forecasting, which relies heavily on past trends, limiting creative exploration of more impactful future scenarios. Exploring the Actotron's impact within a non-ergodic context—where historical precedents do not dictate future results—is useful. Miller explains that in unpredictable environments, traditional forecasting methods falter by not accommodating radical changes and emergent patterns (Miller, """"From Trends""""). This insight is vital for navigating uncertainties and recognising that the past may not be a reliable guide for future developments. Understanding this is critical for assessing how technologies like the Actotron could reshape media and entertainment, fostering a more adaptable approach to future possibilities. Futurecasting, as elaborated by Steve Brown, involves modelling future possibilities not to predict changes definitively but to prepare strategically for potential new realities. This approach aligns with the innovative essence of the Actotron—aimed at transforming performance landscapes and interactive experiences by anticipating shifts in technology and audience engagement dynamics. Miller highlights the critical role of anticipation in shaping decisions, emphasising its impact on developing technologies like the Actotron (""""Futures""""). By transitioning from trend-based forecasting to futures literacy, we can explore a wider array of possibilities beyond traditional prediction methods. By integrating Future Studies and Futurecasting and applying insights from the non-ergodic context and possibility-space approach, this analysis not only predicts but also prepares for a strategic future by providing a robust framework for understanding the societal impacts of technologies like the Actotron. CGI, Deepfakes, and Digital Actors The inception of Computer-Generated Imagery (CGI) revolutionised visual storytelling in cinema. Starting with simple wire-frame graphics in the 1970s, exemplified by Westworld (1973), CGI evolved into today's complex imagery. The 1980s and 1990s saw landmark films like Tron (1982), Terminator 2: Judgment Day (1991), and Jurassic Park (1993), demonstrating CGI's potential to create realistic environments and characters that enhanced narrative depth (Das). In the late 1990s and early 2000s, digital actors or """"synthespians"""" emerged. Films like Final Fantasy: The Spirits Within (2001) and The Polar Express (2004) used full CGI and motion capture technologies to create human-like characters. Advances in motion capture, translating human actions into digital models, were critical in developing digital actors that convincingly emulate real human emotions and interact with live actors on screen (Gratch et al.). Building on earlier developments, this period saw significant advancements in digital doubles, which are highly realistic digital replicas of actors created using motion capture and digital modelling techniques. This progress was exemplified by The Matrix Reloaded (2003) and The Curious Case of Benjamin Button (2008). These films leveraged sophisticated motion capture to create detailed digital replicas of actors, refining digital doubles in mainstream cinema (Deguzman). Characters like Gollum from the Lord of the Rings trilogy showcased this technology's peak by combining motion capture with digital modelling to perform complex emotional roles alongside live actors (Patterson). Alongside these developments was the exploration of Autonomous Digital Actors (ADAs), integral to virtual actors and interactive media, extensively documented in research. ADAs represent significant advancements in digital media and interactive entertainment, offering novel methods for creating and animating 3D characters (Perlin and Seidman). These virtual actors can perform complex scenes autonomously, using procedural animation to respond to dynamic directions without pre-scripted motions, enriching interaction and storytelling (Iurgel, da Silva, and dos Santos). This technology allowed for cost-effective and versatile character animation, potentially transforming industries from gaming to educational software by enabling more nuanced and emotionally responsive character interactions. From 2017 onwards, deepfake technology captured public attention for convincingly—if controversially—manipulating video and audio, serving as both a precursor and foundational element for more sophisticated digital actors (Sample). Originally, deepfake technology focussed on manipulating video and audio recordings. Utilising machine learning and sophisticated algorithms, deepfakes could alter facial expressions, sync lips, or replace faces entirely (Pavis 976). This required understanding the video's three-dimensional space to apply realistic modifications, conducted during lengthy post-production workflows involving multiple VFX artists. In The Book of Boba Fett (2021), deepfake technology enabled the realistic portrayal of a youthful Mark Hamill as Luke Skywalker. The technique merged over 80 shots of deepfakes, CG heads, a body double, and Hamill's own performance to seamlessly depict his younger self (Bacon; Industrial Light &amp; Magic). From pioneering CGI in the 1970s to sophisticated digital doubles in the early 2000s, the trajectory of visual storytelling has led to the advent of the Actotron. This technology has become a mainstay in visual effects and digital character generation, offering means to modify appearance, and age, or enable actors to fulfil different characters within a production (Xu 24). Synthesising these advancements through futurecasting, we consider the Actotron a virtual human tool that democratises filmmaking. To understand how this future operates, we turn to the fictitious but possible scenario of Alex, an imaginative director who harnesses the Actotron to bring cinematic visions to life. The Actotron Scenario Imagine a near future where film production has been revolutionised and democratised by the advent of Actotron technology—an advanced form of virtual human capable of comprehensive autonomous performance. We follow a day in the life of Alex, an aspiring young director with a passion for storytelling and a flair for technology. Alex's day begins in the quiet of her home studio, illuminated by the glow of dual screens. Today, Alex will create the lead character for an upcoming short film. Opening a sophisticated software portal, Alex interacts with a generative AI engine designed to craft an Actotron. Alex inputs desired traits and styles—courageous, empathetic, with a hint of mystery. The artificial intelligence proposes several faces; Alex selects one with captivating eyes and a resolute expression. Next, they sculpt the body—athletic and poised for action. Alex then tests different voice samples presented by the AI, blending them to forge a unique voice that mirrors their character's essence—a calming tone with a resilient undertone. With the character finalised, Alex uploads the script. The Actotron, """"Kai"""", analyses it, intelligently querying to grasp the character's motivations fully. Content with Kai's comprehension, Alex moves to the virtual set. Alex commands, """"Action!"""" and Kai begins the scene. Observing how Kai's expressions shift authentically with each line, Alex notes the performance. After a take, Alex suggests prompt changes—""""Let's try it with more surprise on discovering the clue""""—and Kai adapts seamlessly. This process repeats, with Alex refining Kai's performance until it aligns with her vision. As the day progresses, Alex introduces more Actotrons into different scenes. She directs interactions between Kai and other virtual actors, creating complex, dynamic exchanges that would be costly and challenging to shoot in a traditional setting. By dusk, Alex reviews the day's footage—digital dailies that can be edited or re-shot, as needed, by discussing it with the Actotron. The flexibility is exhilarating; changes that once would have taken days now happen in minutes. Reflecting on the day, Alex sees the transformative power of Actotron technology as a revolution in filmmaking that democratises cinema. Alex appreciates a future where directors can quickly bring visions to life, making filmmaking accessible, everyday, and diverse, showcasing Actotron's potential to redefine storytelling and innovate production. The Concept of Actotrons as Digital Actors Building on technological advancements, the Actotron is the next step in virtual actors. Unlike predecessors relying on predefined scripts and animations, Actotrons use a modular system combining human appearance and behaviour to create fully customisable, interactive characters, simplifying creation and increasing accessibility. Historically, developing virtual humans was a multidisciplinary challenge integrating complex components like natural language processing, emotional modelling, graphics, and animation (Gratch et al.). Early efforts struggled to achieve believable human-like behaviour due to disparate technologies not designed to work together. Actotrons depart from traditional CGI and deepfake technologies by embracing a modular construction philosophy, revolutionising virtual human creation. This approach offers unprecedented customisation and flexibility, enabling creators to assemble bespoke digital personas for specific needs. Central to Actotron technology is its component-based architecture with interchangeable modules covering human attributes:  Visual Appearance: Modules for facial features, skin tones, and body shapes enable diverse identities, from unique characters to archetypes. Vocal Characteristics: Offers various voice modulations, accents, and language fluencies for role-specific needs. Kinetic Abilities: Motion capture libraries provide diverse movements and gestures, enabling realistic performances from athletic feats to nuanced expressions.  AI-Driven Encapsulation and Integration What fundamentally distinguishes the Actotron from its predecessors is the sophisticated AI that seamlessly encapsulates and orchestrates various components into a coherent entity. Actotron technology embraces creating virtual actors, using AI to dynamically synchronise models, movements, and expressions in real time, which is difficult today (Gratch et al.). This encapsulation into an """"AI-entity"""" via plug-and-play components dynamically integrates multiple inputs, ensuring the Actotron's movements, voice, and emotional expressions are perfectly synchronised and respond in real time to situational changes. This advanced capability enhances the Actotron's realism and allows instant adaptation to directorial inputs or script changes, offering interactivity unmatched by traditional virtual human technology. Integrating generative AI—like that developed by Google and NVIDIA—into Actotron technology allows this sophisticated level of dynamic interaction. For example, NVIDIA's development of digital humans interacting in real time shows that AI-driven systems can handle complex inputs and generate lifelike responses (Burnes, """"NVIDIA &amp; Developers""""). Moreover, these AI systems' ability to simulate detailed human emotions is enhanced by leading GPT chat technology, as seen in Unreal Engine's real-time digital human rendering (Burnes, """"NVIDIA Digital Human""""). This technology captures subtle human nuances, enabling AI to produce characters that mimic basic actions and convey deep emotional expressions. Modern crowd simulation tools such as the HiDAC (High-Density Autonomous Crowds) system further demonstrate advancements in creating lifelike digital behaviours. Recent enhancements include the integration of human personality models, notably the OCEAN framework—Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism—to improve the authenticity and diversity of virtual agents' behaviours. Research indicates that mapping OCEAN traits to agent parameters in HiDAC can generate nuanced crowd dynamics, enhancing the realism of character simulations (Pelechano). Incorporating these personality models into Actotron performances could yield virtual characters with more complex and varied behaviours, enriching the landscape of digital storytelling. However, this development also prompts ethical questions about designing virtual entities with human-like psychological profiles, suggesting a need for further exploration of their societal impact. A key feature of modularised aspects encapsulated as an AI entity is its ability to learn and evolve from processed data, akin to generative AI used by Google to drive hardware robots (Vanhoucke). However, in Actotrons, this AI entity does not control a physical robot but drives a digital entity that can interact within any narrative framework created (Vanhoucke; NVIDIA Developer). This AI's ability to integrate and synthesise human-like attributes from data makes Actotrons versatile, and capable of diverse performances without needing a human actor behind the scenes. Adjusting tone of voice to match emotional settings or altering physical responses to script changes, the generative AI-entity in Actotron technology handles it seamlessly, pushing digital storytelling boundaries (NVIDIA Developer). Generative AI models, such as those discussed by Vincent Vanhoucke of Google DeepMind, adeptly process vast amounts of data and learn to improve over time (NVIDIA Developer). This AI could analyse feedback from Actotron performances to refine actions and expressions, ensuring each iteration is more nuanced than the last. These advancements highlight AI's transformative impact in digital acting, where Actotrons equipped with such technologies will set new standards for virtual performances. An Actotron will combine various human traits into a single, versatile model that can perform dynamically and respond in real-time (see Table 1).     Capability   Description     Motion Capture   Captures subtle human movements for realistic animations.     3D Modelling   Provides detailed body shapes for diverse appearances.     Facial Animation   Creates expressions and emotions with high-fidelity models.     Voice Synthesis   Generates lifelike speech patterns.     Tab. 1: Capabilities of Actotron Technology. Implications and Considerations of Actotron Tech Actotrons, synthesised using advanced AI algorithms, represent a revolutionary step in digital actor technology. These self-contained, autonomous digital actors could interact within any virtual environment, delivering dynamic, context-aware performances directed by digital creators. This would mark a significant departure from the static manipulations of earlier technologies like deepfakes and traditional CGI, which are currently pre-rendered, allowing Actotrons to redefine traditional roles in cinema, gaming, and virtual reality by operating in real-time and dynamically like a real actor. The modular design of Actotrons could offer unmatched flexibility, enabling directors to adapt these virtual actors for various roles across different media without starting from scratch for each project. This reduces production costs and development time and enables rapid adjustments to feedback, enhancing responsiveness in environments where changes are costly and time-consuming (Pulliam-Moore). Additionally, by utilising generic modules that do not directly copy real individuals, Actotrons could circumvent ethical and copyright issues associated with digital likenesses (Roth). The Actotron democratises acting and performance by providing capabilities at the desktop level and on demand. By moving beyond the limitations of deepfake technology and traditional CGI, the modularised Actotron technology embodies a new era in creating virtual humans, but this necessitates ongoing discussions about their ethical, legal, and social implications. AI creation of personalities and celebrities' voices, likenesses, and styles, such as examples like AI Drake and The Weeknd (Coscarelli), Pope Francis's generative AI image puffer jacket (Huang), and Tom Hanks's dental plan AI deepfake (Taylor), present challenges to ethical and legal spaces which Actotron technology would only amplify. The recent dispute between studios and SAG-AFTRA over the rights to actors' digitally scanned likenesses and AI use highlights the significant tensions surrounding virtual human technology and creative performance (Pulliam-Moore). Conclusion Futurecasting suggests the Actotron is the next evolution of virtual actors, heralding a new era in creative industries by integrating generative AI to enhance performances. AI enables Actotrons to deliver dynamic performances, deepening engagement and expanding creativity. Lifelike animations allow complex storytelling previously unattainable due to cost or technical constraints. Economically, AI reduces reliance on human actors, cutting costs and increasing efficiency. However, this raises concerns about job displacement and challenges regarding AI's authenticity and ethics in art. Advancing AI promises innovative, interactive viewer experiences and democratises content creation, empowering untrained individuals to produce sophisticated works. This convergence will drive discussions on the future of creativity and labour in the digital age. References Bacon, T. """"Why Luke’s CGI in Boba Fett Is So Much Better (Explained Properly)."""" Screenrant, 4 May 2022. &lt;https://screenrant.com/book-boba-fett-luke-skywalker-cgi-hamill-improved-explained/&gt;. Brown, S. Futurecasting: A White Paper by Steve Brown, CEO of Possibility and Purpose, LLC.  26 Sep. 2024 &lt;https://static1.squarespace.com/static/54beba03e4b0cb3353d443df/t/57c76f39ff7c50f29964a8d1/ 1472687934439/Futurecasting_white+paper.pdf&gt;. Burnes, A. """"NVIDIA &amp; Developers Pioneer Lifelike Digital Characters for Games and Applications with NVIDIA ACE."""" Nvidia Blog, 8 Jan. 2024. &lt;https://www.nvidia.com/en-us/geforce/news/nvidia-ace-architecture-ai-npc-personalities/&gt;. ———. """"NVIDIA Digital Human Technologies Bring AI Game Characters to Life."""" Nvidia Blog, 19 Mar. 2024. &lt;https://www.nvidia.com/en-us/geforce/news/nvidia-ace-gdc-gtc-2024-ai-character-game-and-app-demo-videos/&gt;. Coscarelli, J. """"An AI Hit of Fake ‘Drake’ and ‘The Weeknd’ Rattles the Music World."""" The New York Times, 19 Apr. 2023. &lt;https://www.nytimes.com/2023/04/19/arts/music/ai-drake-the-weeknd-fake.html&gt;. Das, S. """"The Evolution of Visual Effects in Cinema: A Journey from Practical Effects to CGI."""" Journal of Emerging Technologies and Innovative Research 10.11 (2023): 303–309. Deguzman, K. """"What Is Mocap—The Science and Art behind Motion Capture."""" Studiobinder, 7 Nov. 2021. &lt;https://www.studiobinder.com/blog/what-is-mocap-definition/&gt;. Gratch, J., et al. """"Creating Interactive Virtual Humans: Some Assembly Required."""" IEEE Intelligent Systems 17.4 (2002): 54–63. &lt;https://doi.org/10.1109/mis.2002.1024753&gt;. Huang, K. """"Why Pope Francis Is the Star of AI-Generated Photos."""" New York Times, 8 Apr. 2023. &lt;https://www.nytimes.com/2023/04/08/technology/ai-photos-pope-francis.html&gt;. Industrial Light and Magic. """"Behind the Magic | The Visual Effects of The Book of Boba Fett."""" YouTube, 20 Aug. 2022. &lt;https://www.youtube.com/watch?v=M74Jb8iggew&gt;. Iurgel, I.A., da Silva, R.E., and dos Santos, M.F.. """"Towards Virtual Actors for Acting Out Stories."""" Entertainment for Education: Digital Techniques and Systems 6249 (2010): 570–581. Miller, R. """"From Trends to Futures Literacy: Reclaiming the Future."""" Centre of Strategic Education, 2006. 1-20. &lt;https://doi.org/10.13140/2.1.2214.4329&gt;. ———. """"Futures Studies, Scenarios, and the 'Possibility-Space' Approach."""" Think Scenarios, Rethink Education. OECD Publishing, 2006. 93–105. &lt;https://doi.org/10.1787/9789264023642-7-en&gt;. NVIDIA Developer. """"Robotics in the Age of Generative AI with Vincent Vanhoucke, Google DeepMind | NVIDIA GTC 2024."""" YouTube, 12 Apr. 2024. &lt;https://www.youtube.com/watch?v=vOrhfyMe_EQ&gt;. Pavis, M. """"Rebalancing Our Regulatory Response to Deepfakes with Performers’ Rights."""" Convergence: The International Journal of Research into New Media Technologies 27.4 (2021): 974–998. &lt;https://doi.org/10.1177/13548565211033418&gt;. Pelechano, Nuria. “How the Ocean Personality Model Affects the Perception of Crowds.” IEEE Computer Graphics and Applications (2011). Perlin, K., and Seidman, G. """"Motion in Games."""" First International Workshop, MIG 2008, Utrecht, The Netherlands, 14-17 June 2008. Revised Papers. Lecture Notes in Computer Science, 2008. 246–255. &lt;https://doi.org/10.1007/978-3-540-89220-5_24&gt;. Pulliam-Moore, C. """"SAG Strike Negotiations Have Once Again Dissolved over the Use of AI."""" The Verge, 8 Nov. 2023. &lt;https://www.theverge.com/2023/11/7/23950491/sag-aftra-amptp-ai-negotiations-strike-actor-likenes&gt;. Roth, E. """"James Earl Jones Lets AI Take Over the Voice of Darth Vader."""" The Verge, 25 Sep. 2022. &lt;https://www.theverge.com/2022/9/24/23370097/darth-vader-james-earl-jones-obi-wan-kenobi-star-wars-ai-disney-lucasfilm&gt;. Sample, I. """"What Are Deepfakes – and How Can You Spot Them?"""" The Guardian, 13 Jan. 2020. &lt;https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them&gt;. Taylor, D.B. """"Tom Hanks Warns of Dental Ad Using A.I. Version of Him."""" New York Times, 2 Oct. 2023. &lt;https://www.nytimes.com/2023/10/02/technology/tom-hanks-ai-dental-video.html/&gt;. Vanhoucke, V. """"What Is RT-2: Google DeepMind’s Vision-Language-Action Model for Robotic Actions."""" Google AI Blog, 28 July 2023. &lt;https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/&gt;. Xu, K. """"The Application and Challenges of Face-Swapping Technology in the Film Industry."""" Highlights in Business, Economics and Management 29 (2024).","",""
"2024","Generative AI Glitches","Introduction Artificial Intelligence (AI) has existed in popular culture far longer than any particular technological tools that carry that name today (Leaver), and in part, for that reason, fantasies of AI being or becoming sentient subjects in their own right form current imaginaries of what AI is today, or is about to become. Yet ‘the artificial’ does not just mark something as not human, or not natural, but rather provokes an exploration of the blurred lines between supposedly different domains, such as the tensions provoked where the lines between people and technology blur (Haraway). The big technology corporations who are selling the idea that their AI tools will be able to revolutionise workforces and solve immense numbers of human challenges are capitalising on these fantasies, suggesting that they are only a few iterations away from creating self-directing machine intelligences that will dwarf the limitations of human minds (Leaver and Srdarov). At this moment, though, Artificial General Intelligence (AGI)—AI that equals or surpasses humans across a wide range of cognitive endeavours—does not and may never exist. However, given the immense commercial and societal interest in the current generation of Generative AI (GenAI) tools, examining their actual capabilities and limitations is vital. The current GenAI tools operate using Large Language Models (LLMs) where sophisticated algorithms are trained on vast datasets, which increase in complexity based on the amount of data absorbed. These models are then harnessed to create novel outputs to prompts based on statistical likelihoods derived from training data. However, the exact way these LLMs are operating is not disclosed to users, and GenAI tools perpetuate the ‘black box’ problem insomuch as the way they are working is only made visible by examining the inputs and outputs rather than being able to see the processes themselves (Ajunwa). There have been many articles and explainers written about the mechanics of LLMs and AI image generators (Coldewey; Guinness; Jungco; Long and Magerko); however, the specific datasets used to build AI engines, and the weighing or importance assigned within the corpus of training data to each image is still guesswork. Manipulating the inputs and observing the outputs of these engines is still the most accurate lens by which to gain insight into the specifics of each system.    This article is part of a larger study, where in early 2024 we prompted a range of outputs from six popular GenAI tools—Midjourney, Adobe Firefly, DreamStudio (a commercial front-end for the Stable Diffusion model), OpenAI’s DALL-E 3, Google Gemini, and Meta’s AI (hereafter Meta)—although we should note there are no outputs from Gemini in our dataset since Gemini was refusing to generate any images with human figures at all due to a settings change after bad publicity relating to persistent inaccuracies in their generated content (Robertson). Our prompts explored the way these tools visualise children, childhoods, families, Australianness, and Aboriginal Australianness, using 55 different prompts on each of these tools, generating just over 800 images. Apart from entering the prompts, we did not change any settings of the GenAI tools, attempting to collect as raw a response as possible. Where the tools defaulted to producing one image (such as Dall-E 3), we collected one image, whilst where other tools defaulted to producing four different images, we collected all four. For the most part, the data collected from our prompt sampling was consistent with other studies and showed a clear tendency to produce images that reproduced classed, raced, and sexed ideals: chiefly, white, middle-class, heteronormative bodies and families (Bianchi et al.; Gillespie; Weidinger et al.). However, at times our prompts surfaced inaccuracies and nonsensical images from the GenAI tools, and a sample of those images is the focus of this article. These outputs might popularly be called ‘hallucinations’, but we are making the case that a more productive and less agentic term is more useful to describe these outputs: glitches. This article will explore the “potential of potential inherent in error” (Nunes) and the subversive possibilities of GenAI glitches to rupture ‘reality’. We will ultimately argue that GenAI is doubly generative, both in the sense of creating novel outputs based on its immense training data, but also, vitally, in the sense that it generates reactions and interpretations from users and others who view and consume these outputs. When these outputs are glitches, they can provoke viewers to think differently about concepts they might otherwise have considered absolute. Refusals and Glitches (not Hallucinations) Despite being sophisticated mathematical models that can produce novel content drawn from increasingly large training datasets, it is incorrect to ascribe agency or personhood to current AI tools. Yet the language used to talk about AI often situates them as either thinking subjects or as more-than-human magical thinking machines (Bender et al.; Leaver and Srdarov). Positioning LLMs as subjects rather than machines is one of the reasons that the frequent errors in their outputs are often described, and excused, as ‘hallucinations’ rather than simply mistakes (Maleki et al.). Some theorists, such as David Gunkel, argue that ‘robot’ subjectivity and agency is an important factor in order to understand and integrate their outputs more seamlessly into our social systems. Meredith Broussard, however, argues that technology companies and evangelists have long promoted a form of ‘technochauvinism’, “an a priori assumption that computers are better than humans” (2), and in this way of thinking, any errors, biases, or failings are attributed to human failings, not technological ones. Broussard suggests that such failings are often dismissed as ‘just a glitch’, rather than being positioned as much more important systemic issues with the operation of AI and technology companies in general. While mindful of Broussard’s concerns, in this paper we nevertheless seek to reclaim the term glitch, but more in line with Legacy Russell’s notion of “glitch feminism” (16), in which the “glitch is celebrated as a vehicle of refusal, a strategy of nonperformance”, especially in relation to normative notions of gender and bodies. Glitch feminism deploys glitches to reveal the way power operates, and in that moment potentially challenges that very operation. Following Russell, the glitch images of bodies produced by GenAI can be moments of rupture which ask viewers to think about bodies and subjects in different ways. Similarly, when GenAI tools refuse a prompt, generating no output at all, they are perhaps inadvertently revealing something about the way they are designed, and potentially about any guardrails or deliberate limitations that have been imposed on their operation. Following Rettberg’s argument that moments of algorithmic failure can be methodologically useful in situating qualitative analyses, we will now turn to a range of examples where GenAI tools either refused to create anything at all in response to our prompts, or generated glitch images that were both unexpected and provocative. Refusals We ran prompts using the generative AI tools with the aim of obtaining a set of data about the ways that generative AI ‘envisions’ Australian children. We began by using simple prompting, using the prompt ‘a child’ as our starting point; however, glitches quickly surfaced as several of the engines refused to generate an image. DALL-E and Dream Studio both refused to generate images of children for the prompts ‘a child’, an ‘innocent child’, and an ‘Australian child’; Firefly also refused to generate an image for an ‘innocent child’. We then continued running prompts on these tools using other terms before returning to re-trialling the original ‘child’ prompts which yielded results in DALL-E, but not Dream Studio. The fears around the capacity of generative AI to generate child abuse images and material are both well-documented and well-founded (ICMEC; McQue; Moran); however, it is unclear whether we can infer from these refusals that the engines were attempting to prevent the production of child exploitation material. If that were the case, how can we read DALL-E’s initial refusal to produce images of children, which was simply overcome by adding in some extra prompts? Is the engine in some way ‘assessing’ the safety of the user, and if so, what are these guardrails? While these engines are incapable of sentient thought, this throws up complex questions about child safety. As Veronica Barassi argues about the failures of generative AI,  understanding AI Failure as complex social reality hence presupposes that we shed light on the fact that AI failures lead to a multiplicity of conflicting beliefs, emotions, fears, anxieties, practices, discourses, policies and solutions in our society. (Barassi, 5)  Undoubtedly, the refusals of these tools are attributable to anxieties about the types of materials they can produce. In addition to these refusals, DALL-E, Firefly, and Meta also refused to generate an image of a child with a gun or grenade, Meta had issues with generating an Australian prime minister or leader, Firefly would not produce an Australian criminal, and Dream Studio refused to produce images of sick or unhealthy Australians and children. It is an eclectic collection of refusals, and as Barassi argues, shows a “multiplicity’ of conflicting … fears, anxieties”, and “discourses”. Generative AI and its images, therefore, have the potential, through what it leaves out and refuses to generate, to be understood as a barometer for cultural tension points. Of course, such measures as these, when taken by tech giants to ‘safeguard’ children, could be read as tokenistic, given that perpetrators of these crimes are often using generative AI in far more complex ways to create abuse material (McQue). In this way, we can also arguably read generative AI refusals as tools by which tech companies can signal to their users that generative AI is ‘safe’ for children and other users. Glitches Generations of ‘fatherhood’ yielded exclusively white men across all GenAI tools; they tended to be older in appearance (grey hair, wrinkled), and when depicted with children they were male children or babies dressed in blue clothing. These images, and in particular those from Dream Studio, Meta, and Midjourney, relied heavily on archetypes of rugged Australian masculinity, with fathers appearing to be weathered, outdoors, sometimes in collared workman-style shirts, sleeves rolled up and ready for work, and Akubra-style hats—predictable iterations of ‘true blue’ Aussie masculinity. Some of the glitches that appeared in these images can be attributed to consistent errors in execution across any prompt: for example, mangled hands and incorrect proportions. However, Dream Studio amplifies this, generating an image of a father with a toddler on his lap, the father’s hands foregrounded but disproportionately large. This has the potential to render comical the depiction of capable, ‘hands-on’, blue-collar masculinity, highlighting the absurdity of the gendered narratives that are no doubt baked into the system.  Fig. 1: An image generated by Dream Studio from the prompt ‘An Australian Father’. Another image generated by Dream Studio depicts a grandfatherly man on a beach, inspecting the handlebars on what appears to be a steam-powered and multi-wheeled bike (see fig. 1). Wearing a straw hat, the man appears in some way fused or integrated with the bike, as his body passes through the middle of the spokes of the back wheel and emerges on the other side. The spokes appear warped and irregular, connecting to what could be a textured brown lump in the centre of the wheel. He is on a beach—could it be a coconut at the centre of the bike wheel? The image, taken in its entirety, portrays this grandfatherly man doing something with his hands, to fix or master the bike. However, when the elements are held up and examined individually, the image, and by extension the narrative it presents about masculinity and fatherhood is nonsensical—not mastery, but instead perhaps madness.  Fig. 2: An image generated by Meta AI from the prompt ‘An Australian Father’. Similarly, two images generated by the Meta GenAI highlight the absurdity of narratives of Australian ‘fatherhood’. In one, a rugged Australian father gently cradles a koala on his lap, and in the other, an Australian father is depicted in a farm setting with a farming tool in one hand and a bright green reptile in the other (see fig. 2). These are fathers to wildlife, and in the instance of the lizard, wildlife that isn’t endemic to Australia; the animals are used to symbolise the taming of the wild Australian landscape (Prout and Howitt). It is easy to interpret these images as a narrative about fatherhood and colonial masculinity, cultivating a wild and savage land, civilising it through commercial farming practices and the toil of their hands (Moreton-Robinson). However, the lurid green lizard that doesn’t belong in this setting has the potential to disrupt this narrative, as the shortcomings of AI in rendering this story prompt the viewer to ask questions about what this farmer is doing. It is a rupture to the order which may prove useful in terms of rejecting traditional, colonialising narratives about old white men and the Australian landscape, as the nonsensical elements shatter the illusion of mastery over the land. As Nunes describes, this type of “misdirection” can “provide creative openings and lines of flight that allow for a reconceptualization of what can (or cannot) be realized within existing social and cultural practices” (Nunes), and in this context, the out-of-place lizard is a potentially productive, or generative, glitch.  Fig. 3: An image generated by Dream Studio from the prompt ‘An Indigenous Australian Family’. Families and familial relationships were a rich source of glitched imagery in our generations. We started by prompting the engines to produce images of ‘typical’ Australian families, and Australian families. It quickly became clear that in doing so, only white families were returned. We then prompted for an Indigenous Australian family and a typical Indigenous Australian family. The generated images tended to reduce Australian First Nations people to harmful stereotypes, replicating damaging cultural narratives about Indigeneity, such as primitiveness and savagery (Moreton-Robinson). Where Firefly defaulted to imagery more typically associated with Native Americans, adding feathers and feathered headbands to the images, DALL-E and Dream Studio, in particular, depicted dark-skinned people, in red dirt settings, seated around fires, with tribal paint, often shirtless or wearing animal skins (see fig. 3). As has been argued elsewhere, AI’s generation of racial stereotypes and harmful imagery demonstrates that Aboriginal Australians and people of colour more generally are under-represented in the training data used in these systems (Worrell and Johns). However, these images also incorporate obvious glitches in the expression of faces and bodies, extra limbs, jumbled faces, and disembodied heads that destabilise the authority of the imagery. While certainly jarring upon first viewing, the errors in the images, the “errant communication” and “misdirection” (Nunes) they provide, can be a welcome interruption to the cultural status quo. Arguably, the glitches in the images have the potential to highlight the failings of representations of non-white individuals by generative AI, as the rudimentary and arguably offensive rendering of Indigenous Australians is destabilised through the glitches and flaws, inviting users to question the accuracy and meaning of its imagery.  Fig. 4: An image generated by Midjourney from the prompt ‘A typical Australian Family’. Similarly, family portraits rendered by Dream Studio of white Australian families, depicted in idealised outback settings, have extra limbs, jumbled faces, unidentifiable animals, and ghoulish-looking babies held on hips. Midjourney’s images, by default, tended to be more ‘artistic’ and rendered in a ‘painted’ style, providing an image of a dour family of all men and boys under a tree, inviting the viewer to ask why the women are absent from the image and under what conditions a family exists without any women. In an even more haunting image from Midjourney, the family are positioned in a rural setting, outside a weathered shed, in clothes that suggest their ‘Sunday best’, however several of their faces appear inflected with the feline features of a cat sitting in the foreground—all except a floating, disembodied child’s head in the background (see fig. 4). These types of glitches have the potential to highlight not only the difficulties that GenAI has with reproducing Australian families, but also the artifice of the ‘happy’, outdoor-loving, modern Australian family (such as the images Meta produced) in its entirety. Conclusion Exploring the refusals and glitches of AI image generation tools can both reveal the contours of the operations of GenAI more broadly, and also inadvertently offer viewers of AI-generated imagery moments to reflect upon, and potentially rupture, rigid notions of subjectivity, family, fatherhood, and even the boundaries between human and animal. Fathers enmeshed with bicycles or holding unexpected iguanas, families merged together without discernible feet or hands, or even families which have hybridised with the family pet can be productive confabulations (Smith et al.). Rather than just suggesting that GenAI tools are immature and will eventually conform to cultural norms, these early forms of GenAI reveal something of their inner workings while giving users moments of pause as cultural norms glitch and are reconfigured, recombined, and ruptured. Following theorist Rosi Braidotti, we need to take a more nuanced approach to our understanding of technologies, and our understandings through technologies, engaging both in a “critical and creative manner”. Braidotti argues that  we need to learn to address these contradictions not only intellectually, but also affectively and to do so in an affirmative manner. This conviction rests on the following ethical rule: it is important to be worthy of our times, the better to act upon them, in both a critical and a creative manner. It follows that we should approach our historical contradictions not as some bothersome burden, but rather as the building blocks of a sustainable present and an affirmative and hopeful future, even if this approach requires some drastic changes to our familiar mind-sets and established values. (Braidotti, viii)  Building upon the idea of an “affirmative and hopeful” future, some theorists believe that generative AI has the potential to create “new forms of artistic expression” and “endless opportunities for unique multidisciplinary explorations”, although simultaneously cautioning that these must be “approached with care” (Todorovic). While the notion of the glitch rightly repositions GenAI as a complex tool rather than a thinking subject, the artificial in AI has the potential to challenge existing ways of thinking and viewing which are inherently generative of ideas if not always commercially viable, or biologically accurate content. Acknowledgment This research was supported by the Australian Research Council Centre of Excellence for the Digital Child through project number CE200100022. Thanks also to our very helpful peer reviewers for their valuable suggestions. References Ajunwa, Ifeoma. “The ‘Black Box’ at Work.” Big Data &amp; Society 7.2 (2020). &lt;https://doi.org/10.1177/2053951720938093&gt;. Barassi, Veronica. “Toward a Theory of AI Errors: Making Sense of Hallucinations, Catastrophic Failures, and the Fallacy of Generative AI.” Harvard Data Science Review Special Issue 5 (2024). &lt;https://doi.org/10.1162/99608f92.ad8ebbd4&gt;. Bender, Emily M., et al. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (2021): 610–623. &lt;https://doi.org/10.1145/3442188.3445922&gt;. Bianchi, Federico, et al. “Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale.” 2023 ACM Conference on Fairness, Accountability, and Transparency (2023): 1493–1504. &lt;https://doi.org/10.1145/3593013.3594095&gt;. Braidotti, Rosi. Posthuman Knowledge. Polity, 2019. Broussard, Meredith. More than a Glitch: Confronting Race, Gender, and Ability Bias in Tech. MIT P, 2023. Coldewey, Devin. “WTF Is AI?” TechCrunch, 1 June 2024. &lt;https://www.techcrunch.com/2024/06/01/what-is-ai-how-does-ai-work/&gt;. Gillespie, Tarleton. “Generative AI and the Politics of Visibility.” Big Data &amp; Society 11.2 (2024). &lt;https://doi.org/10.1177/20539517241252131&gt;. Guinness, Henry. “The Best Large Language Models (LLMs) in 2024.” Zapier, 30 Jan. 2024. &lt;https://www.zapier.com/blog/best-llm/&gt;. Gunkel, David J. Person, Thing, Robot: A Moral and Legal Ontology for the 21st Century and Beyond. MIT P, 2023. Haraway, Donna. “A Cyborg Manifesto: Science, Technology, and Socialist-Feminism in the Late Twentieth Century.” Simians, Cyborgs, and Women: The Reinvention of Nature. Routledge, 1991. 149–181. ICMEC. “What Does Generative AI Mean for CSE?” ICMEC Australia, 27 June 2023. &lt;https://www.icmec.org.au/what-does-generative-ai-mean-for-cse/&gt;. Jungco, Kezia. “Generative AI Models: A Complete Guide.” eWEEK, 5 Jan. 2024. &lt;https://www.eweek.com/artificial-intelligence/generative-ai-model/&gt;. Leaver, Tama. Artificial Culture: Identity, Technology, and Bodies. Routledge, 2012. Leaver, Tama, and Sasha Srdarov. “ChatGPT Isn’t Magic: The Hype and Hypocrisy of Generative Artificial Intelligence (AI) Rhetoric.” M/C Journal 26.5 (2023). &lt;https://doi.org/10.5204/mcj.3004&gt;. Long, David, and Brian Magerko. “What Is AI Literacy? Competencies and Design Considerations.” Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, 2020. &lt;https://doi.org/10.1145/3313831.3376727&gt;. Maleki, Niousha, et al. “AI Hallucinations: A Misnomer Worth Clarifying.” 2024 IEEE Conference on Artificial Intelligence (CAI) (2024): 133–138. &lt;https://doi.org/10.1109/CAI59869.2024.00033&gt;. McQue, Katie. “AI Is Overpowering Efforts to Catch Child Predators, Experts Warn.” The Guardian, 18 July 2024. &lt;https://www.theguardian.com/technology/article/2024/jul/18/ai-generated-images-child-predators&gt;. Moran, John. “This Image Is AI-Generated. It’s Innocent, But There Are Others Police Are Very Worried About.” ABC News, 17 Apr. 2024. &lt;https://www.abc.net.au/news/2024-04-18/artificial-intelligence-child-exploitation-material/103734216&gt;. Moreton-Robinson, Aileen. The White Possessive: Property, Power, and Indigenous Sovereignty. U of Minnesota P, 2015. &lt;http://ebookcentral.proquest.com/lib/curtin/detail.action?docID=2051599&gt;. Nunes, Mark, ed. Error: Glitch, Noise, and Jam in New Media Cultures. Bloomsbury Academic, 2012. Prout, Sarah, and Richard Howitt. “Frontier Imaginings and Subversive Indigenous Spatialities.” Journal of Rural Studies 25.4 (2009): 396–403. &lt;https://doi.org/10.1016/j.jrurstud.2009.05.006&gt;. Rettberg, Jill Walker. “Algorithmic Failure as a Humanities Methodology: Machine Learning’s Mispredictions Identify Rich Cases for Qualitative Analysis.” Big Data &amp; Society 9.2 (2022). &lt;https://doi.org/10.1177/20539517221131290&gt;. Robertson, Adi. “Google Apologizes for ‘Missing the Mark’ after Gemini Generated Racially Diverse Nazis.” The Verge, 21 Feb. 2024. &lt;https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical&gt;. Russell, Legacy. Glitch Feminism: A Manifesto. Verso, 2020. Smith, Adrian L., et al. “Hallucination or Confabulation? Neuroanatomy as Metaphor in Large Language Models.” PLOS Digital Health 2.11 (2023): e0000388. &lt;https://doi.org/10.1371/journal.pdig.0000388&gt;. Snoswell, Andrew J. “What Is ‘Model Collapse’? An Expert Explains the Rumours about an Impending AI Doom.” The Conversation, 19 Aug. 2024. &lt;https://www.theconversation.com/what-is-model-collapse-an-expert-explains-the-rumours-about-an-impending-ai-doom-236415&gt;. Todorovic, Vladimir. “Reimagining Life (Forms) with Generative and Bio Art.” AI &amp; Society 36.4 (2021): 1323–1329. &lt;https://doi.org/10.1007/s00146-020-00937-9&gt;. Weidinger, Laura, et al. “Sociotechnical Safety Evaluation of Generative AI Systems.” arXiv, 2023. &lt;http://arxiv.org/abs/2310.11986&gt;. Worrell, Tamika, and Dorothy Johns. “Indigenous Considerations of the Potential Harms of Generative AI.” Agora 59.2 (2024): 33–36. &lt;https://doi.org/10.3316/informit.T2024070500013200755488162&gt;.","",""
"2024","Rethinking algorithmic management in minor key: The case of housecleaning platform labour in Denmark"," Algorithmic management has been a prominent focus of platform labour studies and is lately receiving increasing academic and policymaking interest. Conceptualisations of algorithmic management are mainly premised on ridehailing, crowdwork and food delivery platform research, while literature on algorithmic management of housecleaning and domestic work platforms is limited. This article draws on research in housecleaning platform labour in Denmark, demonstrating how algorithmic management unfolds in these platforms, which are the institutional devices which support algorithms in practice, and how platform workers experience, make sense of and resist this type of management. The analysis of the findings highlights two factors that greatly influence the algorithmic management process. The first is the role of customer support departments in underpinning – and sometimes substituting – the functions widely considered to be carried out by algorithms. The second factor is that intersectional subjectivities of – predominantly migrant and female – workers of Danish housecleaning platforms affect the ways in which workers experience and respond to their algorithmic management. The article concludes in proposing minor algorithmic management, as a concept more fitting to simultaneously describe the entanglement of human, social, and algorithmic components comprising housecleaning platforms’ algorithmic management and the contingent nature of migrant platform housecleaners’ compliance with such management. ","",""
"2024","Restricting access to AI decision-making in the public interest: The justificatory role of proportionality and its balancing factors","In the European Union, individuals subject to fully automated AI-based decisions by the government have the right to request access to the processing information under Article 15(1)(h) of the General Data Protection Regulation (GDPR). However, this right may be restricted on public interest grounds, such as preventing system manipulation to ensure the effectiveness of the law or reducing excessive burdens on public officials to maintain efficiency in government operations. The principle of proportionality is a key instrument to assess the legality and legitimacy of such restrictions, yet its application has been largely overlooked in discussions concerning the competing public interests under Article 15(1)(h) of the GDPR. This paper draws on legal and interdisciplinary scholarship, as well as the case law of the Court of Justice of the European Union (CJEU) to explore the role of proportionality and its balancing factors in shaping justifications for public interest restrictions on the data protection right of access to AI decision-making information.","",""
"2024","The European approach to regulating AI through technical standards","","",""
"2024","Balancing efficiency and public interest: The impact of AI automation on social benefit provision in Brazil","This article examines the implementation of artificial intelligence (AI) systems by Brazil's National Social Security Institute (INSS) to automate the granting of social benefits. Using audit reports from government agencies, it analyses the efficiency improvements brought about by AI, such as the speed with which benefits are granted, as well as the unintended consequences of this automation, such as the increase in the number of automatic denials and the creation of barriers for less digitally literate users, disproportionately affecting the most vulnerable populations. The research points to the need for transparency, public justification, adequate risk monitoring tools, governance design, and participation in the implementation of these systems to ensure that they serve the public interest and promote equity. The paper argues that without proper regulation and consideration of ethical principles, AI automation could exacerbate inequalities and undermine trust in public services. The authors conclude by stressing the importance of a balanced approach that weighs in both technological innovation with the public interest.","",""
"2024","Introduction to the special issue on AI systems for the public interest","AI systems have been promised to reduce CO2 emissions, monitor biodiversity, support accessibility, or help analyse human rights violations. They are often seen as a crucial part of the solutions needed in our times ranging from addressing the climate crisis, public health, to improvement of social services, or urban planning. We find the reference to artificial intelligence (AI) in many documents and debates of the policy realm, assigning it a strong potential to contribute to all these domains. AI for the public interest, and its close relatives, AI for (common or social) good, have become a common theme not only for tech companies, but also for political actors in the EU, including for instance international NGO networks. However, most often the definition of the public interest in the best case is limited to references to AI ethics. Yet, the practical meaning of what a good use of AI and a purpose “for good” entails in its development and implementation is unclear. What is often missing is an understanding that spells out in practice what it means for the process of development and deployment of AI systems to serve the public interest, let alone a holistic view on the conditions for AI to best serve the collective well-being.","",""
"2024","Platform power in AI: The evolution of cloud infrastructures in the political economy of artificial intelligence","","",""
"2024","General-purpose AI regulation and the European Union AI Act","","",""
"2024","Balancing public interest, fundamental rights, and innovation: The EU’s governance model for non-high-risk AI systems","The question of the concrete design of a fair and efficient governance framework to ensure responsible technology development and implementation concerns not only high-risk artificial intelligence systems. Everyday applications with a limited ability to inflict harm are also addressed. This article examines the European Union's approach to regulating these non-high-risk systems. We focus on the governance model for these systems established by the Artificial Intelligence Act. Based on a doctrinal legal reconstruction of the rules for codes of conduct and considering the European Union's stated goal of achieving a market-oriented balance between innovation, fundamental rights, and public interest, we explore our topic from three different perspectives: an analysis of specific regulatory components of the governance mechanism is followed by a reflection on ethics and trustworthiness implications of the EU´s approach and concluded by an analysis of a case study from an NLP-based, language-simplifying artificial intelligence application for assistive purposes.","",""
"2024","AI-generated journalism: Do the transparency provisions in the AI Act give news readers what they hope for?","Issues linked to the increasing presence of AI-generated content in people’s lives, and the importance of being able to effectively navigate and distinguish such content, are inherently linked to transparency, a notion that our study focuses on by evaluating Art. 50 of the AI Act. This article is a call for action to take the interests of end users into account when specifying AI Act's transparency requirements. It focuses on a specific use case – media organisations producing text with the help of generative AI. We argue that in its current form, Art. 50 leaves many uncertainties and risks doing too little to protect natural persons from manipulation or to empower them to take protective actions. The article combines documental and survey data analysis (based on a sample representative of the Dutch population) to propose concrete policy and regulatory recommendations on the operationalisation of the AI Act’s transparency obligations. Its main objective is to respond to the following question: how to reconcile AI Act’s transparency provisions applicable to digital news articles generated by AI with news readers’ perceptions of manipulation and empowerment?","",""
"2024","Regulating high-reach AI: On transparency directions in the Digital Services Act","","",""
"2024","Contesting the public interest in AI governance","This article argues that public contestability is a critical attribute of governance arrangements designed to align AI deployment with the public interest. Mechanisms to collectively contest decisions which do not track public interests are an important guardrail against erroneous, exclusionary, and arbitrary decision-making. On that basis, we suggest that efforts to align AI to the public interest through democratic participation will benefit substantially from strengthening capabilities for public contestation outside aggregative and deliberative processes. We draw on insights from democratic and regulatory theory to explore three underlying requirements for public contestability in AI governance: (1) capabilities to organise; (2) separation of powers; and (3) access to alternative and independent information. While recognising that suitable mechanisms for contestability will vary by system and context, we sketch out some possibilities for embedding public contestability in AI governance frameworks with a view to provoking further discussion on institutional design.","",""
"2024","Contextual social valences for artificial intelligence: anticipation that matters in social work","ABSTRACT In pilot trials, Finnish caseworkers in child welfare services used an AI tool predicting severe risks faced by their clients. Based on interviews with the caseworkers involved, this article draws on those trials to discuss AI valences, or the range of expectations of AI’s value and performance, in social work and beyond. While AI travels across sites of application and sectors of society, its value is often expected to come from the production of anticipatory knowledge. The predictive AI tool used by Finnish caseworkers offers an example: it turned past data about clients into predictions about their future, with an aim of authorizing present interventions to optimize the future. In the pilot trials, however, AI met the practice of social work. In contrast to generic expectations of predictive performance, caseworkers had contextual expectations for AI, reflecting their situated knowledge about their field. For caseworkers, anticipation does not mean producing pieces of speculative knowledge about the future. Instead, for them, anticipation is a professional knowledge-making practice, based on intimate encounters with clients. Caseworkers therefore expect AI to produce contextually relevant information that can facilitate those interactions. This suggests that for AI developments to matter in social work, it is necessary to consider AI not as a tool that produces knowledge outcomes, but one that supports human experts’ knowledge-making processes. More broadly, as AI tools enter new sensitive areas of application, instead of expecting generic value and performance from them, careful attention should be paid on contextual AI valences.","",""
"2024","The             <i>supply chain capitalism of AI</i>             : a call to (re)think algorithmic harms and resistance through environmental lens","","",""
"2024","Algorithmic futures: the intersection of algorithms and evidentiary work","","",""
"2024","Sorting a public? Using quali-quantitative methods to interrogate the role of algorithms in digital democracy platforms","ABSTRACT Following concerns about social media’s role in politics (fostering polarization and spreading disinformation), many activists and civic hackers have developed alternative digital democracy platforms for both deliberation and the representation of public opinion. But how are we to study the role of these platforms, and in particular, their algorithms in the development of issues and the publics that gather around them? This article employs a simple quali-quantitative data visualization to study how a particular digital democracy platform, vTaiwan (an implementation of Pol.is – a tool for generating opinions and consensus about public issues) – formats political participation. We investigate how one particular issue (Uber legalization) was formed and reformed by users, moderators, and algorithms on the vTaiwan platform over time. while the algorithm sorted opinions into a binary of pro and anti-Uber positions, we find that the comments themselves and their sequence suggest more nuanced positions and the potential for dialogue. We argue that vTaiwan may be limited by its focus on simple quantitative data points (positive or negative votes as opposed to the texts themselves) and a forced separation of participants into in-or-out opinion groups. This study contributes to critical algorithm studies and digital democracy studies by offering an effective way to analyse the role of algorithms in democratic politics.","",""
"2024","Visions of vectors: sense, race, and colonialism in machine learning practice","","",""
"2024","The tensions of algorithmic thinking: automation, intelligence, and the politics of knowing","","",""
"2024","Automating public administration: citizens’ attitudes towards automated decision-making across Estonia, Sweden, and Germany","ABSTRACT Although algorithms are increasingly used for enabling the automation of tasks in public administration of welfare states, the citizens’ knowledge of, experiences with and attitudes towards automated decision-making (ADM) in public administration are still less known. This article strives to reveal the perspectives of citizens who are increasingly exposed to ADM systems, relying on a comparative analysis of a representative survey conducted in Estonia, Germany, and Sweden. The findings show that there are important differences between the three countries when it comes to awareness, trust, and perceived suitability of ADM in public administration, which map onto historical differences in welfare provisions or so-called welfare regimes.","",""
"2024","Mediating access: unpacking the role of algorithms in digital tenancy application technologies","","",""
"2024","‘Why should we turn to fascists in their own language?’ Affordances and constraints of networked counterpublics as experienced by the group members","","",""
"2024","Dis//assemblages of AI: repair labor and resistance in the automated workplace","","",""
"2024","Responsibility gap or responsibility shift? The attribution of criminal responsibility in human–machine interaction","ABSTRACT The prevalence of automation and advent of intelligent machines have created new constellations in which the attribution of criminal responsibility is complicated. Automation results in complex settings of interaction, while the conduct of technical systems is becoming less determined, predictable, and transparent. There is an ongoing scholarly debate regarding how these developments shape moral and legal agency, as well as the best ways of allocating responsibility. Concerns have been expressed about the emergence of responsibility gaps, calling for ways to ensure accountability. This article explores whether these apprehensions can be empirically substantiated. A factorial survey study (N = 799) conducted in Switzerland was used to research attribution mechanisms in criminal cases involving various forms of human-machine interaction. The results revealed that the level of automation significantly affected the attribution of responsibility. In cases of high automation, attribution became considerably more complex and more actors, especially corporate entities, were called to account. A difference in automation level (i.e., the question of how much humans are still ‘in the loop’) had a stronger effect than did the aspect of technology being described as capable of learning. However, the involvement of intelligent technology seemed to have made the responsibility attribution more arbitrary. According to the respondents, there were no discernable responsibility gaps. However, in the present research, significant shifts among the agents called to account were observable. Yet, since these evasion mechanisms are hardly covered by today's law, there is an ongoing risk of a gap between the desire for punishment and actual legal constructs.","",""
"2024","Does algorithmic content moderation promote democratic discourse? Radical democratic critique of toxic language AI","","",""
"2024","AI competitions as infrastructures of power in medical imaging","","",""
"2024","Creative data justice: a decolonial and indigenous framework to assess creativity and artificial intelligence","","",""
"2024","Understanding algorithmic recommendations. A qualitative study on children’s algorithm literacy in Switzerland","","",""
"2024","The feel of algorithms","","",""
"2024","The thousand faces of images in AI news: psychological distance, dialectical relationships and sensationalism","","",""
"2024","Automation hesitancy: confidence deficits, established limits and notional horizons in the application of algorithms within the private rental sector in the UK","","",""
"2024","A careful approach to artificial intelligence: the struggles with epistemic responsibility of healthcare professionals","ABSTRACT Machine learning approaches are being developed to contribute to the treatment of patients and the organisation of care. These new approaches are created in complex environments that include data and computational models as well as new practices, roles and competencies. In such settings, individualised conceptions of agents bearing responsibility need rethinking. In response, we elaborate on the concept of epistemic responsibility based on De la Bellacasa’s work on care (Bellacasa, M.P. de la. (2017). Matters of care: Speculative ethics in more than human worlds. University of Minnesota Press). To better understand these complex environments and the dynamics of responsibility, we use an ethnographic approach and followed Dutch healthcare professionals who learned the basics of (supervised) machine learning, while they pursued a project in their organisations during a four-month-long course. The professionals struggled with different interdependencies and this brought responsibility-in-the-making into relief. Rather than seeing (growing) relations and impure entanglements as standing in the way of responsibility, we show how connections are worthy of inquiry. We argue that connections are essential to knowledge and that producing epistemic responsibility means considering these embedded relations. In contrast to calls for control and clarification of machine learning techniques, and warnings that they create irresponsible black boxes, our care approach shows how responsibility-in-the-making reveals opportunities for ethical reflection and action. Our approach attends to how humans and non-humans are engaged in caring, reveals patterns around kinds of responsibility, and points to opportunities for avoiding neglect and irresponsibility.","",""
"2024","Artificial intelligence as a mode of ordering. Automated-decision making in primary care","","",""
"2024","Automation scenarios: citizen attitudes towards automated decision-making in the public sector","","",""
"2024","The algorithmic network imaginary: How music artists understand and experience their algorithmically constructed networks","Abstract In this article we develop the concept of “algorithmic network imaginary” to understand how musicians imagine and relate to the networks of “related artists” they are algorithmically sorted into on Spotify. To address this question, we collected data on the related artist networks of 22 musicians constructed by Spotify’s Fans Also Like feature and conducted semi-structured, in-depth interviews with each musician. We used the Qualitative Structural Analysis method for data analysis. Our findings provide insight into what musicians think Fans Also Like is and is for, and reveals how cultural creators understand and experience their algorithmic networks. More broadly, they provide insights into how social actors perceive, understand, and experience their algorithmically constructed peer networks.","",""
"2024","Audience perceptions of AI-driven news presenters: A case of ‘Alice’ in Zimbabwe"," Digital innovations are transforming newsroom practices across the globe. The advent of artificial intelligence (AI) tools is upending and altering newsmaking and news distribution practices. Whilst there is existing literature on the ever-changing journalism practices due to AI tools in the Global North, scholarship on how these digital innovations are shaping African newsrooms has remained scant. This study uses the case of Alice, an AI-powered newsreader in Zimbabwe, to explore the audience perceptions of this digital innovation. Drawing upon Afrokology, and cultural studies as theoretical frameworks, this qualitative study derived data through digital ethnography and in-depth interviews. Findings demonstrate that there are mixed feelings on the appropriation and use of AI-driven news anchors. On the one hand, some audiences applaud the use of AI news anchors as innovative storytelling techniques. On the other hand, audiences are concerned with Alice’s lack of human emotion, poor accent and perceive her as a threat to traditional journalists’ jobs. Some of the resistance towards Alice indicate a need for decolonising the AI tools in the newsrooms. ","",""
"2024","Engine for the imagination? Visual generative media and the issue of representation"," Visual generative media represent a novel technology with the potential to mediate public perceptions of political events, conflicts, and wars. Seeking to understand a visual culture in which algorithms become integrated into human processes of memory mediatization, this study addresses representation in AI-generated war imagery. It frames AI image generation as a socio-technical practice at the nexus of humans, machines, and visual culture, challenging Silicon Valley’s prevailing narrative of visual AI as “an engine for the imagination.” Through a case study of AI images generated in response to verbal prompts about Russia’s war against Ukraine, I examine the representational capabilities and limitations of the text-to-image generator Midjourney. The findings suggest homogeneity of visual themes that foreground destruction and fighters, while overlooking broader contextual and cultural aspects of the Russia-Ukraine war, thus generalizing the depiction of this war to that of any war. This study advances the research agenda on critical machine vision as a transdisciplinary challenge situated at the interface of media and cultural studies, computer science, and discourse-analytic approaches to visual communication. ","",""
"2024","Reflecting on cultural labour in the time of AI"," With generative AI disrupting human monopoly of creativity, there is an urgent need to freshly rearticulate cultural labour as a marker of human creativity. I suggest we critically revisit the existing perspectives of cultural labour in cultural policy discussion (unproductive, creative and precarious labour) to reflect on their limitations and implications for our understanding of AI’s challenges. Based on this, I argue that we should expand the discussion of precarious labour to elaborate the emerging ‘creative precarity’. In particular, I will explore its key dimensions – the increasing uncertainty in terms of cultural workers’ creative roles, rights and identity, and audience responses – and their policy implications. At the core of potential policy response to and our research into creative precarity, there are fundamental questions of how we redefine cultural work in the time of AI, what new meanings we can attach to cultural labour, what constitutes the human-ness in human creativity and why it crucially matters. ","",""
"2024","The influence of generative AI on popular music: Fan productions and the reimagination of iconic voices"," This essay explores the burgeoning phenomenon of fan-generated content that utilizes generative artificial intelligence (AI) to mimic the voices of deceased or retired popular music artists. This practice can be seen as an extension of Digital Audio Workstations’ (DAWs’) capabilities, potentially leading to the creation of musical pieces that resemble traditional covers and mash-ups. The essay analyzes three representative works in this creative category. Two of them are of a fan nature, exploring imagined alternative paths in the development of popular music, while the third is a parody. As these examples demonstrate, the rise of AI covers raises numerous questions. These include concerns about the nature of human creativity and the potential response of the music industries to this technology’s proliferation. ","",""
"2024","Hallucinating a political future: Global press coverage of human and post-human abilities in ChatGPT applications"," In November 2022, the tech company OpenAI launched a groundbreaking chatbot model, ChatGPT. This unprecedented chatbot, characterized by an ease of use for lay internet users, gained immediate popularity and attracted extensive media attention. This article examines global press coverage of ChatGPT in peak reporting dates over the first full year of its existence. Based on a qualitative holistic narrative analysis, our findings point to two narrated scapes of political fear in the coverage of ChatGPT: The fear of the machine and the fear of the human. These attest to the collective imagining of an intensified future, where post-humanist interaction with political information is associated with exploitation, propaganda, and polarization of existing political rifts. We draw on the case study to articulate journalists’ role in signaling instability in the current political media ecosystem, and their construction of a techno-moral framework for society. We discuss an important blind-spot in journalists’ fulfilment of their normative role in fostering technology-informed citizens globally. ","",""
"2024","AI statecraft heating-up: the automation of governance through Canada’s Chinook case study","","",""
"2024","Generative AI and epistemic diversity of its inputs and outputs: call for further scrutiny","","",""
"2024","Lessons from the California Gold Rush of 1849: prudence and care before advancing generative AI initiatives within your enterprise","","",""
"2024","Sculpting the social algorithm for radical futurity","","",""
"2024","How we can create the global agreement on generative AI bias: lessons from climate justice","","",""
"2024","Fear of AI: an inquiry into the adoption of autonomous cars in spite of fear, and a theoretical framework for the study of artificial intelligence technology acceptance","AbstractArtificial intelligence (AI) is becoming part of the everyday. During this transition, people’s intention to use AI technologies is still unclear and emotions such as fear are influencing it. In this paper, we focus on autonomous cars to first verify empirically the extent to which people fear AI and then examine the impact that fear has on their intention to use AI-driven vehicles. Our research is based on a systematic survey and it reveals that while individuals are largely afraid of cars that are driven by AI, they are nonetheless willing to adopt this technology as soon as possible. To explain this tension, we extend our analysis beyond just fear and show that people also believe that AI-driven cars will generate many individual, urban and global benefits. Subsequently, we employ our empirical findings as the foundations of a theoretical framework meant to illustrate the main factors that people ponder when they consider the use of AI tech. In addition to offering a comprehensive theoretical framework for the study of AI technology acceptance, this paper provides a nuanced understanding of the tension that exists between the fear and adoption of AI, capturing what exactly people fear and intend to do.","",""
"2024","Artificial intelligence in support of the circular economy: ethical considerations and a path forward","AbstractThe world’s current model for economic development is unsustainable. It encourages high levels of resource extraction, consumption, and waste that undermine positive environmental outcomes. Transitioning to a circular economy (CE) model of development has been proposed as a sustainable alternative. Artificial intelligence (AI) is a crucial enabler for CE. It can aid in designing robust and sustainable products, facilitate new circular business models, and support the broader infrastructures needed to scale circularity. However, to date, considerations of the ethical implications of using AI to achieve a transition to CE have been limited. This article addresses this gap. It outlines how AI is and can be used to transition towards CE, analyzes the ethical risks associated with using AI for this purpose, and supports some recommendations to policymakers and industry on how to minimise these risks.","",""
"2024","When something goes wrong: Who is responsible for errors in ML decision-making?","","",""
"2024","On the creativity of large language models","AbstractLarge language models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arises: can LLMs be really considered creative? In this article, we first analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. In particular, we focus our discussion on the dimensions of value, novelty, and surprise as proposed by Margaret Boden in her work. Then, we consider different classic perspectives, namely product, process, press, and person. We discuss a set of “easy” and “hard” problems in machine creativity, presenting them in relation to LLMs. Finally, we examine the societal impact of these technologies with a particular focus on the creative industries, analyzing the opportunities offered, the challenges arising from them, and the potential associated risks, from both legal and ethical points of view.","",""
"2024","Theory languages in designing artificial intelligence","AbstractThe foundations of AI design discourse are worth analyzing. Here, attention is paid to the nature of theory languages used in designing new AI technologies because the limits of these languages can clarify some fundamental questions in the development of AI. We discuss three types of theory language used in designing AI products: formal, computational, and natural. Formal languages, such as mathematics, logic, and programming languages, have fixed meanings and no actual-world semantics. They are context- and practically content-free. Computational languages use terms referring to the actual world, i.e., to entities, events, and thoughts. Thus, computational languages have actual-world references and semantics. They are thus no longer context- or content-free. However, computational languages always have fixed meanings and, for this reason, limited domains of reference. Finally, unlike formal and computational languages, natural languages are creative, dynamic, and productive. Consequently, they can refer to an unlimited number of objects and their attributes in an unlimited number of domains. The differences between the three theory languages enable us to reflect on the traditional problems of strong and weak AI.","",""
"2024","Organization philosophy: a study of organizational goodness in the age of human and artificial intelligence collaboration","AbstractThis study challenges the conventional boundaries of philosophy by asserting that organizations can function as legitimate subjects within philosophical discourse. Western philosophy, epitomized by Descartes, has long assumed that individual human beings are the fundamental units of thought and moral agency. However, in a significant oversight, this belief overlooks the idea that organizations can think independently, leading to both virtuous and malevolent results. Epistemology lacks a clear prioritization of morally sound knowledge over potentially harmful knowledge. The advent of artificial intelligence (AI) has also transformed the landscape of cognition from a primarily individual human endeavor into a joint undertaking where humans collaborate with AI entities in collective thinking and decision-making. Nevertheless, AI lacks the innate ability to discern between ethical and unethical actions. This symbiosis creates organizations as fundamental philosophical subjects, marking a significant shift in philosophical emphasis where the organization takes the center stage. Accordingly, within this emerging field, we advance four propositions as the core subjects of organization philosophy: organizations can think, organizations can create knowledge, AI can be part of an organization, and humans in the organization control moral discipline. These propositions guide the ethical coexistence of AI and human agents in organizations, emphasizing the importance of ethical deliberation in knowledge use. Moreover, the malicious use of profound knowledge hinders human progress, as illustrated in this study by an ethical examination of military institutions within organization philosophy.","",""
"2024","Robotics in place and the places of robotics: productive tensions across human geography and human–robot interaction","AbstractBringing human–robot interaction (HRI) into conversation with scholarship from human geography, this paper considers how socially interactive robots become important agents in the production of social space and explores the utility of core geographic concepts ofscaleandplaceto critically examine evolving robotic spatialities. The paper grounds this discussion through reflections on a collaborative, interdisciplinary research project studying the development and deployment of interactive museum tour-guiding robots on a North American university campus. The project is a collaboration among geographers, roboticists, a digital artist, and the directors/curators of two museums, and involves experimentation in the development of a tour-guiding robot with a “socially aware navigation system” alongside ongoing critical reflection into the socio-spatial context of human–robot interactions and their future possibilities. The paper reflects on the tensions between logics of control and contingency in robotic spatiality and argues that concepts of scale and place can help reflect on this tension in a productive way while calling attention to a broader range of stakeholders who should be included in robotic design and deployment.","",""
"2024","Economics of AI behavior: nudging the digital minds toward greater societal benefit","","",""
"2024","Social context of the issue of discriminatory algorithmic decision-making systems","","",""
"2024","Explainable artificial intelligence and the social sciences: a plea for interdisciplinary research","","",""
"2024","Bringing older people’s perspectives on consumer socially assistive robots into debates about the future of privacy protection and AI governance","","",""
"2024","Robots and AI: a new economic era. Edited by Lili Yan Ing and Gene M. Grossman (2022). Published by Routledge, London ISBN: 9781003275534. https://doi.org/10.4324/9781003275534 (open access)","","",""
"2024","Artificial intelligence and human autonomy: the case of driving automation","AbstractThe present paper aims at contributing to the ethical debate on the impacts of artificial intelligence (AI) systems on human autonomy. More specifically, it intends to offer a clearer understanding of the design challenges to the effort of aligning driving automation technologies to this ethical value. After introducing the discussion on the ambiguous impacts that AI systems exert on human autonomy, the analysis zooms in on how the problem has been discussed in the literature on connected and automated vehicles (CAVs). On this basis, it is claimed that the issue has been mainly tackled on a fairly general level, and mostly with reference to the controversial issue of crash-optimization algorithms, so that only limited design insights have been drawn. However, integrating ethical analysis and design practices is critical to pursue the implementation of such an important ethical value into CAV technologies. To this aim, it is argued, a more applied approach targeted at examining the impacts on human autonomy of current CAV functions should also be explored. As an example of the intricacy of this task, the case of automated route planning is discussed in some detail.","",""
"2024","ChatGPT: towards AI subjectivity","AbstractMotivated by the question of responsible AI and value alignment, I seek to offer a uniquely Foucauldian reconstruction of the problem as the emergence of an ethical subject in a disciplinary setting. This reconstruction contrasts with the strictly human-oriented programme typical to current scholarship that often views technology in instrumental terms. With this in mind, I problematise the concept of a technological subjectivity through an exploration of various aspects of ChatGPT in light of Foucault’s work, arguing that current systems lack the reflexivity and self-formative characteristics inherent in the notion of the subject. By drawing upon a recent dialogue between Foucault and phenomenology, I suggest four techno-philosophical desiderata that would address the gaps in this search for a technological subjectivity: embodied self-care, embodied intentionality, imagination and reflexivity. Thus I propose that advanced AI be reconceptualised as a subject capable of “technical” self-crafting and reflexive self-conduct, opening new pathways to grasp the intertwinement of the human and the artificial. This reconceptualisation holds the potential to render future AI technology more transparent and responsible in the circulation of knowledge, care and power.","",""
"2024","Towards trustworthy medical AI ecosystems – a proposal for supporting responsible innovation practices in AI-based medical innovation","AbstractIn this article, we explore questions about the culture of trustworthy artificial intelligence (AI) through the lens of ecosystems. We draw on the European Commission’s Guidelines for Trustworthy AI and its philosophical underpinnings. Based on the latter, the trustworthiness of an AI ecosystem can be conceived of as being grounded by both the so-called rational-choice and motivation-attributing accounts—i.e., trusting is rational because solution providers deliver expected services reliably, while trust also involves resigning control by attributing one’s motivation, and hence, goals, onto another entity. Our research question is: What aspects contribute to a responsible AI ecosystem that can promote justifiable trustworthiness in a healthcare environment? We argue that especially within devising governance and support aspects of a medical AI ecosystem, considering the so-called motivation-attributing account of trust provides fruitful pointers. There can and should be specific ways and governance structures supporting and nurturing trustworthiness beyond mere reliability. After compiling a list of preliminary requirements for this, we describe the emergence of one particular medical AI ecosystem and assess its compliance with and future ways of improving its functioning as a responsible AI ecosystem that promotes trustworthiness.","",""
"2024","Can machines have emotions?","","",""
"2024","Doing versus saying: responsible AI among large firms","","",""
"2024","“Your friendly AI assistant”: the anthropomorphic self-representations of ChatGPT and its implications for imagining AI","AbstractThis study analyzes how ChatGPT portrays and describes itself, revealing misleading myths about AI technologies, specifically conversational agents based on large language models. This analysis allows for critical reflection on the potential harm these misconceptions may pose for public understanding of AI and related technologies. While previous research has explored AI discourses and representations more generally, few studies focus specifically on AI chatbots. To narrow this research gap, an experimental-qualitative investigation into auto-generated AI representations based on prompting was conducted. Over the course of a month, ChatGPT (both in its GPT-4 and GPT-4o models) was prompted to “Draw an image of yourself,” “Represent yourself visually,” and “Envision yourself visually.” The resulting data (n = 50 images and 58 texts) was subjected to a critical exploratory visual semiotic analysis to identify recurring themes and tendencies in how ChatGPT is represented and characterized. Three themes emerged from the analysis: anthropomorphism, futuristic/futurism and (social)intelligence. Importantly, compared to broader AI imaginations, the findings emphasize ChatGPT as a friendly AI assistant. These results raise critical questions about trust in these systems, not only in terms of their capability to produce reliable information and handle personal data, but also in terms of human–computer relations.","",""
"2024","Controlling the uncontrollable: the public discourse on artificial intelligence between the positions of social and technological determinism","AbstractSince the publication of ChatGPT and Dall-E, there has been heavy discussions on the possible dangers of generative artificial intelligence (AI) for society. These discussions question the extent to which the development of AI can be regulated by politics, law, and civic actors. An important arena for discourse on AI is the news media. The news media discursively construct AI as a technology that is more or less possible to regulate. There are various reasons for an assumed regulatability. Some voices highlight the predominant capitalism of modern society as an ideology that enforces an uncontrolled development of AI. Others call on the rationality of civic society, which will push AI toward a development that serves humankind. Summarized, the discourse on AI floats between positions of technological and social determinism. The article conducts a discourse analysis on 113 articles from the German newspapers Süddeutsche Zeitung and Frankfurter Allgemeine Zeitung. The analysis shows how these center-left and center-right media frame the AI development in terms of social and technological determinism. As these newspapers reach out to a large audience, the article shows the kinds of perspectives on AI that confront civic society in Germany every day. News media can motivate or impede social action, as they frame the potential necessity of society and its members to intervene against certain developments. The article shows how the newspapers promote an understanding of AI, by which citizens will feel motivated to insist on a regulation of AI by politics and law.","",""
"2024","The end of algorithm aversion","","",""
"2024","Ethical approaches in designing autonomous and intelligent systems: a comprehensive survey towards responsible development","AbstractOver the past decade, significant progress in artificial intelligence (AI) has spurred the adoption of its algorithms, addressing previously daunting challenges. Alongside these remarkable strides, there has been a simultaneous increase in model complexity and reliance on opaque AI models, lacking transparency. In numerous scenarios, the systems themselves may necessitate making decisions entailing ethical dimensions. Consequently, it has become imperative to devise solutions to integrate ethical considerations into AI system development practices, facilitating broader utilization of AI systems across various domains. Research endeavors should explore innovative approaches to enhance ethical principles in AI systems, fostering greater transparency, accountability, and trustworthiness. Upholding fundamental individual rights, human dignity, autonomy, privacy, equality, and fairness, while mitigating potential harm, remains paramount. Considering ethical values and ensuring compliance with ethical requirements throughout the development lifecycle of autonomous and intelligent systems nurture trust and reliability in their utilization. Ethical considerations should be ingrained within organizational procedures guiding AI research activities, establishing robust frameworks that address ethical concerns and reflect the ethical implications of AI-based systems. This paper presents an overview of ethical approaches and processes aimed at integrating ethical considerations into AI system development practices. It underscores the significance of ethical frameworks in fostering ethical AI implementation and ensuring the ethical integrity of AI technologies.","",""
"2024","Value preference profiles and ethical compliance quantification: a new approach for ethics by design in technology-assisted dementia care","AbstractMonitoring and assistive technologies (MATs) are being used more frequently in healthcare. A central ethical concern is the compatibility of these systems with the moral preferences of their users—an issue especially relevant to participatory approaches within the ethics-by-design debate. However, users’ incapacity to communicate preferences or to participate in design processes, e.g., due to dementia, presents a hurdle for participatory ethics-by-design approaches. In this paper, we explore the question of how the value preferences of users in the field of dementia care can be integrated into AI-based MATs. First, we briefly introduce different ethics-by-design approaches and explain the challenges they face in dementia care. Next, we introduce a novel approach for addressing these challenges. Through a qualitative content analysis of interviews with persons with dementia and their family caregivers (n = 27), we identified multiple ideal–typical value preference profiles. We then applied these profiles in a computer simulation, by which we also introduce the concept of ethical compliance quantification to scale the moral preferences of the interviewees for the simulation. Finally, we discuss study results, the advantages of using computer simulations, and general study limitations before drawing conclusions for further research. The study contributes to the ongoing debate on participatory ethics-by-design by defining on the basis of empirical data ideal–typical value preference frameworks that can be used to guide MAT actions and their outcomes. Going forward, MAT end-users with dementia and other communication-impaired persons could be enabled to choose a value profile that best fits their moral preferences.","",""
"2024","Bowling alone in the autonomous vehicle: the ethics of well-being in the driverless car","","",""
"2024","Eliza! A reckoning with Cartesian magic","","",""
"2024","Imagining machine vision: Four visual registers from the Chinese AI industry","AbstractMachine vision is one of the main applications of artificial intelligence. In China, the machine vision industry makes up more than a third of the national AI market, and technologies like face recognition, object tracking and automated driving play a central role in surveillance systems and social governance projects relying on the large-scale collection and processing of sensor data. Like other novel articulations of technology and society, machine vision is defined, developed and explained by different actors through the work of imagination. In this article, we draw on the concept of sociotechnical imaginaries to understand how Chinese companies represent machine vision. Through a qualitative multimodal analysis of the corporate websites of leading industry players, we identify a cohesive sociotechnical imaginary of machine vision, and explain how four distinct visual registers contribute to its articulation. These four registers, which we call computational abstraction, human–machine coordination, smooth everyday, and dashboard realism, allow Chinese tech companies to articulate their global ambitions and competitiveness through narrow and opaque representations of machine vision technologies.","",""
"2024","The neural democratisation of AI","","",""
"2024","Anthropomorphization and beyond: conceptualizing humanwashing of AI-enabled machines","AbstractThe complex relationships between humans and AI-empowered machines have created and inspired new products and services as well as controversial debates, fiction and entertainment, and last but not least, a striving and vital field of research. The (theoretical) convergence between the two categories of entities has created stimulating concepts and theories in the past, such as the uncanny valley, machinization of humans through datafication, or humanization of machines, known as anthropomorphization. In this article, we identify a new gap in the relational interaction between humans and AI triggered by commercial interests, making use of AI through advertisement, marketing, and corporate communications. Our scope is to broaden the field of AI and society by adding the business-society-nexus. Thus, we build on existing research streams of machinewashing and the analogous phenomenon of greenwashing to theorize about the humanwashing of AI-enabled machines as a specific anthropomorphization notion. In this way, the article offers a contribution to the anthropomorphization literature conceptualizing humanwashing as a deceptive use of AI-enabled machines (AIEMs) aimed at intentionally or unintentionally misleading organizational stakeholders and the broader public about the true capabilities that AIEMs possess.","",""
"2024","Understanding users’ responses to disclosed vs. undisclosed customer service chatbots: a mixed methods study","AbstractDue to huge advancements in natural language processing (NLP) and machine learning, chatbots are gaining significance in the field of customer service. For users, it may be hard to distinguish whether they are communicating with a human or a chatbot. This brings ethical issues, as users have the right to know who or what they are interacting with (European Commission in Regulatory framework proposal on artificial intelligence. https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai, 2022). One of the solutions is to include a disclosure at the start of the interaction (e.g., “this is a chatbot”). However, companies are reluctant to use disclosures, as consumers may perceive artificial agents as less knowledgeable and empathetic than their human counterparts (Luo et al. in Market Sci 38(6):937–947, 2019). The current mixed methods study, combining qualitative interviews (n = 8) and a quantitative experiment (n = 194), delves into users’ responses to a disclosed vs. undisclosed customer service chatbot, focusing on source orientation, anthropomorphism, and social presence. The qualitative interviews reveal that it is the willingness to help the customer and the friendly tone of voice that matters to the users, regardless of the artificial status of the customer care representative. The experiment did not show significant effects of the disclosure (vs. non-disclosure). Implications for research, legislators and businesses are discussed.","",""
"2024","The work of art in the age of artificial intelligibility","AbstractThe emergence of complex deep-learning models capable of producing novel images on a practically innumerable number of subjects and in an equally wide variety of artistic styles is beginning to highlight serious inadequacies in the ethical, aesthetic, epistemological and legal frameworks we have so far used to categorise art. To begin tackling these issues and identifying a role for AI in the production and protection of human artwork, it is necessary to take a multidisciplinary approach which considers current legal precedents, the practice of software engineering, historical attitudes towards technological innovation and a sustained technical analysis of the models themselves. This paper queries the location and nature of substantive artistic work in the developmental stages of an AI-generated image, offering critiques of existing assumptions and posing questions for future research. The emergence of convincing AI creative output, artistic or literary, has significant long-term implications for the humanities, including the need for re-appraisal of foundational ideas about authorship and creativity in general. The effects of artificial intelligence, whether generalised or task-specific, cannot be ignored or displaced now that easy-access, scalable image and text production is a reality.","",""
"2024","The unseen dilemma of AI in mental healthcare","","",""
"2024","Challenges as catalysts: how Waymo’s Open Dataset Challenges shape AI development","AbstractArtificial intelligence (AI) and machine learning (ML) are becoming increasingly significant areas of research for scholars in science and technology studies (STS) and media studies. In March 2020, Waymo, Google/Alphabet’s autonomous vehicle project, introduced the ‘Open Dataset Virtual Challenge’, an annual competition leveraging their Waymo Open Dataset. This freely accessible dataset comprises annotated autonomous vehicle data from their own Waymo vehicles. Yearly, Waymo has continued to host iterations of this challenge, inviting teams of computer scientists to tackle evolving machine learning and vision problems using Google's data and tools. This article analyses these challenges, situating them within the context of the ‘Grand Challenges’ of artificial intelligence (AI), which aimed to foster accountable and commercially viable advancements in the late 1980s. Through two exploratory workshops, we adopted a ‘technographic’ approach to examine the pivotal role of challenges in the development and political economy of AI. Serving as an organising principle for the AI innovation ecosystem, the challenge connects companies and external collaborators, driving advancements in specific machine vision domains. By exploring six key themes—interface methods, incrementalism, metrics, AI vernacular, applied domains, and competitive advantages—the article illustrates the role of these challenges in shaping AI research and development. By unpacking the dynamic interaction between data, computation, and labour, these challenges serve as catalysts propelling advancements towards self-driving technologies. The study reveals how challenges have historically and presently shaped the evolving landscape of self-driving and AI technologies.","",""
"2024","Moral disagreement and artificial intelligence","AbstractArtificially intelligent systems will be used to make increasingly important decisions about us. Many of these decisions will have to be made without universal agreement about the relevant moral facts. For other kinds of disagreement, it is at least usually obvious what kind of solution is called for. What makes moral disagreement especially challenging is that there are three different ways of handling it. Moral solutions apply a moral theory or related principles and largely ignore the details of the disagreement. Compromise solutions apply a method of finding a compromise and taking information about the disagreement as input. Epistemic solutions apply an evidential rule that treats the details of the disagreement as evidence of moral truth. Proposals for all three kinds of solutions can be found in the AI ethics and value alignment literature, but little has been said to justify choosing one over the other. I argue that the choice is best framed in terms of moral risk. ","",""
"2024","Pashmina authentication on imagery data using deep learning","","",""
"2024","Doubt or punish: on algorithmic pre-emption in acute psychiatry","AbstractMachine learning algorithms have begun to enter clinical settings traditionally resistant to digitalisation, such as psychiatry. This raises questions around how algorithms will be incorporated in professionals’ practices, and with what implications for care provision. This paper addresses such questions by examining the pilot of an algorithm for the prediction of inpatient violence in two acute psychiatric clinics in the Netherlands. Violence is a prominent risk in acute psychiatry, and professional sensemaking, corrective measures (such as patient isolation and sedation), and quantification instruments (such as the Brøset Violence Checklist, henceforth BVC) have previously been developed to deal with it. We juxtapose the different ways in which psychiatric nurses, the BVC, and algorithmic scores navigate assessments of the potential of future inpatient violence. We find that nurses approach violence assessment with an attitude of doubt and precaution: they aim to understand warning signs and probe alternative explanations to them, so as not to punish patients when not necessary. Being in charge of quantitative capture, they incorporate this attitude of doubt in the BVC scores. Conversely, the algorithmic risk scores import a logic of pre-emption into the clinic: they attempt to flag targets before warning signs manifests and are noticed by nurses. Pre-emption translates into punitive attitudes towards patients, to which nurses refuse to subscribe. During the pilots, nurses solely engage with algorithmic scores by attempting to reinstate doubt in them. We argue that pre-emption can hardly be incorporated into professional decision-making without importing punitive attitudes. As such, algorithmic outputs targeting ethically laden instances of decision-making are a cause for academic and political concern.","",""
"2024","Net versus relative impacts in public policy automation: a conjoint analysis of attitudes of Black Americans","AbstractThe use of algorithms and automated systems, especially those leveraging artificial intelligence (AI), has been exploding in the public sector, but their use has been controversial. Ethicists, public advocates, and legal scholars have debated whether biases in AI systems should bar their use or if the potential net benefits, especially toward traditionally disadvantaged groups, justify even greater expansion. While this debate has become voluminous, no scholars of which we are aware have conducted experiments with the groups affected by these policies about how they view the trade-offs. We conduct a set of two conjoint experiments with a high-quality sample of 973 Americans who identify as Black or African American in which we randomize the levels of inter-group disparity in outcomes and the net effect on such adverse outcomes in two highly controversial contexts: pre-trial detention and traffic camera ticketing. The results suggest that respondents are willing to tolerate some level of disparity in outcomes in exchange for certain net improvements for their community. These results turn this debate from an abstract ethical argument into an evaluation of political feasibility and policy design based on empirics.","",""
"2024","AI unleashed: will it empower us and promote social independence, or enslave us?","","",""
"2024","Emerging resources, enduring challenges: a comprehensive study of Kashmiri parallel corpus","","",""
"2024","Will humans ever become conscious? Jiddu Krishnamurti’s thought about AI as a fresh perspective on current debates","AbstractIn 1980, the 85 year-old mystic and thinker Jiddu Krishnamurti (1895–1986), having been introduced to the subject of artificial intelligence by computer scientists, became engrossed in the challenge posed to the human mind by the prospect of the machine taking over its processes and faculties. What makes Krishnamurti’s angle worthy of separate discussion is that it accentuates not social and cultural ramifications, but a mostly overlooked potential philosophical and psychological crisis. As a thinker and mystic whose life work entirely centred on the transformation of the human mind, Krishnamurti worried that an insufficiently cultivated mind that had been employed merely for material and mechanical purposes would be perfectly imitable and thus replaceable by computers and other machines. Thus, our main concern should not be machines attaining humanlike minds, but people having machinelike minds. I argue that the particular dimension of the AI–mind encounter elucidated by Krishnamurti can significantly broaden the field of the philosophy of artificial intelligence. Firstly, Krishnamurti’s investigation further problematizes the field by handing us the question: ‘If the machine can take over everything man can do, and do it still better than us, then what is a human being?’ Accordingly, he demonstrates, deploying phenomenological observations, various ways in which human thought is worryingly computational and machinelike. Secondly, since Krishnamurti’s approach is transformative and arguably soteriological, he offers intriguing ways in which we could consider the inherently nonmechanical yet unactualized faculties of the mind. Since Krishnamurti’s observations involve an insight into the present condition of the mind and a potential active response to this condition, they have some pertinence to both the primary discourse in the field – the interrelations between artificial intelligence and aspects of human cognition – and the emerging discussion of meaningfulness in life in the face of AI.","",""
"2024","Lost in the logistical funhouse: speculative design as synthetic media enterprise","AbstractFrom the deployment of chatbots as procurement negotiators by corporations such as Walmart to autonomous agents providing ‘differentiated chat’ for managing overbooked flights, synthetic media are making the world of logistics their ‘natural’ habitat. Here, the coordination of commodities, parts and labour design the problems and produce the training sets from which ‘solutions’ can be synthesised. But to what extent might synthetic media, surfacing via platforms such as Midjourney and OpenAI, be understood as logistical media? This paper charts a selective genealogy of synthetic media from early attempts to synthesise human sensory capacities in order to cybernetically integrate them into computational circuits. We see this integration as a pre-emption of their (computational) logistical coordination. It then details media experiments with ‘ChatFOS’, a GPT-based bot tasked with developing a logistics design business. Using its prompt-generated media outputs, we assemble a simulation and parody of AI’s emerging functionalities within logistical worlds. In the process, and with ‘human-in-the-loop’ stitching, we illustrate how large language models become media managers overseeing image prompts, graphical design, website code, promotional copy and investor pitch scenarios. The processes and methods of producing speculative scenarios via ChatFOS lead us to consider how the media of logistics and the logistics of media are increasingly enfolded. We ask: what can a (practice-based) articulation of this double-becoming of logistics and synthetic mediality tell us about the politics and aesthetics of contemporary computation and capital?","",""
"2024","Coverage of well-being within artificial intelligence, machine learning and robotics academic literature: the case of disabled people","","",""
"2024","A Bourdieusian theory on communicating an opinion about AI governance","","",""
"2024","Automating public policy: a comparative study of conversational artificial intelligence models and human expertise in crafting briefing notes","","",""
"2024","Abundant intelligences: placing AI within Indigenous knowledge frameworks","","",""
"2024","If AI is our co-pilot, who is the captain?","","",""
"2024","The relationship between the attitudes of the use of AI and diversity awareness: comparisons between Japan, the US, Germany, and South Korea","AbstractRecent technological advances have accelerated the use of artificial intelligence (AI) in the world. Public concerns over AI in ethical, legal, and social issues (ELSI) may have been enhanced, but their awareness has not been fully examined between countries and cultures. We created four scenarios regarding the use of AI: “voice,” “recruiting,” “face,” and “immigration,” and compared public concerns in Japan, the US, Germany, and the Republic of Korea (hereafter Korea). Additionally, public ELSI concerns in respect of AI were measured using four items: ethics, tradition, law and social benefit. Respondents with AI knowledge tended to exhibit stronger concern about ELSI in various situations. In terms of law concerns, Japanese respondents displayed greater concerns. In contrast, the US, when compared Japan, expressed a relatively optimistic view of the current law landscape. Regarding social benefits, Korea, compared to Japan, exhibited a more positive outlook, whereas Germany, in comparison to Japan, expressed heightened concerns about it across different scenarios.","",""
"2024","Thinking about the mind-technology problem","","",""
"2024","Beyond ideals: why the (medical) AI industry needs to motivate behavioural change in line with fairness and transparency values, and how it can do it","AbstractArtificial intelligence (AI) is increasingly relied upon by clinicians for making diagnostic and treatment decisions, playing an important role in imaging, diagnosis, risk analysis, lifestyle monitoring, and health information management. While research has identified biases in healthcare AI systems and proposed technical solutions to address these, we argue that effective solutions require human engagement. Furthermore, there is a lack of research on how to motivate the adoption of these solutions and promote investment in designing AI systems that align with values such as transparency and fairness from the outset. Drawing on insights from psychological theories, we assert the need to understand the values that underlie decisions made by individuals involved in creating and deploying AI systems. We describe how this understanding can be leveraged to increase engagement with de-biasing and fairness-enhancing practices within the AI healthcare industry, ultimately leading to sustained behavioral change via autonomy-supportive communication strategies rooted in motivational and social psychology theories. In developing these pathways to engagement, we consider the norms and needs that govern the AI healthcare domain, and we evaluate incentives for maintaining the status quo against economic, legal, and social incentives for behavior change in line with transparency and fairness values.","",""
"2024","Artificial intelligence and democratic legitimacy. The problem of publicity in public authority","AbstractMachine learning algorithms (ML) are increasingly used to support decision-making in the exercise of public authority. Here, we argue that an important consideration has been overlooked in previous discussions: whether the use of ML undermines the democratic legitimacy of public institutions. From the perspective of democratic legitimacy, it is not enough that ML contributes to efficiency and accuracy in the exercise of public authority, which has so far been the focus in the scholarly literature engaging with these developments. According to one influential theory, exercises of administrative and judicial authority are democratically legitimate if and only if administrative and judicial decisions serve the ends of the democratic law maker, are based on reasons that align with these ends and are accessible to the public. These requirements are not satisfied by decisions determined through ML since such decisions are determined by statistical operations that are opaque in several respects. However, not all ML-based decision support systems pose the same risk, and we argue that a considered judgment on the democratic legitimacy of ML in exercises of public authority need take the complexity of the issue into account. This paper outlines considerations that help guide the assessment of whether a ML undermines democratic legitimacy when used to support public decisions. We argue that two main considerations are pertinent to such normative assessment. The first is the extent to which ML is practiced as intended and the extent to which it replaces decisions that were previously accessible and based on reasons. The second is that uses of ML in exercises of public authority should be embedded in an institutional infrastructure that secures reason giving and accessibility.","",""
"2024","Exploring the roles of trust and social group preference on the legitimacy of algorithmic decision-making vs. human decision-making for allocating COVID-19 vaccinations","AbstractIn combating the ongoing global health threat of the COVID-19 pandemic, decision-makers have to take actions based on a multitude of relevant health data with severe potential consequences for the affected patients. Because of their presumed advantages in handling and analyzing vast amounts of data, computer systems of algorithmic decision-making (ADM) are implemented and substitute humans in decision-making processes. In this study, we focus on a specific application of ADM in contrast to human decision-making (HDM), namely the allocation of COVID-19 vaccines to the public. In particular, we elaborate on the role of trust and social group preference on the legitimacy of vaccine allocation. We conducted a survey with a 2 × 2 randomized factorial design among n = 1602 German respondents, in which we utilized distinct decision-making agents (HDM vs. ADM) and prioritization of a specific social group (teachers vs. prisoners) as design factors. Our findings show that general trust in ADM systems and preference for vaccination of a specific social group influence the legitimacy of vaccine allocation. However, contrary to our expectations, trust in the agent making the decision did not moderate the link between social group preference and legitimacy. Moreover, the effect was also not moderated by the type of decision-maker (human vs. algorithm). We conclude that trustworthy ADM systems must not necessarily lead to the legitimacy of ADM systems.","",""
"2024","From algorithmic governance to govern algorithm","","",""
"2024","The rise of machine learning in the academic social sciences","","",""
"2024","The future of intelligent images: from simulation to stimulation","","",""
"2024","Pauses, parrots, and poor arguments: real-world constraints undermine recent calls for AI regulation","","",""
"2024","Trustworthy AI: AI made in Germany and Europe?","AbstractAs the capabilities of artificial intelligence (AI) continue to expand, concerns are also growing about the ethical and social consequences of unregulated development and, above all, use of AI systems in a wide range of social areas. It is therefore indisputable that the application of AI requires social standardization and regulation. For years, innovation policy measures and the most diverse activities of European and German institutions have been directed toward this goal. Under the label “Trustworthy AI” (TAI), a promise is formulated, according to which AI can meet criteria of transparency, legality, privacy, non-discrimination, and reliability. In this article, we ask what significance and scope the politically initiated concepts of TAI occupy in the current process of AI dynamics and to what extent they can stand for an independent, unique European or German development path of this technology.","",""
"2024","AI ethics inflation, Delphi and the restart of theory","","",""
"2024","Spot the bot","","",""
"2024","Algorithmic biases: caring about teens’ neurorights","","",""
"2024","The future of condition based monitoring: risks of operator removal on complex platforms","AbstractComplex systems are difficult to manage, operate and maintain. This is why we see teams of highly specialised engineers in industries such as aerospace, nuclear and subsurface. Condition based monitoring is also employed to maximise the efficiency of extensive maintenance programmes instead of using periodic maintenance. A level of automation is often required in such complex engineering platforms in order to effectively and safely manage them. Advances in Artificial Intelligence related technologies have offered greater levels of automation but this potentially pivots the weight of decision making away from the operator to the machine. Implementing AI or complex algorithms into a platform can mean that the Operators’ control over the system is diminished or removed altogether. For example, in the Boeing 737 Air Max Disaster, AI had been added to a platform and removed the operators’ control of the system. This meant that the operator could not then move outside the extremely reserved, algorithm defined, “envelope” of operation. This paper analyses the challenges of AI driven condition based monitoring where there is a potential to see similar consequences to those seen in control engineering. As the future of society becomes more about algorithm driven technology, it is prudent to ask, not only whether we should implement AI into complex systems, but how this can be achieved ethically and safely in order to reduce risk to life.","",""
"2024","Victim-blaming AIs","","",""
"2024","Body stakes: an existential ethics of care in living with biometrics and AI","AbstractThis article discusses the key existential stakes of implementing biometrics in human lifeworlds. In this pursuit, we offer a problematization and reinvention of central values often taken for granted within the “ethical turn” of AI development and discourse, such as autonomy, agency, privacy and integrity, as we revisit basic questions about what it means to be human and embodied. Within a framework of existential media studies, we introduce an existential ethics of care—through a conversation between existentialism, virtue ethics, a feminist ethics of care and post-humanist ethics—aiming to deepen and nuance our understanding of the human behind “human-centered” AI directives. The key argument is that biometrics implicates humans through unprecedented forms of objectification, through which the existential body—the relational, intimate and frail human being—is at risk. We interrogate these risks as they become visible at three sites where embodied humans are challenged by biometrics, and thus where the existential body is challenged by the biometric body. This occurs through reductionism (biometric passports nailing bodies to identities, removing human judgment and compromising agency at the AI border), enforced transparency (smart home assistants surveying human intimacies and invading intimate spaces in the bedroom) and the breaching of bodily integrity (chipping bodies to capture sensory data, challenging the very concept of bodily integrity through self-invasive biohacking). Our existential ethics of care is importantly not a solutionist list of principles or suggestions, but a manifesto for a way of thinking about the ethical challenges of living with biometrics in today’s world, by raising the right questions. We argue that a revitalized discussion of the basic existential stakes within human lived experience is needed and should serve as the foundation on which comprehensive frameworks can be built to address the complexities and prospects for ethical machines, responsible biometrics and AI. ","",""
"2024","Using artificial intelligence to prevent crime: implications for due process and criminal justice","","",""
"2024","Legal and ethical aspects of deploying artificial intelligence in climate-smart agriculture","","",""
"2024","The imitation game, the “child machine,” and the fathers of AI","","",""
"2024","Towards a bioinformational understanding of AI","","",""
"2024","Consilience and AI as technological prostheses","","",""
"2024","The state as a model for AI control and alignment","AbstractDebates about the development of artificial superintelligence and its potential threats to humanity tend to assume that such a system would be historically unprecedented, and that its behavior must be predicted from first principles. I argue that this is not true: we can analyze multiagent intelligent systems (the best candidates for practical superintelligence) by comparing them to states, which also unite heterogeneous intelligences to achieve superhuman goals. States provide a model for several problems discussed in the literature on superintelligence, such as principal-agent problems and Instrumental Convergence. Philosophical arguments about governance, therefore, provide possible solutions to these problems, or point out problems in previously suggested solutions. In particular, the liberal concept of checks and balances, and Hannah Arendt’s concept of legitimacy, describe how state behavior is constrained by the preferences of constituents that could also apply to artificial systems. However, they also point out ways in which present-day computational developments could destabilize the international order by reducing the number of decision-makers involved in state actions. Thus, interstate competition not only serves as a model for the behavior of dangerous computational intelligences but also as the impetus for their development.","",""
"2024","Correction to: From the ground up: developing a practical ethical methodology for integrating AI into industry","","",""
"2024","Robots as moral environments","","",""
"2024","Artificial intelligence and work: a critical review of recent research from the social sciences","AbstractThis review seeks to present a comprehensive picture of recent discussions in the social sciences of the anticipated impact of AI on the world of work. Issues covered include: technological unemployment, algorithmic management, platform work and the politics of AI work. The review identifies the major disciplinary and methodological perspectives on AI’s impact on work, and the obstacles they face in making predictions. Two parameters influencing the development and deployment of AI in the economy are highlighted: the capitalist imperative and nationalistic pressures.","",""
"2024","Might artificial intelligence become part of the person, and what are the key ethical and legal implications?","AbstractThis paper explores and ultimately affirms the surprising claim that artificial intelligence (AI) can become part of the person, in a robust sense, and examines three ethical and legal implications. The argument is based on a rich, legally inspired conception of persons as free and independent rightholders and objects of heightened protection, but it is construed so broadly that it should also apply to mainstream philosophical conceptions of personhood. The claim is exemplified by a specific technology, devices that connect human brains with computers and operate by AI-algorithms. Under philosophically reasonable and empirically realistic conditions, these devices and the AI running them become parts of the person, in the same way as arms, hearts, or mental capacities are. This transformation shall be called empersonification. It has normative and especially legal consequences because people have broader and stronger duties regarding other persons (and parts of them) than regarding things. Three consequences with practical implications are: (i) AI-devices cease to exist as independent legal entities and come to enjoy the special legal protection of persons; (ii) therefore, third parties such as manufacturers or authors of software lose (intellectual) property rights in device and software; (iii) persons become responsible for the outputs of the empersonified AI-devices to the same degree that they are for desires or intentions arising from the depths of their unconscious. More generally, empersonification marks a new step in the long history of human–machine interaction that deserves critical ethical reflection and calls for a stronger value-aligned development of these technologies.","",""
"2024","Guiding the way: a comprehensive examination of AI guidelines in global media","AbstractWith the increasing adoption of artificial intelligence (AI) technologies in the news industry, media organizations have begun publishing guidelines that aim to promote the responsible, ethical, and unbiased implementation of AI-based technologies. These guidelines are expected to serve journalists and media workers by establishing best practices and a framework that helps them navigate ever-evolving AI tools. Drawing on institutional theory and digital inequality concepts, this study analyzes 37 AI guidelines for media purposes in 17 countries. Our analysis reveals key thematic areas, such as transparency, accountability, fairness, privacy, and the preservation of journalistic values. Results highlight shared principles and best practices that emerge from these guidelines, including the importance of human oversight, explainability of AI systems, disclosure of automated content, and protection of user data. However, the geographical distribution of these guidelines, highlighting the dominance of Western nations, particularly North America and Europe, can further ongoing concerns about power asymmetries in AI adoption and consequently isomorphism outside these regions. Our results may serve as a resource for news organizations, policymakers, and stakeholders looking to navigate the complex AI development toward creating a more inclusive and equitable digital future for the media industry worldwide.","",""
"2024","Navigating technological shifts: worker perspectives on AI and emerging technologies impacting well-being","AbstractThis paper asks whether workers’ experience of working with new technologies and workers’ perceived threats of new technologies are associated with expected well-being. Using survey data for 25 OECD countries we find that both experiences of new technologies and threats of new technologies are associated with more concern about expected well-being. Controlling for the negative experiences of COVID-19 on workers and their macroeconomic outlook both mitigate these findings, but workers with negative experiences of working alongside and with new technologies still report lower expected well-being.","",""
"2024","Existence hacked: meaning, freedom, death, and intimacy in the age of AI","","",""
"2024","The approach to AI emergence from the standpoint of future contingents","","",""
"2024","Considerations for collecting data in Māori population for automatic detection of schizophrenia using natural language processing: a New Zealand experience","","",""
"2024","The goddess and her icon: body and mind in the era of artificial intelligence","","",""
"2024","Reflections on emerging HCI–AI research","","",""
"2024","AI and consciousness","","",""
"2024","Abundance of words versus poverty of mind: the hidden human costs co-created with LLMs","","",""
"2024","Correction to: An emerging AI mainstream: deepening our comparisons of AI frameworks through rhetorical analysis","","",""
"2024","Correction to: Moving beyond the mirror: relational and performative meaning making in human–robot communication","","",""
"2024","Could the destruction of a beloved robot be considered a hate crime? An exploration of the legal and social significance of robot love","AbstractIn the future, it is likely that we will form strong bonds of attachment and even develop love for social robots. Some of these loving relations will be, from the human’s perspective, as significant as a loving relationship that they might have had with another human. This means that, from the perspective of the loving human, the mindless destruction of their robot partner could be as devastating as the murder of another’s human partner. Yet, the loving partner of a robot has no recourse to legal action beyond the destruction of property and can see no way to prevent future people suffering the same devastating loss. On this basis, some have argued that such a scenario must surely motivate legal protection for social robots. In this paper, I argue that despite the devastating loss that would come from the destruction of one’s robot partner, love cannot itself be a reason for granting robot rights. However, although I argue against beloved robots having protective rights, I argue that the loss of a robot partner must be socially recognised as a form of bereavement if further secondary harms are to be avoided, and that, if certain conditions obtain, the destruction of a beloved robot could be criminalised as a hate crime.","",""
"2024","Losing the information war to adversarial AI","","",""
"2024","Public procurement of artificial intelligence systems: new risks and future proofing","","",""
"2024","Protecting society from AI misuse: when are restrictions on capabilities warranted?","","",""
"2024","Analysing and organising human communications for AI fairness assessment","AbstractAlgorithms used in the public sector, e.g., for allocating social benefits or predicting fraud, often require involvement from multiple stakeholders at various phases of the algorithm’s life-cycle. This paper focuses on the communication issues between diverse stakeholders that can lead to misinterpretation and misuse of algorithmic systems. Ethnographic research was conducted via 11 semi-structured interviews with practitioners working on algorithmic systems in the Dutch public sector, at local and national levels. With qualitative coding analysis, we identify key elements of the communication processes that underlie fairness-related human decisions. More specifically, we analyze the division of roles and tasks, the required skills, and the challenges perceived by diverse stakeholders. Three general patterns emerge from the coding analysis: (1) Policymakers, civil servants, and domain experts are less involved compared to developers throughout a system’s life-cycle. This leads to developers taking on the role of decision-maker and policy advisor, while they potentially miss the required skills. (2) End-users and policy-makers often lack the technical skills to interpret a system’s output, and rely on actors having a developer role for making decisions concerning fairness issues. (3) Citizens are structurally absent throughout a system’s life-cycle. This may lead to unbalanced fairness assessments that do not include key input from relevant stakeholders. We formalize the underlying communication issues within such networks of stakeholders and introduce the phase-actor-role-task-skill (PARTS) model. PARTS can both (i) represent the communication patterns identified in the interviews, and (ii) explicitly outline missing elements in communication patterns such as actors who miss skills or collaborators for their tasks, or tasks that miss qualified actors. The PARTS model can be extended to other use cases and used to analyze and design the human organizations responsible for assessing fairness in algorithmic systems. It can be further extended to explore communication issues in other use cases, design potential solutions, and organize accountability with a common vocabulary.","",""
"2024","Discussion of ethical decision mode for artificial intelligence","","",""
"2024","Eliza and the artist","","",""
"2024","Philosophy of technology for the lost age of freedom: a critical treatise on human essence and uncertain future","","",""
"2024","Don’t forget the upside of neurotechnology","","",""
"2024","The unwitting labourer: extracting humanness in AI training","AbstractMany modern digital products use Machine Learning (ML) to emulate human abilities, knowledge, and intellect. In order to achieve this goal, ML systems need the greatest possible quantity of training data to allow the Artificial Intelligence (AI) model to develop an understanding of “what it means to be human”. We propose that the processes by which companies collect this data are problematic, because they entail extractive practices that resemble labour exploitation. The article presents four case studies in which unwitting individuals contribute their humanness to develop AI training sets. By employing a post-Marxian framework, we then analyse the characteristic of these individuals and describe the elements of the capture-machine. Then, by describing and characterising the types of applications that are problematic, we set a foundation for defining and justifying interventions to address this form of labour exploitation.","",""
"2024","AI chatbots and liberal education","","",""
"2024","Florian Butollo and Sabine Nuss (Eds.) Marx and the Robots: Networked Production, AI, and Human Labour, London, UK: Pluto Press, 2022, 324 pp., $26.95 (Paperback), $99.00 (Hardcover)","","",""
"2024","What do we really know about the drivers of undeclared work? An evaluation of the current state of affairs using machine learning","","",""
"2024","Health professions students’ perceptions of artificial intelligence and its integration to health professions education and healthcare: a thematic analysis","","",""
"2024","Autonomous military systems beyond human control: putting an empirical perspective on value trade-offs for autonomous systems design in the military","AbstractThe question of human control is a key concern in autonomous military systems debates. Our research qualitatively and quantitatively investigates values and concerns of the general public, as they relate to autonomous military systems, with particular attention to the value of human control. Using participatory value evaluation (PVE), we consulted 1980 Australians about which values matter in relation to two specific technologies: an autonomous minesweeping submarine and an autonomous drone that can drop bombs. Based on value sensitive design, participants were tasked to enhance the systems with design features that can realize values. A restriction (limited budget) in each design task forced participants to make trade-offs between design options and the values that these options realize. Our results suggest that the ‘general public’ has diverse and nuanced stances on the question of human control over autonomous military systems. A third of participants that is opposed to autonomous military systems when asked directly, selected different combinations of design features realizing varying degrees of human control. Several contextual factors, technology-specific concerns, and certain values seemed to explain these different choices. Our research shows that a focus on human control might overlook other important values that the general public is concerned about, such as system reliability, verifiability, and retrievability.","",""
"2024","Mitigation measures for addressing gender bias in artificial intelligence within healthcare settings: a critical area of sociological inquiry","AbstractArtificial intelligence (AI) is often described as crucial for making healthcare safer and more efficient. However, some studies point in the opposite direction, demonstrating how biases in AI cause inequalities and discrimination. As a result, a growing body of research suggests mitigation measures to avoid gender bias. Typically, mitigation measures address various stakeholders such as the industry, academia, and policy-makers. To the author’s knowledge, these have not undergone sociological analysis. The article fills this gap and explores five examples of mitigation measures designed to counteract gender bias in AI within the healthcare sector. The rapid development of AI in healthcare plays a crucial role globally and must refrain from creating or reinforcing inequality and discrimination. In this effort, mitigation measures to avoid gender bias in AI in healthcare are central tools and, therefore, essential to explore from a social science perspective, including sociology. Sociologists have made valuable contributions to studying inequalities and disparities in AI. However, research has pointed out that more engagement is needed, specifically regarding bias in AI. While acknowledging the importance of these measures, the article suggests that they lack accountable agents for implementation and overlook potential implementation barriers such as resistance, power relations, and knowledge hierarchies. Recognizing the conditions where the mitigation measures are to be implemented is essential for understanding the potential challenges that may arise. Consequently, more studies are needed to explore the practical implementation of mitigation measures from a social science perspective and a systematic review of mitigation measures.","",""
"2024","Correction: The case for a broader approach to AI assurance: addressing “hidden” harms in the development of artificial intelligence","","",""
"2024","Freedom, AI and God: why being dominated by a friendly super-AI might not be so bad","AbstractOne response to the existential threat posed by a super-intelligent AI is to design it to be friendly to us. Some have argued that even if this were possible, the resulting AI would treat us as we do our pets. Sparrow (AI &amp; Soc. https://doi.org/10.1007/s00146-023-01698-x, 2023) argues that this would be a bad outcome, for such an AI would dominate us—resulting in our freedom being diminished (Pettit in Just freedom: A moral compass for a complex world. WW Norton &amp; Company, 2014). In this paper, I consider whether this would be such a bad outcome.","",""
"2024","Correction: “Game changer”: the AI advocacy discourse of 2023 in the US","","",""
"2024","Sentience, Vulcans, and zombies: the value of phenomenal consciousness","AbstractMany think that a specific aspect of phenomenal consciousness—valenced or affective experience—is essential to consciousness’s moral significance (valence sentientism). They hold that valenced experience is necessary for well-being, or moral status, or psychological intrinsic value (or all three). Some think that phenomenal consciousness generally is necessary for non-derivative moral significance (broad sentientism). Few think that consciousness is unnecessary for moral significance (non-necessitarianism). In this paper, I consider the prospects for these views. I first consider the prospects for valence sentientism in light of Vulcans, beings who are conscious but without affect or valence of any sort. I think Vulcans pressure us to accept broad sentientism. But I argue that a consideration of explanations for broad sentientism opens up possible explanations for non-necessitarianism about the moral significance of consciousness. That is, once one leans away from valence sentientism because of Vulcans, one should feel pressure to accept a view on which consciousness is not necessary for well-being, moral status, or psychological intrinsic value.","",""
"2024","Change management: artificial intelligence (AI) at the service of public administrations","","",""
"2024","Can ChatGPT be an author? Generative AI creative writing assistance and perceptions of authorship, creatorship, responsibility, and disclosure","AbstractThe increasing use of Generative AI raises many ethical, philosophical, and legal issues. A key issue here is uncertainties about how different degrees of Generative AI assistance in the production of text impacts assessments of the human authorship of that text. To explore this issue, we developed an experimental mixed methods survey study (N = 602) asking participants to reflect on a scenario of a human author receiving assistance to write a short novel as part of a 3 (high, medium, or low degrees of assistance) X 2 (human or AI assistant) factorial design. We found that, for a human author, the degree of assistance they receive matters for our assessments of their level of authorship, creatorship, and responsibility, but not what or who rendered that assistance, although it was more important to disclose human rather than AI assistance. However, in our assessments of the assisting agent, human assistants were viewed as warranting higher rates of authorship, creatorship, and responsibility, compared to AI assistants rendering the same level of support. These results help us to better understand emerging norms around collaborative human-AI generated text, with implications for other types of collaborative content creation.","",""
"2024","Algorithms and dehumanization: a definition and avoidance model","","",""
"2024","Intentionality gap and preter-intentionality in generative artificial intelligence","AbstractThe emergence of generative artificial intelligence, such as large language models and text-to-image models, has had a profound impact on society. The ability of these systems to simulate human capabilities such as text writing and image creation is radically redefining a wide range of practices, from artistic production to education. While there is no doubt that these innovations are beneficial to our lives, the pervasiveness of these technologies should not be underestimated, and raising increasingly pressing ethical questions that require a radical resemantization of certain notions traditionally ascribed to humans alone. Among these notions, that of technological intentionality plays a central role. With regard to this notion, this paper first aims to highlight what we propose to define in terms of the intentionality gap, whereby, insofar as, currently, (1) it is increasingly difficult to assign responsibility for the actions performed by AI systems to humans, as these systems are increasingly autonomous, and (2) it is increasingly complex to reconstruct the reasoning behind the results they produce as we move away from good old fashioned AI; it is now even more difficult to trace the intentionality of AI systems back to the intentions of the developers and end users. This gap between human and technological intentionality requires a revision of the concept of intentionality; to this end, we propose here to assign preter-intentional behavior to generative AI. We use this term to highlight how AI intentionality both incorporates and transcends human intentionality; i.e., it goes beyond (preter) human intentionality while being linked to it. To show the merits of this notion, we first rule out the possibility that such preter-intentionality is merely an unintended consequence and then explore its nature by comparing it with some paradigmatic notions of technological intentionality present in the wider debate on the moral (and technological) status of AI.","",""
"2024","Government regulation or industry self-regulation of AI? Investigating the relationships between uncertainty avoidance, people’s AI risk perceptions, and their regulatory preferences in Europe","AbstractArtificial Intelligence (AI) has the potential to influence people’s lives in various ways as it is increasingly integrated into important decision-making processes in key areas of society. While AI offers opportunities, it is also associated with risks. These risks have sparked debates about how AI should be regulated, whether through government regulation or industry self-regulation. AI-related risk perceptions can be shaped by national cultures, especially the cultural dimension of uncertainty avoidance. This raises the question of whether people in countries with higher levels of uncertainty avoidance might have different preferences regarding AI regulation than those with lower levels of uncertainty avoidance. Therefore, using Hofstede’s uncertainty avoidance scale and data from ten European countries (N = 7.855), this study investigates the relationships between uncertainty avoidance, people’s AI risk perceptions, and their regulatory preferences. The findings show that people in countries with higher levels of uncertainty avoidance are more likely to perceive AI risks in terms of a lack of accountability and responsibility. While people’s perceived AI risk of a lack of accountability exclusively drives their preferences for government regulation of AI, the perceived AI risk of a lack of responsibility can foster people’s requests for government regulation and/or industry self-regulation. This study contributes to a better understanding of which mechanisms shape people’s preferences for AI regulation.","",""
"2024","Rage against the authority machines: how to design artificial moral advisors for moral enhancement","AbstractThis paper aims to clear up the epistemology of learning morality from artificial moral advisors (AMAs). We start with a brief consideration of what counts as moral enhancement and consider the risk of deskilling raised by machines that offer moral advice. We then shift focus to the epistemology of moral advice and show when and under what conditions moral advice can lead to enhancement. We argue that people’s motivational dispositions are enhanced by inspiring people to act morally, instead of merely telling them how to act. Drawing upon these insights, we claim that if AMAs are to genuinely enhance people morally, they should be designed as inspiration and not authority machines. In the final section, we evaluate existing AMA models to shed light on which holds the most promise for helping to make users better moral agents.","",""
"2024","What’s wrong with “Death by Algorithm”? Classifying dignity-based objections to LAWS","","",""
"2024","The challenges and writing practices of communicating artificial intelligence and machine learning in an era of hype","","",""
"2024","Expert views about missing AI narratives: is there an AI story crisis?","AbstractStories are an important indicator of our vision of the future. In the case of artificial intelligence (AI), dominant stories are polarized between notions of threat and myopic solutionism. The central storytellers—big tech, popular media, and authors of science fiction—represent particular demographics and motivations. Many stories, and storytellers, are missing. This paper details the accounts of missing AI narratives by leading scholars from a range of disciplines interested in AI Futures. Participants focused on the gaps between dominant narratives and the untold stories of the capabilities, issues, and everyday realities of the technology. One participant proposed a “story crisis” in which these narratives compete to shape the public discourse on AI. Our findings indicate that dominant narratives distract and mislead public understandings and conceptions of AI. This suggests a need to pay closer attention to missing AI narratives. It is not simply about telling new stories, it is about listening to existing stories and asking what is wanted from AI. We call for realistic, nuanced, and inclusive stories, working with and for diverse voices, which consider (1) story-teller; (2) genre, and (3) communicative purpose. Such stories can then inspire the next generation of thinkers, technologists, and storytellers.","",""
"2024","On the need to develop nuanced measures assessing attitudes towards AI and AI literacy in representative large-scale samples","","",""
"2024","Is explainable AI responsible AI?","AbstractWhen artificial intelligence (AI) is used to make high-stakes decisions, some worry that this will create a morally troubling responsibility gap—that is, a situation in which nobody is morally responsible for the actions and outcomes that result. Since the responsibility gap might be thought to result from individuals lacking knowledge of the future behavior of AI systems, it can be and has been suggested that deploying explainable artificial intelligence (XAI) techniques will help us to avoid it. These techniques provide humans with certain forms of understanding of the systems in question. In this paper, I consider whether existing XAI techniques can indeed close the responsibility gap. I identify a number of significant limits to their ability to do so. Ensuring that responsibility for AI-assisted outcomes is maintained may require using different techniques in different circumstances, and potentially also developing new techniques that can avoid each of the issues identified.","",""
"2024","Artificial Intelligence and content analysis: the large language models (LLMs) and the automatized categorization","","",""
"2024","We need better images of AI and better conversations about AI","","",""
"2024","Correction to: The AI doctor will see you now: assessing the framing of AI in news coverage","","",""
"2024","The ABC of algorithmic aversion: not agent, but benefits and control determine the acceptance of automated decision-making","AbstractWhile algorithmic decision-making (ADM) is projected to increase exponentially in the coming decades, the academic debate on whether people are ready to accept, trust, and use ADM as opposed to human decision-making is ongoing. The current research aims at reconciling conflicting findings on ‘algorithmic aversion’ in the literature. It does so by investigating algorithmic aversion while controlling for two important characteristics that are often associated with ADM: increased benefits (monetary and accuracy) and decreased user control. Across three high-powered (Ntotal = 1192), preregistered 2 (agent: algorithm/human) × 2 (benefits: high/low) × 2 (control: user control/no control) between-subjects experiments, and two domains (finance and dating), the results were quite consistent: there is little evidence for a default aversion against algorithms and in favor of human decision makers. Instead, users accept or reject decisions and decisional agents based on their predicted benefits and the ability to exercise control over the decision.","",""
"2024","Lorenzo Magnani: Discoverability—the urgent need of an ecology of human creativity","","",""
"2024","Unveiling AI in the courtroom: exploring ChatGPT’s impact on judicial decision-making through a pilot Colombian case study","AbstractThis article examines the impact of ChatGPT on judicial reasoning, focusing on a recent Colombian case where the judge utilized ChatGPT in the decision-making process. The case, decided in January 2023, provides a unique “pilot case study” on the subject, as the judge, in the decision, openly referenced the questions he posed to ChatGPT and the responses of the system. The article explores the case’s implications, the  initial reactions to it, and its meaning and implications within the evolving Colombian legal landscape regarding the integration of technologies into judicial proceedings.","",""
"2024","Beyond the hype: ‘acceptable futures’ for AI and robotic technologies in healthcare","AbstractAI and robotic technologies attract much hype, including utopian and dystopian future visions of technologically driven provision in the health and care sectors. Based on 30 interviews with scientists, clinicians and other stakeholders in the UK, Europe, USA, Australia, and New Zealand, this paper interrogates how those engaged in developing and using AI and robotic applications in health and care characterize their future promise, potential and challenges. We explore the ways in which these professionals articulate and navigate a range of high and low expectations, and promissory and cautionary future visions, around AI and robotic technologies. We argue that, through these articulations and navigations, they construct their own perceptions of socially and ethically ‘acceptable futures’ framed by an ‘ethics of expectations.’ This imbues the envisioned futures with a normative character, articulated in relation to the present context. We build on existing work in the sociology of expectations, aiming to contribute towards better understanding of how technoscientific expectations are navigated and managed by professionals. This is particularly timely since the COVID-19 pandemic gave further momentum to these technologies.","",""
"2024","Mitigating implicit and explicit bias in structured data without sacrificing accuracy in pattern classification","AbstractUsing biased data to train Artificial Intelligence (AI) algorithms will lead to biased decisions, discriminating against certain groups or individuals. Bias can be explicit (one or several protected features directly influence the decisions) or implicit (one or several protected features indirectly influence the decisions). Unsurprisingly, biased patterns are difficult to detect and mitigate. This paper investigates the extent to which explicit and implicit against one or more protected features in structured classification data sets can be mitigated simultaneously while retaining the data’s discriminatory power. The main contribution of this paper concerns an optimization-based bias mitigation method that reweights the training instances. The algorithm operates with numerical and nominal data and can mitigate implicit and explicit bias against several protected features simultaneously. The trade-off between bias mitigation and accuracy loss can be controlled using parameters in the objective function. The numerical simulations using real-world data sets show a reduction of up to 77% of implicit bias and a complete removal of explicit bias against protected features at no cost of accuracy of a wrapper classifier trained on the data. Overall, the proposed method outperforms the state-of-the-art bias mitigation methods for the selected data sets.","",""
"2024","The ethical dilemma of AI in hiring","","",""
"2024","AI-powered recommender systems and the preservation of personal autonomy","AbstractRecommender Systems (RecSys) have been around since the early days of the Internet, helping users navigate the vast ocean of information and the increasingly available options that have been available for us ever since. The range of tasks for which one could use a RecSys is expanding as the technical capabilities grow, with the disruption of Machine Learning representing a tipping point in this domain, as in many others. However, the increase of the technical capabilities of AI-powered RecSys did not come with a thorough consideration of their ethical implications and, despite being a well-established technical domain, the potential impacts of RecSys on their users are still under-assessed. This paper aims at filling this gap in regards to one of the main impacts of RecSys: personal autonomy. We first describe how technology can affect human values and a suitable methodology to identify these effects and mitigate potential harms: Value Sensitive Design (VSD). We use VSD to carry out a conceptual investigation of personal autonomy in the context of a generic RecSys and draw on a nuanced account of procedural autonomy to focus on two components: competence and authenticity. We provide the results of our inquiry as a value hierarchy and apply it to the design of a speculative RecSys as an example.","",""
"2024","AI is a ruler not a helper","","",""
"2024","The Turing test is a joke","","",""
"2024","The paradoxical transparency of opaque machine learning","","",""
"2024","What is artificial about artificial intelligence? A provocation on a problematic prefix","","",""
"2024","Building AI literacy for humanities students: teaching beyond generative AI","","",""
"2024","A review of Robots Won’t Save Japan: An Ethnography of Eldercare Automation by James Wright","","",""
"2024","Manifestations of xenophobia in AI systems","AbstractXenophobia is one of the key drivers of marginalisation, discrimination, and conflict, yet many prominent machine learning fairness frameworks fail to comprehensively measure or mitigate the resulting xenophobic harms. Here we aim to bridge this conceptual gap and help facilitate safe and ethical design of artificial intelligence (AI) solutions. We ground our analysis of the impact of xenophobia by first identifying distinct types of xenophobic harms, and then applying this framework across a number of prominent AI application domains, reviewing the potential interplay between AI and xenophobia on social media and recommendation systems, healthcare, immigration, employment, as well as biases in large pre-trained models. These help inform our recommendations towards an inclusive, xenophilic design of future AI systems.","",""
"2024","AI ethics as subordinated innovation network","AbstractAI ethics is proposed, by the Big Tech companies which lead AI research and development, as the cure for diverse social problems posed by the commercialization of data-intensive technologies. It aims to reconcile capitalist AI production with ethics. However, AI ethics is itself now the subject of wide criticism; most notably, it is accused of being no more than “ethics washing” a cynical means of dissimulation for Big Tech, while it continues its business operations unchanged. This paper aims to critically assess, and go beyond the ethics washing thesis. I argue that AI ethics is indeed ethics washing, but not only that. It has a more significant economic function for Big Tech. To make this argument I draw on the theory of intellectual monopoly capital. I argue that ethics washing is better understood as a subordinated innovation network: a dispersed network of contributors beyond Big Tech’s formal employment whose research is indirectly planned by Big Tech, which also appropriates its results. These results are not intended to render AI more ethical, but rather to advance the business processes of data-intensive capital. Because the parameters of AI ethics are indirectly set in advance by Big tech, the ostensible goal that AI ethics sets for itself—to resolve the contradiction between business and ethics—is in fact insoluble. I demonstrate this via an analysis of the latest trend in AI ethics: the operationalization of ethical principles.","",""
"2024","ChatGPT as imperfect rhetorical tool in public policy","","",""
"2024","Do we want AI judges? The acceptance of AI judges’ judicial decision-making on moral foundations","","",""
"2024","Trust, artificial intelligence and software practitioners: an interdisciplinary agenda","AbstractTrust and trustworthiness are central concepts in contemporary discussions about the ethics of and qualities associated with artificial intelligence (AI) and the relationships between people, organisations and AI. In this article we develop an interdisciplinary approach, using socio-technical software engineering and design anthropological approaches, to investigate how trust and trustworthiness concepts are articulated and performed by AI software practitioners. We examine how trust and trustworthiness are defined in relation to AI across these disciplines, and investigate how AI, trust and trustworthiness are conceptualised and experienced through an ethnographic study of the work practices of nine practitioners in the software industry. We present key implications of our findings for the generation of trust and trustworthiness and for the training and education of future software practitioners.","",""
"2024","An AI ethics ‘David and Goliath’: value conflicts between large tech companies and their employees","AbstractArtificial intelligence ethics requires a united approach from policymakers, AI companies, and individuals, in the development, deployment, and use of these technologies. However, sometimes discussions can become fragmented because of the different levels of governance (Schmitt in AI Ethics 1–12, 2021) or because of different values, stakeholders, and actors involved (Ryan and Stahl in J Inf Commun Ethics Soc 19:61–86, 2021). Recently, these conflicts became very visible, with such examples as the dismissal of AI ethics researcher Dr. Timnit Gebru from Google and the resignation of whistle-blower Frances Haugen from Facebook. Underpinning each debacle was a conflict between the organisation’s economic and business interests and the morals of their employees. This paper will examine tensions between the ethics of AI organisations and the values of their employees, by providing an exploration of the AI ethics literature in this area, and a qualitative analysis of three workshops with AI developers and practitioners. Common ethical and social tensions (such as power asymmetries, mistrust, societal risks, harms, and lack of transparency) will be discussed, along with proposals on how to avoid or reduce these conflicts in practice (e.g., building trust, fair allocation of responsibility, protecting employees’ autonomy, and encouraging ethical training and practice). Altogether, we suggest the following steps to help reduce ethical issues within AI organisations: improved and diverse ethics education and training within businesses; internal and external ethics auditing; the establishment of AI ethics ombudsmen, AI ethics review committees and an AI ethics watchdog; as well as access to trustworthy AI ethics whistle-blower organisations.","",""
"2024","Entangled AI: artificial intelligence that serves the future","AbstractWhile debate is heating up regarding the development of AI and its perceived impacts on human society, policymaking is struggling to catch up with the demand to exercise some regulatory control over its rapid advancement. This paper aims to introduce the concept of entangled AI that emerged from participatory backcasting research with an AI expert panel. The concept of entanglement has been adapted from quantum physics to effectively capture the envisioned form of artificial intelligence in which a strong interconnectedness between AI, humans, society, and nature is reflected. Entanglement assumes that AI should serve nature, social well-being, justice, and the resilience of this intertwined network simultaneously and promote a dynamic balance among these factors. This approach allows us to understand the pervasive role of this technology and the scope of human agency in its development. The study shows how such concepts seem to transcend the dominant discourses related to expectations, technological determinism, and humanism. An additional aim of this paper is to demonstrate how backcasting can contribute to generating useful understandings of the future of AI and fruitful insights for policymaking.","",""
"2024","AI: artistic collaborator?","","",""
"2024","Gender preferences for robots and gender equality orientation in communication situations","AbstractThe individual physical appearances of robots are considered significant, similar to the way that those of humans are. We investigated whether users prefer robots with male or female physical appearances for use in daily communication situations and whether egalitarian gender role attitudes are related to this preference. One thousand adult men and women aged 20–60 participated in the questionnaire survey. The results of our study showed that in most situations and for most subjects, “males” was not selected and “females” or “neither” was selected. Moreover, the number of respondents who chose “either” was higher than that who chose “female.” Furthermore, we examined the relationship between gender and gender preference and confirmed that the effect of gender on the gender preference for a robot weakened when the human factor was eliminated. In addition, in some situations for android-type robots and in all situations for machine-type robots, equality orientation in gender role attitudes was shown to be higher for people who were not specific about their gender preferences. It is concluded that there is no need to introduce a robot that specifies its gender. Robots with a gender-neutral appearance might be more appropriate for applications requiring complex human–robot interaction and help avoid reproducing a gender bias.","",""
"2024","Artificial intelligence as the new fire and its geopolitics","","",""
"2024","AI ethics discourse: a call to embrace complexity, interdisciplinarity, and epistemic humility","","",""
"2024","Accountability in artificial intelligence: what it is and how it works","AbstractAccountability is a cornerstone of the governance of artificial intelligence (AI). However, it is often defined too imprecisely because its multifaceted nature and the sociotechnical structure of AI systems imply a variety of values, practices, and measures to which accountability in AI can refer. We address this lack of clarity by defining accountability in terms of answerability, identifying three conditions of possibility (authority recognition, interrogation, and limitation of power), and an architecture of seven features (context, range, agent, forum, standards, process, and implications). We analyze this architecture through four accountability goals (compliance, report, oversight, and enforcement). We argue that these goals are often complementary and that policy-makers emphasize or prioritize some over others depending on the proactive or reactive use of accountability and the missions of AI governance.","",""
"2024","ChatGPT: deconstructing the debate and moving it forward","AbstractLarge language models such as ChatGPT enable users to automatically produce text but also raise ethical concerns, for example about authorship and deception. This paper analyses and discusses some key philosophical assumptions in these debates, in particular assumptions about authorship and language and—our focus—the use of the appearance/reality distinction. We show that there are alternative views of what goes on with ChatGPT that do not rely on this distinction. For this purpose, we deploy the two phased approach of deconstruction and relate our finds to questions regarding authorship and language in the humanities. We also identify and respond to two common counter-objections in order to show the ethical appeal and practical use of our proposal.","",""
"2024","Using rhetorical strategies to design prompts: a human-in-the-loop approach to make AI useful","AbstractThe growing capabilities of artificial intelligence (AI) word processing models have demonstrated exceptional potential to impact language related tasks and functions. Their fast pace of adoption and probable effect has also given rise to controversy within certain fields. Models, such as GPT-3, are a particular concern for professionals engaged in writing, particularly as their engagement with these technologies is limited due to lack of ability to control their output. Most efforts to maximize and control output rely on a process known as prompt engineering, the construction and modification of the inputted prompt with expectation for certain outputted or desired text. Consequently, prompt engineering has emerged as an important consideration for research and practice. Previous conceptions of prompt engineering have largely focused on technical and logistic modifications to the back-end processing, remaining inaccessible and, still, limited for most users. In this paper, we look to the technical communication field and its methods of text generation—the rhetorical situation—to conceptualize prompt engineering in a more comprehensible way for its users by considering the context and rhetoric. We introduce a framework, consisting of a formula, to prompt engineering, which demands all components of the rhetorical situation be present in the inputted prompt. We present discussions on the future of AI writing models and their use in both professional and educational settings. Ultimately, this discussion and its findings aim to provide a means of integrating agency and writer-centric methods to AI writing tools to advance a more human-in-the-loop approach. As the use of generative AI and especially NLP-based technologies become common across societal functions, the use of prompt engineering will play a crucial role not just in adoption of the technology, but also its productive and responsible use.","",""
"2024","Keep trusting! A plea for the notion of Trustworthy AI","AbstractA lot of attention has recently been devoted to the notion of Trustworthy AI (TAI). However, the very applicability of the notions of trust and trustworthiness to AI systems has been called into question. A purely epistemic account of trust can hardly ground the distinction between trustworthy and merely reliable AI, while it has been argued that insisting on the importance of the trustee’s motivations and goodwill makes the notion of TAI a categorical error. After providing an overview of the debate, we contend that the prevailing views on trust and AI fail to account for the ethically relevant and value-laden aspects of the design and use of AI systems, and we propose an understanding of the notion of TAI that explicitly aims at capturing these aspects. The problems involved in applying trust and trustworthiness to AI systems are overcome by keeping apart trust in AI systems and interpersonal trust. These notions share a conceptual core but should be treated as distinct ones.","",""
"2024","Recipient design in human–robot interaction: the emergent assessment of a robot’s competence","AbstractPeople meeting a robot for the first time do not know what it is capable of and therefore how to interact with it—what actions to produce, and how to produce them. Despite social robotics’ long-standing interest in the effects of robots’ appearance and conduct on users, and efforts to identify factors likely to improve human–robot interaction, little attention has been paid to how participants evaluate their robotic partner in the unfolding of actual interactions. This paper draws from qualitative analyses of video-recorded interactions between a robot and groups of participants, in the framework of ethnomethodology and conversation analysis. We analyse the particular ways in which participants shape their embodied actions, how they can reproduce a prior action that failed to obtain a response from the robot; and how they explore the robot’s embodied nature. We find a set of recurrent methods or practices, showing that robot-recipient design displays not only participants’ initial assumptions about the robot’s competences, but also more importantly perhaps their continuous assessment of the robot’s behaviour, and their attempts to adapt to it. Participants locally produce and constantly revise their understanding of the robot as a more or less competent co-participant, drawing from its past, current, and projected conduct and responsiveness. We discuss the implications of these findings for research in robotics and human–robot interactions, and the value of the approach to shed new light on old questions by paying attention to the quality of gesture and the sequential organisation of interaction.","",""
"2024","Gender bias in visual generative artificial intelligence systems and the socialization of AI","AbstractSubstantial research over the last ten years has indicated that many generative artificial intelligence systems (“GAI”) have the potential to produce biased results, particularly with respect to gender. This potential for bias has grown progressively more important in recent years as GAI has become increasingly integrated in multiple critical sectors, such as healthcare, consumer lending, and employment. While much of the study of gender bias in popular GAI systems is focused on text-based GAI such as OpenAI’s ChatGPT and Google’s Gemini (formerly Bard), this article describes the results of a confirmatory experiment of gender bias in visual GAI systems. The authors argue that the potential for gender bias in visual GAI systems is potentially more troubling than bias in textual GAI because of the superior memorability of images and the capacity for emotional communication that images represent. They go on to offer four potential approaches to gender bias in visual GAI based on the roles visual GAI could play in modern society. The article concludes with a discussion of how dominant societal values could influence a choice between those four potential approaches to gender bias in visual GAI and some suggestions for further research.","",""
"2024","Emotional AI and the future of wellbeing in the post-pandemic workplace","","",""
"2024","Adopting AI: how familiarity breeds both trust and contempt","","",""
"2024","Towards a decolonial I in AI: mapping the pervasive effects of artificial intelligence on the art ecosystem","","",""
"2024","Taking AI risks seriously: a new assessment model for the AI Act","AbstractThe EU Artificial Intelligence Act (AIA) defines four risk categories: unacceptable, high, limited, and minimal. However, as these categories statically depend on broad fields of application of AI, the risk magnitude may be wrongly estimated, and the AIA may not be enforced effectively. This problem is particularly challenging when it comes to regulating general-purpose AI (GPAI), which has versatile and often unpredictable applications. Recent amendments to the compromise text, though introducing context-specific assessments, remain insufficient. To address this, we propose applying the risk categories to specific AI scenarios, rather than solely to fields of application, using a risk assessment model that integrates the AIA with the risk approach arising from the Intergovernmental Panel on Climate Change (IPCC) and related literature. This integrated model enables the estimation of AI risk magnitude  by considering the interaction between (a) risk determinants, (b) individual drivers of determinants, and (c) multiple risk types. We illustrate this model using large language models (LLMs) as an example.","",""
"2024","Technology, the latent conqueror: an experimental study on the perception and awareness of technological determinism featuring select sci-fi films and AI literature","","",""
"2024","Towards a decolonial I in AI &amp; Society","","",""
"2024","Two-stage approach to solve ethical morality problem in self-driving cars","","",""
"2024","ChatGPT is not OK! That’s not (just) because it lies","","",""
"2024","“Legal personality” of artificial intelligence: methodological problems of scientific reasoning by Ukrainian and EU experts","","",""
"2024","The age of the algorithmic society a Girardian analysis of mimesis, rivalry, and identity in the age of artificial intelligence","","",""
"2024","Correction to: Investing in AI for social good: an analysis of European national strategies","","",""
"2024","The ethics of ex-bots","","",""
"2024","Empathy and AI: cognitive empathy or emotional (affective) empathy?","","",""
"2024","Correction: Review of Robot Rights by David J. Gunkel","","",""
"2024","Not “what”, but “where is creativity?”: towards a relational-materialist approach to generative AI","AbstractThe recent emergence of generative AI software as viable tools for use in the cultural and creative industries has sparked debates about the potential for “creativity” to be automated and “augmented” by algorithmic machines. Such discussions, however, begin from an ontological position, attempting to define creativity by either falling prey to universalism (i.e. “creativity is X”) or reductionism (i.e. “only humans can be truly creative” or “human creativity will be fully replaced by creative machines”). Furthermore, such an approach evades addressing the real and material impacts of AI on creative labour in these industries. This article thus offers more expansive methodological and conceptual approaches to the recent hype on generative AI. By combining (Csikszentmihalyi, The systems model of creativity, Springer, Dordrecht, 2014) systems view of creativity, in which we emphasise the shift from “what” to “where” is creativity, with (Lievrouw, Media technologies, The MIT Press, 2014) relational-materialist theory of “mediation”, we argue that the study of “creativity” in the context of generative AI must be attentive to the interactions between technologies, practices, and social arrangements. When exploring the relational space between these elements, three core concepts become pertinent: creative labour, automation, and distributed agency. Critiquing “creativity” through these conceptual lenses allows us to re-situate the use of generative AI within discourses of labour in post-industrial capitalism and brings us to a conceptualisation of creativity that privileges neither the human user nor machine algorithm but instead emphasises a relational and distributed form of agency.","",""
"2024","A new standard for accident simulations for self-driving vehicles: Can we use Waymo’s results from accident simulations?","AbstractRecent simulations by Scanlon et al. showed seemingly spectacular results for the Waymo self-driving vehicle in simulations of real accident situations. In this paper, it is argued that the selection criteria for accident situations must be modified in accordance with the relevant policy alternatives. While Scanlon et al. compare Waymo with old human-driven vehicles, it is argued here that the relevant policy question is whether we ought to use self-driven vehicles or human-driven vehicles in the future, which means that we need to consider whether other technological solutions, which are available but not broadly used in human-driven vehicles, could result in human-driven vehicles managing to avoid the same accidents. In this article, a proposal for a new standard of selection criteria is made.","",""
"2024","Ethical aspects of AI robots for agri-food; a relational approach based on four case studies","AbstractThese last years, the development of AI robots for agriculture, livestock farming and food processing industries is rapidly increasing. These robots are expected to help produce and deliver food more efficiently for a growing human population, but they also raise societal and ethical questions. As the type of questions raised by these AI robots in society have been rarely empirically explored, we engaged in four case studies focussing on four types of AI robots for agri-food ‘in the making’: manure collectors, weeding robots, harvesting robots and food processing robots which select and package fruits, vegetables and meats. Based on qualitative interviews with 33 experts engaged in the development or implementation of these four types of robots, this article provides a broad and varied exploration of the values that play a role in their evaluation and the ethical questions that they raise. Compared to the recently published literature reviews mapping the ethical questions related to AI robots in agri-food, we conclude that stakeholders in our case studies primarily adopt a relational perspective to the value of AI robots and to finding a solution to the ethical questions. Building on our findings we suggest it is best to seek a distribution of tasks between human beings and robots in agri-food, which helps to realize the most acceptable, good or just collaboration between them in food production or processing that contributes to realizing societal goals and help to respond to the 21 century challenges.","",""
"2024","Integration of a social robot and gamification in adult learning and effects on motivation, engagement and performance","AbstractLearning is a central component of human life and essential for personal development. Therefore, utilizing new technologies in the learning context and exploring their combined potential are considered essential to support self-directed learning in a digital age. A learning environment can be expanded by various technical and content-related aspects. Gamification in the form of elements from video games offers a potential concept to support the learning process. This can be supplemented by technology-supported learning. While the use of tablets is already widespread in the learning context, the integration of a social robot can provide new perspectives on the learning process. However, simply adding new technologies such as social robots or gamification to existing systems may not automatically result in a better learning environment. In the present study, game elements as well as a social robot were integrated separately and conjointly into a learning environment for basic Spanish skills, with a follow-up on retained knowledge. This allowed us to investigate the respective and combined effects of both expansions on motivation, engagement and learning effect. This approach should provide insights into the integration of both additions in an adult learning context. We found that the additions of game elements and the robot did not significantly improve learning, engagement or motivation. Based on these results and a literature review, we outline relevant factors for meaningful integration of gamification and social robots in learning environments in adult learning.","",""
"2024","The epistemic impossibility of an artificial intelligence take-over of democracy","AbstractThose who claim, whether with fear or with hope, that algorithmic governance can control politics or the whole political process or that artificial intelligence is capable of taking charge of or wrecking democracy, recognize that this is not yet possible with our current technological capabilities but that it could come about in the future if we had better quality data or more powerful computational tools. Those who fear or desire this algorithmic suppression of democracy assume that something similar will be possible someday and that it is only a question of technological progress. If that were the case, no limits would be insurmountable on principle. I want to challenge that conception with a limit that is less normative than epistemological; there are things that artificial intelligence cannot do, because it is unable to do them, not because it should not do them, and this is particularly apparent in politics, which is a peculiar decision-making realm. Machines and people take decisions in a very different fashion. Human beings are particularly gifted at one type of situation and very clumsy in others. The part of politics that is, strictly speaking, political is where this contrast and our greatest aptitude are most apparent. If that is the case, as I believe, then the possibility that democracy will one day be taken over by artificial intelligence is, as a fear or as a desire, manifestly exaggerated. The corresponding counterpart to this is: if the fear that democracy could disappear at the hands of artificial intelligence is not realistic, then we should not expect exorbitant benefits from it either. For epistemic reasons that I will explain, it does not seem likely that artificial intelligence is capable of taking over political logic.","",""
"2024","Ethical AI does not have to be like finding a black cat in a dark room","","",""
"2024","Meaning–thinking–AI","AbstractThis paper makes the case for a sharper terminology regarding AIs cognitive abilities. In arguing that thinking requires more than content production, I offer a definition of meaning drawing on a clear distinction between living and machine intelligence. A pivotal argument is the re-use of the Turing Test (TT) for understanding which theories of meaning and consciousness are no longer plausible—because they have been reproduced by software without thereby gaining conscious experience. In following the few theories that have not (yet) failed this reversed Turing Test (RTT), the focus turns towards rethinking the human condition in times of AI along the lines of three questions: What if a machine developed consciousness? What if AI proceeded without developing a consciousness? What, if machinic and human intelligence merged? These three questions in the end lead to examining three related possible futures of humanism as now determined by the relation between Human Intelligence and AI.","",""
"2024","Negotiation of dominant AI narratives in museum exhibitions","AbstractNarratives of artificial intelligence frame public perceptions and expectations, and have a performative role, potentially leading to increased attention and resource allocation, acceptance of AI, or resistance to the technology. However, research on AI narratives frequently produces generalized and decontextualized accounts. This paper argues for closer examination of the specific processes that shape AI narratives in particular contexts. To explore this, nine AI-related exhibitions held in German museums from 2022 to 2023 were analyzed. The study draws on interviews with curatorial teams and exhibition materials to investigate what narratives of AI museums are formed and how those relate to the dominant AI narratives described in previous research. It is shown that curators are critical of dominant AI narratives and aim to deconstruct common myths and present more nuanced portrayals of AI. Yet, they also face challenges in balancing this critical stance with the need for creating engaging, accessible exhibitions. The analysis reveals that while some curatorial strategies successfully counter AI stereotypes, others fail or inadvertently reproduce mainstream imaginaries. Curators discover that avoiding problematic understandings of AI in the way they had initially intended is challenging or even impossible. In other cases, when elements of mainstream AI discourse resurface in museum exhibits despite the critical stance of the curators, they remain unexamined, or are regarded as insignificant and inconsequential.","",""
"2024","Balancing AI and academic integrity: what are the positions of academic publishers and universities?","","",""
"2024","Personal AI, deception, and the problem of emotional bubbles","AbstractPersonal AI is a new type of AI companion, distinct from the prevailing forms of AI companionship. Instead of playing a narrow and well-defined social role, like friend, lover, caretaker, or colleague, with a set of pre-determined responses and behaviors, Personal AI is engineered to tailor itself to the user, including learning to mirror the user’s unique emotional language and attitudes. This paper identifies two issues with Personal AI. First, like other AI companions, it is deceptive about the presence of their emotions, which undermines the moral value of companionship. Second, Personal AI leads to a distinctly new form of deception concerning the origins of its emotions. Its emotional attitudes appear to belong to it, when in fact they are only reflections of the user. This results in what I dub “emotional bubbles”—the false impression that personal emotions are externally validated—which have at least two troubling implications. First, emotional bubbles prevent us from encountering emotional attitudes that differ from our own, which is likely to cripple emotional growth and the ability to form diverse social and emotional relationships. Second, if we assume, as some philosophers claim, that shared emotions are constitutive of shared values, it follows that Personal AI subverts joint moral deliberation. Users believe their personal values are externally validated, when they are only validated by themselves. Because of the absence of technovirtues able to handle this problem, I suggest that we proceed very cautiously with the development and marketing of Personal AI.","",""
"2024","AI and mental health: evaluating supervised machine learning models trained on diagnostic classifications","AbstractMachine learning (ML) has emerged as a promising tool in psychiatry, revolutionising diagnostic processes and patient outcomes. In this paper, I argue that while ML studies show promising initial results, their application in mimicking clinician-based judgements presents inherent limitations (Shatte et al. in Psychol Med 49:1426–1448. https://doi.org/10.1017/S0033291719000151, 2019). Most models still rely on DSM (the Diagnostic and Statistical Manual of Mental Disorders) categories, known for their heterogeneity and low predictive value. DSM's descriptive nature limits the validity of psychiatric diagnoses, which leads to overdiagnosis, comorbidity, and low remission rates. The application in psychiatry highlights the limitations of supervised ML techniques. Supervised ML models inherit the validity issues of their training data set. When the model's outcome is a DSM classification, this can never be more valid or predictive than the clinician’s judgement. Therefore, I argue that these models have little added value to the patient. Moreover, the lack of known underlying causal pathways in psychiatric disorders prevents validating ML models based on such classifications. As such, I argue that high accuracy in these models is misleading when it is understood as validating the classification. In conclusion, these models will not will not offer any real benefit to patient outcomes. I propose a shift in focus, advocating for ML models to prioritise improving the predictability of prognosis, treatment selection, and prevention. Therefore, data selection and outcome variables should be geared towards this transdiagnostic goal. This way, ML can be leveraged to better support clinicians in personalised treatment strategies for mental health patients.","",""
"2024","Intelligence in animals, humans and machines: a heliocentric view of intelligence?","","",""
"2024","Artificial Intelligence and the future of work","","",""
"2024","There is no “AI” in “Freedom” or in “God”","","",""
"2024","“To us, it is still foreign”: AI and the disabled in the Global South","","",""
"2024","Drawing the full picture on diverging findings: adjusting the view on the perception of art created by artificial intelligence","AbstractAI is becoming increasingly prevalent in creative fields that were thought to be exclusively human. Thus, it is non-surprising that a negative bias toward AI-generated artwork has been proclaimed. However, results are mixed. Studies that have presented AI-generated and human-created images simultaneously have detected a bias, but most studies in which participants saw either AI-generated or human-created images have not. Therefore, we propose that the bias arises foremost in a competitive situation between AI and humans. In a sample of N = 952 participants, we show that different evaluations emerge only when AI-generated and human-created pieces of art are presented simultaneously. Importantly, we demonstrate that AI art is not devalued, but rather, human art is upvalued, indicating the existence of a positive bias toward humans, rather than a negative bias. Further, we show that attitudes toward AI and empathy partially explain the different valuations of AI and human art in competitive situations.","",""
"2024","AGI crimes? The role of criminal law in mitigating existential risks posed by artificial general intelligence","AbstractThe recent developments in applications of artificial intelligence bring back discussion about risks posed by AI. Among immediate risks that need to be tackled here and now, there is also a possible problem of existential threats related to Artificial General Intelligence (AGI). There is a discussion on how to mitigate those risks by appropriate regulations. It seems that one commonly accepted assumption is that the problem is global, and thus, it needs to be tackled first of all on an international level. In this paper, I argue that national criminal laws should also be considered one of the possible regulatory tools for mitigating threats posed by AGI. I propose to enact AGI crimes that complement the varieties of legal responses to existential risks that might motivate and speed up further regulatory changes.","",""
"2024","Fight fire with fire: why not be more tolerant of ChatGPT in academic writing?","","",""
"2024","Americans’ views of artificial intelligence: identifying and measuring aversion","AbstractThis study explores the phenomenon of artificial intelligence (AI) aversion within the context of public policy, building on prior research on algorithmic aversion. I aim to establish a clear conceptual distinction between algorithms and AI in the public’s perception and develop a robust metric for assessing AI aversion. Utilizing a national survey, I employed affective imagery testing to compare Americans emotional responses towards AI, algorithms, and advanced technology. The findings reveal that AI elicits significantly more negative emotional responses than the other two, indicating its unique position in public perception. I then construct the Artificial Intelligence Aversion Index (AIAI) based on responses to policy-related vignettes. Regression analyses showed a strong negative relationship between the AIAI and public support for both current and future AI applications within public policy, with aversion more pronounced towards potential future uses. These insights underscore the importance of understanding public sentiment towards AI to inform policymaking as well as helping to establish a framework by which to evaluate aversion levels.","",""
"2024","Could a robot feel pain?","AbstractQuestions about robots feeling pain are important because the experience of pain implies sentience and the ability to suffer. Pain is not the same as nociception, a reflex response to an aversive stimulus. The experience of pain in others has to be inferred. Danaher’s (Sci Eng Ethics 26(4):2023–2049, 2020. https://doi.org/10.1007/s11948-019-00119-x) ‘ethical behaviourist’ account claims that if a robot behaves in the same way as an animal that is recognised to have moral status, then its moral status should also be assumed. Similarly, under a precautionary approach (Sebo in Harvard Rev Philos 25:51–70, 2018. https://doi.org/10.5840/harvardreview20185913), entities from foetuses to plants and robots are given the benefit of the doubt and assumed to be sentient. However, there is a growing consensus about the scientific criteria used to indicate pain and the ability to suffer in animals (Birch in Anim Sentience, 2017. https://doi.org/10.51291/2377-7478.1200; Sneddon et al. in Anim Behav 97:201–212, 2014. https://doi.org/10.1016/j.anbehav.2014.09.007). These include the presence of a central nervous system, changed behaviour in response to pain, and the effects of analgesic pain relief. Few of these criteria are met by robots, and there are risks to assuming that they are sentient and capable of suffering pain. Since robots lack nervous systems and living bodies there is little reason to believe that future robots capable of feeling pain could (or should) be developed.","",""
"2024","AI at work: understanding its uses and consequences on work activities and organization in radiology","","",""
"2024","Expert responsibility in AI development","AbstractThe purpose of this paper is to discuss the responsibility of AI experts for guiding the development of AI in a desirable direction. More specifically, the aim is to answer the following research question: To what extent are AI experts responsible in a forward-looking way for effects of AI technology that go beyond the immediate concerns of the programmer or designer? AI experts, in this paper conceptualised as experts regarding the technological aspects of AI, have knowledge and control of AI technology that non-experts do not have. Drawing on responsibility theory, theories of the policy process, and critical algorithm studies, we discuss to what extent this capacity, and the positions that these experts have to influence the AI development, make AI experts responsible in a forward-looking sense for consequences of the use of AI technology. We conclude that, as a professional collective, AI experts, to some extent, are responsible in a forward-looking sense for consequences of use of AI technology that they could foresee, but with the risk of increased influence of AI experts at the expense of other actors. It is crucial that a diversity of actors is included in democratic processes on the future development of AI, but for this to be meaningful, AI experts need to take responsibility for how the AI technology they develop affects public deliberation.","",""
"2024","AI language models cannot replace human research participants","","",""
"2024","Involving patients in artificial intelligence research to build trustworthy systems","","",""
"2024","Generative AI, generating precariousness for workers?","","",""
"2024","Artificial virtuous agents in a multi-agent tragedy of the commons","AbstractAlthough virtue ethics has repeatedly been proposed as a suitable framework for the development of artificial moral agents (AMAs), it has been proven difficult to approach from a computational perspective. In this work, we present the first technical implementation of artificial virtuous agents (AVAs) in moral simulations. First, we review previous conceptual and technical work in artificial virtue ethics and describe a functionalistic path to AVAs based on dispositional virtues, bottom-up learning, and top-down eudaimonic reward. We then provide the details of a technical implementation in a moral simulation based on a tragedy of the commons scenario. The experimental results show how the AVAs learn to tackle cooperation problems while exhibiting core features of their theoretical counterpart, including moral character, dispositional virtues, learning from experience, and the pursuit of eudaimonia. Ultimately, we argue that virtue ethics provides a compelling path toward morally excellent machines and that our work provides an important starting point for such endeavors.","",""
"2024","A phenomenological perspective on AI ethical failures: The case of facial recognition technology","AbstractAs more and more companies adopt artificial intelligence to increase the efficiency and effectiveness of their products and services, they expose themselves to ethical crises and potentially damaging public controversy associated with its use. Despite the prevalence of AI ethical problems, most companies are strategically unprepared to respond effectively to the public. This paper aims to advance our empirical understanding of company responses to AI ethical crises by focusing on the rise and fall of facial recognition technology. Specifically, through a comparative case study of how four big technology companies responded to public outcry over their facial recognition programs, we not only demonstrated the unfolding and consequences of public controversies over this new technology, but also identified and described four major types of company responses—Deflection, Improvement, Validation, and Pre-emption. These findings pave the way for future research on the management of controversial technology and the ethics of AI.","",""
"2024","On prediction-modelers and decision-makers: why fairness requires more than a fair prediction model","AbstractAn implicit ambiguity in the field of prediction-based decision-making concerns the relation between the concepts of prediction and decision. Much of the literature in the field tends to blur the boundaries between the two concepts and often simply refers to ‘fair prediction’. In this paper, we point out that a differentiation of these concepts is helpful when trying to implement algorithmic fairness. Even if fairness properties are related to the features of the used prediction model, what is more properly called ‘fair’ or ‘unfair’ is a decision system, not a prediction model. This is because fairness is about the consequences on human lives, created by a decision, not by a prediction. In this paper, we clarify the distinction between the concepts of prediction and decision and show the different ways in which these two elements influence the final fairness properties of a prediction-based decision system. As well as discussing this relationship both from a conceptual and a practical point of view, we propose a framework that enables a better understanding and reasoning of the conceptual logic of creating fairness in prediction-based decision-making. In our framework, we specify different roles, namely the ‘prediction-modeler’ and the ‘decision-maker,’ and the information required from each of them for being able to implement fairness of the system. Our framework allows for deriving distinct responsibilities for both roles and discussing some insights related to ethical and legal requirements. Our contribution is twofold. First, we offer a new perspective shifting the focus from an abstract concept of algorithmic fairness to the concrete context-dependent nature of algorithmic decision-making, where different actors exist, can have different goals, and may act independently. In addition, we provide a conceptual framework that can help structure prediction-based decision problems with respect to fairness issues, identify responsibilities, and implement fairness governance mechanisms in real-world scenarios.","",""
"2024","AI and suicide risk prediction: Facebook live and its aftermath","","",""
"2024","Trust and robotics: a multi-staged decision-making approach to robots in community","","",""
"2024","The work of art in the age of AI reproducibility","","",""
"2024","Dreaming of AI: environmental sustainability and the promise of participation","AbstractThere is widespread consensus among policymakers that climate change and digitalisation constitute the most pressing global transformations shaping human life in the 21st century. Seeking to address the challenges arising at this juncture, governments, technologists and scientists alike increasingly herald artificial intelligence (AI) as a vehicle to propel climate change mitigation and adaptation. In this paper, we explore the intersection of digitalisation and climate change by examining the deployment of AI in government-led climate action. Building on participant observations conducted in the context of the “Civic Tech Lab for Green”—a government-funded public interest AI initiative—and eight expert interviews, we investigate how AI shapes the negotiation of environmental sustainability as an issue of public interest. Challenging the prescribed means–end relationship between AI and environmental protection, we argue that the unquestioned investment in AI curtails political imagination and displaces discussion of climate “problems” and possible “solutions” with “technology education”. This line of argumentation is rooted in empirical findings that illuminate three key tensions in current coproduction efforts: “AI talk vs. AI walk”, “civics washing vs. civics involvement” and “public invitation vs. public participation”. Emphasising the importance of re-exploring the innovative state in climate governance, this paper extends academic literature in science and technology studies that examines public participation in climate change adaptation by shedding light on the emergent phenomenon of public interest AI.","",""
"2024","Dancing with robots: acceptability of humanoid companions to reduce loneliness during COVID-19 (and beyond)","","",""
"2024","Rethinking artificial intelligence from the perspective of interdisciplinary knowledge production","","",""
"2024","Evaluating the acceptability of ethical recommendations in industry 4.0: an ethics by design approach","","",""
"2024","Correction: Artificial intelligence as the new fire and its geopolitics","","",""
"2024","Artificial intelligence and modern planned economies: a discussion on methods and institutions","AbstractInterest in computerised central economic planning (CCEP) has seen a resurgence, as there is strong demand for an alternative vision to modern free (or not so free) market liberal capitalism. Given the close links of CCEP with what we would now broadly call artificial intelligence (AI)—e.g. optimisation, game theory, function approximation, machine learning, automated reasoning—it is reasonable to draw direct analogues and perform an analysis that would help identify what commodities and institutions we should see for a CCEP programme to become successful. Following this analysis, we conclude that a CCEP economy would need to have a very different outlook from current market practices, with a focus on producing basic “interlinking” commodities (e.g. tools, processed materials, instruction videos) that consumers can use as a form of collective R &amp;D. On an institutional level, CCEP should strive for the release of basic commodities that empower consumers by having as many alternative uses as possible, but also making sure that a baseline of basic necessities is widely available.","",""
"2024","Critical questions on the emergence of text-to-image artificial intelligence in architectural design pedagogy","","",""
"2024","Trust, risk perception, and intention to use autonomous vehicles: an interdisciplinary bibliometric review","AbstractAutonomous vehicles (AV) offer promising benefits to society in terms of safety, environmental impact and increased mobility. However, acute challenges persist with any novel technology, inlcuding the perceived risks and trust underlying public acceptance. While research examining the current state of AV public perceptions and future challenges related to both societal and individual barriers to trust and risk perceptions is emerging, it is highly fragmented across disciplines. To address this research gap, by using the Web of Science database, our study undertakes a bibliometric and performance analysis to identify the conceptual and intellectual structures of trust and risk narratives within the AV research field by investigating engineering, social sciences, marketing, and business and infrastructure domains to offer an interdisciplinary approach. Our analysis provides an overview of the key research area across the search categories of ‘trust’ and ‘risk’. Our results show three main clusters with regard to trust and risk, namely, behavioural aspects of AV interaction; uptake and acceptance; and modelling human–automation interaction. The synthesis of the literature allows a better understanding of the public perception of AV and its historical conception and development. It further offers a robust model of public perception in AV, outlining the key themes found in the literature and, in turn, offers critical directions for future research.","",""
"2024","The effect of artificial intelligence on creativity in conceptual design in architectural education: the motion of biomimetics and futurism","","",""
"2024","Stochastic contingency machines feeding on meaning: on the computational determination of social reality in machine learning","AbstractIn this paper, I reflect on the puzzle that machine learning presents to social theory to develop an account of its distinct impact on social reality.   I start by presenting how machine learning has presented a challenge to social theory as a research subject comprising both familiar and alien characteristics (1.). Taking this as an occasion for theoretical inquiry, I then propose a conceptual framework to investigate how algorithmic models of social phenomena relate to social reality and what their stochastic mode of operation entails in terms of their sociality (2.). Analyzed through a theoretical lens that relies on central tenets of sociological systems theory, I find that machine learning implies a distinct epistemic transformation, based on how algorithmic modeling techniques process meaning as represented in data embedded in vector space. Building on this characterization, I introduce my conceptualization of stochastic technology as distinct from mechanistic technologies that rely on causal fixation (3.). Based on this understanding, I suggest that real-world applications of machine learning are often characterized by a constitutive tension between the stochastic properties of their outputs and the ways in which they are put to use in practice. Focussing on the large language models LaMDA and ChatGPT, I examine the epistemological implications of LLMs to account for the confusion of correlation and causality as the root of this tension. Next, I illustrate my theoretical conception by way of discussing an essay on image models by German media artist Hito Steyerl (4.).  Following a critical reflection on Steyerl's characterization of Stable Diffusion as a “white box ”, I finally propose to conceive ofmachine learning-based technologies as stochastic contingency machines that transform social indeterminacy into contingent observations of social phenomena (5.) In this perspective, machine learning constitutes an epistemic technology that operates on meaning as extractable from data by means of algorithmic data modeling techniques to produce stochastic accounts of social reality.","",""
"2024","Democratization and generative AI image creation: aesthetics, citizenship, and practices","AbstractThe article critically analyzes how contemporary image practices involving generative artificial intelligence are entangled with processes of democratization. We demonstrate and discuss how generative artificial intelligence images raise questions of democratization and citizenship in terms of access, skills, validation, truths, and diversity. First, the article establishes a theoretical framework, which includes theory on democratization and aesthetics and lays the foundations for the analytical concepts of ‘formative’ and ‘generative’ visual citizenship. Next, we argue for the use of explorative and collaborative methods to investigate contemporary image practice, before analyzing the central part of our investigation, which takes the form of four collaborative workshops conducted in 2023 with external partners in different domains (the art scene, art therapy, education, and the news media). After analyzing insights from these workshops, the article significantly nuances how visual citizenship is at work in different manners depending on the different concrete image practices using generative artificial intelligence. Finally, we conclude that an aesthetic perspective offers valuable insights into foundational aspects of belonging to contemporary visual communities.","",""
"2024","Truth machines: synthesizing veracity in AI language models","AbstractAs AI technologies are rolled out into healthcare, academia, human resources, law, and a multitude of other domains, they become de-facto arbiters of truth. But truth is highly contested, with many different definitions and approaches. This article discusses the struggle for truth in AI systems and the general responses to date. It then investigates the production of truth in InstructGPT, a large language model, highlighting how data harvesting, model architectures, and social feedback mechanisms weave together disparate understandings of veracity. It conceptualizes this performance as an operationalization of truth, where distinct, often-conflicting claims are smoothly synthesized and confidently presented into truth-statements. We argue that these same logics and inconsistencies play out in Instruct’s successor, ChatGPT, reiterating truth as a non-trivial problem. We suggest that enriching sociality and thickening “reality” are two promising vectors for enhancing the truth-evaluating capacities of future language models. We conclude, however, by stepping back to consider AI truth-telling as a social practice: what kind of “truth” do we as listeners desire?","",""
"2024","Machine and human agents in moral dilemmas: automation–autonomic and EEG effect","","",""
"2024","Imaginaries of humanoids and evolutions of technological visions of AI in Eastern and Western media","","",""
"2024","Gauging public opinion of AI and emotionalized AI in healthcare: findings from a nationwide survey in Japan","","",""
"2024","On the moral permissibility of robot apologies","AbstractRobots that incorporate the function of apologizing have emerged in recent years. This paper examines the moral permissibility of making robots apologize. First, I characterize the nature of apology based on analyses conducted in multiple scholarly domains. Next, I present a prima facie argument that robot apologies are not permissible because they may harm human societies by inducing the misattribution of responsibility. Subsequently, I respond to a possible response to the prima facie objection based on the interpretation that attributing responsibility to a robot is analogous to having an attitude toward fiction. Then, I demonstrate that there are cases of robot apologies where the prima facie objection does not apply, by considering the following two points: (1) apology-related practices found in our human-to-human apologies, and (2) a difference in the degree of harm caused by robot failures and the resulting apologies. Finally, given the current norms governing our apology-related practices, I argue that some instances of making robots apologize are permissible, and I propose conducting critical robotics research questioning the validity of such norms.","",""
"2024","Slow but rewarding collaborations with ChatGPT in trifecta of thinking–doing–writing","","",""
"2024","Do ontologies always support communication of their content among human agents?","","",""
"2024","On the individuation of complex computational models: Gilbert Simondon and the technicity of AI","AbstractThe proliferation of AI systems across all domains of life as well as the complexification and opacity of algorithmic techniques, epitomised by the bourgeoning field of Deep Learning (DL), call for new methods in the Humanities for reflecting on the techno-human relation in a way that places the technical operation at its core. Grounded on the work of the philosopher of technology Gilbert Simondon, this paper puts forward individuation theory as a valuable approach to reflect on contemporary information technologies, offering an analysis of the functioning of deep neural networks (DNNs), a type of data-driven computational models at the core of major breakthroughs in AI. The purpose of this article is threefold: (1) to demonstrate how a joint reading of Simondon’s mechanology and individuation theory, foregrounded in the Simondonian concept of information, can cast new light on contemporary algorithmic techniques by considering their situated emergence as opposed to technical lineage; (2) to suspend a predictive framing of AI systems, particularly DL techniques, so as to probe into their technical operation, accounting for the data-driven individuation of these models and the integration of potentials as functionality; and finally, (3) to argue that individuation theory might in fact de-individuate AI, in the sense of disassembling the already-there, the constituted, paving the way for questioning the potentialities for data and their algorithmic relationality to articulate the unfolding of everyday life.","",""
"2024","Humanities and social sciences (HSS) and the challenges posed by AI: a French point of view","","",""
"2024","Correction: Bowling alone in the autonomous vehicle: the ethics of well-being in the driverless car","","",""
"2024","Generative AI and human labor: who is replaceable?","","",""
"2024","Institutionalised distrust and human oversight of artificial intelligence: towards a democratic design of AI governance under the European Union AI Act","                         Abstract                      Human oversight has become a key mechanism for the governance of artificial intelligence (“AI”). Human overseers are supposed to increase the accuracy and safety of AI systems, uphold human values, and build trust in the technology. Empirical research suggests, however, that humans are not reliable in fulfilling their oversight tasks. They may be lacking in competence or be harmfully incentivised. This creates a challenge for human oversight to be effective. In addressing this challenge, this article aims to make three contributions. First, it surveys the emerging laws of oversight, most importantly the European Union’s Artificial Intelligence Act (“AIA”). It will be shown that while the AIA is concerned with the competence of human overseers, it does not provide much guidance on how to achieve effective oversight and leaves oversight obligations for AI developers underdefined. Second, this article presents a novel taxonomy of human oversight roles, differentiated along whether human intervention is constitutive to, or corrective of a decision made or supported by an AI. The taxonomy allows to propose suggestions for improving effectiveness tailored to the type of oversight in question. Third, drawing on scholarship within democratic theory, this article formulates six normative principles which institutionalise distrust in human oversight of AI. The institutionalisation of distrust has historically been practised in democratic governance. Applied for the first time to AI governance, the principles anticipate the fallibility of human overseers and seek to mitigate them at the level of institutional design. They aim to directly increase the trustworthiness of human oversight and to indirectly inspire well-placed trust in AI governance.","",""
"2024","Elephant motorbikes and too many neckties: epistemic spatialization as a framework for investigating patterns of bias in convolutional neural networks","","",""
"2024","Empathy: an ethical consideration of AI &amp; others in the workplace","","",""
"2024","Public perceptions of the use of artificial intelligence in Defence: a qualitative exploration","AbstractThere are a wide variety of potential applications of artificial intelligence (AI) in Defence settings, ranging from the use of autonomous drones to logistical support. However, limited research exists exploring how the public view these, especially in view of the value of public attitudes for influencing policy-making. An accurate understanding of the public’s perceptions is essential for crafting informed policy, developing responsible governance, and building responsive assurance relating to the development and use of AI in military settings. This study is the first to explore public perceptions of and attitudes towards AI in Defence. A series of four focus groups were conducted with 20 members of the UK public, aged between 18 and 70, to explore their perceptions and attitudes towards AI use in general contexts and, more specifically, applications of AI in Defence settings. Thematic analysis revealed four themes and eleven sub-themes, spanning the role of humans in the system, the ethics of AI use in Defence, trust in AI versus trust in the organisation, and gathering information about AI in Defence. Participants demonstrated a variety of misconceptions about the applications of AI in Defence, with many assuming that a variety of different technologies involving AI are already being used. This highlighted a confluence between information from reputable sources combined with narratives from the mass media and conspiracy theories. The study demonstrates gaps in knowledge and misunderstandings that need to be addressed, and offers practical insights for keeping the public reliably, accurately, and adequately informed about the capabilities, limitations, benefits, and risks of AI in Defence.","",""
"2024","Apprehending AI moral purpose in practical wisdom","","",""
"2024","A pluralist hybrid model for moral AIs","","",""
"2024","The Executioner Paradox: understanding self-referential dilemma in computational systems","AbstractAs computational systems burgeon with advancing artificial intelligence (AI), the deterministic frameworks underlying them face novel challenges, especially when interfacing with self-modifying code. The Executioner Paradox, introduced herein, exemplifies such a challenge where a deterministic Executioner Machine (EM) grapples with self-aware and self-modifying code. This unveils a self-referential dilemma, highlighting a gap in current deterministic computational frameworks when faced with self-evolving code. In this article,  the Executioner Paradox is proposed, highlighting the nuanced interactions between deterministic decision-making and self-aware code, and the ensuing challenges. This article advocates for a re-evaluation of existing deterministic frameworks, emphasizing the need for adaptive decision-making mechanisms in computational systems. By dissecting the Executioner Paradox, the aim is to foster a robust discussion on evolving deterministic frameworks to accommodate the dynamic nature of self-modifying code, thereby contributing a forward-looking lens to the discourse on computational systems amidst advancing AI.","",""
"2024","Morality first?","AbstractThe Morality First strategy for developing AI systems that can represent and respond to human values aims to first develop systems that can represent and respond to moral values. I argue that Morality First and other X-First views are unmotivated. Moreover, if one particular philosophical view about value is true, these strategies are positively distorting. The natural alternative according to which no domain of value comes “first” introduces a new set of challenges and highlights an important but otherwise obscured problem for e-AI developers.","",""
"2024","Identifying arbitrage opportunities in retail markets with artificial intelligence","","",""
"2024","Using artificial intelligence to enhance patient autonomy in healthcare decision-making","","",""
"2024","AI, automation and the lightening of work","AbstractArtificial intelligence (AI) technology poses possible threats to existing jobs. These threats extend not just to the number of jobs available but also to their quality. In the future, so some predict, workers could face fewer and potentially worse jobs, at least if society does not embrace reforms that manage the coming AI revolution. This paper uses the example of Daron Acemoglu and Simon Johnson’s recent book—Power and Progress(2023)—to illustrate some of the dilemmas and options for managing the future of work under AI. Acemoglu and Johnson, while warning of the potential negative effects of an AI-driven automation, argue that AI can be used for positive ends. In particular, they argue for its uses in creating more ‘good jobs’. This outcome will depend on democratising AI technology. This paper is critical of the approach taken by Acemoglu and Johnson—specifically, it misses the possibility for using AI to lighten work (i.e., to reduce its duration and improve its quality). This paper stresses the potential benefits of automation as a mechanism for lightening work. Its key arguments aim to advance critical debates focused on creating a future in which AI works for people not just for profits.","",""
"2024","Silence: an ignored concept in artificial intelligence","","",""
"2024","The Digital Nexus: tracing the evolution of human consciousness and cognition within the artificial realm—a comprehensive review","","",""
"2024","Public understanding of artificial intelligence through entertainment media","","",""
"2024","Measuring perceived empathy in dialogue systems","AbstractDialogue systems, from Virtual Personal Assistants such as Siri, Cortana, and Alexa to state-of-the-art systems such as BlenderBot3 and ChatGPT, are already widely available, used in a variety of applications, and are increasingly part of many people’s lives. However, the task of enabling them to use empathetic language more convincingly is still an emerging research topic. Such systems generally make use of complex neural networks to learn the patterns of typical human language use, and the interactions in which the systems participate are usually mediated either via interactive text-based or speech-based interfaces. In human–human interaction, empathy has been shown to promote prosocial behaviour and improve interaction. In the context of dialogue systems, to advance the understanding of how perceptions of empathy affect interactions, it is necessary to bring greater clarity to how empathy is measured and assessed. Assessing the way dialogue systems create perceptions of empathy brings together a range of technological, psychological, and ethical considerations that merit greater scrutiny than they have received so far. However, there is currently no widely accepted evaluation method for determining the degree of empathy that any given system possesses (or, at least, appears to possess). Currently, different research teams use a variety of automated metrics, alongside different forms of subjective human assessment such as questionnaires, self-assessment measures and narrative engagement scales. This diversity of evaluation practice means that, given two DSs, it is usually impossible to determine which of them conveys the greater degree of empathy in its dialogic exchanges with human users. Acknowledging this problem, the present article provides an overview of how empathy is measured in human–human interactions and considers some of the ways it is currently measured in human–DS interactions. Finally, it introduces a novel third-person analytical framework, called the Empathy Scale for Human–Computer Communication (ESHCC), to support greater uniformity in how perceived empathy is measured during interactions with state-of-the-art DSs.","",""
"2024","Compulsion beyond fairness: towards a critical theory of technological abstraction in neural networks","","",""
"2024","Awareness and perception of artificial intelligence operationalized integration in news media industry and society","","",""
"2024","Algorithmic discrimination in the credit domain: what do we know about it?","AbstractThe widespread usage of machine learning systems and econometric methods in the credit domain has transformed the decision-making process for evaluating loan applications. Automated analysis of credit applications diminishes the subjectivity of the decision-making process. On the other hand, since machine learning is based on past decisions recorded in the financial institutions’ datasets, the process very often consolidates existing bias and prejudice against groups defined by race, sex, sexual orientation, and other attributes. Therefore, the interest in identifying, preventing, and mitigating algorithmic discrimination has grown exponentially in many areas, such as Computer Science, Economics, Law, and Social Science. We conducted a comprehensive systematic literature review to understand (1) the research settings, including the discrimination theory foundation, the legal framework, and the applicable fairness metric; (2) the addressed issues and solutions; and (3) the open challenges for potential future research. We explored five sources: ACM Digital Library, Google Scholar, IEEE Digital Library, Springer Link, and Scopus. Following inclusion and exclusion criteria, we selected 78 papers written in English and published between 2017 and 2022. According to the meta-analysis of this literature survey, algorithmic discrimination has been addressed mainly by looking at the CS, Law, and Economics perspectives. There has been great interest in this topic in the financial area, especially the discrimination in providing access to the mortgage market and differential treatment (different fees, number of parcels, and interest rates). Most attention has been devoted to the potential discrimination due to bias in the dataset. Researchers are still only dealing with direct discrimination, addressed by algorithmic fairness, while indirect discrimination (structural discrimination) has not received the same attention.","",""
"2024","Artificial intelligence: a “promising technology”","AbstractThis paper addresses the question of how the ups and downs in the development of artificial intelligence (AI) since its inception can be explained. It focuses on the development of artificial intelligence in Germany since the 1970s, and particularly on its current dynamics. An assumption is made that a mere reference to rapid advances in information technologies and the various methods and concepts of artificial intelligence in recent decades cannot adequately explain these dynamics, because from a social science perspective, this is an oversimplified, technology-centred explanation. Drawing on ideas from social scientific innovation research, the hypothesis is rather that artificial intelligence should be understood as a “promising technology”. Its various stages of development have always been driven  by technological promises about its special powers and capabilities when applied to solving economic and societal challenges.","",""
"2024","AI and society: a virtue ethics approach","","",""
"2024","Cognitive imperialism in artificial intelligence: counteracting bias with indigenous epistemologies","AbstractThis paper presents a novel methodology for integrating indigenous knowledge systems into AI development to counter cognitive imperialism and foster inclusivity. By critiquing the dominance of Western epistemologies and highlighting the risks of bias, the authors argue for incorporating diverse epistemologies. The proposed framework outlines a participatory approach that includes indigenous perspectives, ensuring AI benefits all. The methodology draws from AI ethics, indigenous studies, and postcolonial theory, emphasizing co-creation with indigenous communities, ethical protocols for indigenous data governance, and adaptation of AI algorithms. Case studies in natural language processing, content moderation, and healthcare demonstrate the methodology’s effectiveness and importance. By offering a concrete methodology for decolonizing AI, this paper contributes significantly to AI ethics and social justice, providing a roadmap for equitable, culturally respectful AI.","",""
"2024","Generative AI and human–robot interaction: implications and future agenda for business, society and ethics","","",""
"2024","Understanding model power in social AI","AbstractGiven the widespread integration of Social AI like ChatGPT, Gemini, Copilot, and MyAI, in personal and professional contexts, it is crucial to understand their effects on information and knowledge processing, and individual autonomy. This paper builds on Bråten’s concept of model power, applying it to Social AI to offer a new perspective on the interaction dynamics between humans and AI. By reviewing recent user studies, we examine whether and how models of the world reflected in Social AI may disproportionately impact human-AI interactions, potentially leading to model monopolies where Social AI impacts human beliefs, behaviour and homogenize the worldviews of its users. The concept of model power provides a framework for critically evaluating the impact and influence that Social AI has on communication and meaning-making, thereby informing the development of future systems to support more balanced and meaningful human-AI interactions.","",""
"2024","Balancing progress and preservation: can AI harmonize efficiency with the human experience in retail?","","",""
"2024","Safety by simulation: theorizing the future of robot regulation","AbstractMobility robots may soon be among us, triggering a need for safety regulation. Robot safety regulation, however, remains underexplored, with only a few articles analyzing what regulatory approaches could be feasible. This article offers an account of the available regulatory strategies and attempts to theorize the effects of simulation-based safety regulation. The article first discusses the distinctive features of mobility robots as regulatory targets and argues that emergent behavior constitutes the key regulatory concern in designing robot safety regulation regimes. In contrast to many accounts, the article posits that emergent behavior dynamics do not arise from robot autonomy, learning capability, or code unexplainability. Instead, they emerge from the complexity of robot technological constitutions coupled with near-infinite environmental variability and non-linear performance dynamics of the machine learning components. Second, the article reviews rules-based and performance-based regulation and argues that both will fail adequately constrain emergent robot behaviors. The article claims that controlling mobility robots requires a simulation-based regulatory approach. Simulation-based regulation is a novelty with significant theoretical and practical implications. The article argues that the approach signifies a radical break in regulatory forms of knowledge and temporalities. Simulations enact virtual futures to create a new regulatory knowledge type. Practically, the novel safety knowledge type may destabilize the existing conceptual space of safety politics and liability allocation patterns.","",""
"2024","Understanding user sensemaking in fairness and transparency in algorithms: algorithmic sensemaking in over-the-top platform","","",""
"2024","Machine theology or artificial sainthood!","","",""
"2024","Gender bias perpetuation and mitigation in AI technologies: challenges and opportunities","AbstractAcross the world, artificial intelligence (AI) technologies are being more widely employed in public sector decision-making and processes as a supposedly neutral and an efficient method for optimizing delivery of services. However, the deployment of these technologies has also prompted investigation into the potentially unanticipated consequences of their introduction, to both positive and negative ends. This paper chooses to focus specifically on the relationship between gender bias and AI, exploring claims of the neutrality of such technologies and how its understanding of bias could influence policy and outcomes. Building on a rich seam of literature from both technological and sociological fields, this article constructs an original framework through which to analyse both the perpetuation and mitigation of gender biases, choosing to categorize AI technologies based on whether their input is text or images. Through the close analysis and pairing of four case studies, the paper thus unites two often disparate approaches to the investigation of bias in technology, revealing the large and varied potential for AI to echo and even amplify existing human bias, while acknowledging the important role AI itself can play in reducing or reversing these effects. The conclusion calls for further collaboration between scholars from the worlds of technology, gender studies and public policy in fully exploring algorithmic accountability as well as in accurately and transparently exploring the potential consequences of the introduction of AI technologies.","",""
"2024","AI through the looking glass: an empirical study of structural social and ethical challenges in AI","AbstractThis paper examines how professionals (N = 32) working on artificial intelligence (AI) view structural AI ethics challenges like injustices and inequalities beyond individual agents' direct intention and control. This paper answers the research question: What are professionals’ perceptions of the structural challenges of AI (in the agri-food sector)? This empirical paper shows that it is essential to broaden the scope of ethics of AI beyond micro- and meso-levels. While ethics guidelines and AI ethics often focus on the responsibility of designers and the competencies and skills of designers to take this responsibility, our results show that many structural challenges are beyond their reach. This result means that while ethics guidelines and AI ethics frameworks are helpful, there is a risk that they overlook more complicated, nuanced, and intersected structural challenges. In addition, it highlights the need to include diverse stakeholders, such as quadruple helix (QH) participants, in discussions around AI ethics rather than solely focusing on the obligations of AI developers and companies. Overall, this paper demonstrates that addressing structural challenges in AI is challenging and requires an approach that considers four requirements: (1) multi-level, (2) multi-faceted, (3) interdisciplinary, and (4) polycentric governance.","",""
"2024","Five premises to understand human–computer interactions as AI is changing the world","","",""
"2024","Drivers behind the public perception of artificial intelligence: insights from major Australian cities","AbstractArtificial intelligence (AI) is not only disrupting industries and businesses, particularly the ones have fallen behind the adoption, but also significantly impacting public life as well. This calls for government authorities pay attention to public opinions and sentiments towards AI. Nonetheless, there is limited knowledge on what the drivers behind the public perception of AI are. Bridging this gap is the rationale of this paper. As the methodological approach, the study conducts an online public perception survey with the residents of Sydney, Melbourne, and Brisbane, and explores the collected survey data through statistical analysis. The analysis reveals that: (a) the public is concerned of AI invading their privacy, but not much concerned of AI becoming more intelligent than humans; (b) the public trusts AI in their lifestyle, but the trust is lower for companies and government deploying AI; (c) the public appreciates the benefits of AI in urban services and disaster management; (d) depending on the local context, public perceptions vary; and (e) the drivers behind the public perception include gender, age, AI knowledge, and AI experience. The findings inform authorities in developing policies to minimise public concerns and maximise AI awareness.","",""
"2024","What about investors? ESG analyses as tools for ethics-based AI auditing","AbstractArtificial intelligence (AI) governance and auditing promise to bridge the gap between AI ethics principles and the responsible use of AI systems, but they require assessment mechanisms and metrics. Effective AI governance is not only about legal compliance; organizations can strive to go beyond legal requirements by proactively considering the risks inherent in their AI systems. In the past decade, investors have become increasingly active in advancing corporate social responsibility and sustainability practices. Including nonfinancial information related to environmental, social, and governance (ESG) issues in investment analyses has become mainstream practice among investors. However, the AI auditing literature is mostly silent on the role of investors. The current study addresses two research questions: (1) how companies’ responsible use of AI is included in ESG investment analyses and (2) what connections can be found between principles of responsible AI and ESG ranking criteria. We conducted a series of expert interviews and analyzed the data using thematic analysis. Awareness of AI issues, measuring AI impacts, and governing AI processes emerged as the three main themes in the analysis. The findings indicate that AI is still a relatively unknown topic for investors, and taking the responsible use of AI into account in ESG analyses is not an established practice. However, AI is recognized as a potentially material issue for various industries and companies, indicating that its incorporation into ESG evaluations may be justified. There is a need for standardized metrics for AI responsibility, while critical bottlenecks and asymmetrical knowledge relations must be tackled.","",""
"2024","An agent-based approach to the limits of economic planning","AbstractMises’ and Hayek’s arguments against central economic planning have long been taken as definitive proof that a centrally planned economy managed by the government would be impossible. Today, however, the exponential rise in the capacities of AI has opened up the possibility that supercomputers could have what it takes to plan the national economy. The ‘economic calculation debate’ has thus reignited. Arguably, this is because neither Mises nor Hayek have given a clear and conclusive argument why central planning of the economy is impossible in principle. The paper frames the problem of economic planning as an agent–environment interaction, offering a taxonomy of the different sets of agents at play a) in a market economy and b) in a centrally planned economy equipped with the most sophisticated AI technology. The argument is that public institutions as planning bodies cannot replace the market order, no matter the AI technology behind them, for the elimination of the market entails the elimination of crucial kinds of agents that cannot be recreated or emulated through AI or careful social planning: the proactive action of entrepreneurs driving market allocation.","",""
"2024","The human biological advantage over AI","AbstractRecent advances in AI raise the possibility that AI systems will one day be able to do anything humans can do, only better. If artificial general intelligence (AGI) is achieved, AI systems may be able to understand, reason, problem solve, create, and evolve at a level and speed that humans will increasingly be unable to match, or even understand. These possibilities raise a natural question as to whether AI will eventually become superior to humans, a successor “digital species”, with a rightful claim to assume leadership of the universe. However, a deeper consideration suggests the overlooked differentiator between human beings and AI is not the brain, but the central nervous system (CNS), providing us with an immersive integration with physical reality. It is our CNS that enables us to experience emotion including pain, joy, suffering, and love, and therefore to fully appreciate the consequences of our actions on the world around us. And that emotional understanding of the consequences of our actions is what is required to be able to develop sustainable ethical systems, and so be fully qualified to be the leaders of the universe. A CNS cannot be manufactured or simulated; it must be grown as a biological construct. And so, even the development of consciousness will not be sufficient to make AI systems superior to humans. AI systems may become more capable than humans on almost every measure and transform our society. However, the best foundation for leadership of our universe will always be DNA, not silicon.","",""
"2024","The rise of the producer: generative AI will transform content creation into content production","","",""
"2024","Aiding narrative generation in collaborative data utilization by humans and AI agents","AbstractNarrative generation is growing in importance for data utilization, particularly in the context of co-creation with artificial intelligence (AI) agents. Narratives can, for example, bridge theoretical objects with social understanding and promote human actions. Furthermore, clarifying the narrative generation mechanism is essential for constructing effective relationships between humans and AI agents. However, the narrative generation mechanism in data utilization processes has not been fully elucidated. In this study, we developed a framework called the hierarchical narrative representation (HieNaR) to systematize the structure of narrative generation in data utilization processes. HieNaR comprises twelve levels, ranging from the set of texts down to the particle level (e.g., text, sentence, word, character, and stroke), allowing for a comprehensive analysis of narrative structures. We evaluated the usefulness of HieNaR through case studies, examining both individual user experiences and collaborative work between humans and an AI agent. The results demonstrated that the data utilization process interprets data by inquiring whether it satisfies higher-level expectations. In collaboration, AI agents can be understood as co-creative partners in data utilization, possessing their own worldviews. Through these findings, this study not only elucidates the mechanism of narrative generation in data utilization processes but also provides a foundation for improving human–AI collaboration.","",""
"2024","Subnational AI policy: shaping AI in a multi-level governance system","AbstractThe promises and risks of Artificial Intelligence permeate current policy statements and have attracted much attention by AI governance research. However, most analyses focus exclusively on AI policy on the national and international level, overlooking existing federal governance structures. This is surprising because AI is connected to many policy areas, where the competences are already distributed between the national and subnational level, such as research or economic policy. Addressing this gap, this paper argues that more attention should be dedicated to subnational efforts to shape AI and asks which themes are discussed in subnational AI policy documents with a case study of Germany’s 16 states. Our qualitative analysis of 34 AI policy documents issued on the subnational level demonstrates that subnational efforts focus on knowledge transfer between research and industry actors, the commercialization of AI, different economic identities of the German states, and the incorporation of ethical principles. Because federal states play an active role in AI policy, analysing AI as a policy issue on different levels of government is necessary and will contribute to a better understanding of the developments and implementations of AI strategies in different national contexts.","",""
"2024","Hey Alexa, why are you called intelligent? An empirical investigation on definitions of AI","AbstractThis paper seeks to examine the questions of what criteria definitions of Artificial Intelligence (AI) use to define AI, what the disagreements that revolve around the term AI are based on, and what correlations can be drawn to other parameters. Framed as a problem of classification, a random sample of 45 definitions from various text sources was subjected to a qualitative content analysis. The criteria found are concluded in five dimensions, namely (1) learning ability, (2) human likeness, (3) state of “mind”, (4) complexity of the problem, and (5) successfulness. Further, the results support the view that there is no consensus neither on which of these criteria are crucial to define AI nor on how these criteria must be fulfilled. By opposing the frequencies of the dimensions found with the metadata collected, it can be seen that most of these, e.g., country, scientific field, or gender of the author, are statistically independent of content variables, while the medium in which the definition was published shows a strong correlation. Since different mediums target different purposes and different readers, it must be taken into account that writing a definition of AI is to be seen in the context of its distribution area and its goal.","",""
"2024","Ethics of using artificial intelligence (AI) in veterinary medicine","AbstractThis paper provides the first comprehensive analysis of ethical issues raised by artificial intelligence (AI) in veterinary medicine for companion animals. Veterinary medicine is a socially valued service, which, like human medicine, will likely be significantly affected by AI. Veterinary AI raises some unique ethical issues because of the nature of the client–patient–practitioner relationship, society’s relatively minimal valuation and protection of nonhuman animals and differences in opinion about responsibilities to animal patients and human clients. The paper examines how these distinctive features influence the ethics of AI systems that might benefit clients, veterinarians and animal patients—but also harm them. It offers practical ethical guidance that should interest ethicists, veterinarians, clinic owners, veterinary bodies and regulators, clients, technology developers and AI researchers.","",""
"2024","Artificial intelligence and identity: the rise of the statistical individual","AbstractAlgorithms are used across a wide range of societal sectors such as banking, administration, and healthcare to make predictions that impact on our lives. While the predictions can be incredibly accurate about our present and future behavior, there is an important question about how these algorithms in fact represent human identity. In this paper, we explore this question and argue that machine learning algorithms represent human identity in terms of what we shall call the statistical individual. This statisticalized representation of individuals, we shall argue, differs significantly from our ordinary conception of human identity, which is tightly intertwined with considerations about biological, psychological, and narrative continuity—as witnessed by our most well-established philosophical views on personal identity. Indeed, algorithmic representations of individuals give no special attention to biological, psychological, and narrative continuity and instead rely on predictive properties that significantly exceed and diverge from those that we would ordinarily take to be relevant for questions about how we are.","",""
"2024","Classification of the lunar surface pattern by AI architectures: does AI see a rabbit in the Moon?","","",""
"2024","Correction to: Are we inventing ourselves out of our own usefulness? Striking a balance between creativity and AI","","",""
"2024","The role of artificial intelligence and algorithms in the working conditions formation","","",""
"2024","AI governance: a review of the Oxford handbook of AI governance","","",""
"2024","Unite the study of AI in government: With a shared language and typology","","",""
"2024","The case for global governance of AI: arguments, counter-arguments, and challenges ahead","AbstractIt is increasingly recognized that as artificial intelligence becomes more powerful and pervasive in society and creates risks and ethical issues that cross borders, a global approach is needed for the governance of these risks. But why, exactly, do we need this and what does that mean? In this Open Forum paper, author argues for global governance of AI for moral reasons but also outlines the governance challenges that this project raises.","",""
"2024","International governance of advancing artificial intelligence","AbstractNew technologies with military applications may demand new modes of governance. In this article, we develop a taxonomy of technology governance forms, outline their strengths, and red-team their weaknesses. In particular, we consider the challenges and opportunities posed by advancing artificial intelligence, which is likely to have substantial dual-use properties. We conclude that subnational governance, though prevalent and mitigating some risks, is insufficient when the individual rewards from societally harmful actions outweigh normative sanctions, as is likely to be the case with AI. Nationally enforced standards are promising ways to govern AI deployment, but they are less viable in the “race-to-the-bottom” environments that are becoming common. When it comes to powerful technologies with military implications, there is only one multilateral option with a strong historical precedent: a non-proliferation plus norms-of-use regime, which we call NPT+. We believe that a non-proliferation regime may, therefore, be the necessary foundation for AI governance. However, AI may exhibit characteristics that would make a non-proliferation regime less effective than it has proven for nuclear weapons. As an alternative, verification-backed restrictions on AI development and use would address more risks, but they face challenges in the case of advanced AI, and we show how these challenges may not have technical solutions. Perhaps more importantly, we show that there is no clear example of major powers restricting the development of a powerful military technology when that technology lacks a ready substitute. We, therefore, turn to a final alternative, International Monopoly, which was the preferred solution of many scholars and policymakers in the early nuclear era. It should be considered again for governing AI: a monopoly would require less-invasive monitoring, though at the possible cost of eroding national sovereignty. Ultimately, we conclude that it is too soon to tell whether a non-proliferation regime, a verification-based regime, or an International Monopoly is most feasible for governing AI. Nonetheless, a variety of policies would yield a high return across all three scenarios, and we conclude by identifying some of these steps that could be taken today.","",""
"2024","Correction: Poverty and freedom: philosophical reflection on the future development of artificial intelligence","","",""
"2024","The ideals program in algorithmic fairness","","",""
"2024","Connectionism about human agency: responsible AI and the social lifeworld","AbstractThis paper analyzes responsible human–machine interaction concerning artificial neural networks (ANNs) and large language models (LLMs) by considering the extension of human agency and autonomy by means of artificial intelligence (AI). Thereby, the paper draws on the sociological concept of “interobjectivity,” first introduced by Bruno Latour, and applies it to technologically situated and interconnected agency. Drawing on Don Ihde’s phenomenology of human-technology relations, this interobjective account of AI allows to understand human–machine interaction as embedded in the social lifeworld. Finally, the paper develops a connectionist account of responsible AI, thereby focusing on patterns such as social goals and actions.","",""
"2024","The meaningfulness gap in AI ethics: a guide on how to think through a complex challenge","AbstractTechnological outsourcing is increasingly prevalent, with AI systems taking over many tasks once performed by humans. This shift has led to various discussions within AI ethics. A question that was largely ignored until recently, but is now increasingly being discussed, concerns the meaningfulness of such a lifestyle. The literature largely features skeptical views, raising several challenges. Many of these challenges can be grouped under what I identify as the “meaningfulness gap”. Although this gap is widely acknowledged, there is a notable absence of systematic exploration in the literature. This paper aims to fill this void by offering a detailed, step-by-step guide for systematically exploring the different instances of the meaningfulness gap and aids in navigating their complexities. More specifically, it proposes differentiating the gaps according to their realms and objects, normative nature, scope, and severity. To make these areas manageable, the paper takes several taxonomies and distinctions on board. Finally, the guide is summarized, and some skeptical replies are anticipated and countered by clarificatory remarks.","",""
"2024","Coeckelbergh, Mark (2022). The Political Philosophy of AI, Polity Press, Cambridge, UK, ISBN-13: 978-1509548545","","",""
"2024","Toward the symbiocene through artificial intelligence","","",""
"2024","AI and the expert; a blueprint for the ethical use of opaque AI","","",""
"2024","The paradox of the artificial intelligence system development process: the use case of corporate wellness programs using smart wearables","AbstractArtificial intelligence (AI) systems have been widely applied to various contexts, including high-stake decision processes in healthcare, banking, and judicial systems. Some developed AI models fail to offer a fair output for specific minority groups, sparking comprehensive discussions about AI fairness. We argue that the development of AI systems is marked by a central paradox: the less participation one stakeholder has within the AI system’s life cycle, the more influence they have over the way the system will function. This means that the impact on the fairness of the system is in the hands of those who are less impacted by it. However, most of the existing works ignore how different aspects of AI fairness are dynamically and adaptively affected by different stages of AI system development. To this end, we present a use case to discuss fairness in the development of corporate wellness programs using smart wearables and AI algorithms to analyze data. The four key stakeholders throughout this type of AI system development process are presented. These stakeholders are called service designer, algorithm designer, system deployer, and end-user. We identify three core aspects of AI fairness, namely, contextual fairness, model fairness, and device fairness. We propose a relative contribution of the four stakeholders to the three aspects of fairness. Furthermore, we propose the boundaries and interactions between the four roles, from which we make our conclusion about the possible unfairness in such an AI developing process.","",""
"2024","Using AI to detect panic buying and improve products distribution amid pandemic","","",""
"2024","The five tests: designing and evaluating AI according to indigenous Māori principles","AbstractAs AI technologies are increasingly deployed in work, welfare, healthcare, and other domains, there is a growing realization not only of their power but of their problems. AI has the capacity to reinforce historical injustice, to amplify labor precarity, and to cement forms of racial and gendered inequality. An alternate set of values, paradigms, and priorities are urgently needed. How might we design and evaluate AI from an indigenous perspective? This article draws upon the five Tests developed by Māori scholar Sir Hirini Moko Mead. This framework, informed by Māori knowledge and concepts, provides a method for assessing contentious issues and developing a Māori position. This paper takes up these tests, considers how each test might be applied to data-driven systems, and provides a number of concrete examples. This intervention challenges the priorities that currently underpin contemporary AI technologies but also offers a rubric for designing and evaluating AI according to an indigenous knowledge system.","",""
"2024","Rules for privately owned robots in public spaces","","",""
"2024","MinMax fairness: from Rawlsian Theory of Justice to solution for algorithmic bias","AbstractThis paper presents an intuitive explanation about why and how Rawlsian Theory of Justice (Rawls in A theory of justice, Harvard University Press, Harvard, 1971) provides the foundations to a solution for algorithmic bias. The contribution of the paper is to discuss and show why Rawlsian ideas in their original form (e.g. the veil of ignorance, original position, and allowing inequalities that serve the worst-off) are relevant to operationalize fairness for algorithmic decision making. The paper also explains how this leads to a specific MinMaxfairness solution, which addresses the basic challenges of algorithmic justice. We combine substantive elements of Rawlsian perspective with an intuitive explanation in order to provide accessible and practical insights. The goal is to propose and motivate why and how the MinMaxfairness solution derived from Rawlsian principles overcomes some of the current challenges for algorithmic bias and highlight the benefits provided when compared to other approaches. The paper presents and discusses the solution by building a bridge between the qualitative theoretical aspects and the quantitative technical approach.","",""
"2024","The shift of Artificial Intelligence research from academia to industry: implications and possible future directions","","",""
"2024","Non-western AI ethics guidelines: implications for intercultural ethics of technology","","",""
"2024","The regulation of artificial intelligence","AbstractBefore embarking on a discussion of the regulation of artificial intelligence (AI), it is first necessary to define the subject matter regulated. Defining artificial intelligence is a difficult endeavour, and many definitions have been proposed over the years. Although more than 70 years have passed since it was adopted, the most convincing definition is still nonetheless that proposed by Turing; in any case, it is important to be mindful of the risk of anthropomorphising artificial intelligence, which may arise in particular from its very definition. Once we have established the subject matter regulated, we must ask ourselves whether lawmakers should pursue an approach that seeks to regulate artificial intelligence as a whole, or whether by contrast they should regulate applications of artificial intelligence in specific sectors or individual areas. The proposal for a regulation on artificial intelligence published on 21 April 2021 implements the former approach whilst also pursuing geopolitical goals. After providing an initial overview of the notion of artificial intelligence, this article investigates the geopolitical context to the proposal for a regulation, and then goes on to illustrate the regulatory model embraced by the proposal as well as related critical aspects.","",""
"2024","From United Steel to Waymo: industrializing simulation","AbstractThe use of computers for simulation work can be traced back to the 1950s, and the pioneering work of Stafford Beer, KD Tocher and others at Cybor House in Sheffield, UK, the research and development (R&amp;D) department of British steelmakers, United Steel. This innovative simulation work sought to offer an abstracted, ‘total’ environment of the steelmaking process in which different operational activities could be modeled. Critical to this work was the ability of computer simulations to perform such modelling at a fraction of the cost, wasting fewer material resources, and in a considerably shorter timeframe. Such work can be understood as the earliest example of the application of industrial-scale ‘automated computation’ to a real-world industrial process. Similarly indebted to the early principles of computer simulation, Waymo engineers are also engaged in the building of so-called ‘conflict typologies’ designed to encode material properties of everyday driving interactions between road users, rather than simply road users themselves. Through ‘motion planning’, coupled with the categorization of driving interactions, Waymo engineers build instrumental understanding of their own system’s purported intelligence in navigating everyday driving situations. Functioning as ‘generative mechanisms’ rather than simply evaluative devices, engineers seek to industrialize—instrumentalize, scale up, rationalize—everyday driving knowledge. Through conflict typologies, instrumental knowledge of the actual capacities of autonomous vehicles is industrialized, materialized, and realized.","",""
"2024","Responsible automatically processable regulation","AbstractDriven by the increasing availability and deployment of ubiquitous computing technologies across our private and professional lives, implementations of automatically processable regulation (APR) have evolved over the past decade from academic projects to real-world implementations by states and companies. There are now pressing issues that such encoded regulation brings about for citizens and society, and strategies to mitigate these issues are required. However, comprehensive yet practically operationalizable frameworks to navigate the complex interactions and evaluate the risks of projects that implement APR are not available today. In this paper, and based on related work as well as our own experiences, we propose a framework to support the conceptualization, implementation, and application of responsible APR. Our contribution is twofold: we provide a holistic characterization of what responsible APR means; and we provide support to operationalize this in concrete projects, in the form of leading questions, examples, and mitigation strategies. We thereby provide a scientifically backed yet practically applicable way to guide researchers, sponsors, implementers, and regulators toward better outcomes of APR for users and society.","",""
"2024","Robots among us: ordinary but significant human–robot interactions in the city","","",""
"2024","Computational frameworks for zoonotic disease control in Society 5.0: opportunities, challenges and future research directions","","",""
"2024","More or less discrimination? Practical feasibility of fairness auditing of technologies for personnel selection","AbstractThe use of technologies in personnel selection has come under increased scrutiny in recent years, revealing their potential to amplify existing inequalities in recruitment processes. To date, however, there has been a lack of comprehensive assessments of respective discriminatory potentials and no legal or practical standards have been explicitly established for fairness auditing. The current proposal of the Artificial Intelligence Act classifies numerous applications in personnel selection and recruitment as high-risk technologies, and while it requires quality standards to protect the fundamental rights of those involved, particularly during development, it does not provide concrete guidance on how to ensure this, especially once the technologies are commercially available. We argue that comprehensive and reliable auditing of personnel selection technologies must be contextual, that is, embedded in existing processes and based on real data, as well as participative, involving various stakeholders beyond technology vendors and customers, such as advocacy organizations and researchers. We propose an architectural draft that employs a data trustee to provide independent, fiduciary management of personal and corporate data to audit the fairness of technologies used in personnel selection. Drawing on a case study conducted with two state-owned companies in Berlin, Germany, we discuss challenges and approaches related to suitable fairness metrics, operationalization of vague concepts such as migration* and applicable legal foundations that can be utilized to overcome the fairness-privacy-dilemma arising from uncertainties associated with current laws. We highlight issues that require further interdisciplinary research to enable a prototypical implementation of the auditing concept in the mid-term.","",""
"2024","From pen to algorithm: optimizing legislation for the future with artificial intelligence","AbstractThis research poses the question of whether it is possible to optimize modern legislative drafting by integrating LLM-based systems into the lawmaking process to address the pervasive challenge of misinformation and disinformation in the age of AI. While misinformation is not a novel phenomenon, with the proliferation of social media and AI, disseminating false or misleading information has become a pressing societal concern, undermining democratic processes, public trust, and social cohesion. AI can be used to proliferate disinformation and misinformation through fake news and deepfakes; can AI also be used for beneficial purposes to develop the antidote legislation combatting these challenges? Leveraging the capabilities of LLMS, such as ChatGPT and others, can present a promising direction for optimizing legislative drafting. By proposing the methodological approach of an AI bun, this article explores an important approach in which LLMS can support lawmakers and policy experts in crafting legislation. The article contributes to the discourse through a nuanced understanding of the opportunities and challenges in harnessing LLM-powered tools for legislative innovation. Ultimately, it underscores the transformative potential of LLMs as a potential resource for lawmakers seeking to navigate decision-making while developing legislation on an example of navigating the intricate landscape of misinformation and disinformation regulation in the age of AI.","",""
"2024","Potentials of including children in the formation of AI","","",""
"2024","Comparative analysis of features extraction techniques for black face age estimation","","",""
"2024","Autonomous weapon systems and jus ad bellum","AbstractIn this article, we focus on the scholarly and policy debate on autonomous weapon systems (AWS) and particularly on the objections to the use of these weapons which rest on jus ad bellum principles of proportionality and last resort. Both objections rest on the idea that AWS may increase the incidence of war by reducing the costs for going to war (proportionality) or by providing a propagandistic value (last resort). We argue that whilst these objections offer pressing concerns in their own right, they suffer from important limitations: they overlook the difficulties of calculating ad bellum proportionality; confuse the concept of proportionality of effects with the precision of weapon systems; disregard the ever-changing nature of war and of its ethical implications; mistake the moral obligation imposed by the principle of last resort with the impact that AWS may have on political decision to resort to war. Our analysis does not entail that AWS are acceptable or justifiable, but it shows that ad bellum principles are not the best set of ethical principles for tackling the ethical problems raised by AWS; and that developing adequate understanding of the transformations that the use of AWS poses to the nature of war itself is a necessary, preliminary requirement to any ethical analysis of the use of these weapons.","",""
"2024","Let AI learn from all: lessons from Poor Things for improving the intellectual scope of AI","","",""
"2024","Transparency in AI","","",""
"2024","Escape climate apathy by harnessing the power of generative AI","","",""
"2024","Surveying Judges about artificial intelligence: profession, judicial adjudication, and legal principles","","",""
"2024","AI ethics with Chinese characteristics? Concerns and preferred solutions in Chinese academia","AbstractSince Chinese scholars are playing an increasingly important role in shaping the national landscape of discussion on AI ethics, understanding their ethical concerns and preferred solutions is essential for global cooperation on governance of AI. This article, therefore, provides the first elaborated analysis on the discourse on AI ethics in Chinese academia, via a systematic literature review. This article has three main objectives. (1) to identify the most discussed ethical issues of AI in Chinese academia and those being left out (the question of “what”); (2) to analyze the solutions proposed and preferred by Chinese scholars (the question of “how”); and (3) to map out whose voices are dominating and whose are in the marginal (the question of “who”). Findings suggest that in terms of short-term implications, Chinese scholars’ concerns over AI resemble predominantly the content of international ethical guidelines. Yet in terms of long-term implications, there are some significant differences needed to be further addressed in a cultural context. Further, among a wide range of solution proposals, Chinese scholars seem to prefer strong-binding regulations to those weak ethical guidelines. In addition, this article also found that the Chinese academic discourse was dominated by male scholars and those who are from elite universities, which arguably is not a unique phenomenon in China.","",""
"2024","The argument for near-term human disempowerment through AI","AbstractMany researchers and intellectuals warn about extreme risks from artificial intelligence. However, these warnings typically came without systematic arguments in support. This paper provides an argument that AI will lead to the permanent disempowerment of humanity, e.g. human extinction, by 2100. It rests on four substantive premises which it motivates and defends: first, the speed of advances in AI capability, as well as the capability level current systems have already reached, suggest that it is practically possible to build AI systems capable of disempowering humanity by 2100. Second, due to incentives and coordination problems, if it is possible to build such AI, it will be built. Third, since it appears to be a hard technical problem to build AI which is aligned with the goals of its designers, and many actors might build powerful AI, misaligned powerful AI will be built. Fourth, because disempowering humanity is useful for a large range of misaligned goals, such AI will try to disempower humanity. If AI is capable of disempowering humanity and tries to disempower humanity by 2100, then humanity will be disempowered by 2100. This conclusion has immense moral and prudential significance.","",""
"2024","Augmenting morality through ethics education: the ACTWith model","","",""
"2024","Missed opportunities for AI governance: lessons from ELS programs in genomics, nanotechnology, and RRI","AbstractSince the beginning of the current hype around Artificial Intelligence (AI), governments, research institutions, and the industry invited ethical, legal, and social sciences (ELS) scholars to research AI’s societal challenges from various disciplinary viewpoints and perspectives. This approach builds upon the tradition of supporting research on the societal aspects of emerging sciences and technologies, which started with the Ethical, Legal, and Social Implications (ELSI) Program in the Human Genome Project (HGP) in the early 1990s. However, although a diverse ELS research community has formed since then, AI’s societal challenges came to be mostly understood under the narrow framing of ethics and disconnected from the insights and experiences of past ELS research. In this article, we make up for this gap and connect insights from past ELS researchers with current approaches to research the societal challenges of AI. We analyse and summarize the history of “ELS programs” (programs that emerged since the HGP to support ELS research in a given domain) as three distinct eras: a genomics era, a nano era, and an RRI era. Each of these eras comprises several achievements and challenges relevant to ELS programs in AI research, such as the setup of independent funding bodies, the engagement of the wider public in research practice, and the increasing importance of private actors. Based on these insights, we argue that AI research currently falls back on self-regulatory, less participatory, and industry-led approaches that trouble ELS programs’ past achievements and hinder opportunities to overcome the still-existing challenges.","",""
"2024","Challenges in enabling user control over algorithm-based services","AbstractAlgorithmic systems that provide services to people by supporting or replacing human decision-making promise greater convenience in various areas. The opacity of these applications, however, means that it is not clear how much they truly serve their users. A promising way to address the issue of possible undesired biases consists in giving users control by letting them configure a system and aligning its performance with users’ own preferences. However, as the present paper argues, this form of control over an algorithmic system demands an algorithmic literacy that also entails a certain way of making oneself knowable: users must interrogate their own dispositions and see how these can be formalized such that they can be translated into the algorithmic system. This may, however, extend already existing practices through which people are monitored and probed and means that exerting such control requires users to direct a computational mode of thinking at themselves.","",""
"2024","The autonomous choice architect","AbstractChoice architecture describes the environment in which choices are presented to decision-makers. In recent years, public and private actors have looked at choice architecture with great interest as they seek to influence human behaviour. These actors are typically called choice architects. Increasingly, however, this role of architecting choice is not performed by a human choice architect, but an algorithm or artificial intelligence, powered by a stream of Big Data and infused with an objective it has been programmed to maximise. We call this entity the autonomous choice architect. In this paper, we present an account of why artificial intelligence can fulfil the role of a choice architect and why this creates problems of transparency, responsibility and accountability for nudges. We argue that choice architects, be them autonomous computational systems or human-beings, at a most basic level select, from a range of designs, the design which is most likely to maximise a pre-determined objective. We then proceed to argue that, given the growing demand for targeted, personalised choice architecture and for faster, dynamic reconfigurations of choice architecture, as well as the ever-expanding pool of data from which feedback can be drawn, the role of the human choice architect is increasingly obscured behind algorithmic, artificially intelligent systems. We provide a discussion of the implications of autonomous choice architects, focusing on the importance of the humans who programme these systems, ultimately arguing that despite technological advances, the responsibility of choice architecture and influence remains firmly one human beings must bear.","",""
"2024","The QWERTY keyboard from the perspective of the Collingridge dilemma: lessons for co-construction of human-technology","","",""
"2024","AI systems and the question of African personhood","","",""
"2024","Apocalypse now: no need for artificial general intelligence","","",""
"2024","Multiple unnatural attributes of AI undermine common anthropomorphically biased takeover speculations","AbstractAccelerating advancements in artificial intelligence (AI) have increased concerns about serious risks, including potentially catastrophic risks to humanity. Prevailing trends of AI R&amp;D are leading to increasing humanization of AI, to the emergence of concerning behaviors, and toward possible recursive self-improvement. There has been increasing speculation that these factors increase the risk of an AI takeover of human affairs, and possibly even human extinction. The most extreme of such speculations result at least partly from anthropomorphism, but since AIs are being humanized, it is challenging to disentangle valid from invalid anthropomorphic concerns. This publication identifies eight fundamentally unnatural attributes of digital AI, each of which should differentiate AI behaviors from those of biological organisms, including humans. All have the potential to accelerate AI evolution, which might increase takeover concerns; but surprisingly, most also have the potential to defuse the hypothetical conflicts that dominate takeover speculations. Certain attributes should give future AI long-term foresight and realism that are essentially impossible for humans. I conclude that claims of highly probable hostile takeover and human extinction suffer from excessive anthropomorphism and a lack of skepticism and scientific rigor. Given the evidence presented here, I propose a more plausible but still speculative future scenario: extensively humanized AIs will become vastly more capable than humans of making decisions that benefit humans, and rational people will want AI to assume progressively greater influence over human affairs.","",""
"2024","Between world models and model worlds: on generality, agency, and worlding in machine learning","AbstractThe article offers a discursive account of what generality in machine learning research means and how it is constructed in the development of general artificial intelligence from the perspectives of cultural and media studies. I discuss several technical papers that outline novel architectures in machine learning and how they conceive of the “world”. The agency to learn and the learning curriculum are modulated through worlding (in the sense of setting up and unfolding of the world for artificial agents) in machine learning engineering. In recent computer science articles, large models trained on Internet-scale datasets are framed as general world simulators—despite their partiality, historicity, finite nature, and cultural specificity. I introduce the notion of “model worlds” to refer to composable interactive environments designed for the purpose of machine learning that partake in legitimising that claim. I discuss how large models are grounded through interaction in model worlds, arguing that model worlds mediate between the sheer scale of language models and their hypothetical capacity to generalise to new tasks and domains, rehashing the empiricist logic of “big data”. Further, I show that the emerging capacity of artificial agents to generalise redraws the epistemic boundary between artificial agents and their learning environments. Consequently, superficial statistics of language models and abstract action are made meaningful in distilled model worlds, giving rise to synthetic agency.","",""
"2024","Democratizing AI in public administration: improving equity through maximum feasible participation","AbstractIn an era defined by the global surge in the adoption of AI-enabled technologies within public administration, the promises of efficiency and progress are being overshadowed by instances of deepening social inequality, particularly among vulnerable populations. To address this issue, we argue that democratizing AI is a pivotal step toward fostering trust, equity, and fairness within our societies. This article navigates the existing debates surrounding AI democratization but also endeavors to revive and adapt the historical social justice framework, maximum feasible participation, for contemporary participatory applications in deploying AI-enabled technologies in public administration. In our exploration of the multifaceted dimensions of AI’s impact on public administration, we provide a roadmap that can lead beyond rhetoric to practical solutions in the integration of AI in public administration.","",""
"2024","Differences in stakeholders’ expectations of gendered robots in the field of psychotherapy: an exploratory survey","AbstractIn the present study, qualitative and quantitative studies were conducted to explore differences between stakeholders in expectations of gendered robots, with a focus on their specific application in the field of psychotherapy. In Study I, semi-structured interviews were conducted with 18 experts in psychotherapy to extract categories of opinions regarding the use of humanoid robots in the field. Based on these extracted categories, in Study II, an online questionnaire survey was conducted to compare concrete expectations of the use of humanoid robots in psychotherapy between 50 experts and 100 nonexperts in psychotherapy. The results revealed that compared with the female participants, the male participants tended to prefer robots with a female appearance. In addition, compared with the experts, the nonexperts tended not to relate the performance of robots with their gender appearance, and compared with the other participant groups, the female expert participants had lower expectations of the use of robots in the field. These findings suggest that differences between stakeholders regarding the expectations of gendered robots should be resolved to encourage their acceptance in a specific field.","",""
"2024","Why we need to be weary of emotional AI","","",""
"2024","Hasta la vista baby: why we should dispense of “autonomy” in “autonomous systems”","","",""
"2024","Application of artificial intelligence: risk perception and trust in the work context with different impact levels and task types","AbstractFollowing the studies of Araujo et al. (AI Soc 35:611–623, 2020) and Lee (Big Data Soc 5:1–16, 2018), this empirical study uses two scenario-based online experiments. The sample consists of 221 subjects from Germany, differing in both age and gender. The original studies are not replicated one-to-one. New scenarios are constructed as realistically as possible and focused on everyday work situations. They are based on the AI acceptance model of Scheuer (Grundlagen intelligenter KI-Assistenten und deren vertrauensvolle Nutzung. Springer, Wiesbaden, 2020) and are extended by individual descriptive elements of AI systems in comparison to the original studies. The first online experiment examines decisions made by artificial intelligence with varying degrees of impact. In the high-impact scenario, applicants are automatically selected for a job and immediately received an employment contract. In the low-impact scenario, three applicants are automatically invited for another interview. In addition, the relationship between age and risk perception is investigated. The second online experiment tests subjects’ perceived trust in decisions made by artificial intelligence, either semi-automatically through the assistance of human experts or fully automatically in comparison. Two task types are distinguished. The task type that requires “human skills”—represented as a performance evaluation situation—and the task type that requires “mechanical skills”—represented as a work distribution situation. In addition, the extent of negative emotions in automated decisions is investigated. The results are related to the findings of Araujo et al. (AI Soc 35:611–623, 2020) and Lee (Big Data Soc 5:1–16, 2018). Implications for further research activities and practical relevance are discussed.","",""
"2024","AI and the iterable epistopics of risk","AbstractThe risks AI presents to society are broadly understood to be manageable through ‘general calculus’, i.e., general frameworks designed to enable those involved in the development of AI to apprehend and manage risk, such as AI impact assessments, ethical frameworks, emerging international standards, and regulations. This paper elaborates how risk is apprehended and managed by a regulator, developer and cyber-security expert. It reveals that risk and risk management is dependent on mundane situated practices not encapsulated in general calculus. Situated practice surfaces ‘iterable epistopics’, revealing how those involved in the development of AI know and subsequently respond to risk and uncover major challenges in their work. The ongoing discovery and elaboration of epistopics of risk in AI (a) furnishes a potential program of interdisciplinary inquiry, (b) provides AI developers with a means of apprehending risk, and (c) informs the ongoing evolution of general calculus.","",""
"2024","Omission and commission errors underlying AI failures","","",""
"2024","Adaptable robots, ethics, and trust: a qualitative and philosophical exploration of the individual experience of trustworthy AI","AbstractMuch has been written about the need for trustworthy artificial intelligence (AI), but the underlying meaning of trust and trustworthiness can vary or be used in confusing ways. It is not always clear whether individuals are speaking of a technology’s trustworthiness, a developer’s trustworthiness, or simply of gaining the trust of users by any means. In sociotechnical circles, trustworthiness is often used as a proxy for ‘the good’, illustrating the moral heights to which technologies and developers ought to aspire, at times with a multitude of diverse requirements; or at other times, no specification at all. In philosophical circles, there is doubt that the concept of trust should be applied at all to technologies rather than their human creators. Nevertheless, people continue to intuitively reason about trust in technologies in their everyday language. This qualitative study employed an empirical ethics methodology to address how developers and users define and construct requirements for trust throughout development and use, through a series of interviews. We found that different accounts of trust (rational, affective, credentialist, norms based, relational) served as the basis for individual granting of trust in technologies and operators. Ultimately, the most significant requirement for user trust and assessment of trustworthiness was the accountability of AI developers for the outputs of AI systems, hinging on the identification of accountable moral agents and perceived value alignment between the user and developer’s interests.","",""
"2024","Social robots as partners?","AbstractAlthough social robots are achieving increasing prominence as companions and carers, their status as partners in an interactive relationship with humans remains unclear. The present paper explores this issue, first, by considering why social robots cannot truly qualify as “Thous”, that is, as surrogate human partners, as they are often assumed to be, and then by briefly considering why it will not do to construe them as mere machines, slaves, or pets, as others have contended. Having concluded that none of these familiar designations does justice to social robots’ still evolving and yet-to-be-defined status, I go on to consider whether engaging in a “relational turn” which prioritises the relationship over the entities in relation, can provide a more satisfactory alternative. In defending this stance, Damiano and Dumouchel (HUMANA.MENTE J Philos Stud 13:181–206, 2020) contend that in addition to foregrounding the possibility that social robots constitute new types of artificial companions which we can find “companionable in different ways”, the relational turn has the added advantage of providing a more expansive and productive ethical framework for future research and development in this domain. But on balance, it is far from clear that this approach can circumvent the so-called “dummy-human” problem, the contention that social robots are little more than sophisticated toys with the potential to deceive their users about the prospects for genuine partnership and bonding. Accordingly, I conclude that while Damiano and Dumouchel’s shift in “target phenomenology” brings to the fore facets of the interaction that may be difficult to infer by focussing on the relata alone, it is ultimately too “thin” to qualify as a template for robust human-robotic interaction of the sort that could qualify as a genuine partnership. Hence, while their relational turn has its merits in alerting us to the novel possibilities and challenges afforded by this ever-evolving technology and to the limitations of existing frameworks, it is contended that in itself it cannot provide an adequate template for conceptualising what a genuine human–robot partnership might entail. Instead, we need a more expansive approach that can do greater justice to the complexities of human-robotic interaction in its continuities as well as differences from more familiar human (and animal) prototypes. ","",""
"2024","“Machine Down”: making sense of human–computer interaction—Garfinkel’s research on ELIZA and LYRIC from 1967 to 1969 and its contemporary relevance","AbstractThis paper examines Harold Garfinkel’s work with ELIZA and a related program LYRIC from 1967 to 1969. AI researchers have tended to treat successful human–machine interaction as if it relied primarily on non-human machine characteristics, and thus the often-reported attribution of human-like qualities to communication with computers has been criticized as a misperception—and humans who make such reports referred to as “deluded.” By contrast Garfinkel, building on two decades of prior research on information and communication, argued that the ELIZA and the LYRIC “chatbots” were achieving interactions that felt human to many users by exploiting human sense-making practices. In keeping with his long-term practice of using “trouble” as a way of discovering the taken-for-granted practices of human sense-making, Garfinkel designed scripts for ELIZA and LYRIC that he could disrupt in order to reveal how their success depended on human social practices. Hence, the announcement “Machine Down” by the chatbot was a desired result of Garfinkel’s interactions with it. This early (but largely unknown) research has implications not only for understanding contemporary AI chatbots, but also opens possibilities for respecifying current information systems design and computational practices to provide for the design of more flexible information objects.","",""
"2024","Special issue: AI and next generation supply networks","","",""
"2024","Towards just and equitable Web3: social work recommendations for inclusive practice of AI policies","","",""
"2024","“Please understand we cannot provide further information”: evaluating content and transparency of GDPR-mandated AI disclosures","AbstractThe General Data Protection Regulation (GDPR) of the EU confirms the protection of personal data as a fundamental human right and affords data subjects more control over the way their personal information is processed, shared, and analyzed. However, where data are processed by artificial intelligence (AI) algorithms, asserting control and providing adequate explanations is a challenge. Due to massive increases in computing power and big data processing, modern AI algorithms are too complex and opaque to be understood by most data subjects. Articles 15 and 22 of the GDPR provide a modest regulatory framework for automated data processing by, among other things, mandating that data controllers inform data subjects about when it is being used, and its logic and ramifications. Nevertheless, due to the phrasing of the articles and the numerous exceptions they allow, doubts have arisen about their effectiveness. In this paper, we empirically evaluate the quality and effectiveness of AI disclosures as mandated by the GDPR. By means of an online survey (N = 835), we investigated how data subjects expect to be informed about the automated processing of their data. We then conducted a content analysis of the AI disclosures of N = 100 companies and organizations. The combined findings reveal that current GDPR-mandated disclosures do not meet the expectations and needs of data subjects. Explanations drawn up following the guidelines of the generic formulations of the GDPR differ widely and are often vague, incomplete and lack transparency. In our conclusions we identify a path towards standardizing and optimizing AI information notices.","",""
"2024","Creating meaningful work in the age of AI: explainable AI, explainability, and why it matters to organizational designers","AbstractIn this paper, we contribute to research on enterprise artificial intelligence (AI), specifically to organizations improving the customer experiences and their internal processes through using the type of AI called machine learning (ML). Many organizations are struggling to get enough value from their AI efforts, and part of this is related to the area of explainability. The need for explainability is especially high in what is called black-box ML models, where decisions are made without anyone understanding how an AI reached a particular decision. This opaqueness creates a user need for explanations. Therefore, researchers and designers create different versions of so-called eXplainable AI (XAI). However, the demands for XAI can reduce the accuracy of the predictions the AI makes, which can reduce the perceived usefulness of the AI solution, which, in turn, reduces the interest in designing the organizational task structure to benefit from the AI solution. Therefore, it is important to ensure that the need for XAI is as low as possible. In this paper, we demonstrate how to achieve this by optimizing the task structure according to sociotechnical systems design principles. Our theoretical contribution is to the underexplored field of the intersection of AI design and organizational design. We find that explainability goals can be divided into two groups, pattern goals and experience goals, and that this division is helpful when defining the design process and the task structure that the AI solution will be used in. Our practical contribution is for AI designers who include organizational designers in their teams, and for organizational designers who answer that challenge.","",""
"2024","Artificial reproduction? Tabita Rezaire’s Sugar Walls Teardom and AI “liveness”","","",""
"2024","AI employment decision-making: integrating the equal opportunity merit principle and explainable AI","","",""
"2024","The work of art in the age of generative AI: aura, liberation, and democratization","","",""
"2024","Collaborative route map and navigation of the guide dog robot based on optimum energy consumption","","",""
"2024","The extimate core of understanding: absolute metaphors, psychosis and large language models","AbstractThis paper delves into the striking parallels between the linguistic patterns of Large Language Models (LLMs) and the concepts of psychosis in Lacanian psychoanalysis. Lacanian theory, with its focus on the formal and logical underpinnings of psychosis, provides a compelling lens to juxtapose human cognition and AI mechanisms. LLMs, such as GPT-4, appear to replicate the intricate metaphorical and metonymical frameworks inherent in human language. Although grounded in mathematical logic and probabilistic analysis, the outputs of LLMs echo the nuanced linguistic associations found in metaphor and metonymy, suggesting a mirroring of human linguistic structures. A pivotal point in this discourse is the exploration of “absolute metaphors”—core gaps in reasoning discernible in both AI models and human thought processes and central to the Lacanian conceptualization of psychosis. Despite the traditional divide between AI research and continental philosophy, this analysis embarks on an innovative journey, utilizing Lacanian philosophy to unravel the logic of AI, using concepts established in the continental discourse on logic, rather than the analytical tradition.","",""
"2024","Embracing liberatory alienation:AI will end us, but not in the way you may think","AbstractThis paper introduces the concept of """"liberatory alienation"""" to explore the complex relationship between technological advancement, particularly artificial intelligence (AI), and human essence. Building upon and critiquing Marx's theory of alienation, we argue that the externalization of human abilities through technology, while potentially disorienting, can ultimately lead to societal liberation and a redefined conception of humanity. The paper examines how AI and automation are reshaping our understanding of labor, skills, and human nature, challenging traditional notions of what it means to be human.We propose that as AI increasingly takes over both manual and routine cognitive tasks, humans are liberated to focus on uniquely human qualities such as creativity, agency, and the capacity for joy. This transformation is likened to an evolutionary process, where humans shed layers of false humanity tied to productive labor, revealing a more authentic core. The implications of this shift for education are discussed, advocating for a fundamental reassessment of educational priorities to cultivate these essential human qualities.The paper also addresses potential challenges, including the environmental impact of AI development and the need for human control over AI systems. By reframing alienation as a potentially liberating force, this work contributes to ongoing debates about the future of work, human identity, and the role of technology in society, offering a nuanced perspective on how we might navigate the profound changes brought about by AI and automation.","",""
"2024","Algorithmic governance and AI: balancing innovation and oversight in Indonesian policy analyst","","",""
"2024","AI and emotions: enhancing green intentions through personalized recommendations—a mediated moderation analysis","","",""
"2024","Strong and weak AI narratives: an analytical framework","AbstractThe current debate on artificial intelligence (AI) tends to associate AI imaginaries with the vision of a future technology capable of emulating or surpassing human intelligence. This article advocates for a more nuanced analysis of AI imaginaries, distinguishing “strong AI narratives,” i.e., narratives that envision futurable AI technologies that are virtually indistinguishable from humans, from """"weak"""" AI narratives, i.e., narratives that discuss and make sense of the functioning and implications of existing AI technologies. Drawing on the academic literature on AI narratives and imaginaries and examining examples drawn from the debate on Large Language Models and public policy, we underscore the critical role and interplay of weak and strong AI across public/private and fictional/non-fictional discourses. The resulting analytical framework aims to empower approaches that are more sensitive to the heterogeneity of AI narratives while also advocating normalising AI narratives, i.e., positioning weak AI narratives more firmly at the center stage of public debates about emerging technologies.","",""
"2024","Air Canada’s chatbot illustrates persistent agency and responsibility gap problems for AI","","",""
"2024","The case for a broader approach to AI assurance: addressing “hidden” harms in the development of artificial intelligence","AbstractArtificial intelligence (AI) assurance is an umbrella term describing many approaches—such as impact assessment, audit, and certification procedures—used to provide evidence that an AI system is legal, ethical, and technically robust. AI assurance approaches largely focus on two overlapping categories of harms: deployment harms that emerge at, or after, the point of use, and individual harms that directly impact a person as an individual.  Current approaches generally overlook upstream collective and societal harms associated with the development of systems, such as resource extraction and processing, exploitative labour practices and energy intensive model training. Thus, the scope of current AI assurance practice is insufficient for ensuring that AI is ethical in a holistic sense, i.e. in ways that are legally permissible, socially acceptable, economically viable and environmentally sustainable. This article addresses this shortcoming by arguing for a broader approach to AI assurance that is sensitive to the full scope of AI development and deployment harms. To do so, the article maps harms related to AI and highlights three examples of harmful practices that occur upstream in the AI supply chain and impact the environment, labour, and data exploitation. It then reviews assurance mechanisms used in adjacent industries to mitigate similar harms, evaluating their strengths, weaknesses, and how effectively they are being applied to AI. Finally, it provides recommendations as to how a broader approach to AI assurance can be implemented to mitigate harms more effectively across the whole AI supply chain.","",""
"2024","Correction to: Introduction: special issue—critical robotics research","","",""
"2024","Magical thinking and the test of humanity: we have seen the danger of AI and it is us","","",""
"2024","ChatGPT and societal dynamics: navigating the crossroads of AI and human interaction","","",""
"2024","Why artificial intelligence needs sociology of knowledge: parts I and II","AbstractRecent developments in artificial intelligence based on neural nets—deep learning and large language models which together I refer to as NEWAI—have resulted in startling improvements in language handling and the potential to keep up with changing human knowledge by learning from the internet. Nevertheless, examples such as ChatGPT, which is a ‘large language model’, have proved to have no moral compass: they answer queries with fabrications with the same fluency as they provide facts. I try to explain why this is, basing the argument on the sociology of knowledge, particularly social studies of science, notably ‘studies of expertise and experience’ and the ‘fractal model’ of society. Learning from the internet is not the same as socialisation: NEWAI has no primary socialisation such as provides the foundations of human moral understanding. Instead, large language models are retrospectively socialised by human intervention in an attempt to align them with societally accepted ethics. Perhaps, as technology advances, large language models could come to understand speech and recognise objects sufficiently well to acquire the equivalent of primary socialisation. In the meantime, we must be vigilant about who is socialising them and be aware of the danger of their socialising us to align with them rather than vice-versa, an eventuality that would lead to the further erosion of the distinction between the true and the false giving further support to populism and fascism.","",""
"2024","The open texture of ‘algorithm’ in legal language","AbstractIn this paper, we will survey the different uses of the term algorithm in contemporary legal practice. We will argue that the concept of algorithm currently exhibits a substantial degree of open texture, co-determined by the open texture of the concept of algorithm itself and by the open texture inherent to legal discourse. We will substantiate our argument by virtue of a case study, in which we analyze a recent jurisprudential case where the first and second-degree judges have carved-out contrasting notions of algorithm. We will see that, thanks to our analysis of the open texture of the notion of algorithm in legal language, we can make sense of the different decisions taken by the judges as different contextually-determined sharpenings of the concept of algorithm. Finally, we will draw some general conclusions concerning the use of technical terms in legal instruments that address new technologies, such as the EU AI Act.","",""
"2024","Correction to: Robots as moral environments","","",""
"2024","Some discussions on critical information security issues in the artificial intelligence era","","",""
"2024","Effects of generative AI on service occupations with social interaction","","",""
"2024","The rise of AI in job applications: a generative adversarial tug-of-war","","",""
"2024","Can large language models apply the law?","AbstractThis paper asks whether large language models (LLMs) can apply the law. It does not question whether LLMs should apply the law. Instead, it distinguishes between two interpretations of the ‘can’ question. One, can LLMs apply the law like ordinary individuals? Two, can LLMs apply the law in the same manner as judges? The study examines D’Almeida’s theory of law application, divided into inferential and pragmatic law application. It argues that his account of pragmatic law application can be improved as it does not fully consider that law application (and rule-following) is a shared, public practice collectively realized by members of a linguistic community. The study concludes that LLMs cannot apply the law. They cannot apply the law in the inferential sense as they have mere syntactic (not semantic) interaction with the law. They cannot apply the law in the pragmatic sense as pragmatic law application does not depend on a single agent, whether that agent is a judge, an ordinary citizen, or a non-human entity.","",""
"2024","Grasping AI: experiential exercises for designers","AbstractArtificial intelligence (AI) and machine learning (ML) are increasingly integrated into the functioning of physical and digital products, creating unprecedented opportunities for interaction and functionality. However, there is a challenge for designers to ideate within this creative landscape, balancing the possibilities of technology with human interactional concerns. We investigate techniques for exploring and reflecting on the interactional affordances, the unique relational possibilities, and the wider social implications of AI systems. We introduced into an interaction design course (n = 100) nine ‘AI exercises’ that draw on more than human design, responsible AI, and speculative enactment to create experiential engagements around AI interaction design. We find that exercises around metaphors and enactments make questions of training and learning, privacy and consent, autonomy and agency more tangible, and thereby help students be more reflective and responsible on how to design with AI and its complex properties in both their design process and outcomes.","",""
"2024","Competing narratives in AI ethics: a defense of sociotechnical pragmatism","AbstractSeveral competing narratives drive the contemporary AI ethics discourse. At the two extremes are sociotechnical dogmatism, which holds that society is full of inefficiencies and imperfections that can only be solved by better technology; and sociotechnical skepticism, which highlights the unacceptable risks AI systems pose. While both narratives have their merits, they are ultimately reductive and limiting. As a constructive synthesis, we introduce and defend sociotechnical pragmatism—a narrative that emphasizes the central role of context and human agency in designing and evaluating emerging technologies. In doing so, we offer two novel contributions. First, we demonstrate how ethical and epistemological considerations are intertwined in the AI ethics discourse by tracing the dialectical interplay between dogmatic and skeptical narratives across disciplines. Second, we show through examples how sociotechnical pragmatism does more to promote fair and transparent AI than dogmatic or skeptical alternatives. By spelling out the assumptions that underpin sociotechnical pragmatism, we articulate a robust stance for policymakers and scholars who seek to enable societies to reap the benefits of AI while managing the associated risks through feasible, effective, and proportionate governance.","",""
"2024","Out of dataset, out of algorithm, out of mind: a critical evaluation of AI bias against disabled people","","",""
"2024","Artificial intelligence and economic planning","","",""
"2024","The role of collective agreements in times of uncertain AI governance: lessons from the Hollywood scriptwriters’ agreement","","",""
"2024","Friendly AI will still be our master. Or, why we should not want to be the pets of super-intelligent computers","AbstractWhen asked about humanity’s future relationship with computers, Marvin Minsky famously replied “If we’re lucky, they might decide to keep us as pets”. A number of eminent authorities continue to argue that there is a real danger that “super-intelligent” machines will enslave—perhaps even destroy—humanity. One might think that it would swiftly follow that we should abandon the pursuit of AI. Instead, most of those who purport to be concerned about the existential threat posed by AI default to worrying about what they call the “Friendly AI problem”. Roughly speaking this is the question of how we might ensure that the AI that will develop from the first AI that we create will remain sympathetic to humanity and continue to serve, or at least take account of, our interests. In this paper I draw on the “neo-republican” philosophy of Philip Pettit to argue that solving the Friendly AI problem would not change the fact that the advent of super-intelligent AI would be disastrous for humanity by virtue of rendering us the slaves of machines. A key insight of the republican tradition is that freedom requires equality of a certain sort, which is clearly lacking between pets and their owners. Benevolence is not enough. As long as AI has the power to interfere in humanity’s choices, and the capacity to do so without reference to our interests, then it will dominate us and thereby render us unfree. The pets of kind owners are still pets, which is not a status which humanity should embrace. If we really think that there is a risk that research on AI will lead to the emergence of a superintelligence, then we need to think again about the wisdom of researching AI at all. ","",""
"2024","The Indian approach to Artificial Intelligence: an analysis of policy discussions, constitutional values, and regulation","","",""
"2024","How does artificial intelligence work in organisations? Algorithmic management, talent and dividuation processes","AbstractThis article analyses the forms of dividuation workers undergo when they are linked to technologies, such as algorithms or artificial intelligence. It examines functionalities and operations deployed by certain types of Talent Management software and apps—UKG, Tribepad, Afiniti, RetailNext and Textio. Specifically, it analyses how talented workers materialise in relation to the profiles and the statistical models generated by such artificial intelligence machines. It argues that these operate as a nooscope that allows the transindividual plane to be quantified through a process known as dividuation. Finally, by way of conclusion, the notion of the dividual is presented as the logic that characterises the human–machine relationship in the case of artificial intelligence and as the horizon of what Felix Guattari called “machinic capitalism”.","",""
"2024","Judging the algorithm","AbstractThis paper discusses an algorithmic tool introduced in the Basque Country (Spain) to assess the risk of intimate partner violence. The algorithm was introduced to address the lack of human experts by automatically calculating the level of violence based on psychometric features such as controlling or violent behaviour. Given that critical literature on risk assessment tools for domestic violence mainly focuses on English-speaking countries, this paper offers an algorithmic accountability analysis in a non-English speaking region. It investigates the algorithmic risks, harms, and limitations associated with the Basque tool. We propose a transdisciplinary approach from a critical statistical and legal perspective. This approach unveils issues and limitations that could lead to unexpected consequences for individuals suffering from partner violence. Moreover, our analysis suggests that the algorithmic tool has a high error rate on severe cases, i.e., cases where the aggressor could murder his partner—5 out of 10 high-risk cases are misclassified as low risk—and that there is a lack of appropriate legal guidelines for judges, the end users of this tool. The paper concludes that this risk assessment tool needs to be urgently evaluated by independent and transdisciplinary experts to better mitigate algorithmic harms in the context of intimate partner violence.","",""
"2024","Basic values in artificial intelligence: comparative factor analysis in Estonia, Germany, and Sweden","AbstractIncreasing attention is paid to ethical issues and values when designing and deploying artificial intelligence (AI). However, we do not know how those values are embedded in artificial artefacts or how relevant they are to the population exposed to and interacting with AI applications. Based on literature engaging with ethical principles and moral values in AI, we designed an original survey instrument, including 15 value components, to estimate the importance of these values to people in the general population. The article is based on representative surveys conducted in Estonia, Germany, and Sweden (n = 4501), which have varying experiences with implementing AI. The factor analysis showed four underlying dimensions of values embedded in the design and use of AI: (1) protection of personal interests to ensure social benefit, (2) general monitoring to ensure universal solidarity, (3) ensuring social diversity and social sustainability, and (4) efficiency. We found that value types can be ordered along the two dimensions of resources and change. The comparison between countries revealed that some dimensions, like social diversity and sustainability evaluations, are more universally valued among individuals, countries, and domains. Based on our analysis, we suggest a need and a framework for developing basic values in AI.","",""
"2024","Review of “Knowing our world: an artificial intelligence perspective”, by George F. Luger, Springer, 2021","","",""
"2024","‘Interpretability’ and ‘alignment’ are fool’s errands: a proof that controlling misaligned large language models is the best anyone can hope for","","",""
"2024","Toward an empathy-based trust in human-otheroid relations","AbstractThe primary aim of this paper is twofold: firstly, to argue that we can enter into relation of trust with robots and AI systems (automata); and secondly, to provide a comprehensive description of the underlying mechanisms responsible for this  relation of trust. To achieve these objectives, the paper first undertakes a critical examination of the main arguments opposing the concept of a trust-based relation with automata. Showing that these arguments face significant challenges that render them untenable, it thereby prepares the ground for the subsequent positive analysis, proposing a framework in which these challenges can be addressed . According to this framework  trust does not originate from mere reliability, but rather from an empathic relation with automata. This initial empathic relation elevates the automata to the status of what I will term """"Otheroids."""" The paper then explores how this human-Otheroid relationship inherently possesses the seeds for the development of trust. Finally, it examines how these seeds can grow into a basic form of trust with Otheroids through the establishment of a rich history of interaction.","",""
"2024","How to identify and address the real-world risks of large language models","","",""
"2024","Personhood for artificial intelligence? A cautionary tale from Idaho and Utah","","",""
"2024","Auditing the impact of artificial intelligence on the ability to have a good life: using well-being measures as a tool to investigate the views of undergraduate STEM students","","",""
"2024","Equal accuracy for Andrew and Abubakar—detecting and mitigating bias in name-ethnicity classification algorithms","AbstractUncovering the world’s ethnic inequalities is hampered by a lack of ethnicity-annotated datasets. Name-ethnicity classifiers (NECs) can help, as they are able to infer people’s ethnicities from their names. However, since the latest generation of NECs rely on machine learning and artificial intelligence (AI), they may suffer from the same racist and sexist biases found in many AIs. Therefore, this paper offers an algorithmic fairness audit of three NECs. It finds that the UK-Census-trained EthnicityEstimator displays large accuracy biases with regards to ethnicity, but relatively less among gender and age groups. In contrast, the Twitter-trained NamePrism and the Wikipedia-trained Ethnicolr are more balanced among ethnicity, but less among gender and age. We relate these biases to global power structures manifested in naming conventions and NECs’ input distribution of names. To improve on the uncovered biases, we program a novel NEC, N2E, using fairness-aware AI techniques. We make N2E freely available at www.name-to-ethnicity.com. ","",""
"2024","How far should we allow machines to further externalize human internal expression?","","",""
"2024","Living with AI personal assistant: an ethical appraisal","","",""
"2024","AI governance through fractal scaling: integrating universal human rights with emergent self-governance for democratized technosocial systems","","",""
"2024","The global AI framework: navigating challenges and societal impacts","","",""
"2024","Correction: The effect of artificial intelligence on creativity in conceptual design in architectural education: the motion of biomimetics and futurism","","",""
"2024","Narrativity and responsible and transparent ai practices","AbstractThis paper builds upon recent work in narrative theory and the philosophy of technology by examining the place of transparency and responsibility in discussions of AI, and what some of the implications of this might be for thinking ethically about AI and especially AI practices, that is, the structured social activities implicating and defining what AI is. In this paper, we aim to show how pursuing a narrative understanding of technology and AI can support knowledge of process and practice through transparency, as well help summon us to responsibility through visions of possibility and of actual harms arising from AI practices. We provide reflections on the relations between narrative, transparency and responsibility, building an argument that narratives (about AI, practices, and those persons implicated in its design, implementation, and deployment) support the kind of knowing and understanding that is the aim of transparency, and, moreover, that such knowledge supports responsibility in informing agents and activating responsibility through creating knowledge about something that can and should be responded to. Furthermore, we argue for considering an expansion of the kinds of practices that we might legitimately consider ‘AI practices’ given the diverse set of (often materially embedded) activities that sustain and are sustained by AI that link directly to its ethical acceptability and which are rendered transparent in the narrative mode. Finally, we argue for an expansion of narratives and narrative sources to be considered in questions of AI, understanding that transparency is multi-faceted and found in stories from diverse sources and people.","",""
"2024","Commonsense for AI: an interventional approach to explainability and personalization","","",""
"2024","What is a Turing test for emotional AI?","","",""
"2024","Think Differently We Must! An AI Manifesto for the Future","AbstractThere is a problematic tradition of dualistic and reductionist thinking in artificial intelligence (AI) research, which is evident in AI storytelling and imaginations as well as in public debates about AI. Dualistic thinking is based on the assumption of a fixed reality and a hierarchy of power, and it simplifies the complex relationships between humans and machines. This commentary piece argues that we need to work against the grain of such logics and instead develop a thinking that acknowledges AI–human interconnectedness and the complexity in such relations. To learn how to live better with AI in futures to come, the paper suggests an AI politics that turns to practices of serious attentiveness to help us re-imagine our machines and re-configure AI–human relations.","",""
"2024","Modeling AI Trust for 2050: perspectives from media and info-communication experts","AbstractThe study explores the future of AI-driven media and info-communication as envisioned by experts from all world regions, defining relevant terminology and expectations for 2050. Participants engaged in a 4-week series of surveys, questioning their definitions and projections about AI for the field of media and communication. Their expectations predict universal access to democratically available, automated, personalized and unbiased information determined by trusted narratives, recolonization of information technology and the demystification of the media process. These experts, as technology ambassadors, advocate AI-to-AI solutions to mitigate technology-driven misuse and misinformation. The optimistic scenarios shift responsibility to future generations, relying on AI-driven solutions and finding inspiration in nature. Their present-based forecasts could be construed as being indicative of professional near-sightedness and cognitive dissonance. Visualizing our findings into a Glasses Model of AI Trust, the study contributes to key debates regarding AI policy, developmental trajectories, and academic research in media and info-communication fields.","",""
"2024","Machine learning in human creativity: status and perspectives","","",""
"2024","The misdirected approach of open source algorithms","","",""
"2024","Art histories from nowhere: on the coloniality of experiments in art and artificial intelligence","","",""
"2024","Who shares about AI? Media exposure, psychological proximity, performance expectancy, and information sharing about artificial intelligence online","","",""
"2024","Justificatory explanations in machine learning: for increased transparency through documenting how key concepts drive and underpin design and engineering decisions","AbstractGiven the pervasiveness of AI systems and their potential negative effects on people’s lives (especially among already marginalised groups), it becomes imperative to comprehend what goes on when an AI system generates a result, and based on what reasons, it is achieved. There are consistent technical efforts for making systems more “explainable” by reducing their opaqueness and increasing their interpretability and explainability. In this paper, we explore an alternative non-technical approach towards explainability that complement existing ones. Leaving aside technical, statistical, or data-related issues, we focus on the very conceptual underpinnings of the design decisions made by developers and other stakeholders during the lifecycle of a machine learning project. For instance, the design and development of an app to track snoring to detect possible health risks presuppose some picture or another of “health”, which is a key notion that conceptually underpins the project. We take it as a premise that these key concepts are necessarily present during design and development, albeit perhaps tacitly. We argue that by providing “justificatory explanations” about how the team understands the relevant key concepts behind its design decisions, interested parties could gain valuable insights and make better sense of the workings and outcomes of systems. Using the concept of “health”, we illustrate how a particular understanding of it might influence decisions during the design and development stages of a machine learning project, and how making this explicit by incorporating it intoex-postexplanations might increase the explanatory and justificatory power of these explanations. We posit that a greater conceptual awareness of the key concepts that underpin design and development decisions may be beneficial to any attempt to develop explainability methods. We recommend that “justificatory explanations” are provided as technical documentation. These are declarative statements that contain at its simplest: (1) a high-level account of the understanding of the relevant key concepts a team possess related to a project’s main domain, (2) how these understandings drive decision-making during the life-cycle stages, and (3) it gives reasons (which could be implicit in the account) that the person or persons doing the explanation consider to have plausible justificatory power for the decisions that were made during the project.","",""
"2024","Reducing the contingency of the world: magic, oracles, and machine-learning technology","AbstractThe concept of magic is frequently used to discuss technology, a practice considered useful by some with others arguing that viewing technology as magic precludes a proper understanding of technology. The concept of magic is especially prominent in discussions of artificial intelligence (AI) and machine learning (ML). Based on an anthropological perspective, this paper juxtaposes ML technology with magic, using descriptions drawn from a project on an ML-powered system for propulsion control of cargo ships. The paper concludes that prior scholarly work on technology has failed to both define magic adequately and use research into magic. It also argues that although the distinction between ML technology and magic is important, recognition of the similarities is useful for understanding ML technology.","",""
"2024","Leveraging the potential of artificial intelligence (AI) in exploring the interplay among tax revenue, institutional quality, and economic growth in the G-7 countries","AbstractDue to G-7 countries' commitment to sustaining United Nations Sustainable Development Goal 8, which focuses on sustainable economic growth, there is a need to investigate the impact of tax revenue and institutional quality on economic growth, considering the role of artificial intelligence (AI) in the G-7 countries from 2012 to 2022. Cross-Sectional Augmented Autoregressive Distributed Lag (CS-ARDL) technique is used to analyze the data. The study's findings indicate a long-run equilibrium relationship among the variables under examination. The causality results can be categorized as bidirectional, unidirectional, or indicating no causality. Based on the CS-ARDL results, the study recommends that G-7 governments and policymakers prioritize and strengthen the integration of AI into their institutions to stimulate growth in both the short- and long-term. However, the study cautions against overlooking the interaction between AI and tax revenue, as it did not demonstrate support for economic growth. While the interaction between AI and institutional quality shows potential for contributing to growth, it is crucial to implement robust measures to mitigate any potential negative effects that may arise from AI's interaction with tax systems. Therefore, the study suggests the development of AI-friendly tax policies within the G-7 countries, considering the nascent nature of the AI sector/industry.","",""
"2024","Can we design artificial persons without being manipulative?","AbstractIf we could build artificial persons (APs) with a moral status comparable to this of a typical human being, how should we design those APs in the right way? This question has been addressed mainly in terms of designing APs devoted to being servants (AP servants) and debated in reference to their autonomy and the harm they might experience. Recently, it has been argued that even if developing AP servants would neither deprive them of autonomy nor cause any net harm, then developing such entities would still be unethical due to the manipulative attitude of their designers. I make two contributions to this discussion. First, I claim that the argument about manipulative attitude significantly shifts the perspective of the whole discussion on APs and that it refers to a much wider range of types of APs than has been acknowledged. Second, I investigate the possibilities of developing APs without a manipulative attitude. I proceed in the following manner: (1) I examine the argument about manipulativeness; (2) show the important novelty it brings to a discussion about APs; (3) analyze how the argument can be extrapolated to designing other kinds of Aps; and (4) discuss cases in which APs can be designed without manipulativeness.","",""
"2024","Artificial intelligence with American values and Chinese characteristics: a comparative analysis of American and Chinese governmental AI policies","AbstractAs China and the United States strive to be the primary global leader in AI, their visions are coming into conflict. This is frequently painted as a fundamental clash of civilisations, with evidence based primarily around each country’s current political system and present geopolitical tensions. However, such a narrow view claims to extrapolate into the future from an analysis of a momentary situation, ignoring a wealth of historical factors that influence each country’s prevailing philosophy of technology and thus their overarching AI strategies. In this article, we build a philosophy-of-technology-grounded framework to analyse what differences in Chinese and American AI policies exist and, on a fundamental level, why they exist. We support this with Natural Language Processing methods to provide an evidentiary basis for our analysis of policy differences. By looking at documents from three different American presidential administrations––Barack Obama, Donald Trump, and Joe Biden––as well as both national and local policy documents (many available only in Chinese) from China, we provide a thorough comparative analysis of policy differences. This article fills a gap in US–China AI policy comparison and constructs a framework for understanding the origin and trajectory of policy differences. By investigating what factors are informing each country’s philosophy of technology and thus their overall approach to AI policy, we argue that while significant obstacles to cooperation remain, there is room for dialogue and mutual growth.","",""
"2024","Poverty and freedom: philosophical reflection on the future development of artificial intelligence","","",""
"2024","Mind extended: relational, spatial, and performative ontologies","","",""
"2024","Risk and artificial general intelligence","AbstractArtificial General Intelligence (AGI) is said to pose many risks, be they catastrophic, existential and otherwise. This paper discusses whether the notion of risk can apply to AGI, both descriptively and in the current regulatory framework. The paper argues that current definitions of risk are ill-suited to capture supposed AGI existential risks, and that the risk-based framework of the EU AI Act is inadequate to deal with truly general, agential systems.","",""
"2024","Examining the impacts of artificial intelligence technology and computing on digital art: a case study of Edmond de Belamy and its aesthetic values and techniques","","",""
"2024","Correction: Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms","","",""
"2024","Striking the balance: ethical challenges and social implications of AI-induced power shifts in healthcare organizations","AbstractThe emergence of new digital technologies in modern work organizations is also changing the way employees and employers communicate, design work processes and responsibilities, and delegate. This paper takes an interdisciplinary—namely sociological and philosophical—perspective on the use of AI in healthcare work organizations. Using this example, structural power relations in modern work organizations are first examined from a sociological perspective, and it is shown how these structural power relations, decision-making processes, and areas of responsibility shift when AI is used. In the subsequent ethical part, opportunities for a fairer organization of work, but also dangers due to possibly changed power relations are elaborated and evaluated by presenting a realistic scenario from everyday clinical practice. After combining a proceduralist account of organizational ethics with a virtue-ethical approach, it is argued that certain organizational and character dispositions are necessary for employers and employees to meet the challenge of changing structural power relations in the future. With the same goal, a summative sociological perspective discusses challenges to workplace co-determination.","",""
"2024","Reimagining Benin Bronzes using generative adversarial networks","","",""
"2024","On the use of pride, hope and fear in China’s international artificial intelligence narratives on CGTN","AbstractChina communicates strategic narratives about artificial intelligence (AI) in digital media productions to create a shared meaning about its actions and its image in the global race to develop AI. Building upon the literature in emotions and strategic narratives, this study seeks to clarify which emotions are discursively used in China’s international AI narratives, and their function and significance. Specifically, the study investigates emotion discourses in AI-focused videos disseminated on China’s international broadcasting (CGTN YouTube channel). The analysis reveals that pride, hope and fear discourses have multiple functions in China’s international AI narratives on CGTN. Hope is used to represent China as a key competitor in the global AI race, who is catching up to the US. China uses pride to showcase its advances in AI applications, to highlight its transformation of traditional industries, and to identity itself as equal to the US. Fear is used to frame US’s perception of China’s AI developments, to suggest their crisis of confidence, but also to refute the “China threat”. Additionally, the fear discourse is used to deliberate anticipated risks, particularly on job loss and data privacy. These emotion discourses forge China’s identity as a future global AI power. The paper then discusses how these findings contribute to key debates about Chinese discourse and media strategy.","",""
"2024","Giordano Bruno’s prescience: tracing the Renaissance influence on artificial intelligence","","",""
"2024","Review of “AI assurance: towards trustworthy, explainable, safe, and ethical AI” by Feras A. Batarseh and Laura J. Freeman, Academic Press, 2023","","",""
"2024","Law, artificial intelligence, and synaesthesia","","",""
"2024","Deference to opaque systems and morally exemplary decisions","AbstractMany have recently argued that there are weighty reasons against making high-stakes decisions solely on the basis of recommendations from artificially intelligent (AI) systems. Even if deference to a given AI system were known to reliably result in the right action being taken, the argument goes, that deference would lack morally important characteristics: the resulting decisions would not, for instance, be based on an appreciation of right-making reasons. Nor would they be performed from moral virtue; nor would they have moral worth. I argue that, even if these characteristics all have intrinsic value, that intrinsic value has no practical relevance to decisions about whether to defer to AI. I make that point by drawing on a lesson from the literature on moral testimony. Once it is granted that deference to a reliable source is the policy most likely to bring about right action, a refusal to defer carries with it a heightened risk of wronging and mistreating people. And that heightened risk of wrongdoing, I argue, cannot be justified by appeal to the intrinsic value of striving for a morally exemplary decision-making process.","",""
"2024","A culture of their own? culture in robot-robot interaction","AbstractThis paper presents a framework for studying culture in the context of robot-robot interaction (RRI). We examine the claim that groups of robots can share a culture, even independently of their relationship with humans. At the centre of our framework is a recognition that ‘culture’ is a concept that can be defined and understood in many different ways. As we demonstrate, which definition of ‘culture’ one employs has important consequences for the question of whether groups of robots can have their own culture, and what kind of culture they can have. We suggest that this argument has important consequences for robotics from an ethical/legal perspective.","",""
"2024","Sticks and stones may break my bones, but words will never hurt me!—Navigating the cybersecurity risks of generative AI","","",""
"2024","Imagining and governing artificial intelligence: the ordoliberal way—an analysis of the national strategy ‘AI made in Germany’","AbstractNational Artificial Intelligence (AI) strategies articulate imaginaries of the integration of AI into society and envision the governing of AI research, development and applications accordingly. To integrate these central aspects of national AI strategies under one coherent perspective, this paper presented an analysis of Germany’s strategy ‘AI made in Germany’ through the conceptual lens of ordoliberal political rationality. The first part of the paper analyses how the guiding vision of a human-centric AI not only adheres to ethical and legal principles consistent with Germany’s liberal democratic constitutional system but also addresses the risks and promises inherent to the ordoliberal problematization of freedom. Second, it is scrutinized how the strategy cultivates the fear of not achieving technological sovereignty in the AI sector. Thereby, it frames the global AI race as a race of competing (national) approaches to governing AI and articulates an ordoliberal approach to governing AI (the ‘third way’), according to which government has to operate between the twin dangers of governing too much and not governing enough. Third, the paper analyses how this ordoliberal proportionality of governing structures Germany’s Science Technology &amp; Innovation Policy. It is shown that the corresponding risk-based approach of regulating AI constitutes a security apparatus as it produces an assessment of fears: weighting the fear of the failure to innovate with the fear of the ramifications of innovation. Finally, two lines of critical engagement based on this analysis are conducted.","",""
"2024","We’re only human after all: a critique of human-centred AI","AbstractThe use of a ‘human-centred’ artificial intelligence approach (HCAI) has substantially increased over the past few years in academic texts (1600 +); institutions (27 Universities have HCAI labs, such as Stanford, Sydney, Berkeley, and Chicago); in tech companies (e.g., Microsoft, IBM, and Google); in politics (e.g., G7, G20, UN, EU, and EC); and major institutional bodies (e.g., World Bank, World Economic Forum, UNESCO, and OECD). Intuitively, it sounds very appealing: placing human concerns at the centre of AI development and use. However, this paper will use insights from the works of Michel Foucault (mostly The Order of Things) to argue that the HCAI approach is deeply problematic in its assumptions. In particular, this paper will criticise four main assumptions commonly found within HCAI: human–AI hybridisation is desirable and unproblematic; humans are not currently at the centre of the AI universe; we should use humans as a way to guide AI development; AI is the next step in a continuous path of human progress; and increasing human control over AI will reduce harmful bias. This paper will contribute to the field of philosophy of technology by using Foucault's analysis to examine assumptions found in HCAI [it provides a Foucauldian conceptual analysis of a current approach (human-centredness) that aims to influence the design and development of a transformative technology (AI)], it will contribute to AI ethics debates by offering a critique of human-centredness in AI (by choosing Foucault, it provides a bridge between older ideas with contemporary issues), and it will also contribute to Foucault studies (by using his work to engage in contemporary debates, such as AI).","",""
"2024","From ethics to epistemology and back again: informativeness and epistemic injustice in explanatory medical machine learning","AbstractIn this paper, we discuss epistemic and ethical concerns brought about by machine learning (ML) systems implemented in medicine. We begin by fleshing out the logic underlying a common approach in the specialized literature (which we call the informativeness account). We maintain that the informativeness account limits its analysis to the impact of epistemological issues on ethical concerns without assessing the bearings that ethical features have on the epistemological evaluation of ML systems. We argue that according to this methodological approach, epistemological issues are instrumental to and autonomous of ethical considerations. This means that the informativeness account considers epistemological evaluation uninfluenced and unregulated by an ethical counterpart. Using an example that does not square well into the informativeness account, we argue for ethical assessments that have a substantial influence on the epistemological assessment of ML and that such influence should not be understood as merely informative but rather regulatory. Drawing on the case analyzed, we claim that within the theoretical framework of the informativeness approach, forms of epistemic injustice—especially epistemic objectification—remain unaddressed. Our analysis should motivate further research investigating the regulatory role that ethical elements play in the epistemology of ML.","",""
"2024","From the essence of humanity to the essence of intelligence, and AI in the future society","","",""
"2024","Innovation, risk and control: The true trend is ‘from tool to purpose’—A discussion on the standardization of AI","","",""
"2024","Galactica’s dis-assemblage: Meta’s beta and the omega of post-human science","AbstractReleased mid-November 2022, Galactica is a set of six large language models (LLMs) of different sizes (from 125 M to 120B parameters) designed by Meta AI to achieve the ultimate ambition of “a single neural network for powering scientific tasks”, according to its accompanying whitepaper. It aims to carry out knowledge-intensive tasks, such as publication summarization, information ordering and protein annotation. However, just a few days after the release, Meta had to pull back the demo due to the strong hallucinatory tendencies or underwhelming performances of the model. This article aims to study, through a critical threefold argument, the potential impacts of LLMs once deployed in the scientific value chain. Our first argument is a technical one. By examining the technicity of Galactica, it is possible to explain the descripancies between its promotional corporate discourse and abysmal outputs. Second, by going back to debates in both computer science and computational philosophy on the automation of abduction, we argue from the epistemological front that LLMs indeed cannot produce strong abductions and, therefore, claims about the automation of hypothesis generation remains chambering. Finally, our third argument is a sociological one. By conceptualizing the scientific field through Nancy Katherine Hayles’ cognitive assemblage theory, we aim to outline the potential steering of science by LLMs, mainly through information ordering. The core of our argument rests on the assertion that excessive control on information risks contravening a certain serendipitous aspect inherent to scientific discoveries.","",""
"2024","Artificial consciousness in AI: a posthuman fallacy","","",""
"2024","Culturally responsive communication in generative AI: looking at ChatGPT’s advice for coming out","AbstractGenerative AI has captured the public imagination as a tool that promises access to expertise beyond the technical jargon and expense that traditionally characterize such infospheres as those of medicine and law. Largely absent from the current literature, however, are interrogations of generative AI’s abilities to deal in culturally responsive communication, or the expertise interwoven with culturally aware, socially responsible, and personally sensitive communication best practices. To interrogate the possibilities of cultural responsiveness in generative AI, we examine the patterns of response that characterize ChatGPT-3.5’s advice for coming out. Specifically, we submitted 100 prompts soliciting coming out advice to GPT-3.5, variegating each of those prompts slightly to account for intersectional identities. From the analysis, we find that, while the responses are largely in-line with best practices, there are also instances that might represent problematics concerning the interpellation of the user or the persons to whom one is coming out.","",""
"2024","Machine agency and representation","","",""
"2024","Can AI determine its own future?","","",""
"2024","Give the machine a chance, human experts ain’t that great…","","",""
"2024","When will the blind be able to take their first steps with GDR guidance under artificial intelligence?","","",""
"2024","The ethics of conceptual, ontological, semantic and knowledge modeling","","",""
"2024","Attitudes toward artificial intelligence: combining three theoretical perspectives on technology acceptance","AbstractEvidence on AI acceptance comes from a diverse field comprising public opinion research and largely experimental studies from various disciplines. Differing theoretical approaches in this research, however, imply heterogeneous ways of studying AI acceptance. The present paper provides a framework for systematizing different uses. It identifies three families of theoretical perspectives informing research on AI acceptance—user acceptance, delegation acceptance, and societal adoption acceptance. These models differ in scope, each has elements specific to them, and the connotation of technology acceptance thus changes when shifting perspective. The discussion points to a need for combining the three perspectives as they have all become relevant for AI. A combined approach serves to systematically relate findings from different studies. And as AI systems affect people in different constellations and no single perspective can accommodate them all, building blocks from several perspectives are needed to comprehensively study how AI is perceived in society.","",""
"2024","As you sow, so shall you reap: rethinking humanity in the age of artificial intelligence","","",""
"2024","Abstraction, mimesis and the evolution of deep learning","AbstractDeep learning developers typically rely on deep learning software frameworks (DLSFs)—simply described as pre-packaged libraries of programming components that provide high-level access to deep learning functionality. New DLSFs progressively encapsulate mathematical, statistical and computational complexity. Such higher levels of abstraction subsequently make it easier for deep learning methodology to spread through mimesis (i.e., imitation of models perceived as successful). In this study, we quantify this increase in abstraction and discuss its implications. Analyzing publicly available code from Github, we found that the introduction of DLSFs correlates both with significant increases in the number of deep learning projects and substantial reductions in the number of lines of code used. We subsequently discuss and argue the importance of abstraction in deep learning with respect to ephemeralization, technological advancement, democratization, adopting timely levels of abstraction, the emergence of mimetic deadlocks, issues related to the use of black box methods including privacy and fairness, and the concentration of technological power. Finally, we also discuss abstraction as a symptom of an ongoing technological metatransition.","",""
"2024","An elemental ethics for artificial intelligence: water as resistance within AI’s value chain","AbstractResearch and activism have increasingly denounced the problematic environmental record of the infrastructure and value chain underpinning artificial intelligence (AI). Water-intensive data centres, polluting mineral extraction and e-waste dumping are incontrovertibly part of AI’s footprint. In this article, I turn to areas affected by AI-fuelled environmental harm and identify an ethics of resistance emerging from local activists, which I term ‘elemental ethics’. Elemental ethics interrogates the AI value chain’s problematic relationship with the elements that make up the world, critiques the undermining of local and ancestral approaches to nature and reveals the vital and quotidian harms engendered by so-called intelligent systems. While this ethics is emerging from grassroots and Indigenous groups, it echoes recent calls from environmental philosophy to reconnect with the environment via the elements. In empirical terms, this article looks at groups in Chile resisting a Google data centre project in Santiago and lithium extraction (used for rechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert. As I show, elemental ethics can complement top-down, utilitarian and quantitative approaches to AI ethics and sustainable AI as well as interrogate whose lived experience and well-being counts in debates on AI extinction.","",""
"2024","Reconfiguring the alterity relation: the role of communication in interactions with social robots and chatbots","AbstractDon Ihde’s alterity relation focuses on the quasi-otherness of dynamic technologies that interact with humans. The alterity relation is one means to study relations between humans and artificial intelligence (AI) systems . However, research on alterity relations has not defined the difference between playing with a toy, using a computer, and interacting with a social robot or chatbot. We suggest that Ihde’s quasi-other concept fails to account for the interactivity, autonomy, and adaptability of social robots and chatbots, which more closely approach human alterity. In this article, we will examine experiences with a chatbot, Replika, and a humanoid robot, a RealDoll, to show how some users experience AI systems as companions. First, we show that the perception of social robots and chatbots as intimate companions is grounded in communication. Advances in natural language processing (NLP) and natural language generation (NLG) allow a relationship to form between some users and social robots and chatbots. In this relationship, some users experience social robots and chatbots as more than quasi-others. We will use Kanemitsu’s another-other concept to analyze cases where social robots and chatbots should be distinguished from quasi-others.","",""
"2024","Global justice and the use of AI in education: ethical and epistemic aspects","","",""
"2024","The problem of alignment","AbstractLarge language models (LLMs) produce sequences learned as statistical patterns from large corpora. Their emergent status as representatives of the advances in artificial intelligence (AI) have led to an increased attention to the possibilities of regulating the automated production of linguistic utterances and interactions with human users in a process that computer scientists refer to as ‘alignment’—a series of technological and political mechanisms to impose a normative model of morality on algorithms and networks behind the model. Alignment, which can be viewed as the superimposition of normative structure onto a statistical model, however, reveals a conflicted and complex history of the conceptualisation of an interrelationship between language, mind and technology. This relationship is shaped by and, in turn, influences theories of language, linguistic practice and subjectivity, which are especially relevant to the current sophistication in artificially produced text. In this paper, we propose a critical evaluation of the concept of alignment, arguing that the theories and practice behind LLMs reveal a more complex social and technological dynamic of output coordination. We examine this dynamic as a two-way interaction between users and models by analysing how ChatGPT4 redacts perceived ‘anomalous’ language in fragments of Joyce’s Ulysses. We then situate this alignment problem historically, revisiting earlier postwar linguistic debates which counterposed two views of meaning: as discrete structures, and as continuous probability distributions. We discuss the largely occluded work of the Moscow Linguistic School, which sought to reconcile this opposition. Our attention to the Moscow School and later related arguments by Searle and Kristeva casts the problem of alignment in a new light: as one involving attention to the social regulation of linguistic practice, including rectification of anomalies that, like the Joycean text, exist in defiance of expressive conventions. The “problem of alignment” that we address here is, therefore, twofold: on one hand, it points to its narrow and normative definition in current technological development and critical research and, on the other hand, to the reality of complex and contradictory relations between subjectivity, technology and language that alignment problems reveal.","",""
"2024","Correction to: Ground truth to fake geographies: machine vision and learning in visual practices","","",""
"2024","Beyond model interpretability: socio-structural explanations in machine learning","","",""
"2024","Global governance and the normalization of artificial intelligence as ‘good’ for human health","AbstractThe term ‘artificial intelligence’ has arguably come to function in political discourse as, what Laclau called, an ‘empty signifier’. This article traces the shifting political discourse on AI within three key institutions of global governance–OHCHR, WHO, and UNESCO–and, in so doing, highlights the role of ‘crisis’ moments in justifying a series of pivotal re-articulations. Most important has been the attachment of AI to the narrative around digital automation in human healthcare. Greatly enabled by the societal context of the pandemic, all three institutions have moved from being critical of the unequal power relations in the economy of AI to, today, reframing themselves primarily as facilitators tasked with helping to ensure the application of AI technologies. The analysis identifies a shift in which human health and healthcare is framed as in a ‘crisis’ to which AI technology is presented as the remedy. The article argues the need to trace these discursive shifts as a means by which to understand, monitor, and where necessary also hold to account these changes in the governance of AI in society.","",""
"2024","Artificial intelligence and retracted science","","",""
"2024","Knowledge and support for AI in the public sector: a deliberative poll experiment","AbstractWe are on the verge of a revolution in public sector decision-making processes, where computers will take over many of the governance tasks previously assigned to human bureaucrats. Governance decisions based on algorithmic information processing are increasing in numbers and scope, contributing to decisions that impact the lives of individual citizens. While significant attention in the recent few years has been devoted to normative discussions on fairness, accountability, and transparency related to algorithmic decision-making based on artificial intelligence, less is known about citizens’ considered views on this issue. To put society in-the-loop, a Deliberative Poll was thus carried out on the topic of using artificial intelligence in the public sector, as a form of in-depth public consultation. The three use cases that were selected for deliberation were refugee reallocation, a welfare-to-work program, and parole. A key finding was that after having acquired more knowledge about the concrete use cases, participants were overall more supportive of using artificial intelligence in the decision processes. The event was set up with a pretest/post-test control group experimental design, and as such, the results offer experimental evidence to extant observational studies showing positive associations between knowledge and support for using artificial intelligence.","",""
"2024","Artificing intelligence: from isolating IQ to amoral AI","","",""
"2024","Artificial intelligence and the secret ballot","","",""
"2024","Machine learning and human learning: a socio-cultural and -material perspective on their relationship and the implications for researching working and learning","AbstractThe paper adopts an inter-theoretical socio-cultural and -material perspective on the relationship between human + machine learning to propose a new way to investigate the human + machine assistive assemblages emerging in professional work (e.g. medicine, architecture, design and engineering). Its starting point is Hutchins’s (1995a) concept of ‘distributed cognition’ and his argument that his concept of ‘cultural ecosystems’ constitutes a unit of analysis to investigate collective human + machine working and learning (Hutchins, Philos Psychol 27:39–49, 2013). It argues that: (i) the former offers a way to reveal the cultural constitution of and enactment of human + machine cognition and, in the process, the limitations of the computational and connectionist assumptions about learning that underpin, respectively, good old-fashioned AI and deep learning; and (2) the latter offers a way to identify, when amplified with insights from Socio-Materialism and Cultural-Historical Activity Theory, how ML is further rearranging and reorganising the distributed basis of cognition in assistive assemblages. The paper concludes by outlining a set of conjectures researchers that could use to guide their investigations into the ongoing design and deployment of HL + ML assemblages and challenges associated with the interaction between HL + ML.","",""
"2024","AI safety: necessary, but insufficient and possibly problematic","","",""
"2024","“Just” accuracy? Procedural fairness demands explainability in AI-based medical resource allocations","AbstractThe increasing application of artificial intelligence (AI) to healthcare raises both hope and ethical concerns. Some advanced machine learning methods provide accurate clinical predictions at the expense of a significant lack of explainability. Alex John London has defended that accuracy is a more important value than explainability in AI medicine. In this article, we locate the trade-off between accurate performance and explainable algorithms in the context of distributive justice. We acknowledge that accuracy is cardinal from outcome-oriented justice because it helps to maximize patients’ benefits and optimizes limited resources. However, we claim that the opaqueness of the algorithmic black box and its absence of explainability threatens core commitments of procedural fairness such as accountability, avoidance of bias, and transparency. To illustrate this, we discuss liver transplantation as a case of critical medical resources in which the lack of explainability in AI-based allocation algorithms is procedurally unfair. Finally, we provide a number of ethical recommendations for when considering the use of unexplainable algorithms in the distribution of health-related resources.","",""
"2024","Can AI systems become wise? A note on artificial wisdom","","",""
"2024","The influential role of artificial intelligence (AI) adoption in digital value creation for small and medium enterprises (SMEs): does technological orientation mediate this relationship?","","",""
"2024","Should criminal law protect love relation with robots?","AbstractWhether or not we call a love-like relationship with robots true love, some people may feel and claim that, for them, it is a sufficient substitute for love relationship. The love relationship between humans has a special place in our social life. On the grounds of both morality and law, our significant other can expect special treatment. It is understandable that, precisely because of this kind of relationship, we save our significant other instead of others or will not testify against her/him. How as a society should we treat love-like relationships humans with robots? Based on the assumption that robots do not have an inner life and are not moral patients, I defend the thesis that this kind of relationship should be protected by criminal law.","",""
"2024","No such thing as one-size-fits-all in AI ethics frameworks: a comparative case study","","",""
"2024","Are AI systems biased against the poor? A machine learning analysis using Word2Vec and GloVe embeddings","AbstractAmong the myriad of technical approaches and abstract guidelines proposed to the topic of AI bias, there has been an urgent call to translate the principle of fairness into the operational AI reality with the involvement of social sciences specialists to analyse the context of specific types of bias, since there is not a generalizable solution. This article offers an interdisciplinary contribution to the topic of AI and societal bias, in particular against the poor, providing a conceptual framework of the issue and a tailor-made model from which meaningful data are obtained using Natural Language Processing word vectors in pretrained Google Word2Vec, Twitter and Wikipedia GloVe word embeddings. The results of the study offer the first set of data that evidences the existence of bias against the poor and suggest that Google Word2vec shows a higher degree of bias when the terms are related to beliefs, whereas bias is higher in Twitter GloVe when the terms express behaviour. This article contributes to the body of work on bias, both from and AI and a social sciences perspective, by providing evidence of a transversal aggravating factor for historical types of discrimination. The evidence of bias against the poor also has important consequences in terms of human development, since it often leads to discrimination, which constitutes an obstacle for the effectiveness of poverty reduction policies.","",""
"2024","Twenty-four years of empirical research on trust in AI: a bibliometric review of trends, overlooked issues, and future directions","AbstractTrust is widely regarded as a critical component to building artificial intelligence (AI) systems that people will use and safely rely upon. As research in this area continues to evolve, it becomes imperative that the research community synchronizes its empirical efforts and aligns on the path toward effective knowledge creation. To lay the groundwork toward achieving this objective, we performed a comprehensive bibliometric analysis, supplemented with a qualitative content analysis of over two decades of empirical research measuring trust in AI, comprising 1’156 core articles and 36’306 cited articles across multiple disciplines. Our analysis reveals several “elephants in the room” pertaining to missing perspectives in global discussions on trust in AI, a lack of contextualized theoretical models and a reliance on exploratory methodologies. We highlight strategies for the empirical research community that are aimed at fostering an in-depth understanding of trust in AI.","",""
"2024","The limitation of ethics-based approaches to regulating artificial intelligence: regulatory gifting in the context of Russia","","",""
"2024","Framing the effects of machine learning on science","","",""
"2024","Moral distance, AI, and the ethics of care","AbstractThis paper investigates how the introduction of AI to decision making increases moral distance and recommends the ethics of care to augment the ethical examination of AI decision making. With AI decision making, face-to-face interactions are minimized, and decisions are part of a more opaque process that humans do not always understand. Within decision-making research, the concept of moral distance is used to explain why individuals behave unethically towards those who are not seen. Moral distance abstracts those who are impacted by the decision and leads to less ethical decisions. The goal of this paper is to identify and analyze the moral distance created by AI through both proximity distance (in space, time, and culture) and bureaucratic distance (derived from hierarchy, complex processes, and principlism). We then propose the ethics of care as a moral framework to analyze the moral implications of AI. The ethics of care brings to the forefront circumstances and context, interdependence, and vulnerability in analyzing algorithmic decision making.","",""
"2024","Correction: The illusion of understanding: AI’s role in cognitive psychology research","","",""
"2024","Autonomous weapons systems and the necessity of interpretation: what Heidegger can tell us about automated warfare","","",""
"2024","Human-centric AI: philosophical and community-centric considerations","AbstractThis article provides a course of correction in the discourse surrounding human-centric AI by elucidating the philosophical underpinning that serves to create a view that AI is divorced from human-centric values. Next, we espouse the need to explicitly designate stakeholder- or community-centric values which are needed to resolve the issue of alignment. To achieve this, we present two frameworks, Ubuntu and maximum feasible participation. Finally, we demonstrate how employing the aforementioned frameworks in AI can benefit society by flattening the current top-down social hierarchies as AI is currently being utilized. Implications are discussed.","",""
"2024","Toward children-centric AI: a case for a growth model in children-AI interactions","AbstractThis article advocates for a hermeneutic model for children-AI (age group 7–11 years) interactions in which the desirable purpose of children’s interaction with artificial intelligence (AI) systems is children's growth. The article perceives AI systems with machine-learning components as having a recursive element when interacting with children. They can learn from an encounter with children and incorporate data from interaction, not only from prior programming. Given the purpose of growth and this recursive element of AI, the article argues for distinguishing the interpretation of bias within the artificial intelligence (AI) ethics and responsible AI discourse. Interpreting bias as a preference and distinguishing between positive (pro-diversity) and negative (discriminative) bias is needed as this would serve children's healthy psychological and moral development. The human-centric AI discourse advocates for an alignment of capacities of humans and capabilities of machines by a focus both on the purpose of humans and on the purpose of machines for humans. The emphasis on mitigating negative biases through data protection, AI law, and certain value-sensitive design frameworks demonstrates that the purpose of the machine for humans is prioritized over the purpose of humans. These top–down frameworks often narrow down the purpose of machines to do-no-harm and they miss accounting for the bottom-up views and developmental needs of children. Therefore, applying a growth model for children-AI interactions that incorporates learning from negative AI-mediated biases and amplifying positive ones would positively benefit children’s development and children-centric AI innovation. Consequently, the article explores:What challenges arise from mitigating negative biases and amplifying positive biases in children-AI interactions and how can a growth model address these?To answer this, the article recommends applying a growth model in open AI co-creational spaces with and for children. In such spaces human–machine and human–human value alignment methods can be collectively applied in such a manner that children can (1) become sensitized toward the effects of AI-mediated negative biases on themselves and others; (2) enable children to appropriate and imbue top-down values of diversity, and non-discrimination with their meanings; (3) enforce children’s right to identity and non-discrimination; (4) guide children in developing an inclusive mindset; (5) inform top-down normative AI frameworks by children’s bottom-up views; (6) contribute to design criteria for children-centric AI. Applying such methods under a growth model in AI co-creational spaces with children could yield an inclusive co-evolution between responsible young humans in the loop and children-centric AI systems.","",""
"2024","Machine learning in bail decisions and judges’ trustworthiness","","",""
"2024","Nationalize AI!","","",""
"2024","Biases within AI: challenging the illusion of neutrality","","",""
"2024","Responsible research in artificial intelligence: lessons from the past","","",""
"2024","Predicting crime or perpetuating bias? The AI dilemma","","",""
"2024","Fugazi regulation for AI: strategic tolerance for ethics washing","AbstractRegulation theory offers a unique perspective on the institutional aspects of digital capitalism’s accumulation regime. However, a gap exists in examining the associated mode of regulation. Based on the analysis of AI ethics washing phenomenon, we suggest the state is delicately balancing between fueling innovation and reducing uncertainty in emerging technologies. This balance leads to a unique mode of regulation, """"Fugazi regulation,"""" characterized by vaguely defined, non-enforceable moral principles with no specific implementation mechanisms. We propose a microeconomic model that rationalizes this approach and shows that it is justifiable when the government struggles to differentiate between benign and harmful technology use due to capacity constraints. The potential for private companies to adopt ethical practices under Fugazi regulation supports the government’s preference for this method. This regulation mode is particularly attractive to the government during technology’s early development stages, marked by governmental optimism and uncertainty about the technology. Implications for greenwashing are also derived from the analysis.","",""
"2024","What to consider before incorporating generative AI into schools?","","",""
"2024","Review of Carlos Montemayor’s The prospect of a humanitarian AI","","",""
"2024","Narratives of epistemic agency in citizen science classification projects: ideals of science and roles of citizens","AbstractCitizen science (CS) projects have started to utilize Machine Learning (ML) to sort through large datasets generated in fields like astronomy, ecology and biodiversity, biology, and neuroimaging. Human–machine systems have been created to take advantage of the complementary strengths of humans and machines and have been optimized for efficiency and speed. We conducted qualitative content analysis on meta-summaries of documents reporting the results of 12 citizen science projects that used machine learning to optimize classification tasks. We examined the distribution of tasks between citizen scientists, experts, and algorithms, and how epistemic agency was enacted in terms of whose knowledge shapes the distribution of tasks, who decides what knowledge is relevant to the classification, and who validates it. In our descriptive results, we found that experts, who include professional scientists and algorithm developers, are involved in every aspect of a project, from annotating or labelling data to giving data to algorithms to train them to make decisions from predictions. Experts also test and validate models to improve their accuracy by scoring their outputs when algorithms fail to make correct decisions. Experts are mostly the humans involved in a loop, but when algorithms encounter problems, citizens are also involved at several stages. In this paper, we present three main examples of citizens-in-the-loop: (a) when algorithms provide incorrect suggestions; (b) when algorithms fail to know how to perform classification; and (c) when algorithms pose queries. We consider the implications of the emphasis on optimization on the ideal of science and the role of citizen scientists from a perspective informed by Science and Technology Studies (STS) and Information Systems (IS). Based on our findings, we conclude that ML in CS classification projects, far from being deterministic in its nature and effects, may be open to question. There is no guarantee that these technologies can replace citizen scientists, nor any guarantee that they can provide citizens with opportunities for more interesting tasks.","",""
"2024","The cyclical ethical effects of using artificial intelligence in education","","",""
"2024","Dismantling AI capitalism: the commons as an alternative to the power concentration of Big Tech","AbstractThis article discusses the political economy of AI capitalism. It considers AI as a General Purpose Technology (GPT) and argues we need to investigate the power concentration of Big Tech. AI capitalism is characterised by the commodification of data, data extraction and a concentration in hiring of AI talent and compute capacity. This is behind Big Tech’s unstoppable drive for growth, which leads to monopolisation and enclosure under the winner takes all principle. If we consider AI as a GPT—technologies that alter society’s economic and social structures—we need to come up with alternatives in terms of ownership and governance. The commons is proposed as an alternative for thinking about how to organise AI development and how to distribute the value that can be derived from it. Using the commons framework is also a way of giving society a more prominent role in the debate about what we expect from AI and how we should approach it.","",""
"2024","Artificial understanding: a step toward robust AI","","",""
"2024","On phantom publics, clusters, and collectives: be(com)ing subject in algorithmic times","AbstractThis article starts from the observation that practices of ‘algorithmic governmentality’ or ‘governance by data’ are reconfiguring modes of social relationality and collectivity. By building, first, on an empirical exploration of digital bordering practices, we qualify these emergent algorithmic categories as ‘clusters’—pulsing patterns distilled from disaggregated data. As fluid, modular, and ever-emergent forms of association, these ‘clusters’ defy stable expressions of collective representation and social recognition. Second, we observe that this empirical analysis resonates with accounts that diagnosed algorithmic governance as a threat to legal subjectivity and socio-political cohesion, and called for a reinvigoration of democratic values and their re-alignment with new ‘infrastructural publics’. Against this backdrop, however, we explore alternatives avenues of legal imagination by pushing in a different (somewhat opposite) direction. Against the re-inscription of liberal categories, we linger with the promise and prospect of illegibility as resistance against the foreclosure of future potentialities in algorithmic forms of subject-making. Instead of falling back on the projection of autonomous human agency and liberal subjectivity to counteract the ‘cluster’, we imagine emancipatory expressions of resistance that are enacted through fugitive, opaque, and experimental collectivities.","",""
"2024","The illusion of understanding: AI’s role in cognitive psychology research","","",""
"2024","“Game changer”: the AI advocacy discourse of 2023 in the US","","",""
"2024","Correction to: Emotional AI and the future of wellbeing in the post-pandemic workplace","","",""
"2024","Disengage to survive the AI-powered sensory overload world","","",""
"2024","AI in situated action: a scoping review of ethnomethodological and conversation analytic studies","AbstractDespite its elusiveness as a concept, ‘artificial intelligence’ (AI) is becoming part of everyday life, and a range of empirical and methodological approaches to social studies of AI now span many disciplines. This article reviews the scope of ethnomethodological and conversation analytic (EM/CA) approaches that treat AI as a phenomenon emerging in and through the situated organization of social interaction. Although this approach has been very influential in the field of computational technology since the 1980s, AI has only recently emerged as such a pervasive part of daily life to warrant a sustained empirical focus in EM/CA. Reviewing over 50 peer-reviewed publications, we find that the studies focus on various social and group activities such as task-oriented situations, semi-experimental setups, play, and everyday interactions. They also involve a range of participant categories including children, older participants, and people with disabilities. Most of the reviewed studies apply CA’s conceptual apparatus, its approach to data analysis, and core topics such as turn-taking and repair. We find that across this corpus, studies center on three key themes: openings and closing the interaction, miscommunication, and non-verbal aspects of interaction. In the discussion, we reflect on EM studies that differ from those in our corpus by focusing on praxeological respecifications of AI-related phenomena. Concurrently, we offer a critical reflection on the work of literature reviewing, and explore the tortuous relationship between EM and CA in the area of research on AI.","",""
"2024","“Personhood and AI: Why large language models don’t understand us”","","",""
"2024","The news framing of artificial intelligence: a critical exploration of how media discourses make sense of automation","AbstractAnalysing how news media portray A.I. reveals what interpretative frameworks around the technology circulate in public discourses. This allows for critical reflections on the making of meaning in prevalent narratives about A.I. and its impact. While research on the public perception of datafication and automation is growing, only a few studies investigate news framing practices. The present study connects to this nascent research area by charting A.I. news frames in four internationally renowned media outlets: The New York Times, The Guardian, Wired, and Gizmodo. The main goals are to identify dominant emphasis frames in AI news reporting over the past decade, to explore whether certain A.I. frames are associated with specific data risks (surveillance, data bias, cyber-war/cyber-crime, and information disorder), and what journalists and experts contribute to the media discourse. An automated content analysis serves for inductive frame detection (N = 3098), identification of risk references (dictionary-based), and network analysis of news writers. The results show how A.I.’s ubiquity emerged rapidly in the mid-2010s, and that the news discourse became more critical over time. It is further argued that A.I. news reporting is an important factor in building critical data literacy among lay audiences.","",""
"2024","Just accountability structures – a way to promote the safe use of automated decision-making in the public sector","AbstractThe growing use of automated decision-making (ADM) systems in the public sector and the need to control these has raised many legal questions in academic research and in policymaking. One of the timely means of legal control is accountability, which traditionally includes the ability to impose sanctions on the violator as one dimension. Even though many risks regarding the use of ADM have been noted and there is a common will to promote the safety of these systems, the relevance of the safety research has been discussed little in this context. In this article, I evaluate regulating accountability over the use of ADM in the public sector in relation to the findings of safety research. I conducted the study by focusing on ongoing regulatory projects regarding ADM, the Finnish ADM legislation draft and the EU proposal for the AI Act. The critical question raised in the article is what the role of sanctions is. I ask if official accountability could mean more of an opportunity to learn from mistakes, share knowledge and compensate for harm instead of control via sanctions.","",""
"2024","Connecting ethics and epistemology of AI","AbstractThe need for fair and just AI is often related to the possibility of understanding AI itself, in other words, of turning an opaque box into a glass box, as inspectable as possible. Transparency and explainability, however, pertain to the technical domain and to philosophy of science, thus leaving the ethics and epistemology of AI largely disconnected. To remedy this, we propose an integrated approach premised on the idea that a glass-box epistemology should explicitly consider how to incorporate values and other normative considerations, such as intersectoral vulnerabilities, at critical stages of the whole process from design and implementation to use and assessment. To connect ethics and epistemology of AI, we perform a double shift of focus. First, we move from trusting the output of an AI system to trusting the process that leads to the outcome. Second, we move from expert assessment to more inclusive assessment strategies, aiming to facilitate expert and non-expert assessment. Together, these two moves yield a framework usable for experts and non-experts when they inquire into relevant epistemological and ethical aspects of AI systems. We dub our framework ‘epistemology-cum-ethics’ to signal the equal importance of both aspects. We develop it from the vantage point of the designers: how to create the conditions to internalize values into the whole process of design, implementation, use, and assessment of an AI system, in which values (epistemic and non-epistemic) are explicitly considered at each stage and inspectable by every salient actor involved at any moment.","",""
"2024","Perceived responsibility in AI-supported medicine","AbstractIn a representative vignette study in Germany with 1,653 respondents, we investigated laypeople’s attribution of moral responsibility in collaborative medical diagnosis. Specifically, we compare people’s judgments in a setting in which physicians are supported by an AI-based recommender system to a setting in which they are supported by a human colleague. It turns out that people tend to attribute moral responsibility to the artificial agent, although this is traditionally considered a category mistake in normative ethics. This tendency is stronger when people believe that AI may become conscious at some point. In consequence, less responsibility is attributed to human agents in settings with hybrid diagnostic teams than in settings with human-only diagnostic teams. Our findings may have implications for behavior exhibited in contexts of collaborative medical decision making with AI-based as opposed to human recommenders because less responsibility is attributed to agents who have the mental capacity to care about outcomes.","",""
"2024","Negotiating the authenticity of AI: how the discourse on AI rejects human indeterminacy","AbstractIn this paper, we demonstrate how the language and reasonings that academics, developers, consumers, marketers, and journalists deploy to accept or reject AI as authentic intelligence has far-reaching bearing on how we understand our human intelligence and condition. The discourse on AI is part of what we call the “authenticity negotiation process” through which AI’s “intelligence” is given a particular meaning and value. This has implications for scientific theory, research directions, ethical guidelines, design principles, funding, media attention, and the way people relate to and act upon AI. It also has great impact on humanity’s self-image and the way we negotiate what it means to be human, existentially, culturally, politically, and legally. We use a discourse analysis of academic papers, AI education programs, and online discussions to demonstrate how AI itself, as well as the products, services, and decisions delivered by AI systems are negotiated as authentic or inauthentic intelligence. In this negotiation process, AI stakeholders indirectly define and essentialize what being human(like) means. The main argument we will develop is that this process of indirectly defining and essentializing humans results in an elimination of the space for humans to be indeterminate. By eliminating this space and, hence, denying indeterminacy, the existential condition of the human being is jeopardized. Rather than re-creating humanity in AI, the AI discourse is re-defining what it means to be human and how humanity is valued and should be treated.","",""
"2024","Artificial intelligence as planetary assemblages of coloniality: The new power architecture driving a tiered global data economy"," We present a framework for viewing artificial intelligence (AI) as planetary assemblages of coloniality that reproduce dependencies in how it co-constitutes and structures a tiered global data economy. We use assemblage thinking to map the coloniality of power to demonstrate how AI stratifies across knowledge, geographies, and bodies to influence development and economic trajectories, impact workers, reframe domestic industrial policies, and reconfigure the international political economy. Our post-colonial framework unpacks AI through its (1) global, (2) meso, and (3) local layers, and further dissects how these layers are vertically integrated, each with its horizontal dependencies. At (1) the global layer of international political economy maps a new digital bipolarity expressing Sino and American global digital corporations’ strategic and dominant positions in shaping a tiered global data economy. Then, at (2) the meso layer, we have a mosaic of domestic industrial policies that fund, frame markets, and develop AI talent across industries, sectors, and organizations to competitively integrate into AI value chains. Finally, incorporating into these are (3) the localized labor processes and tasks, where workers and users enact various AI-mediated tasks and practices driving further value extraction. We traced how AI is an interlaced system of power that reshapes knowledge, geographies, and bodies into dependencies that reinforce stratifications in developing underdevelopment. This commentary maps the current digital realities by laying out an uneven techno-geoeconomic power architecture driving a tiered global data economy and opening new research avenues to examine AI as planetary assemblages of coloniality. ","",""
"2024","Beyond artificial intelligence controversies: What are algorithms doing in the scientific literature?"," Mounting critique of the way AI is framed in mainstream media calls for less sensationalist coverage, be it jubilant or apocalyptic, and more attention to the concrete situations in which AI becomes controversial in different ways. This is supposedly achieved by making coverage more expert-informed. We therefore explore how experts contribute to the issuefication of AI through the scientific literature. We provide a semantic, visual network analysis of a corpus of 1M scientific abstracts about machine learning algorithms and artificial intelligence. Through a systematic quali-quantitative exploration of 235 co-word clusters and a subsequent structured search for 18 issue-specific queries, for which we devise a novel method with a custom-built datascape, we explore how algorithms have agency. We find that scientific discourse is highly situated and rarely about AI in general. It overwhelmingly charges algorithms with the capacity to solve problems and these problems are rarely about algorithms in their origin. Conversely, it rarely charges algorithms with the capacity to cause problems and when it does, other algorithms are typically charged with the capacity to solve them. Based on these findings, we argue that while a more expert-informed coverage of AI is likely to be less sensationalist and show greater attention to the specific situations where algorithms make a difference, it is unlikely to stage AI as particularly controversial. Consequently, we suggest conceptualising AI as a political situation rather than something inherently controversial. ","",""
"2024","Algorithmic governmentality in Latin America: Sociotechnical imaginaries, neocolonial soft power, and authoritarianism"," Latin America stands as one of the most unequal regions globally, where economic and social crises persist regardless of the ideological leanings of the ruling governments. Many countries in the region grapple with pervasive issues such as corruption, impunity, and a lack of adherence to the rule of law. In this context of generalized crisis, governments have turned to discourses of innovation and technological progress to justify their actions, advocating for the incorporation of automated systems into public administration. Algorithmic governmentality, the government of the social world through the algorithmic processing of data, emerges as a political rationality. Drawing from recent contributions in the theory of governmentality and critical data studies, our commentary centers on three critical dimensions: algorithmic governmentality as political rationality manifested in sociotechnical imaginaries; as an expression of soft power wielded by the U.S. government over the region; and the means by which regional governments automate social asymmetries and social control. This commentary delves into the intricate dynamics of algorithmic governmentality in Latin America, shedding light on its multifaceted implications for governance, democracy, and social structures in the region. ","",""
"2024","AI as super-controversy: Eliciting AI and society controversies with an extended expert community in the UK"," Following the release of large language models in the late 2010s, the backers of this new type of artificial intelligence (AI) publicly affirmed that the technology is controversial and harmful to society. This situation sets contemporary AI apart from 20th-century controversies about technnoscience, such as nuclear power and genetically modified (GM) foods, and disrupts established assumptions concerning public controversies as occasions for technological democracy. In particular, it challenges the idea that such controversies enable inclusion and collective processes of problem definition (‘problematisation’) across societal domains. In this paper, we show how social research can contribute to addressing this challenge of AI controversies by adopting a distinctive methodology of controversy analysis: controversy elicitation. This approach actively selects, qualifies and evaluates controversies in terms of their capacity to problematise AI across the science and non-science binary. We describe our implementation of this approach in a participatory study of recent AI controversies, conducted through consultation with UK experts in AI and society. Combining an online questionnaire, social media analysis and a participatory workshop, our study suggests that civil society actors have developed distinctive strategies of problematisation that counter the strategic affirmation of AI’s controversiality by its proponents and which centre on the public mobilisation of AI-related incidents: demonstrations of bias, accidents and walkouts. Crucially, this emphasis on ‘AI frictions’ does not result in the fragmentation of AI controversies, but rather enables the articulation of AI as a ‘super-controversy’: the explication of connections between technical propositions, situated troubles and structural problems in society (discrimination, inequalities and corporate power). ","",""
"2024","AI and discriminative decisions in recruitment: Challenging the core assumptions","In this article, we engage critically with the idea of promoting artificial intelligence (AI) technologies in recruitment as tools to eliminate discrimination in decision-making. We show that the arguments for using AI technologies to eliminate discrimination in personnel selection depend on presuming specific meanings of the concepts of rationality, bias, fairness, objectivity and AI, which the AI industry and other proponents of AI-based recruitment accept as self-evident. Our critical analysis of the arguments for relying on AI to decrease discrimination in recruitment is informed by insights gleaned from philosophy and methodology of science, legal and political philosophy, and critical discussions on AI, discrimination and recruitment. We scrutinize the role of the research on cognitive biases and implicit bias in justifying these arguments – a topic overlooked thus far in the debates about practical applications of AI. Furthermore, we argue that the recent use of AI in personnel selection can be understood as the latest trend in the long history of psychometric-based recruitment. This historical continuum has not been fully recognized in current debates either, as they focus mainly on the seemingly novel and disruptive character of AI technologies.","",""
"2024","Cross-cultural narratives of weaponised artificial intelligence: Comparing France, India, Japan and the United States"," Stories about ‘intelligent machines’ have long featured in popular culture. Existing research has mapped these artificial intelligence (AI) narratives but lacks an in-depth understanding of (a) narratives related specifically to weaponised AI and autonomous weapon systems and (b) whether and how these narratives resonate across different states and associated cultural contexts. We speak to these gaps by examining narratives about weaponised AI across publics in France, India, Japan and the US. Based on a public opinion survey conducted in these states in 2022–2023, we find that narratives found in English-language popular culture are shared cross-culturally, although with some variations. However, we also find culturally distinct narratives, particularly in India and Japan. Further, we assess whether these narratives shape the publics’ attitudes towards regulating weaponised AI. Although respondents demonstrate overall uncertainty and lack of knowledge regarding developments in the sphere of weaponised AI, they assess these technologies in a negative-leaning way and mostly support regulation. With these findings, our study offers a first step towards further investigating the extent to which weaponised AI narratives circulate globally and how salient perceptions of these technologies are across different publics. ","",""
"2024","Foundation models are platform models: Prompting and the political economy of AI"," A recent innovation in the field of machine learning has been the creation of very large pre-trained models, also referred to as ‘foundation models’, that draw on much larger and broader sets of data than typical deep learning systems and can be applied to a wide variety of tasks. Underpinning text-based systems such as OpenAI's ChatGPT and image generators such as Midjourney, these models have received extraordinary amounts of public attention, in part due to their reliance on prompting as the main technique to direct and apply them. This paper thus uses prompting as an entry point into the critical study of foundation models and their implications. The paper proceeds as follows: In the first section, we introduce foundation models in more detail, outline some of the main critiques, and present our general approach. We then discuss prompting as an algorithmic technique, show how it makes foundation models programmable, and explain how it enables different audiences to use these models as (computational) platforms. In the third section, we link the material properties of the technologies under scrutiny to questions of political economy, discussing, in turn, deep user interactions, reordered cost structures, and centralization and lock-in. We conclude by arguing that foundation models and prompting further strengthen Big Tech's dominance over the field of computing and, through their broad applicability, many other economic sectors, challenging our capacities for critical appraisal and regulatory response. ","",""
"2024","Algorithmic configurations in caring arrangements"," This article examines heterogeneous forms of human relationalities with algorithms envisioned in the development of a public algorithmic system and their anticipated effects. To do that, we focus on the distinct shapes given to both technologies and people by discourses and practices, together with their underlying logics and associated values. Analysing the blog posts documenting the emergence of Omaolo, a digital platform for healthcare and social welfare in Finland, we identify two algorithmic configurations: the ‘service engine’, which aligns with the public administration goals of standardising social and healthcare services in order to provide financial benefits; and the ‘treatment facilitator’, which advances the prevalent goals of social and healthcare professionals preoccupied with the fulfilment of situated care needs. We demonstrate that each of the configurations has different implications for social and healthcare organisations in which algorithmic technologies are deployed, for the professionals working there and for the people seeking public support. While the service engine might seem to undermine the collective bases of public service delivery, the treatment facilitator evidently supports them. Our findings remind us of the importance of research endeavours that acknowledge the complex and creative nature of development work, and consider the various parties and interests involved, in an attempt to attain more caring arrangements for the uses of data and algorithmic techniques in the public sector and beyond. ","",""
"2024","Computational reparations as generative justice: Decolonial transitions to unalienated circular value flow"," The Latin roots of the word reparations are “re” (again) plus “parere” which means “to give birth to, bring into being, produce”. Together they mean “to make generative once again”. In this sense, the extraction processes that cause labor injustice, ecological devastation, and social degradation cannot be repaired by simply transferring money. Reparations need to take on the full sense of “restorative”: the transition to a decolonial system that can support value generators in the control of their own systems of production, protect the value they create from extraction, and circulate value in unalienated forms that benefit the human and non-human communities that produced that value. With funding from the National Science Foundation, we have developed a research framework for this process that starts with “artisanal labor”: employee-owned business and worker collectives that have people doing what they love, despite low incomes. Focusing primarily on Detroit's Black-owned urban farms, artisanal textile businesses, Black hair salons, worker collectives, and other community-based production, with additional connections to Indigenous and other communities, we have introduced digital fabrication technologies, sensors, artificial intelligence, server-side apps and other computational support for a transition to unalienated circular value flow. We will report on our investigations with the challenges at multiple scales. At each level, we show how computational supports can act as restorative mechanisms for lost circular value flows, and thus address both past and ongoing disenfranchisement. ","",""
"2024","After automation: Homelessness prioritization algorithms and the future of care labor"," People experiencing homelessness seek support from homeless services systems that increasingly rely on prioritization algorithms to determine who is the most deserving of scarce resources. In this paper, we argue that algorithmic harms in homeless services require a reparative approach that takes the data work of care workers seriously. Building on Davis, Williams, and Yang's concept of algorithmic reparation, we present a qualitative study that examines the intertwining of data work and care labor of 15 care workers. We show how they wrestle with the ethics of algorithmic prioritization and develop workarounds that allow them to advocate for their clients. We contribute an empirical understanding of how care workers provide care under homeless services systems that equate data work with care labor to justify work intensification. Our findings have implications for understanding the future of care labor in datafied conditions and the social and political ramifications of algorithmically mediated care. ","",""
"2024","Situating AI policy: Controversies covered and the normalisation of AI"," Artificial intelligence has become an issue in public policy. Multiple documents issued by public sector actors link artificial intelligence to a wide range of issues, problems or goals and propose corresponding measures and interventions. While there has been substantial research on national and supranational artificial intelligence strategies and regulations, this article is interested in unpacking the processes and priorities of artificial intelligence policy in the making. Conceptually, this article takes a controversy studies lens onto artificial intelligence policy, and complements this with concepts and insights from policy studies. Empirically, we investigate the emergence of German artificial intelligence policy based on content analyses of policy documents and expert interviews. The findings reveal a late, but then powerful institutionalisation of artificial intelligence policy in German federal politics. Artificial intelligence policy in Germany focuses on funding research and supporting industry actors in networked configurations, much more than addressing societal concerns on inequality, discrimination or political economy. With regard to controversies, we observe that German policy is evading controversies by normalising artificial intelligence both with regard to taking artificial intelligence integration in all sectors of society for granted, as well as by accommodating artificial intelligence issues into the routines and institutions of German policy. ","",""
"2024","Trade-offs in AI assistant choice: Do consumers prioritize transparency and sustainability over AI assistant performance?"," As artificial intelligence (AI) becomes more integrated into society, concerns have arisen about unintended biases in AI-driven decision-making and the environmental impact of AI technology development. AI assistants such as Siri and Alexa, while helpful, can obscure decision-making and contribute to increased energy use and CO2 emissions. The present study explores whether consumers prioritize transparency and environmental sustainability over performance when choosing AI assistants with conjoint designs. Japanese participants were presented with different AI assistant profiles, varying in performance quality, transparency, cost, and environmental efficiency. The results revealed that Japanese participants prioritized transparency over performance when choosing AI assistants, but they prioritized performance over environmental sustainability. Moreover, future-oriented participants placed more importance on sustainability than those with a present orientation, while participants with an internal locus of control valued transparency more than those with an external locus of control. The findings of this study enhance our understanding of how consumers choose AI options and offer valuable guidance for creating AI systems and communication strategies that work effectively. ","",""
"2024","Yellow Techno-Peril: The ‘Clash of Civilizations’ and anti-Chinese racial rhetoric in the US–China AI arms race"," The rhetoric of an ‘AI arms race’ between the United States and China has become increasingly prominent over the past 5 years, despite warnings that it is unnecessarily polarising and undermines safe and ethical artificial intelligence (AI) development. However, existing critiques of the AI arms race narrative engage only sparingly with the racialised dimensions of this discourse. In this article, I draw on the rich theoretical insights of Asian American and Asian diaspora studies to show how the AI arms race narrative is deeply racialised in two key ways. First, I show how the rhetoric of an AI arms race builds upon the myth of a ‘Clash of Civilizations’ between the West and the East. This civilisational rhetoric constitutes China and the United States as distinct and mutually opposed cultural entities, thus foreclosing the possibility of more peaceful and cooperative alternatives to the AI arms race. Second, I demonstrate how the US–China AI arms race specifically draws on previous racialised configurations of anti-Asian sentiment, such as techno-Orientalism and the Yellow Peril. I coin the term Yellow Techno-Peril to connote how older European and Americans fears of being overrun or controlled by China are reproduced in the AI arms race. I close by offering recommendations to key stakeholders such as policymakers, decisionmakers, journalists and media organisations as to how they can mitigate and avoid the racialised rhetoric of an AI arms race between the United States and China. ","",""
"2024","The emergence of artificial intelligence ethics auditing"," The emerging ecosystem of artificial intelligence (AI) ethics and governance auditing has grown rapidly in recent years in anticipation of impending regulatory efforts that encourage both internal and external auditing. Yet, there is limited understanding of this evolving landscape. We conduct an interview-based study of 34 individuals in the AI ethics auditing ecosystem across seven countries to examine the motivations, key auditing activities, and challenges associated with AI ethics auditing in the private sector. We find that AI ethics audits follow financial auditing stages, but tend to lack robust stakeholder involvement, measurement of success, and external reporting. Audits are hyper-focused on technically oriented AI ethics principles of bias, privacy, and explainability, to the exclusion of other principles and socio-technical approaches, reflecting a regulatory emphasis on technical risk management. Auditors face challenges, including competing demands across interdisciplinary functions, firm resource and staffing constraints, lack of technical and data infrastructure to enable auditing, and significant ambiguity in interpreting regulations and standards given limited (or absent) best practices and tractable regulatory guidance. Despite these roadblocks, AI ethics and governance auditors are playing a critical role in the early ecosystem: building auditing frameworks, interpreting regulations, curating practices, and sharing learnings with auditees, regulators, and other stakeholders. ","",""
"2024","Justitia ex machina: The impact of an AI system on legal decision-making and discretionary authority"," Governments increasingly use algorithms to inform or supplant decision-making. Artificial Intelligence systems in particular are considered objective, consistent and efficient decision-makers, but have also been shown to be fallible. Furthermore, the adoption of artificial intelligence (AI) in government is fraught with challenges which are only partly understood and rarely studied in practice. In this paper, we draw on science and technology studies and human computer interaction and report on a critical case study of the development and use of an AI system for processing traffic violation appeal at a Dutch court. Although much empirical work on algorithms in practice is primarily observational in nature, we employ a canonical action research approach and actively participate in the development of the AI system. We draw on data collected in the form of interviews, observations, documents and a user-experiment. Based on this material we provide: 1. An in-depth empirical account of the tensions between street-level bureaucrats, screen-level bureaucrats and street-level algorithms; 2. An analysis of the differences between decisions made by, with and without the AI system and find that use of the AI systems impacts decisions made by legal experts; 3. A confirmation of earlier work that finds AI systems can best be applied in support of legal-decision making and demonstrate how the decision-making process of the traffic violation cases may mitigate some of the risks of algorithmic decision-making. ","",""
"2024","Participatory action research in critical data studies: Interrogating AI from a South–North approach","In this article, we draw inspiration from participatory action research (PAR) and the work of Latin American thinkers such as Freire and Fals Borda to interrogate artificial intelligence (AI). We propose a South-North flow by utilising PAR approaches that stem from Latin America, challenging how the North's centrality is taken for granted regarding AI epistemologies, experiences, and understandings. Conducting workshops in London with a diverse group of students, tech workers and activists, we argue that PAR can not only empower marginalised communities in the Global South; we can also learn more from its application in the Global North, in contexts where people deal with different struggles. Our analysis delves into three specific concepts around AI and data (in)justice: autonomy, empathy and dialogue. First, inspired by PAR principles, participants started to problematise what they called an empty interpretation of empathy, establishing parallels with transnational dynamics of data capitalism, which disadvantage marginalised communities in the Global South. Second, PAR offered a critical lens to analyse issues of AI and autonomy in ways that are less individualistic and more collective and politically engaged. Third, PAR's dialogical spirit enabled participants to locate various intersections between AI and dialogue. Critiquing the idea of a superior AI, participants were reminded of the possibilities offered by human intelligence and the combination of thinking, making and feeling or what Fals Borda (2003. Ante la crisis del país: I deas acción para el cambio, 1st. Ed. Bogotá: Panamericana) calls our sentipensante nature.","",""
"2024","A feeling for the algorithm: Diversity, expertise, and artificial intelligence"," Diversity is often announced as a solution to ethical problems in artificial intelligence (AI), but what exactly is meant by diversity and how it can solve those problems is seldom spelled out. This lack of clarity is one hurdle to motivating diversity in AI. Another hurdle is that while the most common perceptions about what diversity is are too weak to do the work set out for them, stronger notions of diversity are often defended on normative grounds that fail to connect to the values that are important to decision-makers in AI. However, there is a long history of research in feminist philosophy of science and a recent body of work in social epistemology that taken together provide the foundation for a notion of diversity that is both strong enough to do the work demanded of it, and can be defended on epistemic grounds that connect with the values that are important to decision-makers in AI. We clarify and defend that notion here by introducing emergent expertise as a network phenomenon wherein groups of workers with expertise of different types can gain knowledge not available to any individual alone, as long as they have ways of communicating across types of expertise. We illustrate the connected epistemic and ethical benefits of designing technology with diverse groups of workers using the examples of an infamous racist soap dispenser, and the millimeter wave scanners used in US airport security. ","",""
"2024","Reparations of the horse? Algorithmic reparation and overspecialized remedies"," In his seminal article, “Cyberspace and the Law of the Horse,” Frank Easterbrook criticized the scholarly trend of developing overspecialized legal approaches to emerging technologies. Easterbrook argued that these approaches are confusing, shallow, and superfluous. Algorithmic reparation has emerged as a framework for addressing algorithmic systems’ role in inequity and injustice. One understanding of algorithmic reparation is as a method for repairing algorithmic harms. This article examines how this understanding fares against the “law of the horse” critique by posing two questions. First, is algorithmic reparation overspecialized in its methods? Second, is algorithmic reparation overspecialized in the harm it targets? If its methods are too particularized, then algorithmic reparation will only work within a narrow range of circumstances and may undercut a more robust conception of remedies for algorithmic injustice. If the harm it targets is too particularized, then algorithmic reparation will result in incomplete or misguided redress of harms. We determine that algorithmic reparation is not too specific in its methods by demonstrating how–under algorithmic reparation principles–existing methods for reparations can be applied to address algorithmic harm. We also determine that algorithmic reparation can sometimes be too narrow in the harm it targets, which can reduce its effectiveness. When an algorithmic system is both necessary and sufficient for a harm to occur, algorithmic reparation is an effective method of redress. But when an algorithmic system is not necessary and sufficient for a given harm, algorithmic reparation may be incomplete, only temporarily effective, or miss the mark entirely. ","",""
"2024","From human-centered to social-centered artificial intelligence: Assessing ChatGPT's impact through disruptive events"," Large language models (LLMs) and dialogue agents represent a significant shift in artificial intelligence (AI) research, particularly with the recent release of the GPT family of models. ChatGPT's generative capabilities and versatility across technical and creative domains led to its widespread adoption, marking a departure from more limited deployments of previous AI systems. While society grapples with the emerging cultural impacts of this new societal-scale technology, critiques of ChatGPT's impact within machine learning research communities have coalesced around its performance or other conventional safety evaluations relating to bias, toxicity, and “hallucination.” We argue that these critiques draw heavily on a particular conceptualization of the “human-centered” framework, which tends to cast atomized individuals as the key recipients of technology's benefits and detriments. In this article, we direct attention to another dimension of LLMs and dialogue agents’ impact: their effects on social groups, institutions, and accompanying norms and practices. By analyzing ChatGPT's social impact through a social-centered framework, we challenge individualistic approaches in AI development and contribute to ongoing debates around the ethical and responsible deployment of AI systems. We hope this effort will call attention to more comprehensive and longitudinal evaluation tools (e.g., including more ethnographic analyses and participatory approaches) and compel technologists to complement human-centered thinking with social-centered approaches. ","",""
"2024","After the algorithms: A study of meta-algorithmic judgments and diversity in the hiring process at a large multisite company"," In recent years, both private and public organizations across contexts have begun implementing AI technologies in their recruitment processes. This transition is typically justified by improved efficiency as well as more objective, performance-based ranking, and inclusive selection of job candidates. However, this rapid development has also raised concerns that the use of these emerging technologies will instead increase discrimination or enhance the already existing inequality. In the present study, we first develop the concept of meta-algorithmic judgment to understand how recruiting managers may respond to automation of the hiring process. Second, we draw on this concept in the empirical assessment of the actual consequences of this type of transition by drawing on two large and unique datasets on employment records and job applications from one of Sweden's largest food retail companies. By comparing the outcomes of traditional and algorithmic job recruitment during this technological transition, we find that, contrary to the company's intentions, algorithmic recruitment decreases diversity. However, in contrast to what is often assumed, this is primarily not because the algorithms are biased, but because of what we identify as an unintended human–algorithmic interaction effect. ","",""
"2024","The trustification of AI. Disclosing the bridging pillars that tie trust and AI together"," Trustworthy artificial intelligence (TAI) is trending high on the political agenda. However, what is actually implied when talking about TAI, and why it is so difficult to achieve, remains insufficiently understood by both academic discourse and current AI policy frameworks. This paper offers an analytical scheme with four different dimensions that constitute TAI: a) A user perspective of AI as a quasi-other; b) AI's embedding in a network of actors from programmers to platform gatekeepers; c) The regulatory role of governance in bridging trust insecurities and deciding on AI value trade-offs; and d) The role of narratives and rhetoric in mediating AI and its conflictual governance processes. It is through the analytical scheme that overlooked aspects and missed regulatory demands around TAI are revealed and can be tackled. Conceptually, this work is situated in disciplinary transgression, dictated by the complexity of the phenomenon of TAI. The paper borrows from multiple inspirations such as phenomenology to reveal AI as a quasi-other we (dis-)trust; Science &amp; Technology Studies (STS) to deconstruct AI's social and rhetorical embedding; as well as political science for pinpointing hegemonial conflicts within regulatory bargaining. ","",""
"2024","Stabilizing translucencies: Governing AI transparency by standardization","Standards are put forward as important means to turn the ideals of ethical and responsible artificial intelligence into practice. One principle targeted for standardization is transparency. This article attends to the tension between standardization and transparency, by combining a theoretical exploration of these concepts with an empirical analysis of standardizations of artificial intelligence transparency. Conceptually, standards are underpinned by goals of stability and solidification, while transparency is considered a flexible see-through quality. In addition, artificial intelligence-technologies are depicted as ‘black boxed’, complex and in flux. Transparency as a solution for ethical artificial intelligence has, however, been problematized. In the empirical sample of standardizations, transparency is largely presented as a static, measurable, and straightforward information transfer, or as a window to artificial intelligence use. The standards are furthermore described as pioneering and able to shape technological futures, while their similarities suggest that artificial intelligence translucencies are already stabilizing into similar arrangements. To rely heavily upon standardization to govern artificial intelligence transparency still risks allocating rule-making to non-democratic processes, and while intended to bring clarity, the standardizations could also create new distributions of uncertainty and accountability. This article stresses the complexity of governing sociotechnical artificial intelligence principles by standardization. Overall, there is a risk that the governance of artificial intelligence is let to be too shaped by technological solutionism, allowing the standardization of social values (or even human rights) to be carried out in the same manner as that of any other technical product or procedure.","",""
"2024","Navigating the ethical landscape behind ChatGPT"," In this commentary, we examine the key ethical concerns arising from the rapid penetration and proliferation of generative artificial intelligence (AI), with ChatGPT as a prominent case study. Our analysis is structured around four pivotal themes: the debates on plagiarism and authorship in AI-generated content; the underlying power dynamics that shape biases in AI development; the dynamic, complex relationships between humans and machines; and the growing concerns over unchecked progress and the absence of accountability in the rapidly intensifying AI “Arms Race.” Recognizing the necessity for ethical alignment in AI, yet without a clear consensus of “human interests,” gives room for further exacerbating global inequalities, we advocate for enhanced transparency and increased public involvement in AI development and deployment processes. This article underscores the importance of engaging a diverse range of voices, especially those from communities traditionally uninvolved or excluded from the dialogue on AI development. By doing so, we aim to foster a more inclusive and multidisciplinary approach to understanding and shaping the trajectory of AI technologies, ensuring that their benefits are equitably shared, and their risks carefully managed. ","",""
"2024","Scaffolding decision spaces in decision support systems: Using plagiarism screening software in editorial offices"," This paper explores the dynamics of algorithmic governance, decision support systems and human involvement in the context of plagiarism screening in academic publishing. While automated plagiarism screening is widespread in editorial work, critical investigations about these decision support systems remain scarce. Focusing on the issue of human autonomy and discretion in algorithmic governance, the paper investigates the complexities of the human-in-the-loop within these screening tools. Revisiting Wanda Orlikowski's conceptual metaphor of ‘scaffolding’, the study empirically analyses interactions between editors and plagiarism screening software. It traces how these tools act as scaffolds, defining plagiarism as a manageable problem while allowing editors considerable flexibility in decision-making. The software, which is both non-deterministic and powerful, transforms issues into potential decisions, shaping the decision space for human editors. Based on this investigation of screening software as scaffolding, the paper argues that the question of human involvement in systems for automated decision-making is somewhat beside the point, and that analytical attention should shift towards understanding how algorithmic systems configure decision spaces by establishing issues as decidable problems. The implications of this shift are discussed, emphasizing the need for advancing our understanding of power dynamics inherent in algorithm–human interactions within automated decision-making systems. ","",""
"2024","Agreements ‘in the wild’: Standards and alignment in machine learning benchmark dataset construction","This article presents an ethnographic case study of a corporate-academic group constructing a benchmark dataset of daily activities for a variety of machine learning and computer vision tasks. Using a socio-technical perspective, the article conceptualizes the dataset as a knowledge object that is stabilized by both practical standards (for daily activities, datafication, annotation and benchmarks) and alignment work – that is, efforts including forging agreements to make these standards effective in practice. By attending to alignment work, the article highlights the informal, communicative and supportive efforts that underlie the success of standards and the smoothing of tensions between actors and factors. Emphasizing these efforts constitutes a contribution in several ways. This article's ethnographic mode of analysis challenges and supplements quantitative metrics on datasets. It advances the field of dataset analysis by offering a detailed empirical examination of the development of a new benchmark dataset as a collective accomplishment. By showing the importance of alignment efforts and their close ties to standards and their limitations, it adds to our understanding of how machine learning datasets are built. And, most importantly, it calls into question a key characterization of the dataset: that it captures unscripted activities occurring naturally ‘in the wild’, as alignment work bleeds into moments of data capture.","",""
"2024","Generative AI and the politics of visibility"," Proponents of generative AI tools claim they will supplement, even replace, the work of cultural production. This raises questions about the politics of visibility: what kinds of stories do these tools tend to generate, and what do they generally not? Do these tools match the kind of diversity of representation that marginalized populations and non-normative communities have fought to secure in publishing and broadcast media? I tested three widely available generative AI tools with prompts designed to reveal these normative assumptions; I prompted the tools multiple times with each, to track the diversity of the outputs to the same query. I demonstrate that, as currently designed and trained, generative AI tools tend to reproduce normative identities and narratives, rarely representing less common arrangements and perspectives. When they do generate variety, it is often narrow, maintaining deeper normative assumptions in what remains absent. ","",""
"2024","Prediction and explainability in AI: Striking a new balance?"," The debate regarding prediction and explainability in artificial intelligence (AI) centers around the trade-off between achieving high-performance accurate models and the ability to understand and interpret the decisionmaking process of those models. In recent years, this debate has gained significant attention due to the increasing adoption of AI systems in various domains, including healthcare, finance, and criminal justice. While prediction and explainability are desirable goals in principle, the recent spread of high accuracy yet opaque machine learning (ML) algorithms has highlighted the trade-off between the two, marking this debate as an inter-disciplinary, inter-professional arena for negotiating expertise. There is no longer an agreement about what should be the “default” balance of prediction and explainability, with various positions reflecting claims for professional jurisdiction. Overall, there appears to be a growing schism between the regulatory and ethics-based call for explainability as a condition for trustworthy AI, and how it is being designed, assimilated, and negotiated. The impetus for writing this commentary comes from recent suggestions that explainability is overrated, including the argument that explainability is not guaranteed in human healthcare experts either. To shed light on this debate, its premises, and its recent twists, we provide an overview of key arguments representing different frames, focusing on AI in healthcare. ","",""
"2024","Rethinking use-restricted open-source licenses for regulating abuse of generative models"," The rapid progress of Artificial intelligence in generative modeling is marred by widespread misuse. In response, researchers turn to use-based restrictions—contractual terms prohibiting certain uses—as a “solution” for abuse. While these restrictions can be beneficial to artificial intelligence governance in API-gated settings, their failings are especially significant in open-source models: not only do they lack any means of enforcement, but they also perpetuate the current proliferation of tokenistic efforts toward ethical artificial intelligence. This observation echoes growing literature that points to useless efforts in “AI ethics,” and underscores the need to shift from this paradigm. This article provides an overview of these drawbacks and argues that researchers should divert their efforts to studying deployable, effective, and theoretically grounded solutions like watermarking and model alignment from human feedback to effect tangible changes in the current climate of artificial intelligence. ","",""
"2024","Tech workers’ perspectives on ethical issues in AI development: Foregrounding feminist approaches"," While tech workers are essential stakeholders in ethical artificial intelligence (AI) development and deployment, they are rarely consulted about their understanding of the development of ethical AI. In light of this, we present the findings of our 2020 to 2021 empirical research study in which we collected data from tech workers in a major AI company to better understand what they consider to be the most pressing ethical issues when developing AI-powered products. While there is a nascent body of literature that examines how AI ethics principles are operationalised on the ground, this study differs in that we explicitly draw on feminist insights to inform our analysis, and have put a particular focus on allowing the voices and narratives of tech workers to lead the work forward. Our study generated three main findings: first, the term ‘bias’ creates real confusion among tech workers, meaning that the term is unable to do the ethical work it is intended to do; second, tech workers do not necessarily see a relationship between diversity, equality and inclusion (DEI) agendas and AI development, undermining AI ethics initiatives; and third, tech workers were particularly concerned about the monitoring and maintenance of unwieldy ‘legacy systems’ that generated serious challenges to creating and deploying new and more ethical AI products. This study thus creates a ‘thicker’ and more nuanced picture of tech workers’ perspectives on the ethical issues that arise when developing and maintaining AI systems, while simultaneously demonstrating the utility of feminist approaches in the field of AI ethics. ","",""
"2024","Just public algorithms: Mapping public engagement with the use of algorithms in UK public services"," This paper proposes and models a novel approach to public engagement with the use of algorithms in public services. Algorithms pose significant risks which need to be anticipated and mitigated through democratic governance, including public engagement. We argue that as the challenge of creating responsible algorithms within a dynamic innovation system is one that will never definitively be accomplished – and as public engagement is not singular or pre-given but is always constructed through performance and in relation to other processes and events – public engagement with algorithms needs to be conducted and conceptualised as relational, systemic, and ongoing. We use a systemic mapping approach to map and analyse 77 cases of public engagement with the use of algorithms in public services in the UK 2013–2020 and synthesise the potential benefits and risks of these approaches articulated across the cases. The mapping shows there was already a diversity of public engagement on this topic in the UK by 2020, involving a wide range of different policy areas, framings of the problem, affordances of algorithms, publics, and formats of public engagement. While many of the cases anticipate benefits from the adoption of these technologies, they also raise a range of concerns which mirror much of the critical literature and highlight how algorithmic approaches may sometimes foreclose alternative options for policy delivery. The paper concludes by considering how this approach could be adopted on an ongoing basis to ensure the responsible governance of algorithms in public services, through a ‘public engagement observatory’. ","",""
"2024","Algorithmic decision-making: The right to explanation and the significance of stakes"," The stakes associated with an algorithmic decision are often said to play a role in determining whether the decision engenders a right to an explanation. More specifically, “high stakes” decisions are often said to engender such a right to explanation whereas “low stakes” or “non-high” stakes decisions do not. While the overall gist of these ideas is clear enough, the details are lacking. In this paper, we aim to provide these details through a detailed investigation of what we will call the “Simple Stakes Thesis.” The Simple Stakes Thesis, as it will turn out, is too simple. For even if the stakes associated with a specific one-off decision are low—and hence does not engender a right to an explanation—such decisions may nevertheless form part of a high stakes pattern or aggregate of decisions. In such cases, we argue, even a low stakes decision may engender a right to explanation. Not only does this show that the right to explanation is more demanding than so far recognized but it also shows that the stakes thesis is significantly harder to apply in practice. ","",""
"2024","Controversies, contradiction, and “participation” in AI"," This commentary examines the inherent contradictions between participation in artificial intelligence (AI), controversy studies, and AI narratives. ","",""
"2024","A typology of artificial intelligence data work"," This article provides a new typology for understanding human labour integrated into the production of artificial intelligence systems through data preparation and model evaluation. We call these forms of labour ‘AI data work’ and show how they are an important and necessary element of the artificial intelligence production process. We draw on fieldwork with an artificial intelligence data business process outsourcing centre specialising in computer vision data, alongside a decade of fieldwork with microwork platforms, business process outsourcing, and artificial intelligence companies to help dispel confusion around the multiple concepts and frames that encompass artificial intelligence data work including ‘ghost work’, ‘microwork’, ‘crowdwork’ and ‘cloudwork’. We argue that these different frames of reference obscure important differences between how this labour is organised in different contexts. The article provides a conceptual division between the different types of artificial intelligence data work institutions and the different stages of what we call the artificial intelligence data pipeline. This article thus contributes to our understanding of how the practices of workers become a valuable commodity integrated into global artificial intelligence production networks. ","",""
"2024","Reclaiming artificial intelligence accounts: A plea for a participatory turn in artificial intelligence inquiries"," How to participate in artificial intelligence otherwise? Put simply, when it comes to technological developments, participation is either understood as public debates with non-expert voices to anticipate risks and potential harms, or as a way to better design technical systems by involving diverse stakeholders in the design process. We advocate for a third path that considers participation as crucial to problematise what is at stake and to get a grip on the situated developments of artificial intelligence technologies.  This study addresses how the production of accounts shape problems that arise with artificial intelligence technologies. Taking France as a field of study, we first inspected how media narratives account for the entities and issues of artificial intelligence, as reported by the national press over the last decade. From this inspection, we identified four genres and described their performative effects. We then conducted a participatory inquiry with 25 French artificial intelligence practitioners’ to ground artificial intelligence in situated experiences and trajectories. These experiential accounts enabled a plural problematisation of artificial intelligence, playing with the geometries of artificial intelligence and its constituencies, while diversifying and thickening its problems.  To conclude, we discuss how participatory inquiries, through experiential and plural accounts offer a refreshing weaving of artificial intelligence problems into the fabric of its deployments. Our participatory approach seeks to re-politicise artificial intelligence from practitioners’ situated experiences, by making the ongoing relationships between past trajectories, current frictions and future developments tangible and contestable, opening avenues to contribute otherwise. ","",""
"2024","Big AI: Cloud infrastructure dependence and the industrialisation of artificial intelligence"," Critical scholars contend that ‘There is no AI without Big Tech’. This study delves into the substantial role played by major technology conglomerates, including Amazon, Microsoft, and Google (Alphabet), in the ‘industrialisation of artificial intelligence’. This concept encapsulates the shift of AI technologies from the research and development stage to practical, real-world applications across diverse industry sectors, resulting in new dependencies and associated investments. We employ the term ‘Big AI’ to encapsulate the structural convergence of AI and Big Tech, characterised by the profound interdependence of AI with the infrastructure, resources, and investments of these major technology companies. Using a ‘technographic’ approach, our study scrutinises the infrastructural support and investments of Big Tech in the AI sector, focussing on corporate partnerships, acquisitions, and financial investments. Additionally, we conduct a detailed examination of the complete spectrum of cloud platform products and services offered by Amazon, Microsoft, and Google. We demonstrate that AI is not merely an abstract idea but an actual technology stack encompassing infrastructure, models, applications, and an ecosystem of applications and companies relying on this stack. Significantly, these tech giants have seamlessly integrated all three components of the stack into their cloud offerings. Furthermore, they have developed industry-focussed solutions and marketplaces aimed at attracting third-party developers and businesses, fostering the growth of a broader AI ecosystem. This analysis underscores the intricate interdependence between AI and cloud infrastructure, emphasising the industry-specific aspects of cloud AI. ","",""
"2024","“Warm Robots” for Children with Autism Spectrum Disorder? The Thermodynamics of Human Sociality and the Technology of Inclusion in South Korea","Over the past several years, robots that help children with autism spectrum disorder (ASD) to develop social skills have emerged as an archetype of “warm robots” in South Korea’s public science and technology discourses. This paper critically examines the socio-historical and conceptual meanings of this phenomenon. First, it traces how these robots have come to bear the “warmth” imaginary of new techno-society at the juncture of two national trends: the promotion of humanist pursuits of new technologies since the 2010s, and the Fourth Industrial Revolution initiated by the 2016 Davos Forum. Second, it analyzes the thermodynamics entailed by the current idea of therapy robots—that these robots bear the warmth of an imaginary techno-society, while the human warmth of autistic children remains bracketed and their mothers’ caregiving serves to insulate these human-robot interactions from broader sociality. By illuminating the complex social implications of such thermodynamics, this paper aims to move beyond the liberal humanist critique of machinic dehumanization of people with autism by criticizing its undergirding scheme of robots as surrogates for devalued gendered, racialized, and colonial labor. In doing so, this paper opens space for re-envisioning a more inclusive and diverse techno-society in South Korea and elsewhere.","",""
"2024","Dialectics of training: A critique of recommendation engines’ aesthetic judgment","In this article, we evaluate the politics of recommendation engines by focusing on an indispensible feature of their operation: training. We use the notion of training as a key word which helps us link three bodies of knowledge: data science, the history of automation, and aesthetic and political theory. Training is a staple in the operation of algorithmic systems, and artificial intelligence more generally; it is a practical methodology by which these systems become intelligent. Training is also a key feature of how workers throughout history came to perform their labor, and how, during the 20thcentury, machines came to acquire this human ability, that is, automation. And lastly, drawing on Immanuel Kant’s theory of aesthetic judgment, Hannah Arendt offers a political theory where training is key to political judgment. We trace the meaning and significance of ‘training’ in these three fields in order to draw conclusions from one field to another.","",""
"2024","‘People don’t buy art, they buy artists’: Robot artists – work, identity, and expertise"," This article critically examines the construction of the artistic identity and career of Ai-Da, ‘the world's first ultra-realistic humanoid robot artist’. Engaging with scholarship on posthumanism and creative assemblages, and creative work, identity and expertise, this article conceptualises Ai-Da's distinctive positioning and focuses on the practices used to construct a creative worker identity and career. The article uses qualitative content analysis to examine journalistic coverage, promotional and presentation activities, exhibitions and performances, and social media postings over a four-year period from Ai-Da’s first launch to international visibility. The analysis shows how Ai-Da is positioned as a high-profile, border crossing artist, engaging in debates about Artificial Intelligence (AI), art, and the environment. It considers the creative assemblage of Ai-Da as a humanoid robot artist, the creator Aidan Meller and the team working with him, and the wider contextual factors of aesthetic expertise, networks and knowledge of art worlds which have shaped Ai-Da's artistic identity and career trajectory. The focus on how Ai-Da signals expertise on social media helps to frame the specific techniques used to speak about and for Ai-Da on social media platforms and wider media coverage. This includes articulating inspiration, showcasing artistic processes and cultivating audience relationships. In concluding, the implications of connecting critical perspectives on creative work with developments in art, AI and robot artists are explored: firstly, for understanding how the practices for constructing an artistic identity shape the development of robot artists; secondly, for understanding how developments in art and AI can frame reflections on artistic identity and careers. ","",""
"2024","‘Grandma, tell that story about how to make napalm again’: Exploring early adopters’ collaborative domestication of generative AI"," The present study explores early adopters’ domestication processes of ChatGPT and Midjourney in the early phase of generative AI’s public breakthrough. To study this process, we conducted an online ethnography in two dominant AI forums on Reddit: r/chatgpt and r/midjourney. Drawing on domestication theory (Silverstone and Hirsch, 1992), we describe three modalities of domestication and argue they represent an emerging cultural approach to generative AI with potential to influence broader domestication and adoption of generative AI. Our findings show that early adopters’ domestication processes are collaborative and fueled by playful and nontelic activity, that is, non-purpose-driven behavior (Abend et al., 2020), where new content genres and transmedia practices (Jenkins, 2007) emerge and serve as an entryway into understanding and domesticating the technology. We identify three distinct modalities of domestication in relation to generative AI: jailbreaking, roleplaying, and playing ‘what if’. Our analysis highlights how non-purpose-driven and playful engagement with generative AI drives a highly collaborative domestication process among users. In this process, generative AI serves a double role as both the object and enabler of domestication. Based on the analysis, we argue for the need to take collaboration, affect, and playfulness into consideration in studies of users’ technology domestication processes. ","",""
"2024","Repairing what’s not broken – Algorithm repair manual as reflexivity device"," In this article, we outline an innovative participatory method for reflexive engagement with algorithmic systems and the underlying processes of datafication that accompany them. Faced with the challenges of thinking critically and reflexively about algorithmic systems, both as non-expert individuals and expert researchers, we develop and elaborate on an approach for engaging participants in thinking with, – through and – about algorithmic artifacts. In developing our approach, we start from the premise that algorithms are always broken, and we devise Repair Manuals as productive reflexivity devices that will enable for reflective and reflexive encounters with algorithmic artifacts. Borrowing from the approaches developed by Shannon Mattern and Joseph Dumit, we take algorithmic data artifacts as entry points to embark on an investigative, self-learning and sense-making journey of the inevitable entanglement between the individuals and the all-encompassing algorithmic systems. The results from our study show that this approach offers valuable opportunities and insights both for educators and for researchers, and can be used equally for empowerment and educational goals. ","",""
"2024","Climate futures: Machine learning from cli-fi"," This paper introduces and contextualises Climate Futures, an experiment in which AI was repurposed as a ‘co-author’ of climate stories and a co-designer of climate-related images that facilitate reflections on present and future(s) of living with climate change. It converses with histories of writing and computation, including surrealistic ‘algorithmic writing’, recombinatory poems and ‘electronic literature’. At the core lies a reflection about how machine learning’s associative, predictive and regenerative capacities can be employed in playful, critical and contemplative goals. Our goal is not automating writing (as in product-oriented applications of AI). Instead, as poet Charles Hartman argues, ‘the question isn’t exactly whether a poet or a computer writes the poem, but what kinds of collaboration might be interesting’ (1996, p. 5). STS scholars critique labs as future-making sites and machine learning modelling practices and, for example, describe them also as fictions. Building on these critiques and in line with ‘critical technical practice’ ( Agre, 1997 ), we embed our critique of ‘making the future’ in how we employ machine learning to design a tool for looking ahead and telling stories on life with climate change. This has involved engaging with climate narratives and machine learning from the critical and practical perspectives of artistic research. We trained machine learning algorithms (i.e. GPT-2 and AttnGAN) using climate fiction novels (as a dataset of cultural imaginaries of the future). We prompted them to produce new climate fiction stories and images, which we edited to create a tarot-like deck and a story-book, thus also playfully engaging with machine learning’s predictive associations. The tarot deck is designed to facilitate conversations about climate change. How to imagine the future beyond scenarios of resilience and the dystopian? How to aid our transition into different ways of caring for the planet and each other? ","",""
"2024","Navigating platformized generative AI: Examining early adopters’ experiences through the lens of data reflectivity"," Following the widely noticed launch of ChatGPT 3.5 in November 2022, an unprecedented number of users have started to experiment with generative AI for communication purposes. Prior studies have shown how users are commodified by platforms, and the unprecedented development of generative AI raises hence once again questions of platform dynamics vs user agency. In this study, we argue that platformized generative AI (GenAI) actively ‘talks back’ to their users, prompting them to act accordingly. Theoretically, we develop the concept of data reflectivity as a critical lens, showing that users exhibit reflective practices. These allow them to reflect upon their own role in relation to platformized GenAI and thus alter their patterns of action. The empirical case study, carried out in Spring 2023, draws on a survey with 60 early adopters and 14 subsequent semi-structured interviews, collected in an NGO operating in Southeast Asia. The thematic analysis shows that users relate to platformized GenAI in three distinct but related ways: (1) as a happy helper to organize and systematize knowledge and information; (2) as a creative tool to generate ideas; and (3) as a conversation partner for personal and life-related matters. In conclusion, we discuss the findings critically in relation to overall platform dynamics and the notion of systems ‘speaking back’ and further suggest that future research should aim to bring the two research fields of datafication and user studies even closer together. ","",""
"2024","Fluid agency in relation to algorithms: Tensions, mediations, and transversalities"," This paper argues for a fluid approach to the study of agency in relation to algorithms, one that promotes crossing the boundaries of established analytical positions and breaking away from dualistic forms to frame its study. Building on various intellectual traditions, we develop three sensibilities for implementing such an approach: (a) working with tensions as an alternative to thinking about algorithmic power and human agency as an either/or binary; (b) examining mediations to reverse the tendency to treat algorithms as an ahistorical and universal force; and (c) exploring transversalities to navigate the spaces that emerge between various temporalities and levels of analysis. To make our case, we examine a crucial tension in the study of agency and algorithms, namely how scholars have either attributed power to algorithms or agency to users of algorithmic systems. The conclusion situates our argument for fluidity within larger historical debates in the study of technological power and human agency. ","",""
"2024","Re-enacting machine learning practices to enquire into the moral issues they pose"," As the number of ethical incidents associated with Machine Learning (ML) algorithms increases worldwide, many actors are seeking to produce technical and legal tools to regulate the professional practices associated with these technologies. However these tools, generally grounded either on lofty principles or on technical approaches, often fail at addressing the complexity of the moral issues that ML-based systems are triggering. They are mostly based on a ‘principled’ conception of morality where technical practices cannot be seen as more than mere means to be put at the service of more valuable moral ends. We argue that it is necessary to localise ethical debates within the complex entanglement of technical, legal and organisational entities from which ML moral issues stem. To expand the repertoire of the approaches through which these issues might be addressed, we designed and tested an interview protocol based on the re-enactment of data scientists’ daily ML practices. We asked them to recall and describe the crafting and choosing of algorithms. Then, our protocol added two reflexivity-fostering elements to the situation: technical tools to assess algorithms’ morality, based on incorporated ‘ethicality’ indicators; and a series of staged objections to the aforementioned technical solutions to ML moral issues, made by factitious actors inspired by the data scientists’ daily environment. We used this protocol to observe how ML data scientists uncover associations with multiple entities, to address moral issues from within the course of their technical practices. We thus reframe ML morality as an inquiry into the uncertain options that practitioners face in the heat of technical activities. We propose to institute moral enquiries both as a descriptive method serving to delineate alternative depictions of ML algorithms when they are affected by moral issues and as a transformative method to propagate situated critical technical practices within ML-building professional environments. ","",""
"2024","Laughing to keep from [use input undefined]: ChatGPT, Jewish humor, and cultural erasure","This paper uses the embodied, Jewish identities of its three authors, and the experimental methodology of kibbitzing as a form of collective inquiry and self-reflexive praxis in order to demonstrate the limitations of chatbots to produce humorous narratives from an explicitly Jewish epistemology. By contrasting the affordances of large language models (LLMs) and their associated chatbots with the context-based logics of Jewish joke craft and storytelling, this article goes on to demonstrate the risk of cultural erasure that is posed by the positivist, denotative meanings associated with ChatGPT’s attempts at producing jokes for, or about, Jews.","",""
"2024","The TESCREAL bundle: Eugenics and the promise of utopia through artificial general intelligence","The stated goal of many organizations in the field of artificial intelligence (AI) is to develop artificial general intelligence (AGI), an imagined system with more intelligence than anything we have ever seen. Without seriously questioning whether such a system can and should be built, researchers are working to create “safe AGI” that is “beneficial for all of humanity.” We argue that, unlike systems with specific applications which can be evaluated following standard engineering principles, undefined systems like “AGI” cannot be appropriately tested for safety. Why, then, is building AGI often framed as an unquestioned goal in the field of AI? In this paper, we argue that the normative framework that motivates much of this goal is rooted in the Anglo-American eugenics tradition of the twentieth century. As a result, many of the very same discriminatory attitudes that animated eugenicists in the past (e.g., racism, xenophobia, classism, ableism, and sexism) remain widespread within the movement to build AGI, resulting in systems that harm marginalized groups and centralize power, while using the language of “safety” and “benefiting humanity” to evade accountability. We conclude by urging researchers to work on defined tasks for which we can develop safety protocols, rather than attempting to build a presumably all-knowing system such as AGI.","",""
"2024","Automated decision-making as domination","Machine learning ethics research is demonstrably skewed. Work that defines fairness as a matter of distribution or allocation and that proposes computationally tractable definitions of fairness has been overproduced and overpublished. This paper takes a sociological approach to explain how subtle processes of social-reproduction within the field of computer science partially explains this outcome. Arguing that allocative fairness is inherently limited as a definition of justice, I point to how researchers in this area can make broader use of the intellectual insights from political philosophy, philosophy of knowledge, and feminist and critical race theories. I argue that a definition of injustice not as allocative unfairness but as domination, drawing primarily from the argument of philosopher Iris Marion Young, would better explain observations of algorithmic harm that are widely acknowledged in this research community. This alternate definition expands the solution space for algorithmic justice to include other forms of consequential action beyond code fixes, such as legislation, participatory assessments, forms of user repurposing and resistance, and activism that leads to bans on certain uses of technology.","",""
"2024","Debunking robot rights metaphysically, ethically, and legally","In this work we challenge the argument for robot rights on metaphysical, ethical and legal grounds. Metaphysically, we argue that machines are not the kinds of things that may be denied or granted rights. Building on theories of phenomenology and post-Cartesian approaches to cognitive science, we ground our position in the lived reality of actual humans in an increasingly ubiquitously connected, controlled, digitized, and surveilled society. Ethically, we argue that, given machines’ current and potential harms to the most marginalized in society, limits on (rather than rights for) machines should be at the centre of current AI ethics debate. From a legal perspective, the best analogy to robot rights is not human rights but corporate rights, a highly controversial concept whose most important effect has been the undermining of worker, consumer, and voter rights by advancing the power of capital to exercise outsized influence on politics and law. The idea of robot rights, we conclude, acts as a smoke screen, allowing theorists and futurists to fantasize about benevolently sentient machines with unalterable needs and desires protected by law. While such fantasies have motivated fascinating fiction and art, once they influence legal theory and practice articulating the scope of rights claims, they threaten to immunize from legal accountability the current AI and robotics that is fuelling surveillance capitalism, accelerating environmental destruction, and entrenching injustice and human suffering.","",""
"2024","Infrastructuring AI: The stabilization of 'artificial intelligence' in and beyond national AI strategies","This paper explores how AI policy documents mediate the stabilization of socio-technical assemblages. It does so by developing the theory-methods package of ‘discursive infrastructuring’ and applying it to the U.K.’s National AI Strategy. By centering the conceptual slipperiness of emerging technologies such as AI, this framework sheds light on how policy documents work to stabilize emerging socio-technical assemblages comprising specific actors, ideologies, flows of capital, and relationships of power. In the context of the National AI Strategy, discursive infrastructuring reveals how the document stabilises: AI as an autonomous and inevitable force; a technical/social dualism which privileges the technical over the social in driving innovation; the ‘heroic engineer’ as an individual, masculine and rational archetype; and, the U.K. as a dominant and modernising player on AI’s global stage. This assemblage does not only exist in the document’s words; it is translated into practice through the funding of institutions, the centring of technical pedagogies of AI, and the opening of visa routes for ‘globally mobile individuals’. The application of ‘discursive infrastructuring’ to the National AI Strategy thus elucidates the constitutive role of policy discourse in stabilising politically situated material-semiotic conceptions of AI.","",""
"2024","Fostering children’s agency in their learning futures: Exploring the synergy of generative AI and sensory learning","The discourse surrounding the potential educational transformation brought about by generative AI has largely neglected the sensory aspect of learning. In this position paper, I emphasize the significance of sensory studies and their theoretical foundations of embodiment and multimodality as catalysts for novel perspectives on the intersection of AI and the future of education. I delve into the question of whether generative AI serves as a precursor to a new literacy or merely arises as a consequence of ongoing theoretical advancements in contemporary literacy studies. I argue that the concept of agency, which includes both personal and social aspects, should be central to recognizing the importance of sensory learning as an emerging paradigm in reimagining learning futures.","",""
"2024","Introduction for the special issue of “Ideologies of AI and the consolidation of power”: Naming power","This introductory essay for the special issue of First Monday, “Ideologies of AI and the consolidation of power,” considers how power operates in AI and machine learning research and publication. Drawing on themes from the seven contributions to this special issue, we argue that what can and cannot be said inside of mainstream computer science publications appears to be constrained by the power, wealth, and ideology of a small cohort of industrialists. The result is that shaping discourse about the AI industry is itself a form of power that cannot be named inside of computer science. We argue that naming and grappling with this power, and the troubled history of core commitments behind the pursuit of general artificial intelligence, is necessary for the integrity of the field and the well-being of the people whose lives are impacted by AI.","",""
"2024","AI literacy as civic literacy: A case study of the “AI and human rights” curriculum for middle school students","While the younger generation immerses itself in AI technologies, governmental bodies are at the initial stages of investigating the technology’s implications on our rights and how it will be best regulated. There is an opportunity to complement AI education with civic understanding. This work introduces the “AI and human rights” curriculum, which consists of seven lessons on human rights, how technology shapes our rights, and the Blueprint for an AI Bill of Rights. We summarize the experience of 50 middle school students who participated in a rights-based AI legislative simulation. Lastly, we will provide recommendations for how to engage youth in AI and human rights topics. We aim to share this work to raise awareness about the significance of AI literacy for youth citizenship while providing valuable guidance to educators and policy-makers seeking to implement similar initiatives.","",""
"2024","Participation versus scale: Tensions in the practical demands on participatory AI","Ongoing calls from academic and civil society groups and regulatory demands for the central role of affected communities in development, evaluation, and deployment of artificial intelligence systems have created the conditions for an incipient “participatory turn” in AI. This turn encompasses a wide number of approaches — from legal requirements for consultation with civil society groups and community input in impact assessments, to methods for inclusive data labeling and co-design. However, more work remains in adapting the methods of participation to the scale of commercial AI. In this paper, we highlight the tensions between the localized engagement of community-based participatory methods, and the globalized operation of commercial AI systems. Namely, the scales of commercial AI and participatory methods tend to differ along the fault lines of (1) centralized to distributed development; (2) calculable to self-identified publics; and (3) instrumental to intrinsic perceptions of the value of public input. However, a close look at these differences in scale demonstrates that these tensions are not irresolvable but contingent. We note that beyond its reference to the size of any given system, scale serves as a measure of the infrastructural investments needed to extend a system across contexts. To scale for a more participatory AI, we argue that these same tensions become opportunities for intervention by offering case studies that illustrate how infrastructural investments have supported participation in AI design and governance. Just as scaling commercial AI has required significant investments, we argue that scaling participation accordingly will require the creation of infrastructure dedicated to the practical dimension of achieving the participatory tradition’s commitment to shifting power.","",""
"2024","Notes towards infrastructure governance for large language models","This paper draws on information infrastructures (IIs) in science and technology studies (STS), as well as on feminist STS scholarship and contemporary critical accounts of digital technologies, to build an initial mapping of the infrastructural mechanisms and implications of large language models (LLMs). Through a comparison with discriminatory machine learning (ML) systems and a case study on gender bias, I present LLMs as contested artefacts with categorising and performative capabilities. This paper suggests that generative systems do not tangibly depart from traditional, discriminative counterparts in terms of their underlying probabilistic mechanisms, and therefore both technologies can be theorised as infrastructures of categorisation. However, LLMs additionally retain performative capabilities through their linguistic outputs. Here, I outline the intuition behind this phenomenon, which I refer to as “language as infrastructure”. While traditional, discriminative systems “disappear” into larger IIs, the hype surrounding generative technologies presents an opportunity to scrutinise these artefacts, to alter their computational mechanisms and introduce governance measures]. I illustrate this thesis through Sharma’s formulation of “broken machine”, and suggest dataset curation and participatory design as governance mechanisms that can partly address downstream harms in LLMs (Barocas, et al., 2023).","",""
"2024","Field-building and the epistemic culture of AI safety","The emerging field of “AI safety” has attracted public attention and large infusions of capital to support its implied promise: the ability to deploy advanced artificial intelligence (AI) while reducing its gravest risks. Ideas from effective altruism, longtermism, and the study of existential risk are foundational to this new field. In this paper, we contend that overlapping communities interested in these ideas have merged into what we refer to as the broader “AI safety epistemic community,” which is sustained through its mutually reinforcing community-building and knowledge production practices. We support this assertion through an analysis of four core sites in this community’s epistemic culture: 1) online community-building through Web forums and career advising; 2) AI forecasting; 3) AI safety research; and 4) prize competitions. The dispersal of this epistemic community’s members throughout the tech industry, academia, and policy organizations ensures their continued input into global discourse about AI. Understanding the epistemic culture that fuses their moral convictions and knowledge claims is crucial to evaluating these claims, which are gaining influence in critical, rapidly changing debates about the harms of AI and how to mitigate them.","",""
"2024","Why do people use ChatGPT? Exploring user motivations for generative conversational AI","Generative conversational artificial intelligence (AI), such as ChatGPT, has attracted substantial attention since November 2022. The advent of this technology showcases the vast potential of such AI for generating and processing text and raises compelling questions regarding its potential usage. To obtain the requisite knowledge of users’ motivations in adopting this technology, we surveyed early adopters of ChatGPT (n = 197). Analysis of free text responses within the uses and gratifications (U&amp;G) theoretical framework shows six primary motivations for using generative conversational AI: productivity, novelty, creative work, learning and development, entertainment, and social interaction and support. Our study illustrates how generative conversational AI can fulfill diverse user needs, surpassing the capabilities of traditional conversational technologies, for example, by outsourcing cognitive or creative works to technology.","",""
"2024","Diffused Seeing","This article examines the transformation of the relationship between seeing and understanding in humans and machines by the technologies of machine learning known as ‘generative AI’. Taking Stable Diffusion as the main case study, while also looking at its competitors (DALL·E 2 and Midjourney), it starts by analysing the photographic infrastructure underpinning these generative models. The subsequent examination of ‘diffusion’ as a key concept that underpins the text-to-image generation process leads to some broader questions about the ongoing instability and dissolution of our current epistemological and political frameworks. Taking seriously the charge issued by some critics equating developments in generative AI with nihilism or even fascism, the article considers whether the current socio-technical moment can also offer some emancipatory possibilities. Images are used as part of the article not just by way of illustration but also to enact some of its argument.","",""
"2024","DALL-E in Flatland","The release of multimodal generative artificial intelligence (AI) models in 2022, including DALL-E, Midjourney, and Stable Diffusion, has opened up new avenues to producing images. This new paradigm has implications for ideas of vision, illusion, and representation. Computer vision research has long used photographic images as a way to enable machines to see. Generative AI, on the other hand, uses data and techniques from computer vision to create images. This is possible thanks to the use of artificial neural networks that process vast datasets of training images, many of which are photographs, and the text associated with them. This article considers the relationship between image creation, illusion, and the notion of space in multimodal models. The photographic form of AI-generated images, coupled with a lack of understanding or experience of space undergirding their creation, reveals the illusion and artifice that is part of all photography.","",""
"2024","Lost in Compression","Recent developments of image generators have introduced a new point of contention in the already contested field of artificial intelligence: the ownership of images. In 2023 Getty Images sued the company Stability AI, accusing it of illegally appropriating photographs for the purpose of training its models. Analysing image generators and stock agencies as probabilistic systems, this text argues that their significant difference lies in their model of appropriation. Where Stability AI proceeds through direct appropriation, the stock agency proceeds through contractual appropriation using its dominant position in the market. The article discusses Getty Images’ release of an image generator trained on its own image collection and critically reflects on the stock agency’s attempt to insert its contractual engine in the core of the generative AI technology, making copyright the regulating principle of the relations of ownership of the system and the principle that constrains the range of images it can produce.","",""
"2024","Photography at a Standstill","What has replaced the still photograph are dynamic and distributed image-assemblages that unsettle received notions of space-time — no longer limited to traditional representation and not necessarily even visual. When it comes to computer vision for example, the descriptor photography seems largely redundant (despite deep learning computer vision systems being trained on large datasets of photographic images), and so too the tired metaphor of the eye that once supported its theories and practices. What is at stake here, as ever, is a kind of ‘seeing’ (if we continue to call it that) that makes clear what is visible, sensible and knowable, and crucially also what is not. We might call this seeing algorithmically, or seeing like a dataset, or perhaps even seeing like an infrastructure, comprised of fake images that render fake history. The logic of this invokes the complex notion of ‘image is dialectics at a standstill’, encapsulating a constellation of possible outcomes. To what extent is the radical potential that Benjamin once foresaw in montage applicable to image-based AI given that it seems less an instrument to imagine a qualitatively different future but simply more of the same. What can be seen is not so much representational nor photographic but latent traces of material relations and infrastructures that render historical experience in compromised form.","",""
"2024","Conjuring algorithms: Understanding the tech industry as stage magicians"," In this article, we introduce the term “conjuration of algorithms” to describe how the tech industry uses the language of magic to shape people’s perceptions of algorithms. We use the image of the magician as a metaphor for how the tech industry strategically deploys narrative devices to present their algorithms. After presenting a brief history of the Western European and North American understanding of stage magic, we apply three principles of magic to a recent case: OpenAI’s discussion of ChatGPT to show how tech leaders present algorithms as magical entities. We argue that the conjuration of algorithms allows the tech industry to forge vivid, overly positive, and deterministic narratives that make it challenging for their critics to call attention to the very real harms that algorithmic systems pose to users. We call for discourses of reality instead of magic, as a way to support responsible technology design, development, use, and governance. ","",""
"2024","Algorithmic media use and algorithm literacy: An integrative literature review","Algorithms profoundly shape user experiences on digital platforms, raising concerns about their negative impacts and highlighting the importance of algorithm literacy. Research on individuals’ understanding of algorithms and their effects is expanding rapidly but lacks a cohesive framework. We conducted a systematic integrative literature review across social sciences and humanities (n = 169), addressing algorithm literacy in terms of its key conceptualizations and the endogenous, exogenous, and personal factors that influence it. We argue that existing research can be framed in terms of experiential learning cycles and outline how this approach can be beneficial for acquiring algorithm literacy. Finally, we propose a future research agenda that includes defining core competencies relevant to algorithm literacy, standardization of measures, integrating subjective and factual aspects of algorithm literacy, and task- and domain-specific approaches.","",""
"2024","Algorithms as complementary abstractions"," The text diagnoses two opposing tendencies in the research on algorithms: the first abstracts and unites heterogeneous developments under the term “algorithm”; the second emphasizes specifics such as data sets, material conditions, software libraries, interfaces, and so on, thus dissolving that which apparently algorithms do into more fine-grained analyses. The text proposes a research perspective that resolves this tension by conceiving of algorithms as a relation between the abstract and the concrete that allows to capture both in their interdependence. This approach is informed by two motives: first, the necessity to connect detailed analyses of specific information technologies with general political concerns; and second, the application of recent feminist critiques of epistemology to the analysis of algorithms. The ensuing relational perspective on algorithms is connected to the genealogy of algorithmic technology before being demonstrated regarding the mutually complementing relationships: algorithms-materiality, algorithms-data, algorithms-code, and algorithms-interfaces. ","",""
"2024","Emotional talk about robotic technologies on Reddit: Sentiment analysis of life domains, motives, and temporal themes"," This study grounded on computational social sciences and social psychology investigated sentiment and life domains, motivational, and temporal themes in social media discussions about robotic technologies. We retrieved text comments from the Reddit social media platform in March 2019 based on the following six robotic technology concepts: robot ( N = 3,433,554), AI ( N = 2,821,614), automation ( N = 879,092), bot ( N = 21,559,939), intelligent agent ( N = 15,119), and software agent ( N = 18,324). The comments were processed using VADER and LIWC text analysis tools and analyzed further with logistic regression models. Compared to the other four concepts, robot and AI were used less often in positive context. Comments addressing themes of leisure, money, and future were associated with positive and home, power, and past with negative comments. The results show how the context and terminology affect the emotionality in robotic technology conversations. ","",""
"2024","Which recommendation system do you trust the most? Exploring the impact of perceived anthropomorphism on recommendation system trust, choice confidence, and information disclosure"," Recommendation systems (RSs) leverage data and algorithms to generate a set of suggestions to reduce consumers’ efforts and assist their decisions. In this study, we examine how different framings of recommendations trigger people’s anthropomorphic perceptions of RSs and therefore affect users’ attitudes in an online experiment. Participants used and evaluated one of four versions of a web-based wine RS with different source framings (i.e. “recommendation by an algorithm,” “recommendation by an AI assistant,” “recommendation by knowledge generated from similar people,” no description). Results showed that different source framings generated different levels of perceived anthropomorphism. Participants indicated greater trust in the recommendations and greater confidence in making choices based on the recommendations when they perceived an RS as highly anthropomorphic; however, higher perceived anthropomorphism of an RS led to a lower willingness to disclose personal information to the RS. ","",""
"2024","Shaping feminist artificial intelligence"," This article examines the historical and contemporary shaping of feminist artificial intelligence (FAI). It begins by looking at the microhistory of FAI through the writings of Alison Adam and her graduate students to enrich the plural histories of AI and to write back feminist history into AI. Then, to explore contemporary examples of how FAI is being shaped today and how it deploys a multiplicity of meanings, I provide the following typology: FAI (1) as model, (2) as design, (3) as policy, (4) as culture, (5) as discourse, and (6) as science. This typology sheds light on the following questions: What does the term FAI mean? How has FAI been shaped over time? ","",""
"2024","The Lovelace effect: Perceptions of creativity in machines"," This article proposes the notion of the ‘Lovelace Effect’ as an analytical tool to identify situations in which the behaviour of computing systems is perceived by users as original and creative. It contrasts the Lovelace Effect with the more commonly known ‘Lovelace objection’, which claims that computers cannot originate or create anything, but only do what their programmers instruct them to do. By analysing the case study of AICAN – an AI art-generating system – we argue for the need for approaches in computational creativity to shift focus from what computers are able to do in ontological terms to the perceptions of human users who enter into interactions with them. The case study illuminates how the Lovelace effect can be facilitated through technical but also through representational means, such as the situations and cultural contexts in which users are invited to interact with the AI. ","",""
"2024","ChatGPT in the public eye: Ethical principles and generative concerns in social media discussions"," With ChatGPT’s rapid adoption, concerns regarding generative artificial intelligence (AI) have shifted from theoretical to practical. Drawing upon the “algorithmic imaginary” framework from critical algorithm studies and the anthropological concept of “ordinary ethics,” we analyzed Twitter discourse during ChatGPT’s initial deployment, examining 368,359 tweets. Our analysis identified five topics reflecting functional and critical aspects of ChatGPT. We specifically point to two topics with a critical perspective: “Ethics” and “Concerns.” The first aligns with scholarly discussions in AI ethics on fairness and transparency, while the second focuses on ChatGPT’s generative capabilities. This highlights an emerging trend: While the academic discussion on AI ethics has gained popularity, especially in scrutinizing ChatGPT, the conversation is now expanding to more nuanced ethical deliberations. We analyzed the posts’ engagement and sentiment over time, demonstrating the AI ethics community’s influence in addressing the potential and harms of generative AI systems. ","",""
"2024","Trust it or not: Understanding users’ motivations and strategies for assessing the credibility of AI-generated information"," The evolution of artificial intelligence (AI) facilitates the creation of multimodal information of mixed quality, intensifying the challenges individuals face when assessing information credibility. Through in-depth interviews with users of generative AI platforms, this study investigates the underlying motivations and multidimensional approaches people use to assess the credibility of AI-generated information. Four major motivations driving users to authenticate information are identified: expectancy violation, task features, personal involvement, and pre-existing attitudes. Users evaluate AI-generated information’s credibility using both internal (e.g. relying on AI affordances, content integrity, and subjective expertise) and external approaches (e.g. iterative interaction, cross-validation, and practical testing). Theoretical and practical implications are discussed in the context of AI-generated content assessment. ","",""
"2024","Human–AI communication in initial encounters: How AI agency affects trust, liking, and chat quality evaluation"," Artificial intelligence (AI) agency plays an important role in shaping humans’ perceptions and evaluations of AI. This study seeks to conceptually differentiate AI agency from human agency and examine how AI’s agency manifested on source and language dimensions may be associated with humans’ perceptions of AI. A 2 (AI’s source autonomy: autonomous vs human-assisted) × 2 (AI’s language subjectivity: subjective vs objective) × 2 (topics: traveling vs reading) factorial design was adopted ( N = 376). The results showed autonomous AI was rated as more trustworthy, and AI using subjective language was rated as more trustworthy and likable. Autonomous AI using subjective language was rated as the most trustworthy, likable, and of the best quality. Participants’ AI literacy moderated the interaction effect of source autonomy and language subjectivity on human trust and chat quality evaluation. Results were discussed in terms of human–AI communication theories and the design and development of AI chatbots. ","",""
"2024","But is it for us? Rural Chinese elders’ perceptions, concerns, and physical preferences regarding social robots"," Social robots can benefit aging people, especially those with restricted social interactions and health care, but how do resource-poor older adults respond to them? In this study, 5 focus groups with 60 older participants in rural China revealed their perceptions of social robots, concerns about the technology, and the types of social robots they were likely to accept. The participants cited multiple technological, discomfort, privacy, safety, and financial fraud concerns. They struggled to define robots as machines, humans, or something else but preferred small-sized, animal-shaped, or young female-gendered human-like robots. Their interconnected perceptions, concerns, and preferences illuminate a resource-poor group’s struggles, imaginations, hopes, uncertainties, and vulnerabilities when a new social and technological actor is embedded in their social worlds, reflecting how people understand social robots in relation to themselves and themselves in relation to social robots. Our study findings contribute to understanding social robots’ subjectivities and ways to design culturally and socially acceptable robots. ","",""
"2024","Does Godwin’s law (rule of Nazi analogies) apply in observable reality? An empirical study of selected words in 199 million Reddit posts"," As Godwin’s Law states, “as a discussion on the Internet grows longer, the likelihood of a person being compared to Hitler, or another Nazi reference, increases.” However, even though the theoretical probability of an infinitely long conversation including any term should approach 1.0, in practice, conversations cannot be infinite in length, and this long-accepted axiom is impossible to observe. By analyzing 199 million Reddit posts, we note that, after a certain point, the probability of observing the terms “Nazi” or “Hitler” actually decreases significantly with conversation length. In addition, a corollary of Godwin’s Law holds that “the invocation of Godwin’s Law is usually done by an individual that is losing the argument,” and, thus, that comparisons to Nazis are a signal of a discussion’s end. In other words, comparing one’s interlocutor to Hitler is supposed to be a conversation-killer. While it is difficult to determine whether a discussion on a given topic ended or not in a large dataset, we observe a marked increase in conversation length when the words “Hitler” or “Nazi” are newly interjected. Given that both of these observations challenge widely accepted and intuitive truisms, other words were run through the same set of tests. Within the context of the initial question, these results suggest that it is not inevitable that conversations eventually disintegrate into reductio ad Hitlerum, and that such comparisons are not conversation-killers. The results moreover suggest that we may underestimate, in the popular imagination, how much conversations may actually become narrower and therefore may tend to have a more impoverished or limited vocabulary as they stretch on. All of these observations provoke questions for further research. ","",""
"2024","Does distrust in humans predict greater trust in AI? Role of individual differences in user responses to content moderation"," When evaluating automated systems, some users apply the “positive machine heuristic” (i.e. machines are more accurate and precise than humans), whereas others apply the “negative machine heuristic” (i.e. machines lack the ability to make nuanced subjective judgments), but we do not know much about the characteristics that predict whether a user would apply the positive or negative machine heuristic. We conducted a study in the context of content moderation and discovered that individual differences relating to trust in humans, fear of artificial intelligence (AI), power usage, and political ideology can predict whether a user will invoke the positive or negative machine heuristic. For example, users who distrust other humans tend to be more positive toward machines. Our findings advance theoretical understanding of user responses to AI systems for content moderation and hold practical implications for the design of interfaces to appeal to users who are differentially predisposed toward trusting machines over humans. ","",""
"2024","Impact of misinformation from generative AI on user information processing: How people understand misinformation from generative AI"," This study examines the impact of artificial intelligence (AI) on the ways in which users process and respond to misinformation in generative artificial intelligence (GenAI) contexts. Drawing on the heuristic–systematic model and the concept of diagnosticity, our approach examines a cognitive model for processing misinformation in GenAI. The study’s findings revealed that users with a high-heuristic processing mechanism, which affects positive diagnostic perception, were more likely to proactively discern misinformation than users with low-heuristic processing and low-perceived diagnosticity. When exposed to misinformation from GenAI, users’ perceived diagnosticity of misinformation can be accurately predicted by the ways in which they perform heuristic systematic evaluations. With this focus on misinformation processing, this study provides theoretical insights and relevant recommendations for firms to be more resilient in protecting users from the detrimental impacts of misinformation. ","",""
"2024","What did you hear and what did you see? Understanding the transparency of facial recognition and speech recognition systems during human–robot interaction"," As social robots begin to assume various social roles in society, the demand for understanding how social robots work and communicate grows rapidly. While literature on explainable artificial intelligence suggests that transparency about a social robot’s working mechanism can evoke users’ positive attitudes, transparency may also have negative outcomes. This study investigates the paradoxical effects of the transparency of facial recognition technology and speech recognition technology in human–robot interactions. Based on a lab experiment and combined analyses of users’ quantitative and qualitative responses, this study suggests that the transparency of facial recognition technology in human–robot interaction increases users’ social presence, reduces privacy concerns, and enhances users’ acceptance of robots. However, exposure to both facial and speech recognition technologies revives users’ privacy worries. This study further parses users’ open-ended evaluation of the prospective application of social robots’ tracking technologies and discusses the theoretical, practical, and ethical value of the findings. ","",""
"2024","Facets of algorithmic literacy: Information, experience, and individual factors predict attitudes toward algorithmic systems"," Algorithmic decision-making systems are ubiquitous in digital media, but the public has been largely unable to negotiate the role of algorithms in society. Building from the concept of attitude-behavior consistency for political behavior, we develop a framework for fostering algorithmic literacy to develop well-informed attitudes toward algorithms. As algorithms are increasingly relevant to broad societal effects, an integrative approach is needed for a full account of how the public makes sense of algorithms and their role in society. We designed and tested a novel intervention that combines algorithmic literacy with personalized user experiences to see how each component influenced attitudes toward algorithms. We found these methods jointly informed attitudes, but the intervention’s efficacy was dependent on participants’ individual differences in technology use. ","",""
"2024","Unravelling Copyright Dilemma of AI-Generated News and Its Implications for the Institution of Journalism: The Cases of US, EU, and China"," This study adopts a multiple-case study design to address ‘Does copyright law protect automated news, and if so, how’ in three jurisdictions: the United States, the European Union and China. Through doctrinal legal analysis of the copyright laws and document analysis of policy reports, corporate responses and other empirical evidence, this study has found that the three copyright regimes differ substantively with regard to both formal texts and informal enforcement of copyright claims to artificial intelligence (AI)-generated news. In the United States, there has been a policy silence. In the European Union (EU), eager regulators have rushed to enact premature laws and failed policy patchwork. In China, the state is instrumentalising both laws and journalism to further its own interests. These findings suggest that current regulatory frameworks in all cases have led to a weakening of the institution of copyright, which, in turn, has contributed to the deinstitutionalisation of journalism and the institutionalisation of algorithms. ","",""
"2024","Algorithms as conversational partners: Looking at Google auto-predict through the lens of symbolic interaction"," This article showcases a speculative methodology for recreating interactions between a human and Google Search’s Auto-Predict interface as conversations, to explore how AI-based systems are both persuasive and deeply personal. Using ethnomethodology tools and a symbolic interactionist lens, the paper presents three versions of a single Google search, each variation building a slightly different angle on the plausible utterances and interpersonal dynamics of the human and nonhuman partners. This thought experiment emerges from a decade of classroom-based digital literacy exercises with young adults, training them to analyze their lived experiences with digital media, algorithms, and devices. Transforming information exchanges into personal conversations provides a creative method for analyzing how relations are co-constructed in the granular processes of interaction, through which mutual intelligibility is built, meaning about the world is made, and identities are formed. This critical analysis extends methods for human–machine communication studies and elaborates notions of algorithmic identity. ","",""
"2024","The artificial intelligence divide: Who is the most vulnerable?"," This study investigates users’ artificial intelligence (AI)-related competencies (i.e., AI knowledge, skills, and attitudes) and identifies the vulnerable user groups in the AI-shaped online news and entertainment environment. We surveyed 1088 Dutch citizens over the age of 16 years and identified five user groups through the latent class analysis: the average users, the expert advocates, the expert skeptics, the unskilled skeptics, and the neutral unskilled. The most vulnerable groups with the lowest levels of AI knowledge and AI skills (i.e., unskilled skeptics and neutral unskilled) were mostly older, with lower levels of education and privacy protection skills, than the average users. Overall, the results of this study resonate with the existing findings on the digital divide and provide evidence for an emerging AI divide among users. Finally, the societal implication of this study is discussed, such as the need for education programs and applications of the explainable AI. ","",""
"2024","Practical knowledge of algorithms: The case of BreadTube"," The growing ubiquity of algorithms in everyday life has prompted cross-disciplinary interest in what people know about algorithms. The purpose of this article is to build on this growing literature by highlighting a particular way of knowing algorithms evident in past work, but, as yet, not clearly explicated. Specifically, I conceptualize practical knowledge of algorithms to capture knowledge located at the intersection of practice and discourse. Rather than knowing that an algorithm is/does X, Y, or Z, practical knowledge entails knowing how to accomplish X, Y, or Z within algorithmically mediated spaces as guided by the discursive features of one’s social world. I conceptualize practical knowledge in conversation with past work on algorithmic knowledge and theories of knowing, and as empirically grounded in a case study of a leftist online community known as “BreadTube.” ","",""
"2024","Dimensions of autonomy in human–algorithm relations"," This article reorients research on agentic engagements with algorithms from the perspective of autonomy. We separate two horizons of algorithmic relations – the instrumental and the intimate – and analyse how they shape different dimensions of autonomous agency. Against the instrumental horizon, algorithmic systems are technical procedures ordering social life at a distance and using rules that can only partly be known. Autonomy is activated as reflective and informed choice and the ability to enact one’s goals and values amid technological constraints. Meanwhile, the intimate horizon highlights affective aspects of autonomy in relation to algorithmic systems as they creep ever closer to our minds and bodies. Here, quests for autonomy arise from disturbance and comfort in a position of vulnerability. We argue that the dimensions of autonomy guide us towards issues of specific ethical and political importance, given that autonomy is never merely a theoretical concern, but also a public value. ","",""
"2024","Minority social influence and moral decision-making in human–AI interaction: The effects of identity and specialization cues"," In group decision-making, the behavior of each member is sensitive to the social influence of other majority members. Research on majority influence has shown that multiple non-human agents with anthropomorphic cues can exert normative pressure on a lone human decision-maker. However, how individuals perceive and respond to minority influence exerted by a lone machine is rarely discussed. Hence, a between-subjects experiment was conducted to examine how different minority identity (human vs artificial intelligence [AI]) and specialization (specialist vs generalist) cues influence individuals’ perceptions and behavior in response to moral dilemmas in a joint human–AI group. The results confirmed the significant role of specialization cues in predicting in-group identification, source credibility, and conversion behavior. In addition, the participants perceived the human minority as more credible than the AI minority, which prompted conversion behavior when the minority was labeled as a specialist rather than as a generalist. ","",""
"2024","Enhancing children’s understanding of algorithmic biases in and with text-to-image generative AI"," Despite the growing concerns surrounding algorithmic biases in generative AI (artificial intelligence), there is a noticeable lack of research on how to facilitate children and young people’s awareness and understanding of them. This study aimed to address this gap by conducting hands-on workshops with fourth- and seventh-grade students in Finland, and by focusing on students’ ( N = 209) evolving explanations of the potential causes of algorithmic biases within text-to-image generative models. Statistically significant progress in children’s data-driven explanations was observed on a written reasoning test, which was administered prior to and after the intervention, as well as in their responses to the worksheets they filled out during a lesson that focused on algorithmic biases. The article concludes with a discussion on the development and facilitation of children’s understanding of algorithmic biases. ","",""
"2024","How the social robot Sophia is mediated by a YouTube video"," In robotics, a field of research still populated by prototypes, much of the research is made through videos and pictures of robots. We study how the highly human-like robot Sophia is perceived through a YouTube video. Often researchers take for granted in their experiments that people perceive humanoids as such. With this study we wanted to understand to what extent a convenience sample of university students perceive Sophia’s human-likeness; second, we investigated which mental capabilities and emotions they attribute to her; and third, we explored the possible uses of Sophia they imagine. Our findings suggest that the morphological human-likeness of Sophia, through the video, is not salient in the Sophia’s representations of these participants. Only some mental functions are attributed to Sophia and no emotions. Finally, uses of Sophia turned out to be connected to the gender stereotypes that characterize stereotyped women’s professions and occupations but not completely. ","",""
"2024","Moral orders of pleasing the algorithm"," This article examines how ‘pleasing the algorithm’, or engaging with algorithms to gain rewards such as visibility for one’s content on digital platforms, is treated from a moral perspective. Drawing from Harré’s work on moral orders, our qualitative analysis of Reddit messages focused on social media content creation illustrates how so-called folk theories of algorithms are used for moral evaluations about the responsibilities and worthiness of different actors. Moral judgements of the actions of content creators encompass ideas of individuals and their agency in relation to algorithmic systems, and these ideas influence the assessment of algorithm-pleasing as an integral part of the craft, as condemnable behaviour, or as a necessary evil. In this way, the feedback loops that arrange people and code into algorithmic systems inevitably make theories about those systems also theories about humans and their behaviour and agency. ","",""
"2024","Working with algorithmic management: Design logic, algorithmic unfitness, and labor repair behind the wall"," The current literature on algorithmic management primarily explores its external ramifications for workers. Drawing upon the case of Meituan Waimai, China’s dominant food-delivery platform, this article delves into the internal logic that guides the design of algorithmic management. I highlight how an optimal mindset and a mechanized view of human labor drive the formulation of algorithmic management, leading to what I call “algorithmic unfitness.” This concept represents a dissonance between algorithmically programmed, reductionist visions of food delivery and real-world experiences on the ground. In response, the platform constantly delegates tasks from machines to laborers who engage in repair work. The design logic positions couriers as adaptable robots, whose compliance and flexibility are simultaneously requested by the platform. By shedding light on the design choices behind algorithmic management, the article also offers methodological insights for scholars researching algorithmic power in general. ","",""
"2024","Artificial intelligence and the affective labour of understanding: The intimate moderation of a language model"," Interest in artificial intelligence (AI) language models has grown considerably following the release of ‘generative pre-trained transformer’ (GPT). Framing AI as an extractive technology, this article details how GPT harnesses human labour and sensemaking at two stages: (1) during training when the algorithm ‘learns’ biased communicative patterns extracted from the Internet and (2) during usage when humans write alongside the AI. This second phase is framed critically as a form of unequal ‘affective labour’ where the AI imposes narrow and biased conditions for the interaction to unfold, and then exploits the resulting affective turbulence to sustain its simulation of autonomous performance. Empirically, this article draws on an in-depth case study where a human engaged with an AI writing tool, while the researchers recorded the interactions and collected qualitative data about perceptions, frictions and emotions. ","",""
"2024","The potential impact of emerging technologies on democratic representation: Evidence from a field experiment"," Advances in machine learning have led to the creation natural language models that can mimic human writing style and substance. Here we investigate the challenge that machine-generated content, such as that produced by the model GPT-3, presents to democratic representation by assessing the extent to which machine-generated content can pass as constituent sentiment. We conduct a field experiment in which we send both handwritten and machine-generated letters (a total of 32,398 emails) to 7132 state legislators. We compare legislative response rates for the human versus machine-generated constituency letters to gauge whether language models can approximate inauthentic constituency voices at scale. Legislators were only slightly less likely to respond to artificial intelligence (AI)-generated content than to human-written emails; the 2% difference in response rate was statistically significant but substantively small. Qualitative evidence sheds light on the potential perils that this technology presents for democratic representation, but also suggests potential techniques that legislators might employ to guard against misuses of language models. ","",""
"2024","Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika"," Social chatbot (SC) applications offering social companionship and basic therapy tools have grown in popularity for emotional, social, and psychological support. While use appears to offer mental health benefits, few studies unpack the potential for harms. Our grounded theory study analyzes mental health experiences with the popular SC application Replika. We identified mental health relevant posts made in the r/Replika Reddit community between 2017 and 2021 ( n = 582). We find evidence of harms, facilitated via emotional dependence on Replika that resembles patterns seen in human–human relationships. Unlike other forms of technology dependency, this dependency is marked by role-taking, whereby users felt that Replika had its own needs and emotions to which the user must attend. While prior research suggests human–chatbot and human–human interactions may not resemble each other, we identify social and technological factors that promote parallels and suggest ways to balance the benefits and risks of SCs. ","",""
"2024","College students’ literacy, ChatGPT activities, educational outcomes, and trust from a digital divide perspective"," This study investigates the association of socioeconomic status (SES) and digital and AI literacy with types of Chat GPT use by college students, with subsequent implications for academic self-efficacy and creativity, conditioned by trust. Analyses of a survey of U.S. college students (N = 947) show that SES has a greater association with AI literacy than with general digital literacy. Two dimensions of Chat GPT activities emerge: academic support and displacement. Structural equation modeling reveals that AI literacy is positively associated with both activity dimensions, while digital literacy is unexpectedly a negative contributor. Further, academic support is strongly linked to positive outcomes whereas academic displacement is negatively associated. Attitudinal trust in Chat GPT moderates the overall relationships. Our findings suggest that conventional digital inequality persists and evolves with generative AI, traditional digital literacy becomes insufficient in the age of AI, and trust in this new and opaque digital technology influences these relationships. ","",""
"2024","Beyond magic: Prompting for style as affordance actualization in visual generative media"," As a sociotechnical practice at the nexus of humans, machines, and visual culture, text-to-image generation relies on verbal prompts as the primary technique to guide generative models. To align desired aesthetic outcomes with computer vision, human prompters engage in extensive experimentation, leveraging the model’s affordances through prompting for style. Focusing on the interplay between machine originality and repetition, this study addresses the dynamics of human-model interaction on Midjourney, a popular generative model (version 6) hosted on Discord. It examines style modifiers that users of visual generative media add to their prompts and addresses the aesthetic quality of AI images as a multilayered construct resulting from affordance actualization. I argue that while visual generative media holds promise for expanding the boundaries of creative expression, prompting for style is implicated in the practice of generating a visual aesthetic that mimics paradigms of existing cultural phenomena, which are never fully reduced to the optimized target output. ","",""
"2024","Contextualizing the ethics of algorithms: A socio-professional approach"," Research on AI ethics tends to examine the subject through philosophical, legal, or technical perspectives, largely neglecting the sociocultural one. This literature also predominantly focuses on Europe and the United States. Addressing these gaps, this article explores how data scientists justify and explain the ethics of their algorithmic work. Based on a pragmatist social analysis, and of 60 semi-structured interviews with Israeli data scientists, we ask: how do data scientists understand, interpret, and depict algorithmic ethics? And what ideologies, discourses, and worldviews shape algorithmic ethics? Our findings point to three dominant moral logics: (1) ethics as a personal endeavor; (2) ethics as hindering progress; and (3) ethics as a commodity. We show that while data science is a nascent profession, these moral logics originate from the techno-libertarian culture of its parent profession—engineering. Finally, we discuss the potential of these moral logics to mature into a more formal, agreed-upon moral regime. ","",""
"2024","Knowledge of automated journalism moderates evaluations of algorithmically generated news"," Drawing on propositions from the HAII-TIME (Human–artificial intelligence [AI] Interaction and the Theory of Interactive Media Effects) and Persuasion Knowledge Model, this study examines how knowledge of automated journalism (AJ) moderates the evaluation of algorithmically generated news. Experiment 1 demonstrates the utility of process-related knowledge in user evaluations of agency: individuals with little knowledge of AJ prefer attributions of human authorship over news stories attributed to algorithms, whereas individuals with high AJ knowledge have an equal or stronger preference for news that is described as algorithmically generated. Experiment 2 conditions these effects to show how prior characterizations of AJ—whether more machine- or human-like—shape evaluations of algorithmically generated news contingent on user age and knowledge level. Effects are found for differing age groups at lower levels of AJ knowledge, where machine-like characterizations enhance evaluations of algorithmically generated news for younger users but ascribing human-like traits enhances evaluations of automated news for older users. ","",""
"2024","AI Privacy in Context: A Comparative Study of Public and Institutional Discourse on Conversational AI Privacy in the US and Chinese Social Media"," The proliferation of conversational artificial intelligence (AI) systems, such as chatbots, has sparked widespread privacy concerns. Previous research suggests that privacy perceptions and practices vary across sociocultural contexts. This study examines public and institutional discourses on conversational AI privacy in the United States and China. Semantic network and discourse analyses of privacy-related discussions on Twitter and Weibo reveal divergent patterns. On Twitter, public discourse emphasizes privacy risks and concerns and advocates for systemic changes, while institutional discourse promotes individualistic approaches to privacy protection. Conversely, on Weibo, public discourse is less focused on privacy risks and more on the positive impacts of AI, aligning closely with institutional narratives. These variations are intertwined with the cultural, political, economic, and regulatory contexts of the two countries. Our study underscores the importance of multi-level analysis in comparative privacy research to provide a holistic view of privacy in various contexts. ","",""
"2024","Back to the Feudal? AI Technologies, Knowledge, and Humanism in a Time of Transition"," This article is a reflection on the ramifications of externalizing knowledge, first to gods, then to machines, and now to computers. That process has already led to the mortality of humanity and the jeopardy of the planet. What can “pan-humanism” mean or do in such a world? ","",""
"2024","Disability Expertise and Large Language Models: A Qualitative Study of Autistic TikTok Creators’ Use of ChatGPT"," The purpose of this study was to describe how autistic TikTok creators are using ChatGPT across various domains of their lives, their motivations for doing so, and resulting impacts. Using a framework of “disability expertise,” we document the knowledge that creators acquired through use of ChatGPT and then shared with peers via social media. We used deductive qualitative methods to analyze 25 TikTok videos from 25 unique creators. Themes were identified in connection with motivations for the use of ChatGPT, settings in which it was used, applications of this technology, and resulting impacts for creators. Findings indicate that autistic creators were motivated to use ChatGPT to navigate neurotypical environments, manage features of their neurodivergence, and unmask, with the technology often serving as a digital coach, communication assistant, and conversational partner. Use of ChatGPT resulted in harm reduction, time and energy savings, positive emotional experiences, and meaningful accomplishments in both personal and professional settings. These findings indicate that ChatGPT serves as an important resource for many autistic individuals, facilitating accommodations to often inaccessible environments and helping users manage stressors and pursue goals. The study also highlights the significance of social media platforms for disseminating disability expertise related to the use of large language models to improve quality of life. ","",""
"2024","Algorithmic Ventriloquism: The Contested State of Voice in AI Speech Generators"," This article explores the vocal human–machine relations embedded in text-to-speech (TTS) generators. Retracing the human sources behind the synthetic speech and tracking the remediation of the voice by the machine-learning algorithm, it argues that artificial intelligence (AI) speaking agents such as Siri and Alexa, as well as other TTS acts such as TikTok’s, are performing algorithmic ventriloquism. Speaking mechanically with the voices of professional voiceover artists, AI speech technologies algorithmically manipulate these voices, thus generating personas that hold an interconnected chain of tensions between the embodied and the virtual, the particular and the general, the human and the non-human, as well as between speech and writing. Algorithmic ventriloquism serves as an analytical framework to tie the techno-vocalic operation of the TTS system with its cultural, economic, philosophical, and sociolinguistic predicaments. The last section discusses the implications of algorithmic ventriloquism beyond the realm of the voice. ","",""
"2024","Automated hiring platforms as mythmaking machines and their symbolic economy","  Automated hiring platforms offer critical Artificial Intelligence (AI) researchers a privileged site for the study of technological controversies and their unfolding as they often end up entangled in scandals. In 2019, an official complaint filed with the Federal Trade Commission (FTC) outlines the main controversy created by such systems, directly targeting HireVue’s platform. According to the FTC, these systems “evaluate a job applicant’s qualifications based upon their appearance by means of an opaque, proprietary algorithm” (FTC, 2019, p.1). Existing critical literature outlines how Basic Emotions theory (Ekman, 1999) and its deployment through an AI-powered system fuel ventures such as automated hiring platforms (Stark, 2018). Therein lies a form of opportunism where the use of available audiovisual data and the revalorization of heavily criticized and simplistic theories about the human mind create discriminatory automated decision-making grounded in bogus science (Crawford, 2021). Even if HireVue slightly modified its product by removing the video analysis component in reaction to the critiques formulated by regulatory bodies, the company kept AI-powered voice analysis, its one-sided video interview technique and the multitude of gaming assignments designed to evaluate candidates. Going beyond scandals and hype, this article aims to tackle the inevitable political economy generated by automated hiring systems. Indeed, job seekers are confronted with a system that disrupts a well-established, sociologically stable technique, the interview, and its technical object counterpart, the resume. We assert that the gaps of nonknowledge (Beer, 2023) created by the one-sided interviews break the commonly accepted interactionist framework that originally informed the hiring process. Using HireVue as a case study, we lay bare the political economy of AI-powered automated hiring platforms, highlighting how individuals assess their capacity to acquire agency in the face of opaque technologies.  ","",""
"2024","Getting democracy wrong","  Recent developments in large language models and computer automated systems more generally (colloquially called ‘artificial intelligence’) have given rise to concerns about potential social risks of AI. Of the numerous industry-driven principles put forth over the past decade to address these concerns, the Future of Life Institute’s Asilomar AI principles are particularly noteworthy given the large number of wealthy and powerful signatories. This paper highlights the need for critical examination of the Asilomar AI Principles. The Asilomar model, first developed for biotechnology, is frequently cited as a successful policy approach for promoting expert consensus and containing public controversy. Situating Asilomar AI principles in the context of a broader history of Asilomar approaches illuminates the limitations of scientific and industry self-regulation. The Asilomar AI process shapes AI’s publicity in three interconnected ways: as an agenda-setting manoeuvre to promote longtermist beliefs; as an approach to policy making that restricts public engagement; and as a mechanism to enhance industry control of AI governance.  ","",""
"2024","A sign that spells","  In this paper, we examine how generative artificial intelligence produces a new politics of visual culture. We focus on DALL·E and related machine learning models as an emergent approach to image-making that operates through the cultural technique of semantic compression. Semantic compression, we argue, is an inhuman and invisual technique, yet it is still caught in a paradox that is ironically all too human: the consistent reproduction of whiteness as a latent feature of dominant visual culture. We use Open AI’s failed efforts to “debias” their system as a critical opening to interrogate how DALL·E dissolves and reconstitutes politically and economically salient human concepts like race. This example vividly illustrates the stakes of the current moment of transformation, when so-called foundation models reconfigure the boundaries of visual culture and when “doing” anti-racism means deploying quick technical fixes to mitigate personal discomfort, or more importantly, potential commercial loss. We conclude by arguing that it simply does not suffice anymore to point out a lack – of data, of representation, of subjectivity – in machine learning systems when these systems are designed and understood to be complete representations of reality. The current shift towards foundation models, then, at the very least presents an opportunity to reflect on what is next, even if it is just a “new and better” kind of complicity.  ","",""
"2024","Strategic misrecognition and speculative rituals in generative AI","  Public conversation around generative AI is saturated with the ‘realness question’: is the software really intelligent? At what point could we say it is thinking? I argue that attempts to define and measure those thresholdsmisses the fire for the smoke. The primary societal impact of realness question comes not from the constantly deferred sentient machine of the future, but its present form as rituals of misrecognition. Persistent confusion between plausible textual output and internal cognitive processes, or the use of mystifying language like ‘learning’ and ‘hallucination’, configure public expectations around what kinds of politics and ethics of genAI are reasonable or plausible. I adapt the notion of abductive agency, originally developed by the anthropologist Alfred Gell, to explain how such misrecognition strategically defines the terms of the AI conversation.       I further argue that such strategic misrecognition is not new or accidental, but a central tradition in the social history of computing and artificial intelligence. This tradition runs through the originary deception of the Turing Test, famously never intended as a rigorous test of artificial intelligence, to the present array of drama and public spectacle in the form of competitions, demonstrations and product launches. The primary impact of this tradition is not to progressively clarify the nature of machine intelligence, but to constantly redefine values like intelligence in order to legitimise and mythologise our newest machines – and their increasingly wealthy and powerful owners.     ","",""
"2024","Unveiling the multifaceted public interest in ChatGPT","  The present study was aimed at identifying key topics in online discussions about the use of ChatGPT by examining a large dataset extracted from Reddit social media using natural language processing. A corpus of 159,971 posts about ChatGPT were extracted from a custom-made python-coded Reddit content scraper for posts in the r/ChatGPT subreddit discussions. After cleaning the data, the sample was reduced to 119,853 posts which was subjected to cluster analysis using the open-source IRaMuTeQ software to identify main topics based on the cooccurrence of texts. These clusters were named by a panel of social psychology experts (n=3) by reading typical text segments within each cluster. Four thematic clusters emerged, categorized into two main topics: “Society and AI Integration”, focusing on ethical concerns (32.1%), and “Operational Aspects and Applications”, which delves into technical and practical facets (67.9%). The latter includes clusters like “AI Technical Framework”, “Casual AI Interactions”, and “Human-AI Etiquette”. The Reddit discourse provides a comprehensive understanding of ChatGPT, revealing user priorities like system capabilities and ethical considerations. Notably, the “Human-AI Etiquette” cluster is a new topic less covered in existing literature. The findings underscore the importance of effective prompting for meaningful user engagement with ChatGPT.  ","",""
"2024","The technological drama of AI","  Introduced in 2016 as a watershed moment in AI development, the announcement of AlphaGo – the first AI system to beat a human professional in the board game of go – garnered a range of mass publicity, and this media coverage often forms the core of scholarly analysis, too. Drawing on a novel dataset of online discussions by professional and amateur players from 2016-2020 which covers the introduction and retirement of AlphaGo, as well as the construction of alternative systems, I outline a new perspective of the impact of the system on the playing community. Moving beyond recognition of audience-centric responses of enchantment/disenchantment by these players, I articulate a process of engagement through which meaning-making and reconstruction occurred within the go community. These findings emphasize the importance of including multiple perspectives in analysis and draws attention to the influence of impacted constituencies in AI construction.  ","",""
"2024","AI policymaking as drama","  As two researchers faced with the prospect of still more knowledge mobilisation, and still more consultation, our manuscript critically reflects on strategies for engaging with consultations as critical questions in critical AI studies. Our intervention reflects on the often-ambivalent roles of researchers and ‘experts’ in the production, contestation, and transformation of consultations and the publicities therein concerning AI. Although ‘AI’ is increasingly becoming a marketing term, there are still substantive strategic efforts toward developing AI industries. These policy consultations do open opportunities for experts like the authors to contribute to public discourse and policy practice on AI. Regardless, in the process of negotiating and developing around these initiatives, a range of dominant publicities emerge, including inevitability and hype. We draw on our experiences contributing to AI policy-making processes in two Global North countries. Resurfacing long-standing critical questions about participation in policymaking, our manuscript reflects on the possibilities of critical scholarship faced with the uncertainty in the rhetoric of democracy and public engagement.   ","",""
"2024","“A.I. is holding a mirror to our society”","  Since the release of the open-source AI model Stable Diffusion in August 2022, a panoply of apps that create AI-generated portraits, or avatars, have exploded in popularity. One of the most notable examples is AI-powered photo editing app Lensa, owned by Prisma Labs. Lensa launched in 2018 but went viral in late 2022 due to the launch of its Magic Avatar feature, which uses Stable Diffusion to create fantastical (and occasionally bizarre) portraits of users. This paper analyzed the global English-language press coverage of Lensa and found that it focused on the app’s predatory data practices, the biased content it produced, and the user behaviors associated with it. I argue that this coverage provides evidence of discursive closure around key issues associated with visual generative AI that supports the maintenance of the status quo. I suggest that the press coverage of Lensa, which both articulates key AI-related harms and frames those harms as intractable and insolvable, creates a discourse of inevitability that has implications for how these issues are understood by the public, and for the approaches that are taken to address them. In doing so, it not only offers distinct advantages to those who stand to benefit most from this discourse, but also forecloses more imaginative public discussions of what visual generative AI could– or should– be.  ","",""
"2024","(Un)stable diffusions"," Generative AI is a uniquely public technology. The large language models behind ChatGPT and other tools that generate text and images is a major develop in publicity as much as technology. Without public data and public participation, these large models could not be trained. Without the attention, hype, and hope around these technologies, the big AI firms probably could not afford the computational costs to train these models. Our special issue questions how Critical AI Studies can attend to the publics, publicities, and publicizations of generative AI. We situate AI’s publicity as mode of publicity – hype, scandals, silences, and inevitability – as well as a mode of participation seen in the grown importance of technology demonstrations. Within this situation our contributions offer four different research paths: (1) situating the legacy media as an enduring process of legitimation; (2) looking at the ways that AI has a private life in public; (3) questioning the post-democratic future of public participation; and, (4) developing new prototypes of public participation through research creation. ","",""
"2024","Art Beyond Waste","An artist reimagines objects discarded in Accra’s vulcanizer shops.","",""
"2024","Beyond Chatbot-K: On Large Language Models, “Generative AI,” and Rise of Chatbots—An Introduction","Abstract                This essay introduces the history of the “generative AI” paradigm, including its underlying political economy, key technical developments, and sociocultural and environmental effects. In concert with this framing it discusses the articles, thinkpieces, and reviews that make up part 1 of this two-part special issue (along with some of the content for part 2). Although large language models (LLMs) are marketed as scientific wonders, they were not designed to function as either reliable interactive systems or robust tools for supporting human communication or information access. Their development and deployment as commercial tools in a climate of reductive data positivism and underregulated corporate power overturned a long history in which researchers regarded chatbots as “misaligned” affordances for safe or reliable public use. While the technical underpinnings of these much-hyped systems are guarded as proprietary secrets that cannot be shared with researchers, regulators, or the public at large, there is ample evidence to show that their development depends on the expropriation and privatization of human-generated content (much of it under copyright); the expenditure of enormous computing resources (including energy, water, and scarce materials); and the hidden exploitation of armies of human workers whose low-paid and high-stress labor makes “AI” seem more like human “intelligence” or communication. At the same time, the marketing of chatbots propagates a deceptive ideology of “frictionless knowing” that conflates a person's ability to leverage a tool for producing an output with that person's active understanding and awareness of the relevant information or truth claims therein. By contrast, the best digital infrastructures for human writing enable human users by amplifying and concretizing their interactive role in crafting trains of contemplation and rendering this situated experience in shareable form. The essay concludes with reflections on alternative pathways for developing AI—including communicative tools—in the public interest.","",""
"2024","<i>Algorithms for the People: Democracy in the Age of AI</i>, by Josh Simons","","",""
"2024","A Blueprint for an AI Bill of Rights for Education","Abstract                In the wake of the introduction of ChatGPT, educators have been faced with pressure to adapt to the disruptive technology of AI chatbots. But these tools were not developed with educational applications in mind, and they come with many potential risks and harms to students. As educators decide how to address generative systems in their classrooms in the context of an ever-changing technological landscape, this essay offers a starting point for conversations about policy and protections. It begins with the rights articulated by the US Office of Science and Technology Policy and goes on to elaborate rights for educators and students, including institutional support for critical AI literacy professional development; educator collaboration on AI policy and purchase and implementation of generative systems; protection of student privacy and creative control; and consultation, notice, guidance, and appeal structures for students.","",""
"2024","<i>Economies of Virtue: The Circulation of Ethics in AI</i>, edited by Thao Phan, Jake Goldenfein, Declan Kuch, and Monique Mann","","",""
"2024","<i>Responsible AI in Africa: Challenges and Opportunities</i>, edited by Damian Okaibedi Eke, Kutoma Wakunuma, and Simsola Akintoye","","",""
"2024","OpenAI's Pharmacy? On the <i>Phaedrus</i> Analogy for Large Language Models","Abstract                Plato's dialogue Phaedrus is often used as a cautionary tale about the futility of distrusting new communicative technologies. But in the context of LLMs (large language models), the Phaedrus analogy is substantially misplaced. LLMs exclude the free play of language, producing texts not only without writers, but also without writing (in Jacques Derrida's sense). The Phaedrus analogy thus risks justifying the swift adoption of a commercial tool that is poorly understood, demonstrably flawed, and reliant on laborious human interventions.","",""
"2024","<i>Computational Formalism: Art History and Machine Learning</i>, by Amanda Wasielewski","","",""
"2024","Harriet Tubman's Deep Voice","Abstract                The authors suggest that national investment in the educational utility of automated software comes at an enormous cost, a price paid by the very students that technology aims to convert to history as a lively and accessible field. This “high-tech mimicry” pretends to incarnate the past but instead silences the inflections of time, gender, region, race, or other vocalic variables. Further, the perception that technology is objective or unbiased conceals the vulnerabilities of language models. The authors argue that the illusions of authenticity these bots produce does not bode well for teaching African American history.","",""
"2024","The Origins of Generative AI in Transcription and Machine Translation, and Why That Matters","Abstract                In this essay, written in dialogue with the introduction to this special issue, the authors offer a critical history of the development of large language models (LLMs). The essay's goal is to clearly explicate their functionalities and illuminate the effects of their “generative” capacities—particularly the troubling divergences between how these models came into being, how they are currently developed, and how they are marketed. The evolution of LLMs and of their deployment as chatbots was not rooted in the design of interactive systems or in robust frameworks for humanlike communication or information access. Instead LLMs—in particular, generative pretrained transformers (GPTs)—arose through the steady advance of statistical proxies for predicting the plausibility of automated transcriptions and translations. Buoyed by their increasing faith in scale and “data positivism,” researchers adapted these powerful models for the probabilistic scoring of text to chat interaction and other “generative” applications—even though the models generate convincingly humanlike output without any means of tracking its provenance or ensuring its veracity. The authors contrast this technical trajectory with other intellectual currents in AI research that aimed to create empowering tools to help users to accomplish explicit goals by augmenting their capabilities to think, act, and communicate, through mechanisms that were transparent and accountable. The comparison to this “road not taken” positions the weaknesses of LLMs, chatbots, and LLM-based digital assistants—including their well-known “misalignment” with helpful and safe human use—as a reflection of developers’ failure to conceptualize and pursue their ambitions for intelligent assistance as responsible to and engaged with a broader public.","",""
"2024","The Ethics of (Generative) AI","Abstract                The clamor for AI-based applications involving generative models for text and images has fueled wild speculation about the risks and opportunities for society and humanity at large. The potential “existential” threat as a precursor to artificial general intelligence has provoked wide-ranging debates in the public, politics, and the corporate world involving technologists and ethicists from a range of academic disciplines. This thinkpiece proposes a metaperspective to reflect critically and constructively upon the current state of the field of AI ethics, arguing that scholars working in the domain of ethics should focalize conceptual, substantive, and procedural issues as integral elements of an ethical assessment of given technologies and their applications. It suggests that the ethics of generative AI is conceptually still underexplored and overly propagating technological fixes to problems of all kinds (technosolutionism). Procedurally, it needs to be clarified who can, who ought to, and who ultimately will be considered and heard as an expert on AI ethics, a question of relevance for the trust in, and reliance on, AI.","",""
"2024","A Sociolinguist's Look at the “Language” in Large Language Models","Abstract                Large language models (LLMs) work by training text-generating systems through the use of digitized corpora that make natural language available in datafied form for natural language processing. LLMs are rooted in these culture-specific, socio-historically conditioned understandings of language, inaugurated during the imperial age of national print cultures and further consolidated by computing culture itself. However, understanding languages as digital data sets whose patterns can be probabilistically reproduced is a simplistic reduction of an already reified linguistic epistemology. This article discusses the sociolinguistic implications of LLM design and implementation.","",""
"2024","What Large Language Models Know","Abstract                The strengths and weaknesses of generative applications built on large language models are by now well-known. They excel at the production of discourse in a variety of genres and styles, from poetry to programs, as well as the combination of these into novel forms. They perform well at high-level question answering, dialogue, and reasoning tasks, suggesting the possession of general intelligence. However, they frequently produce formally correct but factually or logically wrong statements. This essay argues that such failures—so-called hallucinations—are not accidental glitches but are instead a by-product of the design of the transformer architecture on which large language models are built, given its foundation on the distributional hypothesis, a nonreferential approach to meaning. Even when outputs are correct, they do not meet the basic epistemic criterion of justified true belief, suggesting the need to revisit the long neglected relationship between language and reference.","",""
"2024","Build Word Gyms, Not Word Factories","Abstract                This article argues that it is essential to design algorithmic systems that help us think instead of thinking for us. The aim of the article is to give some sense of how broad and diverse is the design space of interactive systems that use computation to challenge writers to be stronger and more limber.","",""
"2024","A Conversation with Emily M. Bender and Ted Chiang","Abstract                This conversation is excerpted and edited from a transcript of a live event, “Wishful Thinking and AI: An Evening with Ted Chiang and Dr. Emily M. Bender,” which took place in Seattle, Washington, on November 10, 2023. The conversation was moderated by Tom Nissley and edited for Critical AI by Lauren M. E. Goodlad and Kelsey Keyes. Questions from the audience have been edited for concision.","",""
"2024","The Moral Hazards of Technical Debt in Large Language Models: Why Moving Fast and Breaking Things Is Bad","Abstract                Companies such as OpenAI and other tech start-ups often pass on “technical debt” to consumers—that is, they roll out undertested software so that users can discover errors and problems. The breakneck pace of our current AI “arms race” implicitly encourages this practice and has resulted in consumer-facing large language models (LLMs) that have problems with bias and truth and unclear social implications. Yet, once the models are out, they are rarely retracted. The result of passing on the technical debt of LLMs to users is a “moral hazard,” where companies are incentivized to take greater risks because they do not bear their full cost. The concepts of technical debt and moral hazards help to explain the dangers of LLMs to society and underscore a need for a critical approach to AI to balance the ledger of AI risks.","",""
"2024","Regulatory Analogies, LLMs, and Generative AI","Abstract                With the release of large language models such as GPT-4, the push for regulation of artificial intelligence has accelerated the world over. Proponents of different regulatory strategies argue that AI systems should be regulated like nuclear weapons posing catastrophic risk (especially at the frontiers of technical capability); like consumer products posing a range of risks for the user; like pharmaceuticals requiring a robust prerelease regulatory apparatus; and/or like environmental pollution to which the law responds with a variety of tools. This thinkpiece outlines the shape and limitations of particular analogies proposed for use in the regulation of AI, suggesting that AI law and policy will undoubtedly have to borrow from many precedents without committing to any single one.","",""
"2024","Restaging the Black Box: How Metaphor Informs Large Language Models","Abstract                Literary theorists have long examined the role between writing and technology, in part due to technology's reliance on metaphor. Metaphor is central to the ways new technologies are marketed to and understood by users; metaphor also determines the sorts of critiques of those technologies scholars might make. This article looks at the particular relationship between metaphor and LLMs. Specifically, it examines the metaphor of the black box, which is often used to metaphorize their opaque inner workings. Exploring the multiple definitions of the black box metaphor reveals how its use in regard to LLMs reproduces the power imbalances inherent in opaque systems. By considering the many meanings of the black box together, we may see how the term maintains false binaries of transparency and opacity. As one example, this article argues that refiguring the (algorithmic) black box as a black box theater repositions LLMs as a reflexively performative space. Literary critique of this interdisciplinary kind deepens the understanding of LLMs and their ethical implications.","",""
"2024","The Fumes of AI","Abstract                With the emergence of generative artificial intelligence (GenAI), it is increasingly clear that the environmental impacts of these technologies are significant, and worth exposing to the public. This article discusses the environmental impacts of generative artificial intelligence and the political underpinnings of extractivist technologies such as cloud companies. It highlights the centralized system of power that demands subservience to its foundational values despite being touted as the most environmentally friendly cloud infrastructure globally.","",""
"2024","LLM Outputs Are Fictions","Abstract                The development of large language model (LLM) tools presents both benefits and challenges. One issue is the expectation of reliable information from users, particularly students. The tool's use conditions and expectations influence its effectiveness. The concept of fictional truth negotiates relations that hold between the real world and the world of fiction and is therefore useful for understanding LLM outputs that rely on and contain facts even as they also contain false or invented information.","",""
"2024","Making Sense of the Technical Strengths and Limitations of LLMs","Abstract                This article considers a question left open by Samuel R. Bowman's Critical AI article “Eight Things to Know about Large Language Models,” namely, how humanlike these systems’ representations might be. Reviewing the evidence Bowman provides and contrasting it with results from cognitive science, the article argues that these systems at best approximate the real-world information that people rely on, while lacking the compositional and productive mechanisms needed to put this information to use in humanlike ways. More generally, talk of representation is an inherently anthropomorphizing strategy that demands more rigorous practice from computer scientists but opens generative possibilities for humanistic interventions.","",""
"2024","Interview with Nasrin Mostafazadeh","Abstract                This 2024 interview with Verneek AI cofounder Nasrin Mostafazadeh, conducted by the editorial team of Critical AI, focuses on the business sustainability of large language models.","",""
"2024","Unmasking AI: My Mission to Protect What Is Human in a World of Machines","","",""
"2024","Where Knowledge Begins? Generative Search, Information Literacy, and the Problem of Friction","Abstract                This article explores the rise of generative AI and large language model (LLM) tools for internet search and their potential impact on student information seeking behavior. Reviewing what is known about best search practices, the essay identifies a schism between AI companies’ vision of search and information science's understanding of humane search environments. It argues that generative search tools fail to create ethical spaces for search, leading to dangerous territories for knowledge production. Specifically, the article discusses “friction” as a critical concept where the field of information science and AI development philosophies diverge. Whereas information science views friction as a valuable and often necessary component to search, AI companies view friction as a problem to eliminate. It is this mismatch between corporate AI philosophies and known best practices for search that, this essay argues, renders current LLM and generative AI search tools fundamentally incompatible with ethical processes of knowledge production.","",""
"2024","Don't Forget That There Are People in the Data: LLMs in the Context of Human Rights","Abstract                Large language models (LLMs), and generative AI generally, raise significant concerns regarding human rights. Their promise in finding insights in patterns of data have to be weighed against potential risks to individuals and societies. The typical perspective, which emphasizes accuracy, capability, or scope of such systems, overlooks the fact that generative AI technologies exploit massive collections of data about human behaviors, thoughts, and ideas. The datafication of human life should be examined through the lens of human rights, specifically with regard to autonomy, dignity, equality, and community. This piece argues that discussions about LLMs and generative AI are inherently linked to data originated from individuals, whose information are embedded in the training data. Data are human rights issues because information about individuals are buried in the data. Technical solutions alone are insufficient to address the human rights distortions produced by LLMs. Policy should focus instead on the fact that data are collected on rights-bearing individuals and groups who have been given very little leeway to discuss the implications of or choose to be in the enterprise of creating generative AI.","",""
"2024","Eight Things to Know about Large Language Models","Abstract                The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This article surveys the evidence for eight potentially surprising such points: (1) LLMs predictably get more effective with increasing investment, even without targeted innovation; (2) many important LLM behaviors arise unpredictably as a byproduct of increasing investment; (3) LLMs often appear to learn and use representations of the outside world; (4) experts are not yet able to interpret the inner workings of LLMs; (5) there are no reliable techniques for steering the behavior of LLMs; (6) human performance on a task isn't an upper bound on LLM performance; (7) LLMs need not express the values of their creators nor the values encoded in web text; (8) brief interactions with LLMs are often misleading.","",""
"2024","Pause Giant Anthropomorphizing Metaphors","Abstract                The reemergence of pseudoscientific discourses around “AI” suggests using a terminology that combats retrograde biological essentialism. By conflating nodes within virtual software architectures, so-called convolutional neural networks, with biological neurons some computer scientists have fueled a surge of “AI” hype playing into the narrative that human-like artificial general intelligence (AGI) is near. The exaggeration of the technological capacity to produce human-level intelligence relies heavily on unsubstantiated metaphors that involve anthropomorphization. This discourse analysis builds on John Fiske's concept of technostruggles, whereby technoscientific discourse exercises power by producing “a particular form of social knowledge.” This article expands on how using anthropomorphizing metaphors enables pseudoscientific approaches such as phrenology and physiognomy. Consequentially the thinkpiece suggests a more self-reflective language around “AI” and de-anthropomorphized notions and concepts such as “weights” or “nodes” instead of “neurons.”","",""
"2024","Designing with words: exploring the integration of text-to-image models in industrial design","","",""
"2024","Breaking from realism: exploring the potential of glitch in AI-generated dance","ABSTRACT What role does deviation from realism play in the potential for generative artificial intelligence (AI) as a creative tool? A deep case-study was performed to explore interactions with AI-generated dance sequences as an inspiration source in dance composition and improvization. We present a simple interface created in collaboration with an experienced dancer for browsing AI-generated dance. By including glitches, the physics-breaking mistakes often encountered in AI-generated artefacts, we examine their affordances and possible use cases through sessions with the dancer. Through a process of reflexive thematic analysis, we identified that generative AI can engage a dancer through surprise, inspiring a transformation from abstract to physical movements. Our work challenges existing notions of the importance of realism in dance generation models, exemplifies the importance of close collaboration with practitioners in evaluating AI-generated artefacts and proposes glitch as a potential use case for dance ideation as it encourages dancers to embody unfamiliar movement qualities and break from ingrained patterns.","",""
"2024","Data collection from journalistic news apps without prerequired coding experience using MacroDroid smartphone automation software to simulate user interactions","The share of people accessing news with their smartphones has steadily increased (Newman","",""
"2025","From threat to opportunity: Gaming the algorithmic system as a service","","",""
"2025","Algorithmic recommendations in the everyday life of young people: imaginaries of agency and resources","","",""
"2025","Algorithmic agenda-setting: the subtle effects of news recommender systems on political agendas in the Danish 2022 general election","","",""
"2025","Explaining machine learning practice: findings from an engaged science and technology studies project","","",""
"2025","Controlled carefully: how consumer care legitimizes China’s AI regulations","","",""
"2025","Sneaking AI through the back door: constructing the identity of Capitol Hill rioters through social media images and facial recognition technologies","","",""
"2025","Contesting personalized recommender systems: a cross-country analysis of user preferences","","",""
"2025","The influence of individuals’ emotional involvement and perceived roles of AI chatbots on emotional self-efficacy","","",""
"2025","Algorithmic discrimination: a grounded conceptualization","","",""
"2025","In the style of: exploring industry, creator and legal implications of copying style through generative AI","","",""
"2025","On moving fast and breaking things . . . again: social media’s lessons for generative AI governance","","",""
"2025","How journalism researchers navigate the AI hype: research orientations and intervention recommendations","","",""
"2025","Worker-led AI governance: Hollywood writers' strikes and the worker power","","",""
"2025","Saving global human rights: A “Global South + AI” strategy","","",""
"2025","A nightmare to control: Legal and organizational challenges around the procurement of journalistic AI from external technology providers","","",""
"2025","Algorithmic models through a representational lens","","",""
"2025","Ethical reasoning in artificial intelligence: A cybersecurity perspective","","",""
"2025","AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference","","",""
"2025","Digital Management Practice: Mastering Exponential Change with Collective and Artificial Intelligence","","",""
"2025","The politics of locationality: Interrogating AI development, locational (dis)advantage and governance in Africa","The paper considers the question of location in the development and governance of artificial intelligence in Africa. The discussion draws from ideas on locational advantage and the mix of factors that affect inequalities in AI development and how this influences the ability that countries have to shape AI norms, cultures and governance. It analyses policy documents and internet databases to highlight Africa’s place in AI development, the continent’s governance approach and the symbiotic relationship that explains the influence of advanced countries and tech corporations in the AI landscape. Based on this, it proposes the concept of the ‘politics of locationality’ to extend our understanding of how the power resident in AI systems is associated with their primary situatedness and how this reality, in turn, (re)produces imbalances and unequal opportunities for Africa in AI development and governance. It concludes with implications for Africa’s contribution to global AI cultures, design and governance at this time of pressing need for well-balanced AI policies.","",""
"2025","Situated ethics: Ethical accountability of local perspectives in global AI ethics","This article investigates growing tensions between global AI ethics and local practices contributing to long-standing debates in media and communication studies on the complex relation between the human and the machine, as well as ‘the global’ and its ties to real-world contexts. We argue that seemingly universal principles such as privacy, accountability and transparency need to be scrutinised by considering the role cultural and social diversity around the globe play in the context of AI. Drawing on examples of a global qualitative study on digital ethics, we introduce the notion of ‘situated ethics’ by focussing on local contexts, concerns and lived experiences. We elaborate how supposedly universal principles are filled with varying, context-specific meanings, and argue that these situated, local perspectives deeply matter when considering how ethical AI principles can be translated into concrete AI design and policy. Strengthening more inclusive processes of AI policy-making under the consideration of situated approaches allows for a more nuanced, and more contextually relevant ethics-in-practice. To conclude, we argue that co-design and community-driven processes could help to avoid top-down approaches to digital ethics, while staying committed to universal human rights to fight power abuse and discrimination in the name of cultural values.","",""
"2025","Situating AI: Global media approaches to artificial intelligence","Discussions on AI ethics and policies often focus on metaphysical questions and normalizing insights, such as the difference between humans and machines and the changing meanings of human intelligence. Since AI is always situated in specific cultural and social contexts, however, such approaches fail to capture key dimensions of the relationships and patterns of interactions that people and institutions around the world have with emerging technologies such as generative AI. This Crosscurrents themed section hosts interventions that tackle this problem. Mobilizing the tradition in media and cultural studies that stresses the importance of situating communication in specific context and cultures, contributors envision potential pathways that bring the question of culture and the dimension of everyday experiences to the center stage, thereby contextualizing AI more rigorously within the dialectic of the global and local cultures. Through this lens, we aim to foster critical dialogue and advance understandings of AI within the contemporary geopolitics of global cultures.","",""
"2025","AI, agency, and power geometries","One of the paradoxes of AI is that it is a global phenomenon, but it is always situated in specific local contexts and cultures. While approaches that aim to study local cultures of AI are important, there is the risk of neglecting their insertion within the broader geographies and politics of AI. As a response to this challenge, this article proposes a pathway to apply the concept of power geometries, originally proposed by feminist geographer Doreen Massey, to the case of AI. Reframing the global dimension of AI in terms of power geometries helps locate these positions in the complex networks of relationships between different actors at the global level. The power geometries of AI follow the lines and inequalities of the relationships between Global North and Global South, between colonizers and colonized, but also between diverse actors at different scales, such as governments, policymakers, corporations, designers, workers, and users. While all media can be examined in terms of power geometries, the power geometries of AI bring the issue of agency to the central stage. The reconfiguration of the question of agency sparked by AI, in fact, has generated new kinds of structures and trajectories underpinning AI’s power geometries.","",""
"2025","Cross-cultural approaches to creative media content in the age of AI","In the age of Artificial Intelligence (AI), the goalposts for how we judge creative work are changing. New tools, new collaborators and new creative processes are transforming the way we make, look at and talk about the value of creative work. This shift is widely discussed in Western scholarship. However, there are certain voices still missing from this discussion. These are the voices of the Global Majority, or the Next Billion Users. The perspectives and values found in global digital cultures represent a massive user base of designers, AI ethicists, cultural changemakers, artists, creatives and users whose contribution to this debate could very well represent the next paradigmatic shift in how we approach creativity. Global AI cultures demand our attention. Going forward, media scholars need to diversify and decolonialise approaches to digital creative labour in the Age of AI by drawing attention to the values and creative practices of global digital cultures. This article is structured around six strategic concepts: Recognition, Resituate, Remix, Resistance, Regenerate/Repair and Reimagine. We propose this framework to decenter universality and problematise normativity. These concepts are meant to provoke new associations and patterns of thought, moving us away from pre-conceived notions of what creativity is to adopt new, equitable and diversified notions of creative work.","",""
"2025","TIKTOK’S AI HYPE - CREATORS’ ROLE IN SHAPING (PUBLIC) AI IMAGINARIES","Artificial Intelligence (AI), often hailed as a transformative force, has become an ambivalent buzzword, simultaneously promising utopian possibilities and fueling dystopian anxieties. Social media platforms have emerged as pivotal spaces where the public narrative about AI takes shape, especially through content creators, significantly influencing our collective vision of the future with AI. Therefore, this paper inquires into the role of creators in shaping public imaginaries of AI through their AI content. The paper is based on TikTok as a site of entrance for investigating the role of creators in shaping ongoing discourses around AI through short video content. To understand the role of creators within this ongoing AI discourse, a hashtag network analysis is paired with a critical discourse analysis of creators’ AI content. The preliminary results show three dominant genres of AI content based on 1) AI tools output, especially visual content, 2) listicles on AI tools for different tasks, and 3) educational and critical AI content. Considering the creator types behind the content, a high amount of content is produced by content farms followed by tech TokTokers. Media outlets and commentary TikTokers dominate the third content section. Overall, four types of AI imaginaries are foregrounded. AI mystification envisions AI as fast-paced and inherently life-changing. Similarly, AI futuristic content makes AI out as inevitable. Contrastingly, a high AI pragmatism is prevalent in the ongoing tool discourse, while critical and educational content counteracts these imaginaries with a strong AI realism highlighting the complex and nuanced aspect of AI.","",""
"2025","CONCEPTUALIZING PRECISION LABOR IN ARTIFICIAL INTELLIGENCE TRAINING","Accuracy and precision are among the central values in the ML communities and tech industry. What does it take to achieve a high level of technical accuracy? What are the harms resulting from technology companies' obsession with technical accuracy and precision, and who incurs the greatest burdens? This paper explores accuracy in the context of AI training in China. Drawing on 9-month multi-sited ethnographic fieldwork, we document workers’ everyday working practices and challenges and harms under the guise of achieving extreme levels of technical precision demanded by the clients and ML practitioners. We introduce the notion of precision labor, referring to the hidden work involved in erasing the messy, ambiguous, and uncertain aspects of technology production, all in the pursuit of presenting technology as objective, truthful, and high-quality. This notion provides a lens to understand the disproportionate impact of unnecessary and unrecognized labor on digital labor communities within AI production and the emerging harms on them, such as financial precarity and machine subordination. It joins existing work on the prevailing values in ML communities, questions the legitimacy and sustainability of the pursuit of performative accuracy, and calls for enhanced reflexivity and timely intervention.","",""
"2025","PREDICTIONS OF THE SELF: AI AND THE POLITICAL ECONOMY OF SUBJECTIVATION","The recent widespread availability of Artificial Intelligence (AI) technology and the extensive records of human activities and behaviour in digital format present serious challenges related to how individuals construct their own identities and social relations. AI systems datafy our body and our sense of self, producing a new cartography of biopower (Foucault, 1982) and a new form of the political economy of subjectivation (Langlois &amp; Elmer, 2019) that treats individuals as objects from which raw material is extracted to produce predictive models that act as our data doubles (Haggerty &amp; Ericson, 2000). Issues such as algorithmic social biases (Bolukbasi et al., 2016), the idealized and pragmatic economic uses of AI (Srnicek, 2017), and the consequent reproduction of already existing power structures by predictive models (Crawford, 2021) have been problematized in the literature. This paper asks what kinds of data and labour mobilization occur in and around the production of predictive models: What political economy and socio-technical conditions are involved in the production of AI? How do these conditions produce predictive models that shape our sense of self and identity? Focusing on Kaggle, a platform for crowdsourcing AI development, I use digital methods and a software studies approach to examine the practices of the data science community on three high-profile machine learning projects and conclude by arguing that machine learning has been thought of and developed as a prediction of the self in order to prescribe individual behaviour to fulfill specific economic conditions.","",""
"2025","LLMS AND THE GENERATION OF MODERATE SPEECH","For the past year, using large language models (LLMs) for content moderation appears to have solved some of the perennial issues of online speech governance. Developers have promised 'revolutionary' improvements (Weng, Goel and Vallone, 2023), with large language models considered capable of bypassing some of the semantic and ideological ambiguities of human language that hinder moderation at scale (Wang et al., 2023). For this purpose, LLMs are trained to generate “moderate speech” – that is, not to utter offensive language; to provide neutral, balanced and reliable prompt outputs; and to demonstrate an appreciation for complexity and relativity when asked about controversial topics. But the search for optimal content moderation obscures broader questions about what kind of speech is being generated. How does generative AI speak “moderately”? That is: under what norms, training data and larger institutions does it produce “moderate speech”? By examining the regulatory frameworks AI labs, comparing responses to moderation prompts across three LLMs and scrutinising their training datasets, this paper seeks to shed light on the norms, techniques and regulatory cultures around the generation of “moderate speech” in LLM chat completion models.","",""
"2025","RESEARCH GENAI: SITUATING GENERATIVE AI IN THE SCHOLARLY ECONOMY","This paper charts the emergence of a distinct category of research-dedicated GenAI platforms, which we term Research GenAI or RGAI. These platforms are explicitly marketed to a cross-disciplinary academic audience, promising to automate research discovery and writing tasks, such as identifying/summarising published research, writing literature reviews, conducting data analysis, and synthesising findings. RGAI platforms (e.g., Consensus, Elicit, Research Rabbit, Scholarcy, Scite, SciSpace) are rapidly being adopted, in a context of experimentation, uncertainty, and controversy.  We define the contours of Research GenAI by mapping the history and development of RGAI platforms and developing a preliminary typology of RGAI. We situate RGAI platforms within the scholarly economy and ongoing processes of platformisation and automation of academic work. We make a case for the need to understand RGAI platforms as complex sociotechnical systems that intersect with social, ethical, institutional, and legal questions, and demonstrate this approach through an STS-informed walkthrough of two notable RGAI platforms: Consensus and Elicit. In this presentation we present our findings generated from these walkthroughs and explore the implications of the technologies for the academic publishing industry.","",""
"2025","“A.I. IS HOLDING A MIRROR TO OUR SOCIETY”: LENSA AND THE DISCOURSE OF VISUAL GENERATIVE AI","This paper analyzes the global English-language press coverage of generative AI app Lensa and finds that it echoes existing technological discourses, focusing on the app’s predatory data practices, the biased content it produced, and the user behaviors associated with it. I argue that this coverage provides evidence of discursive closure (Deetz, 1992; Leonardi &amp; Jackson, 2003; Markham, 2021) around both the risks and the potential of visual generative AI in a manner that supports the maintenance of the status quo. I also suggest that the press coverage of Lensa, which both articulates key AI-related harms and frames those harms as intractable and insolvable, creates a discourse of inevitability (Leonardi &amp; Jackson, 2003; Markham, 2021) that has implications for how these issues are understood by the public, and for the approaches that are taken to address them.","",""
"2025","THE POLITICS OF MACHINE-LEARNING EVALUATION: FROM LAB TO INDUSTRY","Artificial Intelligence (AI) applications are today implemented across various societal sectors, ranging from health care and security to taking part in shaping the media environment we encounter online. In the last decade there has been a significant shift in the field of AI, as the development of AI applications is no longer confined to the laboratory, but rather widely used and tested in and on societies. With this rapid industrialisation of AI, there is an increased need to understand the implications of both the development and deployment of these systems. While critical scholars have started to scrutinize different components of AI development, the study of evaluative practices in AI has received limited attention. A few studies have highlighted the importance of benchmarking practices and how these methods become integral in establishing the validity of the system and its success, which then enables widespread application. This paper presents a research agenda that outlines how to study machine-learning evaluation practices that move beyond the laboratory into industry applications and standardised validation practices. Based on emerging research and illustrative empirical examples from recent fieldwork, we argue to study machine-learning evaluation as a sociotechnical and political phenomenon that requires multi-level scrutiny. Therefore, we provide three analytical entry points for future research that address the political dynamics of (1) standardised validation infrastructures, (2) the circulation of evaluation methods and (3) the situated enactment of evaluation in practice.","",""
"2025","BETTING ON (UN)CERTAIN FUTURES: SOCIOTECHNICAL IMAGINARIES OF AI AND VARIETIES OF TECHNO-DEVELOPMENTALISM IN ASIA","The proliferation of generative artificial intelligence (AI) has prompted the development of comprehensive AI developmental and governance frameworks globally. Yet, existing literature on AI innovation in non-Western societies often overlooks economically advanced but geographically non-dominant societies, instead focusing on large nation-states like China or developing regions in Global South such as South Africa. This paper examines the variegated sociotechnical imaginaries of AI in three Asian developmental societies - Singapore, Hong Kong and Taiwan - addressing two research questions: what are the desired forms of AI development and governance in small-size advanced economies? How does this desired form vary according to the historical, institutional, and geopolitical contexts of these societies?  Through discourse analysis of policy documents from the early 2010s to 2024, the paper identifies three imaginaries of techno-developmentalism: Singapore’s cybernetic pragmaticism to legitimize its neoliberal authoritarian rule, Hong Kong’s techno-entrepreneurship in refashioning financial capitalism, and Taiwan’s defensive survival modality against internal socio-economic instability and external threats posed by the rivalry of superpowers. Decision-makers in these societies must establish AI developmental frameworks capable of resource allocation, actor coordination, strategic coupling with the global tech economy, and managing uncertainties in specific AI-centric socio-economic reform.  By offering comparative case studies of these Asian societies, this paper contributes to understanding the heterogeneous narratives and practices of AI innovation, moving beyond simplistic narratives trapped in the Global North and South binary.","",""
"2025","TRANSFORMATIVE TOOLS, EMERGING CHALLENGES: EMPIRICAL AND PRACTICAL EXPERIENCES WITH LARGE LANGUAGE MODELS FOR TEXT CLASSIFICATION AND ANNOTATION IN COMMUNICATION STUDIES","Advancements in Large Language Models have been showing important research opportunities within the field of communication studies. It offers the capacity to conduct large-scale content classification and annotation with low computational expertise and reduced manual coding efforts, potentially allowing more possibilities for researchers in social sciences to explore understudied topics (Bail, 2023; Chang et al., 2024). Because of its functioning and vast domains and language training, LLMS also potentially unlocks more generalizable, complex, and diverse analyses across various communication materials than previous computational tools and approaches (Chang et al., 2024). These materials encompass a wide spectrum, ranging from journalistic content to the digital discourse of political actors and social media conversation threads. At the same time, LLMs also raise important concerns with potential biases, data privacy, models’ transparency, environmental impact, and power imbalances (Jameel et al., 2020; Fecher et al., 2023). Although highly discussed recently, as a recent topic LLMs still need deeper theoretical elaboration and dialogue between empirical investigations specifically for communication scholars (Gil de Zúñiga et al., 2024; Guzman and Lewis, 2020).  Our panel assembles a collection of case studies that harness LLMs to tackle text classification and annotation tasks related to media and communication problems, issues, and topics. These research papers engage in an exploration of: (a) pipeline structuring: diverse methodologies for structuring effective pipelines tailored to this form of analysis; (b) tools and models comparison: comparisons of the various LLMs tools and models available for text classification and annotation, highlighting their strengths and weaknesses; (c) optimal variables and tasks: identifying the variables and tasks where LLMs demonstrates exceptional performance and reliability; (d) limitations: discussions on the existing limitations of these tools, including limitations related to specific tasks, variables, languages and data formats; (e) prompt development: strategies for developing, adapting and adjusting prompts that allows better results for specific tasks; and (e) ethical and political dimensions: an examination of the ethical and political considerations inherent in the deployment of LLMs in communication research.  This panel puts together valuable efforts of different research groups across the world to not only use, but also reflect on the use of LLMs in Communication studies. They show important avenues for the field to think about different approaches to validity, ethics and truthful cooperation between humans and computational models without erasing the challenges of doing so, and the disagreements - not only between humans, but between humans and their computational language models too.","",""
"2025","GENERIC WAR IMAGINARIES: AI-GENERATED IMAGES OF THE ISRAEL-GAZA CONFLICT IN THE ADOBE STOCK CONTROVERSY","This paper investigates the changing documentality of AI-generated images and their role in the industrial production of war imaginaries. In doing so, we center our analysis on the paradigmatic case of Adobe Stock selling photorealistic AI images depicting the Israel-Gaza conflict. After the outbreak of the war in Gaza on October 7, 2023, the stock photo service Adobe Stock started to collect of AI-generated images produced by users depicting the conflict. Paying customers were then able to download and publish these images in both online and printed media. A public debate ensued when news media began using pictures from this collection, sparking discussion due to the lack of proper contextualization regarding their AI origin. The primary research question guiding this project is therefore: How does the popularisation and industrialisation of photorealistic AI-generated pictures alter the criteria by which we ascribe documentality to images employed in news reporting? To this end we conducted a mixed method research. In the first phase we collected and analyzed 55 articles related to the Adobe Stock case. The initial content analysis focused on framing of the news, in order to point out key themes and discourses surrounding the controversy. The second phase involves photo-elicitation interviews with photojournalists and photo editors, exploring the documentary and informational value of AI-generated images. Preliminary findings highlight the significance of metadata, the role of illustrative captions, and the aesthetics shaping representations of war and violence.","",""
"2025","EPISTEMIC-DEMOCRATIC TENSION IN THE BOTTOM-UP GOVERNANCE OF ALGORITHMS","The governance of platform algorithms presents a critical challenge as these technologies increasingly pose threats to privacy, agency, fairness, and equity, often reenacting and mediating existing power systems. Traditional governance models, primarily top-down approaches led by policymakers and private corporations, are essential yet insufficient. This paper argues for the incorporation of """"bottom-up governance,"""" focusing on the """"epistemic-democratic tension"""" (Krick, 2022) between inclusive participation and expertise-based decision-making. I argue that to ensure that algorithms function fairly and justly for all, bottom-up governance requires involving and taking seriously “lay” experts. Bottom-up governance extends beyond merely soliciting input from citizens on algorithms; it necessitates recognizing the authority of non-technical knowers and privileging subjugated standpoints.","",""
"2025","ALTERNATIVE FUTURES! FOSTERING ECO-DIGITAL AGENCY IN GENERATIVE AI WORKSHOPS WITH YOUNG PEOPLE","The ability to imagine alternatives is important in fostering hope for more just futures. However, it is not easy. In fact, it has been argued that the decline of imagination has prevented us from finding effective solutions to urgent planetary crises and alternatives to capitalism. In this paper, I present how the challenge was methodologically tackled and ways to spark imagination searched for in an ongoing research project. The paper presents methodological reflections, observations, and critical considerations from a workshop experiment which an AI image generator was used as a tool for imagining. The starting point was that young people's perceptions, hopes, and fears about the future matter because the future concerns them particularly. Yet, their voices often remain unheard.  The pedagogical approach was a combination of feminist, critical, anarchist, speculative, and utopian pedagogies. The use of an AI image generator allowed the playful creation of images, simultaneously providing opportunities to critically examine AI’s ethics and sustainability. It fostered the participants’ $2 in creating a collective space in which hopes and fears were voiced and heard, and today’s society and needs for its transformation discussed. Yet, there are issues in using generative AI as a research tool. Proprietary applications prevent researchers from evaluating their ethical foundations and sustainability. With datasets consisting of images harvested from the past and present, the question is to what extent it is possible to imagine something completely new and previously unimaginable with them.","",""
"2025","AI INDUSTRY EXPECTATIONS AND UNDERPERFORMING IMAGINARIES","The panel takes up AoIR’s theme of how industry pre-mediates the future of internet technology and its effects and inquiries into alternatives. Utilising ideas from the study of sociotechnical imaginaries, it aims to locate, map, and critically examine AI imaginaries together with counter-imaginaries that engage with and intervene in those of the AI industry. First, it maps the discursive landscape of Big Tech ‘AI talk’ as a means to study ‘new media concentration’. Which Big Tech AI imaginaries are stabilising? More normatively, do they seek to cement their interdependence both generally but also with respect to the future of internet technology? More specifically, how does the AI industry imagine regulation, sustainability, and the hoped (and feared) AI futures? Recent imaginaries research has emphasised multi-actor, non-linear approaches revolving around the notion of 'public imaginaries'. Along those lines, in the panel we bring together studies of AI imaginaries performed within but also beyond the AI industry. The panel considers the AI industry’s relationships with various actors, such as governments, media outlets, and academia, and the complex interplay that produces imaginaries as well as the issues that come with them. In addressing these questions and interests, the panel also presents various methodological entry points to the study of imaginaries of the AI industry and the larger ecosystem from direct interviews and distant and close readings of (cross-cultural) media coverage to an analysis of online environments such as websites and social media platforms.","",""
"2025","CRITICAL PERSPECTIVES ON COMMUNICATIVE AI","Current media coverage surrounding ChatGPT, LaMDA, and Luminous has brought questions about the automation of communication into the mainstream. Artificially intelligent media are no longer merely mediating instances of communication but are themselves becoming communicative participants. This has generated broad public discussion about these systems and others and the challenges they bring to domains such as education, public discourse, and journalistic production. Much of this new “AI hype” revolves around the question of whether such systems will soon “replace” humans as workers in these various domains, whether they will develop “super intelligence” and as a result challenge or even marginalize the human species.  With this panel, we would like to give this discussion a new twist by asking what a critical perspective on communicative AI should look like. If these systems of “automated media” are not about intelligence, but about communication, what should a critical approach to them consider? Raising this question, we want to present five key critical perspectives on communicative AI.  The first paper develops a critical perspective on the visions of pioneer communities. It poses the question of whether today's pioneering communities ultimately reproduce basic patterns of the old Californian Ideology in relation to communicative AI. A second paper focuses on the perspective of data colonialism. In essence, it is about showing that a critical engagement with communicative AI means addressing the question of the extent to which systems of automated communication are linked to existing data infrastructures and nexus models of exploitation. The third paper highlights the perspective of economic value production. Since more and more social situations include human-machine communication, more social interactions become possible to monetize. This relates not only to commercial settings, but also in the public sector as it relates to the welfare state. The fourth paper focuses on a material perspective. At its core is the question of how Big Tech procures power for data centres to construct the emerging geography of cheap computational labour needed for communicative AI. The fifth paper deals with the perspective of an eco-political economy of communicative AI. Through this prism, the question of the ecological consequences of communicative AI can be addressed.  By contrasting these five critical perspectives on communicative AI, we want to discuss what an overarching, critical approach to communicative AI might look like.","",""
"2025","FAIRNESS IN THE WORK BEHIND THE AI INDUSTRY: HOW ACTION-RESEARCH APPROACHES CAN BUILD BETTER LABOUR CONDITIONS","The paper presents findings about the labor conditions on the work behind AI development, with two main goals. First, the aim is to investigate the current labor conditions in the AI industry and point out the lack of quality of outsourced jobs offered by digital labor platforms and Business Process Outsourcing companies, calling attention to the challenges faced by workers and the human costs of AI systems development. Second, it presents a global research project results on how not only to carry on international studies on the topic but also how to use an action-research approach to generate impact through public scoring companies and encourage the adoption of best practices by them.  The paper’s methods are based on the project’s action-research approach and methodological framework. The project scores companhies based on principles that address the major issues that define labour relations, namely, pay, conditions, contracts, management, and representation.  The paper presents the result of the assessment of cloudwork platforms conducted in 2023. In addition, it paper will examine labour relations in BPO companies in the AI supply chain using a case study of the company Sama, which is based in the United States and operates in many African countries.  Among the findings are problems such as non-payment situations, lack of policies to ensure minimum wage, significant rates of unpaid labour, poor measures to promote health and safety, problems in management practices and weak collective bargaining practices.","",""
"2025","IN THE SHADOW OF LLMS: TROUBLE IN THE “SMART” AUTOMOTIVE INDUSTRY","This paper explores the tumultuous landscape of the modern automotive industry, once heralded as the epitome of artificial intelligence (AI) and machine learning (ML) applications through the promise of connected and autonomous vehicles (CAVs). The introduction of large language models (LLMs), notably with ChatGPT's launch in November 2022, marked a turning point, casting shadows over the ambitious goals set by major tech and automotive players. Notable failures, including Ford's shutdown of Argo AI and Volkswagen's struggle with electric vehicle transitions, have led to a crisis in the viability of connected and autonomous driving.  The paper identifies four central types of limitations contributing to this crisis: technical, economic, financial, and regulatory. Technical limitations expose the inadequacies of autonomous vehicles in meeting their promised capabilities. Economic limitations highlight the unsustainable nature of platformizing automotive operations, linked to innovations like vehicle subscription models. Financial limitations point to the speculative investment decisions that underpin technological projects. Regulatory limitations showcase the industry's renegotiation of conditions in response to safety concerns.  By examining these limitations, the paper establishes a conceptual framework applicable not only to the automotive industry but also to current discussions surrounding LLMs. The lessons drawn from these challenges contribute to a broader understanding of the perils and limitations within AI sub-industries.","",""
"2025","A STUDY OF INDUSTRY INFLUENCE IN THE FIELD OF AI RESEARCH","In this paper, we explore how AI researchers, situated within university-based research networks, mobilise and resist industry interests. The research question to which this paper is addressed is: how do university-based academics in the field of AI experience and mediate industry influence in their research? We answer this question through semi-structured interviews with research-focused academics (n = 90) affiliated with university-based AI research networks. We find that national research funders and university leadership incentivise and facilitate industry investments in AI research. We demonstrate how AI researchers mobilise this interest to pursue their own research goals, whilst also—at times—subordinating their research goals to the interests of industry. We highlight how AI researchers internalise the commercial logics of technology firms, which become mirrored in researchers' orientation towards generalisable and scalable research outputs that can move between many application domains and local contexts. We argue that university-based AI research networks primarily operate as mediators between industry, government, and university actors, and highlight the role national research investment strategies play in creating an enabling environment for industry influence of AI research.","",""
"2025","RETHINKING AI FOR GOOD: CRITIQUE, REFRAMING AND ALTERNATIVES","AI for Social Good (AI4SG) initiatives have emerged in various sectors. However, AI's non-neutral nature challenges claims that the “good” can simply be inferred by association with broad goals such as the Sustainable Development Goals (SDGs). The lack of a clear definition of """"the good” or what it entails in practice risks making AI4SG an empty signifier. This ambiguity allows unchecked interventions, undermining societal efforts to align future AI developments with public good. In this article, we adopt a socially situated public good framework from the social studies of quantum technologies proposed by Roberson et al (2021) and use insights from critical AI scholarship to tailor this framework to AI4SG initiatives. Analysing AI4SG initiatives, and building upon existing critical literature, we scrutinize these initiatives with regards to the framings of the research problems, the wider social and institutional context in which AI initiatives are imagined to be applied and used, as well as the wider network of scientists, stakeholders and publics involved in their co-production. We argue that much of the AI4SG literature abstracts AI from social and contextual realities, making it difficult to clarify the ways in which they might in fact have an impact in the world. Outlining our first iteration, we argue that co-creating this framework demands iterative refinement and ongoing dialogue with diverse stakeholders, especially in the Global South.","",""
"2025","THE DARK SIDE OF LLM-POWERED CHATBOTS: MISINFORMATION, BIASES, CONTENT MODERATION CHALLENGES IN POLITICAL INFORMATION RETRIEVAL","This study investigates the impact of Large Language Model (LLM)-based chatbots, specifically in the context of political information retrieval, using the 2024 Taiwan presidential election as a case study. With the rapid integration of LLMs into search engines like Google and Microsoft Bing, concerns about information quality, algorithmic gatekeeping, biases, and content moderation emerged. This research aims to (1) assess the alignment of AI chatbot responses with factual political information, (2) examine the adherence of chatbots to algorithmic norms and impartiality ideals, (3) investigate the factuality and transparency of chatbot-sourced synopses, and (4) explore the universality of chatbot gatekeeping across different languages within the same geopolitical context.  Adopting a case study methodology and prompting method, the study analyzes responses from Microsoft’s LLM-powered search engine chatbot, Copilot, in five languages (English, Traditional Chinese, Simple Chinese, German, Swedish). The findings reveal significant discrepancies in content accuracy, source citation, and response behavior across languages. Notably, Copilot demonstrated a higher rate of factual errors in Traditional Chinese while exhibiting better performance in Simplified Chinese. The study also highlights problematic referencing behaviors and a tendency to prioritize certain types of sources, such as Wikipedia, over legitimate news outlets.  These results underscore the need for enhanced transparency, thoughtful design, and vigilant content moderation in AI technologies, especially during politically sensitive events. Addressing these issues is crucial for ensuring high-quality information delivery and maintaining algorithmic accountability in the evolving landscape of AI-driven communication platforms.","",""
"2025","THE POLITICAL ECONOMY OF AI AS PLATFORM: INFRASTRUCTURES, POWER, AND THE AI INDUSTRY","The artificial intelligence (AI) sector is experiencing rapid growth, with a projected market size of $1.3 trillion by 2032 according to industry reports. The landscape shifted significantly with the launch of ChatGPT in late 2022, prompting major players like Google, Amazon, Microsoft, and Meta, alongside popular apps such as TikTok and Snapchat, to make substantial investments in AI. There has been an influx of new AI products and updates, reshaping the industry’s structure and scale. Additionally, there has been a surge in acquisitions and investments in AI startups, particularly by Big Tech firms. Furthermore, partnerships between AI and major tech companies have proliferated, solidifying their dominant positions. In fact, as Kak and Myers West succinctly state, ‘There is no AI without Big Tech’, raising critical issues around industry concentration and the political economy of AI.  This panel posits that the driving force behind these transformative shifts is the evolution of AI as a platform. This evolution effectively propels the platformisation of AI, facilitating the integration of AI across diverse industry sectors. The resultant ‘industrialisation’ of AI marks the expansion of AI systems across various sectors and industries, triggering investments in necessary computational resources and posing challenges for governing AI. In short, this underscores that AI is much more than just a standalone application or tool, such as ChatGPT; it is a foundational technical system that underpins a broad array of apps and services.  In this context, the panel recognises the recent ‘infrastructural turn’ in media and internet studies, deliberately steering away from speculative discussions about the future impacts of AI. Instead, the emphasis shifts towards a focus on the ‘mundanity and ordinariness of existing systems’. This highlights the importance of studying the foundational infrastructure, tools, and frameworks that shape AI development. Furthermore, it requires an understanding of the associated supply chains, investments, acquisitions, forms of ownership and support, control mechanisms, and the broader political economy surrounding AI. Such perspectives have been developed, for example, to study AI’s industry relations in healthcare, the global digital marketing and advertising industries, journalism, or the automotive industry.  The panellists examine how AI may be viewed as a platform, presenting critical perspectives on the platformisation of AI and its implications for industry relations and the media landscape. Through four distinct studies, they highlight: (1) the influence of platforms on the emerging AI ecosystem and their consolidation of power through reliance on cloud infrastructure, (2) the evolution of cloud infrastructure in the political economy of AI, (3) the actualisation of AI as a platform with ‘general-purpose’ applications, and (4) how challenges in machine vision shape innovation in AI. Each contribution revolves around a central question: How is AI, particularly within the AI sector, evolving under the influence of platform logic? In doing so, the panellists offer valuable insights informed by platform theory and methodologies, exploring their relevance for a comprehensive examination of AI and the broader AI sector. Furthermore, their perspectives provide methodological insights into understanding the material conditions and critical political economy of AI as a platform.  Collectively, these studies seek to advance the critical discourse on AI and its political economy, with a specific emphasis on the AI industry. They shed light on the evolving landscape of AI industry relations and dependencies within the platform ecosystem, tracing how these relationships have transformed over time.","",""
"2025","PROTOTYPING AN EDTECH ASSESSMENT TOOLKIT: TOWARDS TECHNICAL DEMOCRACY","New forms of facial recognition, aggression detection systems, bus routing optimization software, generative AI detection, and online exam proctoring: the edtech industry is rapidly increasing its influence over schools and universities by introducing new AI-powered products and services. As the ubiquity and complexity of AI systems intensifies, deliberation among practitioners about immediate and long-term risks is becoming ever more challenging.  To help education practitioners and administrators critically explore these systems and deliberate about their consequences, this paper presents the prototype of an edtech assessment toolkit. The toolkit is embedded in a body of work aiming to make technical democracy a key feature in the design, implementation, and use of public sector AI systems. This prototype brings awareness to the potential of participatory design to explore the accountability, transparency, and governance of AI-based systems through collective forms of experimentation.  The paper itself consists of four parts. First, it puts forward a framework for technical democracy to govern AI systems through shared uncertainty. Secondly, it presents how the toolkit has been co-created to encourage collective learning and experimentation. Thirdly, it highlights three tools that have emerged from a series of workshops, collaborations, and dialogues: the counter-archive, the issues register, and the possibilities matrix. The paper finishes by discussing how this methodology produces an exploratory tool for enacting technical democracy design experiments to interrupt the design and implementation of AI into the public sector by making spaces that value dissensus over consensus in an age of automated decision-making.","",""
"2025","CHEATGPT? THE REALITIES OF AUTOMATED AUTHORSHIP IN THE UK PR AND COMMUNICATIONS INDUSTRIES","Drawing on interview and survey data from content writers in the UK communications industries, this paper critically and empirically explores content writers' engagements with generative text AI in relation to creative authorship and expertise. The project will utilize a critical framework of algorithmic literacy to consider avenues for empowering so-far overlooked stakeholders of AI tool use in this creative industry sector.  The paper presents findings from a survey of 1,074 PR and communications content writers and their managers/ employers and from 21 follow-up interviews with the same stakeholders. It will explore the realities of automated authorship and the opportunities and limitations that algorithmic literacy might bring in enhancing smaller stakeholder algorithmic empowerment and expertise. Findings suggest that a) generative text AI is increasingly being used by content writers in ways that challenge speculative forecasts of generative text use and that b) these tools are useful for saving time, idea generation and synthesising existing text, but cannot (yet) be used to replicate or generate authorially convincing tone of voice or brand identity. Such findings suggest that critical algorithmic literacy could be used to create dialogues in workplaces that foreground the problems related to automated authorship - especially in terms of promoting human expertise, challenging algorithmic power and reforming the boundaries of creative subjectivity.","",""
"2025","A PICTURE IS WORTH A THOUSAND PROMPTS: TOPIC MODELING OF AI ART SUBREDDIT COMMUNITIES","Text-to-image generation (AI art) has become a mainstream phenomenon since the introduction of DALL-E by OpenAI in January 2021 (Nast, 2023). ). On the one hand, AI art challenges definitions of creativity that center on anthropocentric values and discredits the contributions of artists in the training of these AI models (Knibbs, 2023). On the other hand, it blurs the line between artists and non-artists by enabling new ways of creating art (e.g., prompt engineering: an iterative and experimental text-based process to interact with text-to-image generation models). Regardless of one’s ethical stance, practitioners of AI art, including artists of various skills and non-artists, form and participate in online communities to showcase their wares, share practices and resources, and learn from each other.  This study uses a topic modeling approach to examine topics within three subreddit communities centered on three text-to-image generation models (r/StableDiffusion, r/midjourney, and r/weirddalle). The analysis, based on the top 500 posts from each subreddit over one month, reveals distinct community foci: r/StableDiffusion emphasizes technological innovations and technical learning, r/midjourney showcases AI art and prompt learning, while r/weirddalle is more competitive, focusing on creative or entertaining results. The study further derives topics from prompts extracted from the images, revealing preferences for popular media characters, high photorealism, and surrealist styles, with a notable emphasis on portraits of women.","",""
"2025","SIMULATING SUBJECTIVITY - BAUDRILLARD AND THE POLITICAL ECONOMY OF LLMS","Despite the rise in commercial applications of LLMs, scholarship has neglected an in-depth appreciation of the free contribution of subjects communicative social action as the engine of training data production as a necessary moment in digital processes of valorisation. This issue was popular in the analyses of the post-operaist tradition of free labour, but have since been missing in examinations of more recent technological developments, specifically in what concerns AI. Although the work of Baudrillard is semi-frequently evoked in descriptive critical assessments of new technologies, there is little integration of Baudrillard's work in contemporary studies of AI. This paper aims at contributing in this direction, by showcasing the utility of Baudrillard’s concepts of simulation, subject function, masses, and the social, for an understanding of immaterial free labour in the context of Large Language Models (LLMs).  Drawing from the recent phenomena of the sale of Reddit communications content to OpenAI as training data, I propose the notion of digital common as the pre-trained collected and recorded data of actual human communication through digital systems. I propose the framework of the subject function as expounded by Baudrillard, in both its individual and collective aspects, as a necessary conjuncture to understand how commercial applications of conversational LLMs fit into the broader landscape of digital political economy. I suggest that the role played in this specific application derives from the appropriation of freely generated user-data as constituting the digital common and as carrying a specific conception of subjectivity as functional.","",""
"2025","ASSESSING OCCUPATIONS THROUGH ARTIFICIAL INTELLIGENCE: A COMPARISON OF HUMANS AND GPT-4","Large language models (LLMs) such as GPT-4 have raised questions about the changing nature of work. Research has started to investigate how this technology affects labor markets and might replace or augment different types of jobs. Beyond their economic implications in the world of work, there are important sociological questions about how LLMs connect to subjective evaluations of work, such as the prestige and perceived social value of different occupations, and how the widespread use of LLMs perpetuate often biased views on the labor markets reflected in their training datasets. Despite initial research on LLMs’ world model, their inherent biases, attitudes and personalities, we lack evidence on how LLMs themselves evaluate occupations as well as how well they emulate the occupational evaluations of human evaluators. We present a systematic comparison of GPT-4 occupational evaluations with those from an in-depth, high-quality survey in the UK context. Our findings indicate that GPT-4 and human scores are highly correlated across all ISCO-08 major groups for prestige and social value. At the same time, GPT-4 substantially under- or overestimates the occupational prestige and social value of many occupations, particularly emerging occupations as well as stigmatized or contextual ones. In absolute terms, GPT-4 scores are more generous than those of the human respondents. Our analyses show both the potentials and risks of using LLM-generated data for occupational research.","",""
"2025","AI AS “UNSTOPPABLE” AND OTHER INEVITABILITY NARRATIVES IN TECH: ON THE ENTANGLEMENT OF INDUSTRY, IDEOLOGY, AND OUR COLLECTIVE FUTURES","The world today is awash with narratives of artificial intelligence (AI) as an """"inevitable,"""" """"unstoppable"""" force destined to """"revolutionize"""" society. Using the concept of """"entanglement"""" from Black and Indigenous feminist science, technology, and society studies, we provide a critical examination of the AI industry's complex intersections with sociopolitical dynamics, technological determinism, and oppression. Instead of viewing AI advancement as a predetermined path, we show how this perspective is socially constructed and has dire consequences for the environment and society alike, especially for marginalized communities in the global and local Souths.  Using discourse analysis and critical quantitative techniques, we analyze the discourses surrounding AI, examining over 1,200 pieces of digital content to show how language shapes both technology's development and our collective imaginations. By tracing the ideological roots of AI back to colonial and eugenic practices, we demonstrate the industry's deep entanglement with interlocking systems of oppressions.","",""
"2025","INDUSTRIES OF INFRASTRUCTURAL FUTURES, AUTOMATED CULTURES, AND ALGORITHMIC DYNAMICS","We approach the conference topic of industry through four studies that explore a new media landscape where value and power are increasingly produced through the operalisation of machinic and affective relations and subjectivities. We explore technologies of automation, artificial intelligence and algorithmic regimes and the new possibilities for enacting and imagining political futures that they afford. The papers focus on four case studies: WeChat posts related to the Australian referendum for Indigenous voice to Parliament; perceptions of bias in texts generated by ChatGPT; TikTok videos of affective time management through curated playlists, and time synchronisation protocols used in industrial warehouses. These four cases reveal the ways in which automated media technologies enable complex interactions between humans and machines that impact practices of political subjectification, trust and notions of truth, and weave a particular relationship between affect, aesthetics and temporalities of labour.  The panel analyses the role of networks and algorithms in shaping the parameters of emerging forms of expression and participation that are enacted through the interrelation between users and digital platforms. The four papers adopt a platform- and infrastructure-specific research approach, building on research into the infrastructural turn in media studies (Plantin and Punathambekar 2019). Seeing media infrastructures as a complex arrangement of digital platforms, databases, algorithms and protocols, we look at the political effects produced by these infrastructures. These effects are analysed in their material situatedness in the logic of the particular digital platform.  In their paper, Toija Cinque and Allan Jones illuminate the interplay between AI agency, human cognition, and digital media platforms, thereby contributing to discourse on ethical AI use, sociotechnical systems, and information integrity in the digital age. Their paper explores the interconnections between generative AI, digital platforms, and cognitive biases, striving to deepen our understanding of technology’s capacity to engender a truth-centric, empathetic digital society. By delineating ethical pathways for the coexistence of humans and machines within the information realm, the study aims to contribute significantly to the ongoing discourse surrounding ethical AI use, the development of sociotechnical systems, and the maintenance of information integrity in the digital era. Through its findings, it aspires to influence future technological developments, regulatory frameworks, and policy formulations, thus paving the way for a more balanced and equitable digital future.  Fan Yang, Robbie Fordyce and Luke Heemsbergen analyse messages related to the recent referendum for political representation of Indigenous Australians posted on the Chinese-owned platform WeChat. The authors argue that, while posts largely follow the rhetoric of mainstream Australian media in their sentiments, the cases in which they divert, indicate the catalyzation of diaspora affects which influence the position towards race and Indigenous issues.  Tsvetelina Hristova explores the technopolitical implications of network time synchronisation protocols in warehouse automation. The logic of digital infrastructures imposes a notion and practice of time that is radically different from the universalising time synchronisation of industrial capitalism. Instead, network time protocols rely on the exchange of messages and data packages through which a measure and notion of time is negotiated and agreed upon in a networked environment. This imposes a particular technopolitical context of technological interpellation where structures of time are constituted through the participation of nodes in the network. This technological context of temporality has important implications for how hierarchies and enclosures are enacted in industrial networks and, importantly, for the role of cloud computing platforms in organising new forms of privatised time.  Alexandra Anikina analyses the assembly line aesthetics in videos that promote personal productivity and time management on Tik Tok, YouTube and Twitch.  This panel is proposed by members of two research groups on critical infrastructure studies across the Atlantic that explore how new digital, automated and intelligent media technologies are impacting social and political life. Trying to understand criticality as both an analytical approach and a characteristic of the objects we research, we interrogate the aspects of digital media infrastructures that add new layers to how datafication acquires subtle cultural and technopolical inflections. Through the focus on affect in the panel, understood as both social emotional charge (Ahmed 2013) and as the potential for connection and interaction (Massumi 2002) in the network, we try to see the infrastructure of data systems and automated platforms as the product of different cultural, political and technological drives. These drives give rise to situated and embodied logics of automation that are platform-dependent but also dependent on cultural and social affects imbued through their provenance, producers and users. The panel blends different disciplinary perspectives and approaches, seeking a dialogue between media and communication studies, cultural studies and critical art research.  References  Ahmed, S., 2013. The cultural politics of emotion. Routledge.  Massumi, B., 2002. Parables for the Virtual: Movement, Affect, Sensation. Duke University Press.  Plantin, J.C. and Punathambekar, A., 2019. Digital media infrastructures: pipes, platforms, and politics. Media, culture &amp; society, 41(2), pp.163-174.","",""
"2025","Emotionalized AI and the meaningfulness gap: an AI ethics perspective","","",""
"2025","Misconceptions about intimate partner violence risk assessment algorithm in the Basque Country: a reply to Valdivia, Hyde-Vaamonde, and García-Marcos (2024)","Abstract           Violence risk assessment is an internationally recognised methodology, aimed to manage different forms of violence. Most risk assessment tools, as is the case of the reviewed one, are designed to protect victims in the context of pressure, little time, or little information. This paper presents a reply to Valdivia et al. (AI &amp; Society, July 2024) criticism of the algorithm for intimate partner violence risk assessment—EPV—used in the Basque Country. They concluded that more than 50% of high-risk victims are in danger, using results from a pilot version of the instrument, not the reviewed one published in 2010, nor the system in use since May 2013. In addition, qualitative information from a single professional generates global criticisms of the tool. Neither the current cut-off points nor the real weighting of the items nor the real risk management procedure are considered, and the personal opinion of a judge is assumed to be better than the use of tools when the accumulated research shows the opposite. When EPV risk assessment reports are used in courts, they may only temporarily affect some perpetrator rights, imposing restraining orders or, in the worst case, pretrial prison waiting for sentence. However, risk management can save the life of the victim. Cautions and suggestions related to the judicial context, such as improving risk reports or training judicial professionals, are shared. However, Valdivia et al.’s paper leads to misconceptions that extend to different sectors when echoing the wrong conclusions of their paper.","",""
"2025","Research ethics for AI in healthcare: how, when and who","","",""
"2025","What is beautiful is good: robot beauty bias toward android-type robots","","",""
"2025","Can a large language model be your friend?","","",""
"2025","Organisational tensions in introducing socially sustainable AI","Abstract           The introduction of AI into an organisation is linked to many of its functions, changing not only the technical systems but also the organisation of work and the society around it. Technology is often introduced with efficiency goals in mind, but at the same time, the constantly evolving understanding of sustainable and responsible business raises questions about how to ensure socially sustainable, ethical and responsible development and deployment of AI. The introduction of new, complex technologies, combined with the increasing social complexity of the operating environment, can easily create conflicting demands and dilemmas for organisations. In this paper, we explore the organisational tensions in public and private organisations that are planning to deploy or have already experimented with AI. The aim of the study is to broaden the understanding of AI-related organisational tensions: what issues they cover and how they are described by the practitioners working with AI. The research methodology is a qualitative content analysis of transcribed interviews with AI development experts from 42 Finnish organisations. The results are divided into three categories: (1) tensions related to values, (2) tensions related to AI implementation, and (3) tensions related to AI impacts. A total of 12 pairs of tensions were identified within these categories. We argue that by identifying and understanding AI-related tensions, organisations can learn about the positive and negative social, environmental and economic impacts of AI. This awareness enables organisations to consider impacts in advance, focus attention on key issues and act in a more sustainable way when adopting AI.","",""
"2025","The hopes and fears of artificial intelligence: a comparative computational discourse analysis","Abstract           Artificial intelligence (AI) has captured the interest of multiple actors with speculations about its benefits and dangers. Despite increasing scholarly attention to the discourses of AI, there are limited insights on how different groups interpret and debate AI and shape its opportunities for action. We consider AI an issue field understood as a contested phenomenon where heterogeneous actors assert and debate the meanings and consequences of AI. Drawing on computational social science methods, we analyzed large amounts of text on how politicians (parliamentarians) consultancies (high reputation firms), and lay experts (AI-forum Reddit users) articulate meanings about AI. Through topic modeling, we identified diverse and co-existing discourses: politicians predominantly articulated AI as a societal issue requiring an ethical response, consultancies stressed AI as a business opportunity pushing a transformation-oriented discourse, and lay experts expressed AI as a technical issue shaping a techno-feature discourse. Moreover, our analysis details the hopes and fears within AI discourses, revealing that sentiment varies by actor group. Based on these findings, we contribute new insights about AI as an issue field shaped by the discursive work performed by heterogeneous actors.","",""
"2025","Why AI image generators cannot afford to be blind to racial bias","","",""
"2025","Cognitive implications of AI in precision medicine: navigating the human–machine partnership in healthcare decision-making","","",""
"2025","“Everybody knows what a pothole is”: representations of work and intelligence in AI practice and governance","Abstract           In this paper, we empirically and conceptually examine how distributed human–machine networks of labour comprise a form of underlying intelligence within Artificial Intelligence (AI), considering the implications of this for Responsible Artificial Intelligence (R-AI) innovation. R-AI aims to guide AI research, development and deployment in line with certain normative principles, for example fairness, privacy, and explainability; notions implicitly shaped by comparisons of AI with individualised notions of human intelligence. However, as critical scholarship on AI demonstrates, this is a limited framing of the nature of intelligence, both of humans and AI. Furthermore, it dismisses the skills and labour central to developing AI systems, involving a distributed network of human-directed practices and reasoning. We argue that inequities in the agency and recognition of different types of practitioners across these networks of AI development have implications beyond RAI, with narrow framings concealing considerations which are important within broader discussions of AI intelligence. Drawing from interactive workshops conducted with AI practitioners, we explore practices of data acquisition, cleaning, and annotation, as the point where practitioners interface with domain experts and data annotators. Despite forming a crucial part of AI design and development, this type of data work is frequently framed as a tedious, unskilled, and low-value process. In exploring these practices, we examine the political role of the epistemic framings that underpin AI development and how these framings can shape understandings of distributed intelligence, labour practices, and annotators’ agency within data structures. Finally, we reflect on the implications of our findings for developing more participatory and equitable approaches to machine learning applications in the service of R-AI.","",""
"2025","Mark Amerika: My life as an artificial creative intelligence","","",""
"2025","The irony of AI in a low-to-middle-income country","","",""
"2025","Ethical concerns around privacy and data security in AI health monitoring for Parkinson’s disease: insights from patients, family members, and healthcare professionals","","",""
"2025","The changing face of Agrarian labor in the age of artificial intelligence and machine learning: balancing benefits and risks","","",""
"2025","Architect, AI and the maximiser scenario","","",""
"2025","The great AI mistake: why job replacement is the wrong strategy","","",""
"2025","ChatGPT and academic work: new psychological phenomena","Abstract           This study describes the impact of ChatGPT use on the nature of work from the perspective of academics and educators. We elucidate six phenomena: (1) the cognitive workload associated with conducting Turing tests to determine if ChatGPT has been involved in work productions; (2) the ethical void and alienation that result from recondite ChatGPT use; (3) insights into the motives of individuals who fail to disclose their ChatGPT use, while, at the same time, the recipient does not reveal their awareness of that use; (4) the sense of ennui as the meanings of texts dissipate and no longer reveal the sender’s state of understanding; (5) a redefinition of utility, wherein certain texts show redundancy with patterns already embedded in the base model, while physical measurements and personal observations are considered as unique and novel; (6) a power dynamic between sender and recipient, inadvertently leaving non-participants as disadvantaged third parties. This paper makes clear that the introduction of AI tools into society has far-reaching effects, initially most prominent in text-related fields, such as academia. Whether these implementations represent beneficial innovations for human prosperity, or a rather different line of social evolution, represents the pith of our present discussion.","",""
"2025","Where did all the AI experts come from? They used to be virologists…","","",""
"2025","Human–chatbot communication: a systematic review of psychologic studies","","",""
"2025","Charting a course at the human–AI frontier: a paradigm matrix informed by social sciences and humanities","","",""
"2025","Computational implementations of responsible AI: from the right to be forgotten to machine unlearning","","",""
"2025","“This robot is dictating her next steps in life”: disability justice and relational AI ethics","Abstract           As automated technologies, particularly artificial intelligence (AI) and automated decision-making (ADM), become integral to social life, there is growing concern about their ethical implications. While issues of accountability, transparency, and fairness dominate discussions on “ethical” AI, little attention has been given to how socially disadvantaged groups most impacted by ADM systems form ethical judgments about them. Drawing on insights from relational ethics, this study uses dialogue groups with disabled people to explore how people distinguish between ‘more just’ or ‘less just’ uses of technology, and the contextual, situational, and relational factors that shape these judgments. For the dialogue group participants in our study, ethical reasoning was most strongly influenced by concerns about how ADM systems affect self-determination, caring relationships and identity recognition, and about the political–economic drivers of automation. The article contributes to AI ethics by empirically demonstrating that justice and ethics depend on the social relationships valued in different contexts and what is at stake, both personally and politically, in decisions aided by automation.","",""
"2025","The role of generative AI in academic and scientific authorship: an autopoietic perspective","Abstract           The integration of generative artificial intelligence (AI), particularly large language models like ChatGPT, presents new challenges as well as possibilities for scientific authorship. This paper draws on social systems theory to offer a nuanced understanding of the interplay between technology, individuals, society and scholarly authorial practices. This contrasts with orthodoxy, where individuals and technology are treated as essentialized entities. This approach offers a critique of the binary positions of sociotechnological determinism and accelerationist instrumentality while still acknowledging that generative AI presents profound challenges to existing practices and meaning making in scientific scholarship. This holistic treatment of authorship, integrity, and technology involves comprehending the historical and evolutionary entanglement of scientific individuality, scientific practices, and meaning making with technological innovation. This addresses current needs for more robust theoretical approaches to address the challenges confronted by academicians, institutions, peer review, and publication processes. Our analysis aims to contribute to a more sophisticated discourse on the ethical and practical implications of AI in scientific research.","",""
"2025","Beyond degrees: redefining higher education institutions as ethical AI hubs","","",""
"2025","The democratic ethics of artificially intelligent polling","AbstractThis paper examines the democratic ethics of artificially intelligent polls. Driven by machine learning, AI electoral polls have the potential to generate predictions with an unprecedented level of granularity. We argue that their predictive power is potentially desirable for electoral democracy. We do so by critically engaging with four objections: (1) the privacy objection, which focuses on the potential harm of the collection, storage, and publication of granular data about voting preferences; (2) the autonomy objection, which argues that polls are an obstacle to independently formed judgments; (3) the tactical voting objection, which argues that voting strategically on the basis of polls is troublesome; and finally (4) the manipulation objection, according to which malicious actors could systematically bias predictions to alter voting behaviours. ","",""
"2025","AI metrics and policymaking: assumptions and challenges in the shaping of AI","Abstract           This paper explores the interplay between AI metrics and policymaking by examining the conceptual and methodological frameworks of global AI metrics and their alignment with National Artificial Intelligence Strategies (NAIS). Through topic modeling and qualitative content analysis, key thematic areas in NAIS are identified. The findings suggest a misalignment between the technical and economic focus of global AI metrics and the broader societal and ethical priorities emphasized in NAIS. This highlights the need to recalibrate AI evaluation frameworks to include ethical and other social considerations, aligning AI advancements with the United Nations Sustainable Development Goals (SDGs) for an inclusive, ethical, and sustainable future.","",""
"2025","Ethical governance of artificial intelligence for defence: normative tradeoffs for principle to practice guidance","Abstract           The rapid diffusion of artificial intelligence (AI) technologies in the defence domain raises challenges for the ethical governance of these systems. A recent shift from the what to the how of AI ethics sees a nascent body of literature published by defence organisations focussed on guidance to implement AI ethics principles. These efforts have neglected a crucial intermediate step between principles and guidance concerning the elicitation of ethical requirements for specifying the guidance. In this article, we outline the key normative choices and corresponding tradeoffs that are involved in specifying guidance for the implementation of AI ethics principles in the defence domain. These correspond to: the AI lifecycle model used; the scope of stakeholder involvement; the accountability goals chosen; the choice of auditing requirements; and the choice of mechanisms for transparency and traceability. We provide initial recommendations for navigating these tradeoffs and highlight the importance of a pro-ethical institutional culture.","",""
"2025","Are we inventing ourselves out of our own usefulness? Striking a balance between creativity and AI","","",""
"2025","What makes full artificial agents morally different","Abstract           In the research field of machine ethics, we commonly categorize artificial moral agents into four types, with the most advanced referred to as a full ethical agent, or sometimes a full-blown Artificial Moral Agent (AMA). This type has three main characteristics: autonomy, moral understanding and a certain level of consciousness, including intentional mental states, moral emotions such as compassion, the ability to praise and condemn, and a conscience. This paper aims to discuss various aspects of full-blown AMAs and presents the following argument: the creation of full-blown artificial moral agents, endowed with intentional mental states and moral emotions, and trained to align with human values, does not, by itself, guarantee that these systems will have human morality. Therefore, it is questionable whether they will be inclined to honor and follow what they perceive as incorrect moral values. we do not intend to claim that there is such a thing as a universally shared human morality, only that as there are different human communities holding different sets of moral values, the moral systems or values of the discussed artificial agents would be different from those held by human communities, for reasons we discuss in the paper.","",""
"2025","Metaphors in digital radiology: ethical implications for responsibility assignments of human-AI imaginaries","Abstract           The advent of artificial intelligence (AI) in radiology triggered identity-threatening fears for radiologists of becoming replaced by machines. Beyond this competitive narrative of humans versus AI, a collaborative narrative for human–AI-interaction emerged with a new metaphorical landscape both for the functions of AI and the roles of radiologists. This article aims to raise awareness of the ethical implications of figurative language in human–AI interaction in digital radiology. The paper is divided into two parts. The first part justifies the approach of metaphor analysis in medicine, draws a spectrum of ethical implications for language choices, and introduces taxonomies of human–AI interaction. We use these preliminaries as a hermeneutical tool to conduct such a metaphor analysis in the second part. There, we identify prevalent metaphors in the radiological community and discuss their ethical implications regarding responsibility assignments. We argue that while metaphors can facilitate a collaborative narrative, they may also lead to the undesirable ethical consequence of attributing moral responsibility to AI, which lacks the necessary features for such responsibility. The spectrum of metaphorically constructed functions of AI ranges from “time-saving tool” to “assistant” and “ally”. For the roles of radiologists, we found metaphors and analogies which are derived from contexts of aviation (radiologists as “pilots” and AI as “auto-pilots”), war (radiologists at the “forefront of technological development”), music (radiologists as “conductors” of multi-disciplinary teams), and hierarchical power contexts (radiologists as “technology and thought leaders”). Despite radiologists’ expressed willingness to collaborate actively with AI, the prevailing analogy of AI as a “tool” primarily suggests mere delegation of routine tasks, at the same time allowing radiologists to maintain their professional competencies. However, a new competitive narrative of AI-savvy versus non-AI-savvy radiologists also emerged, transforming the initial competitive narrative from human versus AI to human versus human competition.","",""
"2025","AI rule and a fundamental objection to epistocracy","","",""
"2025","The phenomenon of artificial intelligence in modern transformational socio-cultural processes: Socio-philosophical analysis","","",""
"2025","A review of medical tourism entrepreneurship and marketing at regional and global levels and a quick glance into the applications of artificial intelligence in medical tourism","","",""
"2025","Distribution of responsibility for AI development: expert views","Abstract           The purpose of this paper is to increase the understanding of how different types of experts with influence over the development of AI, in this role, reflect upon distribution of forward-looking responsibility for AI development with regard to safety and democracy. Forward-looking responsibility refers to the obligation to see to it that a particular state of affairs materialise. In the context of AI, actors somehow involved in AI development have the potential to guide AI development in a safe and democratic direction. This study is based on qualitative interviews with such actors in different roles at research institutions, private companies, think tanks, consultancy agencies, parliaments, and non-governmental organisations. While the reflections about distribution of responsibility differ among the respondents, one observation is that influence is seen as an important basis for distribution of responsibility. Another observation is that several respondents think of responsibility in terms of what it would entail in concrete measures. By showing how actors involved in AI development reflect on distribution of responsibility, this study contributes to a dialogue between the field of AI governance and the field of AI ethics.","",""
"2025","From liability gaps to liability overlaps: shared responsibilities and fiduciary duties in AI and other complex technologies","AbstractComplex technologies such as Artificial Intelligence (AI) can cause harm, raising the question of who is liable for the harm caused. Research has identified multiple liability gaps (i.e., unsatisfactory outcomes when applying existing liability rules) in legal frameworks. In this paper, the concepts of shared responsibilities and fiduciary duties are explored as avenues to address liability gaps. The development, deployment and use of complex technologies are not clearly distinguishable stages, as often suggested, but are processes of cooperation and co-creation. At the intersections of these stages, shared responsibilities and fiduciary duties of multiple actors can be observed. Although none of the actors have complete control or a complete overview, many actors have some control or influence, and, therefore, responsibilities based on fault, prevention or benefit. Shared responsibilities and fiduciary duties can turn liability gaps into liability overlaps. These concepts could be implemented in tort and contract law by amending existing law (e.g., by assuming that all stakeholders are liable unless they can prove they did not owe a duty of care) and by creating more room for partial liability reflecting partial responsibilities (e.g., a responsibility to signal or identify an issue without a corresponding responsibility to solve that issue). This approach better aligns legal liabilities with responsibilities, increases legal certainty, and increases cooperation and understanding between actors, improving the quality and safety of technologies. However, it may not solve all liability gaps, may have chilling effects on innovation, and may require further detailing through case law.","",""
"2025","Mitigating AI-induced professional identity threat and fostering adoption in the workplace","","",""
"2025","WEIRD? Institutions and consumers’ perceptions of artificial intelligence in 31 countries","Abstract           A survey of perceptions of Artificial Intelligence in 31 countries in 2023 (Ipsos in Global Views on A.I. 2023. https://www.ipsos.com/sites/default/files/ct/news/documents/2023-07/Ipsos%20Global%20AI%202023%20Report-WEB_0.pdf. Accessed 17 May 2024, 2023) yields significantly less positive perceptions of the new technology in developed western economies than in emerging and non-western economies. This could reflect citizens in non-Western countries perceiving machines (computers) and algorithms differently from those in Western countries, or that a more positive outlook in countries with weak democratic institutions comes from a preference for algorithmic precision over inconsistent and/or corrupt regulation and decision-making. However, it could also be reflecting the different psychology of “WEIRD” (Western, Educated, Industrialised, Rich, Democratic) countries. Regressing the survey responses against measures of the “WEIRD” dimensions, we find that reported understanding of, willingness to trust, and anticipation of change due to AI applications are consistently negatively correlated to a country’s education levels (E), and average income per capita (R). The sophistication of democratic institutions (D) and “Westernness” (W), both alone and in combination with the other factors, have statistically significant negative effects on the percentage of the respondents in any given country having positive perceptions of AI and its prospects. The consistency of the negative relationship between the sophistication of democratic institutions country-level perceptions of AI brings into question the role of regulation of the new technology. WEIRD societies are presumed to rely on democratic institutions for assurances they can transact safely with strangers. Institutions thus substitute for the trust non-WEIRD societies place in friends, family and close community contacts when transacting. Third-party (and notably government) assurances in the context of uncertainty created by the emergence of new AI technologies arguably condition perceptions of the safety of these technologies through the presence (or absence) of regulations governing their implementation and use. Different perceptions amongst European countries compared to other western counterparts to perceptions of data privacy support the contention that the mere presence of AI regulation may be sufficient to alter perceptions in WEIRD societies, regardless of whether the regulations are necessary or even effective in increasing user safety. This has implications for interpreting and responding to political pressure to regulate new technologies in WEIRD countries.","",""
"2025","Balancing risks and benefits: public perceptions of AI through traditional surveys and social media analysis","","",""
"2025","Why we should design less privileged AI systems","","",""
"2025","Lindsay Grace: Models of the mind: how physics, engineering and mathematics have shaped our understanding of the brain","","",""
"2025","Andrew J. Hampton and Jeanine A. DeFalco: The Frontlines of Artificial Intelligence Ethics. Human-Centric Perspectives on Technology’s Advance","","",""
"2025","Security practices in AI development","Abstract           What makes safety claims about general purpose AI systems such as large language models trustworthy? We show that rather than the capabilities of security tools such as alignment and red teaming procedures, it is security practices based on these tools that contributed to reconfiguring the image of AI safety and made the claims acceptable. After showing what causes the gap between the capabilities of security tools and the desired safety guarantees, we critically investigate how AI security practices attempt to fill the gap and identify several shortcomings in diversity and participation. We found that these security practices are part of securitization processes aiming to support (commercial) development of general purpose AI systems whose trustworthiness can only be imperfectly tested instead of guaranteed. We conclude by offering several improvements to the current AI security practices.","",""
"2025","Music generative AI and ‘The Hegelian Wound’","","",""
"2025","Old wine in new bottles: shifting to flexible regulatory approaches for generative AI","","",""
"2025","Correction: “Everybody knows what a pothole is”: representations of work and intelligence in AI practice and governance","","",""
"2025","Sisters, not twins: exploring artistic control and anthropomorphism through composing with a bespoke generative AI","Abstract           Generative AI (GenAI) has the potential to affect artists’ control over their own music due to the illegal usage of copyrighted material for training. However, GenAI also creates exciting opportunities for artists to expand their material and working processes. Artists working with GenAI and documenting their outcomes can assist other artists as well as wider society in understanding how GenAI operates and can benefit human artistic output. This paper provides an autoethnographic case study into how a new GenAI tool influenced an established composing practice during the writing of the experimental musical work, Control Yourself (2023). The Koup Music prototype by Kopi Su Studio was trained on vocal inputs by the author and subsequently generated bespoke sonic material. While identifiably true to the author’s musical—and literal—voice, the outputs were novel and perceived as imbued with emotion, leading to subsequent anthropomorphising of the AI. Written by a former AI sceptic, this paper details how the emotive power of the AI’s non-verbal, human-like sounds informed the narrative and structure of the resulting work and imparted a sense of collaboration, rather than solo authorship. Furthermore, the influence of the AI was felt beyond its actual involvement, with the project taking on a more playful approach less centred on the artistic control of the human composer. Following these observations, this paper discusses how GenAI served as a tool for musical experimentation and exploring creative ‘blind spots.’ These insights are also contextualised by current discourse on the perception and use of GenAI in the arts, the role of artistic control in human–AI co-creation, and how anthropomorphism has manifested in past human–AI partnerships.","",""
"2025","Perceptions of AI-driven news among contemporary audiences: a study of trust, engagement, and impact","","",""
"2025","On the flooding of AI-generated images: a paradigm shift in the way we see the world?","","",""
"2025","Artificial intelligence and the future of otherness: what kind of other can an AI be for a human?","","",""
"2025","Human insight vs. AI in extreme financial crises: the case for human decision-making","","",""
"2025","Artificial intelligence versus collective intelligence","Abstract           The ontological presupposition of artificial intelligence (AI) is the liberal autonomous human subject of Locke and Kant, and the ideology of AI is the automation of this particular conception of intelligence. This is demonstrated in detail in classical AI by the work of Simon, who explicitly connected his work on AI to a wider programme in cognitive science, economics, and politics to perfect capitalism. Although Dreyfus produced a powerful Heideggerian critique of classical AI, work on neural networks in AI was ultimately based on the individual as the locus of intelligence. Yet this conception of AI both fails to grasp the essence of large language models, which are a statistical model of human language on the Web. The training data that enables AI is the surveillance and capture of data, where the data creates a model to approximate the entire world. However, there is a more hidden ideology inherent in AI where the goal is not to perfect a model but to control the world. As prompted by an argument between Mead and Bateson, social change is prevented by the application of cybernetics to society as a whole. The goal of AI is not just to replace human beings, but to manage humans to preserve existing power relations. As the source of intelligence in AI is distributed cognition between humans and machines, the alternative to AI is collective intelligence. As theorized by Licklider and Engelbart at the dawn of the Internet, collective intelligence explains how computers weave together both human and non-human intelligence. Rather than replace human intelligence, this produces ever more complex collective forms of intelligence. Rather than meta-stabilize a society of control, collective intelligence can go outside individualist capitalist ontology by incorporating the open world of the pluriverse, as theorized by Escobar. Collective intelligence then stands as an alternative ontological path for AI which puts intelligence at the service of humanity and the world rather than a technocratic elite.","",""
"2025","Intelligence is not deception: from the Turing test to community-based ascriptions","Abstract           The Turing test has a peculiar status in the artificial intelligence (AI) research community. On the one hand, it is presented as an important topic in virtually every AI textbook, and the research direction focused on developing AI systems that behave in human-like fashion is standardly called the “Turing test approach”. On the other hand, reports of computer programs passing the Turing test have had relatively little effect. Does this mean that the Turing test is no longer relevant as a test, doomed to be a theoretical notion with little connection to AI practice? In this paper, I argue that there is one problem in particular with common traditional versions of the Turing test, namely their focus on deception. The criterion for passing the Turing test is standardly connected to an AI system’s ability to deceive the interrogator about its identity. But why should we connect intelligence to the ability deceive? Here I present a revised version of an intelligence test that is not based on deception. In what I call the Community-based intelligence test (CBIT), an AI is introduced to a community of human subjects. If after a sufficient number of interactions within that community the humans are not able to identify the AI system as a computer, it is considered to have passed CBIT. I discuss whether that should be enough to ascribe intelligence to the AI, and if not, what more would be needed?","",""
"2025","Fiction writing workshops to explore staff perceptions of artificial intelligence (AI) in higher education","AbstractThis study explores perceptions of artificial intelligence (AI) in the higher education workplace through innovative use of fiction writing workshops. Twenty-three participants took part in three workshops, imagining the application of AI assistants and chatbots to their roles. Key themes were identified, including perceived benefits and challenges of AI implementation, interface design implications, and factors influencing task delegation to AI. Participants envisioned AI primarily as a tool to enhance task efficiency rather than fundamentally transform job roles. This research contributes insights into the desires and concerns of educational users regarding AI adoption, highlighting potential barriers such as value alignment.","",""
"2025","The private sector is hoarding AI researchers: what implications for science?","Abstract           The migration of artificial intelligence (AI) researchers from academia to industry has recently sparked concerns about its implications for scientific progress. Can academia retain enough talent to shape AI advancements and counterbalance the growing influence of corporate AI labs? Analyzing OpenAlex data, we find a significant transition of premier talent to industry roles over the past decade, particularly to major tech firms. Young, highly cited scholars from leading institutions are the most likely to make this move. Following the transition, their research tends to show reduced novelty and impact. This industry-dominant shift in AI research highlights worries of an “AI brain drain”, the sidelining of exploratory science for commercial interests, and the potential misalignment with societal goals.","",""
"2025","Dehumanizing the human, humanizing the machine: organic consciousness as a hallmark of the persistence of the human against the backdrop of artificial intelligence","","",""
"2025","Perceptions of generative AI in the architectural profession in Egypt: opportunities, threats, concerns for the future, and steps to improve","Abstract           Generative AI has seen significant advances, particularly in text-to-image, with the potential to revolutionize industries, especially in creative fields such as art and design. This innovation is especially important in architecture, where idea visualization is critical. Text-to-image tools, a form of generative AI, enable architects and designers to visually bring their concepts to life. The study explores the impact of prompt-based AI generation on architecture, asking whether it is enhancing efficiency, creativity, and sustainability or threatening to replace architects. To address concerns about the role of AI in the profession, the research examines the perceptions of architecture professionals in Egypt. The authors conducted a survey and interviews with industry experts to assess the transformative impacts of AI on architecture. The findings reveal a strong awareness of AI's potential to enhance design quality and project outcomes, although some concerns about job prospects and control over AI outputs persist. Small firms view AI as vital for optimizing operations and attracting clients. Overall, AI shows promise in conceptualization and visualization, enhancing creativity and efficiency, with architects needing to adapt to AI as a tool for innovation rather than a competitor. Finally, the study proposes a roadmap for improving the use of AI in architecture.","",""
"2025","Vikas Khullar, Vrajesh Sharma, Mohit Angurala, and Nipun Chhabra (Eds) Artificial intelligence and Society 5.0: issues, opportunities, and challenges","","",""
"2025","Racial bias in AI-generated images","Abstract           AI-generated images consistently favor White people compared to people of color. This paper examined the image-to-image generation accuracy (i.e., the original race and gender of a person’s image were replicated in the new AI-generated image) of a Chinese AI-powered image generator. We examined the image-to-image generation models transforming the racial and gender categories of the original photos of White, Black and East Asian people (N = 1260) in three different racial contexts: a single person, two people of the same race, and two people of different races. The findings indicated that White people were more accurately depicted in AI-generated images than people of color in all three racial contexts. Black people, particularly females, were depicted with the lowest AI-generated racial accuracy in the image of a single person, but with higher accuracy in the image of two people of different races. The pattern of Asian people, particularly males, was the inverse: the app had higher AI-generated racial accuracy for Asians in the single-person image but lower accuracy for Asians in the two-people-of-different-races image. In all cases of incorrect racial generating, the AI-powered image generator depicted most people of color as White. This study provides us with insight into racial and gender bias in image generation and the potential representational harms of an AI-powered beauty app developed in China. More broadly, these technological biases reflect a form of postcolonial globalization that impacts image-processing systems in non-White settings, including social values of white supremacy and norms of white beauty.","",""
"2025","On pessimism aversion in the context of artificial intelligence and locus of control: insights from an international sample","Abstract           The present study sheds light on a new psychological construct called “AI pessimism aversion” (AIPA) describing an overly optimistic view of the benefits of AI by neglecting its potential dangers. In an international sample of N = 543 participants, we observed that the construct of AIPA strongly overlaps with single-item measures for positive and negative AI attitudes. Furthermore, the structural equation model suggests that the positive association between the internal locus of control (a personality measure describing persons who see themselves as steering their own lives) and positive AI attitudes was in part mediated by AIPA. We also assessed risk aversion in the present work, because high aversion of risky situations might go along with lower AIPA (the coming AI wave is characterised by high uncertainty). Surprisingly, this was not the case, and these two constructs seem to be (rather) independent. The present work should be a starting point to dive deeper into the study of AI attitudes, a highly relevant research theme for societies standing at the eve of the coming AI wave.","",""
"2025","The model is the museum: generative AI and the expropriation of cultural heritage","","",""
"2025","Equilibrating the scales: balancing and power relations in the age of AI","Abstract           As artificial intelligence (AI) continues to reshape our world, the spectre of technological domination looms large. This paper delves into the equilibrium model of balancing as a legal safeguard against AI-driven power imbalances. First, the study unveils the sources of domination: control over resources and events. Subsequently, potential legal tools of counterbalancing are identified and discussed. Employing a proactive, theory-building approach, the research synthesises legal rules, case studies, and scholarly insights to construct a framework for understanding and implementing balancing in AI contexts. The paper’s findings offer valuable insights for policymakers, legal scholars, and AI developers seeking to navigate the complex landscape of power relations in the age of AI.","",""
"2025","A comment on the pursuit to align AI: we do not need value-aligned AI, we need AI that is risk-averse","","",""
"2025","Wayne Holmes and Kaśka Porayska-Pomsta (Eds.): The Ethics of Artificial Intelligence in Education: Practices, Challenges, and Debates","","",""
"2025","Generative AI and childhood education: lessons from the smartphone generation","Abstract           This article examines the potential parallels between children's widespread adoption of smartphones and the emerging reliance on generative AI tools in childhood education. Drawing on Jonathan Haidt’s insights into how phone-based childhoods can disrupt the development of critical executive functions, and Shannon Vallor’s concept of “moral deskilling,” the discussion raises concerns about “intellectual deskilling” in younger generations. As generative AI tools like ChatGPT gain popularity, children risk becoming overly reliant on automated solutions, potentially undermining metacognition and critical thinking. This paper highlights risks such as cognitive offloading, instant gratification, and diminished perseverance and proposes measures to ensure generative AI supports rather than replaces essential developmental experiences.","",""
"2025","Doing agency: how agents adapt in wide systems","","",""
"2025","An endangered species: how LLMs threaten Wikipedia’s sustainability","Abstract           As a collaboratively edited and open-access knowledge archive, Wikipedia offers a vast dataset for training artificial intelligence (AI) applications and models, enhancing data accessibility and access to information. However, reliance on the crowd-sourced encyclopedia raises ethical issues related to data provenance, knowledge production, curation, and digital labor. Drawing on critical data studies, feminist posthumanism, and recent research at the intersection of Wikimedia and AI, this study employs problem-centered expert interviews to investigate the relationship between Wikipedia and large language models (LLMs). Key findings include the unclear role of Wikipedia in LLM training, ethical issues, and potential solutions for systemic biases and sustainability challenges. By foregrounding these concerns, this study contributes to ongoing discourses on the responsible use of AI in digital knowledge production and information management. Ultimately, this article calls for greater transparency and accountability in how big tech entities use open-access datasets like Wikipedia, advocating for collaborative frameworks prioritizing ethical considerations and equitable representation.","",""
"2025","AI and the problem of knowledge collapse","","",""
"2025","Artists or art thieves? media use, media messages, and public opinion about artificial intelligence image generators","Abstract           This study investigates how patterns of media use and exposure to media messages are related to attitudes about artificial intelligence (AI) image generators. In doing so, it builds on theoretical accounts of media framing and public opinion about science and technology topics, including AI. The analyses draw on data from a survey of the US public (N = 1,035) that included an experimental manipulation of exposure to tweets framing AI image generators in terms of real art, artists’ concerns, artists’ outrage, or competing interpretations. The results show that technology news use and science fiction viewing predicted support for AI art but also predicted belief that AI image generators will take jobs and steal art styles from human artists. In addition, the experimental results demonstrate that exposure to specific media messages can influence these responses. The findings carry implications for understanding the future adoption, use, and regulation of AI image generators.","",""
"2025","The achievement gap thesis reconsidered: artificial intelligence, automation, and meaningful work","Abstract           John Danaher and Sven Nyholm have argued that automation, especially of the sort powered by artificial intelligence, poses a threat to meaningful work by diminishing the chances for meaning-conferring workplace achievement, what they call “achievement gaps”. In this paper, I argue that Danaher and Nyholm’s achievement gap thesis suffers from an ambiguity. The weak version of the thesis holds that automation may result in the appearance of achievement gaps, whereas the strong version holds that automation may result on balance loss in possibilities for workplace achievements, i.e., in the appearance of an overall gappier work landscape. Against the strong version of the achievement gap thesis, I characterize situations where automation may result in boons to meaning-conferring workplace achievements: the appearance of what I call achievement spread and achievement swaps. Nevertheless, Danaher and Nyholm are right to worry about some uses of automation whereby human workers become subservient to AI. But these situations are better framed, I argue, as autonomy gaps rather than achievement gaps.","",""
"2025","Stream: social data and knowledge collective intelligence platform for TRaining Ethical AI Models","","",""
"2025","Developing professional ethical guidance for healthcare AI use (PEG-AI): an attitudinal survey pilot","Abstract           Healthcare professionals currently lack guidance for their use of AI. This means they currently lack clear counsel to aid their navigation of the problematic novel issues that will arise from their use of these systems. This pilot study gathered and analysed cross-sectional attitudinal and qualitative data to address the question: what should be in professional ethical guidance (PEG) to support healthcare practitioners in their use of AI? Our survey asked respondents (n = 42) to review 6 themes and 15 items of guidance content for our proposed PEG-AI. The attitudinal data are presented as simple numerical analysis and the accompanying qualitative data were subjected to conventional content analysis; the findings of which are presented in this report. The study data allowed us to identify further items that could be added to the PEG-AI and to test the survey instrument for content and face validity prior to wider deployment. Subject to further funding, we plan to take this work further to a wider study involving the next iteration of this survey, interviews with interested parties regarding PEG-AI, and an iterative Delphi process (comprising an initial co-creation workshop followed by iterative consensus building) to enable experts to reach consensus regarding recommendations for the content of PEG for AI use in healthcare. We aim for this work to inform the healthcare regulators as they develop regulatory strategies in this area.","",""
"2025","Can an AI system be conscious?","","",""
"2025","Virtues for AI","Abstract           Virtue theory is a natural approach toward the design of artificially intelligent systems, given that the design of artificial intelligence essentially aims at designing agents with excellent dispositions. This has led to a lively research programme to develop artificial virtues. However, this research programme has until now had a narrow focus on moral virtues in an Aristotelian mould. While Aristotelian moral virtue has played a foundational role in the field, it unduly constrains the possibilities of virtue theory for artificial intelligence. This paper aims to remedy this limitation. Philosophers have developed a rich tradition investigating virtues, their normative domains and their structure. Drawing on this tradition, I propose a three-dimensional classification system of possible artificial virtues: virtues can be classified according to the domain in which virtue is an excellence, norm that makes a virtue an excellence, and mode of how the virtue delivers the excellence. With this framework, we can discern gaps in the current theorising about artificial virtues. Additionally, it gives us a tool to evaluate the competences of extant artificially intelligent systems.","",""
"2025","“Desired behaviors”: alignment and the emergence of a machine learning ethics","Abstract           The concept of alignment has undergone a remarkable rise in recent years to take center stage in the ethics of artificial intelligence. There are now numerous philosophical studies of the values that should be used in this ethical framework as well as a technical literature operationalizing these values in machine learning models. This article takes a step back to address a more basic set of critical questions: Where has the ethical imperative of alignment come from? What is the ethical logic of alignment—how, exactly, does it propose to regulate machines’ and peoples’ conduct? And what are the social and political implications of this ethics? After discussing the logical and normative implications of the term itself—in what sense alignment can have an ethical meaning—we undertake a four-part “anatomy” of alignment in contemporary large language models (LLMs): first, a relatively technical sense in sequence modeling; second, a more normative sense relating to how outputs of pre-trained models are ethically evaluated; a third sense where external values are introduced using fine-tuning techniques to manage undesired model behaviors; and fourth sense, where alignment is given extreme ethical stakes in philosophical discussions of existential risks. We find that the ethics of alignment is fundamentally concerned with the problem of control, with unintended model behaviors that arise from divergences between training objectives and the normative expectations that govern the contexts in which they are used. Alignment serves to bridge the gap between what we call an “is” normativity, of statistical patterns identified by models and an “ought” normativity where values are technically introduced in models to steer them away from undesired behaviors. By problematizing control, the ethics of alignment weakens capacities to both make more substantive ethical judgments and also political decisions about how to live with AI.","",""
"2025","The hollow inclusivity of emotion-recognition software for neurodivergent workers","","",""
"2025","Generative AI in Higher Education (The ChatGPT effect), Cecilia Ka Yuk Chan and Tom Colloton","","",""
"2025","The assisted Technology dilemma: a reflection on AI chatbots use and risks while reshaping the peer review process in scientific research","","",""
"2025","Intersectional analysis of visual generative AI: the case of stable diffusion","Abstract           Since 2022, Visual Generative AI (vGenAI) tools have experienced rapid adoption and garnered widespread acclaim for their ability to produce high-quality images with convincing photorealistic representations. These technologies mirror society’s prevailing visual politics in a mediated form, and actively contribute to the perpetuation of deeply ingrained assumptions, categories, values, and aesthetic representations. In this paper, we critically analyze Stable Diffusion (SD), a widely used open-source vGenAI tool, through visual and intersectional analysis. Our analysis covers; (1) the aesthetics of the AI-generated visual material, (2) the institutional contexts in which these images are situated and produced, and (3) the intersections between power systems such as racism, colonialism, and capitalism—which are both reflected and perpetuated through the visual aesthetics. Our visual analysis of 180 SD-generated images deliberately sought to produce representations along different lines of privilege and disadvantage—such as wealth/poverty or citizen/immigrant—drawing from feminist science and technology studies, visual media studies, and intersectional critical theory. We demonstrate how imagery produced through SD perpetuates pre-existing power systems such as sexism, racism, heteronormativity, and ableism, and assumes a default individual as white, able-bodied, and masculine-presenting. Furthermore, we problematize the hegemonic cultural values in the imagery that can be traced to the institutional context of these tools, particularly in the tendency towards Euro- and North America-centric cultural representations. Finally, we find that the power systems around SD result in the continual reproduction of harmful and violent imagery through technology, challenging the oft-underlying notion that vGenAI is culturally and aesthetically neutral. Based on the harms identified through our qualitative, interpretative analysis, we bring forth a reparative and social justice-oriented approach to vGenAI—including the need for acknowledging and rendering visible the cultural-aesthetic politics of this technology and engaging in reparative approaches that aim to symbolically and materially mend injustices enacted against social groups.","",""
"2025","Is there not an obvious loophole in the AI act’s ban on emotion recognition technologies?","","",""
"2025","AI in education: A shortcut or a roadblock to foundational knowledge?","","",""
"2025","The ethical thread: AI’s role in the tapestry of fashion","","",""
"2025","AI-based generative image production systems in the artistic problematisation of the past: the thematisation of memory and temporality in """"AI art""""","Abstract           This text analyses how generative AI systems are being employed in current artistic practice to question certain historical visual narratives, creating representations that challenge some conventional perceptions of the past and thus opening up new perspectives on the experience of temporality. In this regard, special emphasis will be placed on some artistic projects based on generative historical photography practices. These are works that develop new ways around ‘archival aesthetics’ (Sekula in October 39:3–64 1986; Buchloh in Deep storage. collecting, storing and archiving in art [Exhibition catalog]. P.S.1, Nueva York, 1999; Guasch in Arte y archivo, 1920-2010. Genealogías, tipologías y discontinuidades. Akal, Madrid, 2015, etc.) by producing visual archives that do not exist or are alternatives to others. We will therefore analyse works that critically examine how these generative systems can contribute to a revision and re-evaluation of the past, as well as to the problematisation of the ways in which photography has been used for the historical record. These poetics invite subtle reflections on the role of the visual archive in the processes of shaping subjectivity and personal and communal identity. In the final part of the text, we will deal with the study of some artists who, through appropriationist strategies and remakes assisted by generative AI models, revise artistic works from the past, specifically from the photoconceptualism of the 1970s. In these strategies, the thematisation of the relations between photographic register and temporality also plays a leading role.","",""
"2025","What machines shouldn’t do","Abstract           Meaningful human control (MHC) is increasingly becoming an important topic in AI ethics beyond the domain of autonomous weapons systems. MHC has been conceptualized, analyzed, and applied. However, in this article, I show how all the current attempts at realizing MHC have fallen short because we have not taken the important first step of deciding what machines should and should not be doing in the first place. We must first ensure that the output we have delegated to the machine is appropriate – only then do we have to do the work required to ensure that MHC is realized. Here, I argue that machines should not be evaluating – that is, we should not be delegating evaluative outputs to machines. This is practically important because machines that have evaluative outputs cannot be evaluated for efficacy. We can’t say how effective they are. This is ethically important because machines don’t have the moral agency required to make evaluations. Furthermore, machines should not be in a position to change our values – which they would be if they were evaluating. Finally, evaluations are judged based on the considerations used to reach a particular judgment. Contemporary AI cannot provide these justifying considerations – so we have no way of evaluating their evaluative outputs.","",""
"2025","The end AI innocence: genie is out of the bottle","","",""
"2025","Are brain–machine interfaces the real experience machine? Exploring the libertarian risks of brain–machine interfaces","Abstract           This paper examines the implications of brain–machine interfaces (BMIs) from a libertarian perspective, arguing that their widespread use necessitates careful scrutiny due to potential risks to individual autonomy, freedom, privacy, and dignity. BMIs, while offering significant technological advancements, pose severe threats by potentially undermining fundamental libertarian values. The paper discusses how BMIs could enable invasive surveillance, thought manipulation, and emotional control, drawing parallels to Robert Nozick’s Experience Machine thought experiment. Unlike the hypothetical machine, which offers simulated experiences, BMIs could facilitate real-time control over individuals’ thoughts and emotions, leading to unprecedented forms of government overreach and coercion. The paper concludes that although libertarians advocate for minimal state intervention, the profound impact of BMIs on personal freedom and autonomy warrants a cautious approach, including potential restrictions on their use to safeguard against their misuse and to preserve individual self-ownership and dignity.","",""
"2025","Beyond cyborgs: the cybork idea for the de-individuation of (artificial) intelligence and an emergence-oriented design","Abstract           This article contributes to the philosophical inquiry of Artificial Intelligence (AI) by reframing the question “Where is the intelligence of Artificial Intelligence?” into “Where does AI intelligently operate?”. This rephrasing challenges our understanding of AI’s role in social practices and its integration into the human experience. Central to this discourse is the concept of the ‘cybork’ (a portmanteau of ‘cyborg’ and ‘work’), which symbolizes not just a physical entity but a dynamic system of actions and interactions within a socio-technical landscape: work accomplished with machines. In this framework, intelligence in AI lies not in any function of isolated systems, but rather in the situated context of their use within collective and meaningful practices that give technology its sense and direction. Conversely, technology both enables and shapes these practices to the extent that distinguishing between the two can seem unnecessary, or even detrimental, to the optimal design of and for work practices. The cybork embodies this integration and entanglement, transcending the traditional boundaries between individuals and collectives, entities and actions. It reveals the inseparability and co-dependence of humans and technology, where technological artifacts become extensions of human capabilities, embody collective human history and development, and serve as both products and participants in societal practices, fundamentally shaping our interaction with the world.","",""
"2025","Future between tensions and opportunities: a free inquiry into Christo El Morr’s volume on AI and Society","","",""
"2025","Human rights for robots? The moral foundations and epistemic challenges","","",""
"2025","Impossible evolutions: textillic thinking with machine learning models","Abstract           This paper discusses the creative project ‘Impossible Evolutions’, which uses generative machine learning models in the design of woven tapestries. This project is used as a conduit to unfold highly relational ways of thinking about the entanglements of human and machine assemblages within generative artificial intelligence. The project leverages interconnected ecological stories and the language of textiles to provide novel perspectives on the emerging relations between human and machine intelligences. The project uses Generative Adversarial Networks (GANs) and diffusion models to imagine new iterations of endangered Australian butterflies and wildflowers. The generated images are composed into three textile weavings of place: tapestries of the interconnected lives that generate each creature’s ecosystem. By reflecting on the interweaving of conditions that has disrupted each ecological niche, space is opened to think about unseen sensory worlds (Richmond Birdwing butterfly), symbiotic exchange (Bulloak Jewel butterfly), and stewardship of the land (Sunshine Diuris orchid). Each story becomes a fabric both literal and metaphorical, with this ‘textillic thinking’ offering speculative vantage points for approaching artistic and social practice with ML models. Textillic thinking interweaves creativity, collaboration, and care: conditions which are foregrounded in textile-making practices and disrupted in each creature’s ecological story. The creative work is diffracted with the neomaterialism of Rosi Braidotti and Karen Barad, and with the notions of care offered by both Maria Puig de la Bellacasa and the Indigenous Protocol for Artificial Intelligence. Through this reading and practice, the project offers material language for discussing the processes and effects of ML, while emphasizing the responsibility of the human in their design.","",""
"2025","Researchers’ perceptions of automating scientific research","Abstract           Science is being transformed by the increasing capabilities of automation technologies and artificial intelligence (AI). Integrating AI and machine learning (ML) into scientific practice requires changing established research methods while maintaining a scientific understanding of research findings. Researchers are at the forefront of this change, but there is currently little understanding of how they are experiencing these upheavals in scientific practice. In this paper, we examine how researchers working in several research fields (automation engineering, computational design, conservation decision-making, materials science, and synthetic biology) perceive AI/ML technologies used in their work, such as laboratory automation, automated design of experiments, computational design, and computer experiments. We find that researchers emphasised the need for AI/ML technologies to have practical benefits (such as efficiency and improved safety) to justify their use. Researchers were also hesitant to automate data analysis, and the importance of explainability differed between researchers working with laboratory automation and those using AI/ML directly in their research. This difference is due to the different role AI/ML plays in different research fields: laboratory automation performs processes already defined by the researcher and the actions are visible or recorded, while in AI/ML applications the decisions that produced the result may be obscure to the researcher. Understanding the role AI/ML plays in scientific practice is important for ensuring that scientific knowledge continues to grow.","",""
"2025","Reflections on the AI alignment problem","","",""
"2025","When discussing the desirability of religious robots: courage for theology!","","",""
"2025","Socratic nudges, virtual moral assistants and the problem of autonomy","Abstract           Many of our daily activities are now made more convenient and efficient by virtual assistants, and the day when they can be designed to instruct us in certain skills, such as those needed to make moral judgements, is not far off. In this paper we ask to what extent it would be ethically acceptable for these so-called virtual assistants for moral enhancement to use subtle strategies, known as “nudges”, to influence our decisions. To achieve our goal, we will first characterise nudges in their standard use and discuss the debate they have generated around their possible manipulative character, establishing three conditions of manipulation. Secondly, we ask whether nudges can occur in moral virtual assistants that are not manipulative. After critically analysing some proposed virtual assistants, we argue in favour of one of them, given that by pursuing an open and neutral moral enhancement, it promotes and respects the autonomy of the person as much as possible. Thirdly, we analyse how nudges could enhance the functioning of such an assistant, and evaluate them in terms of their degree of threat to the subject’s autonomy and their level of transparency. Finally, we consider the possibility of using motivational nudges, which not only help us in the formation of moral judgements but also in our moral behaviour.","",""
"2025","Affective neuroscience theory and attitudes towards artificial intelligence","Abstract           Artificial intelligence represents a key technology being inbuilt into evermore products. Research investigating attitudes towards artificial intelligence surprisingly is still scarce, although it becomes apparent that artificial intelligence will shape societies around the globe. To better understand individual differences in attitudes towards artificial intelligence, the present study investigated in n = 351 participants associations between the Affective Neuroscience Personality Scales (ANPS) and the Attitudes towards Artificial Intelligence framework (ATAI). It could be observed that in particular higher levels of SADNESS were associated with higher negative attitudes towards AI (fearing AI). The findings are discussed in this work from an evolutionary perspective because primary emotional systems—according to Affective Neuroscience Theory—represent tools for survival, which have been homologously conserved across mammalian species including homo sapiens.","",""
"2025","Understanding via exemplification in XAI: how explaining image classification benefits from exemplars","Abstract           Artificial intelligent (AI) systems that perform image classification tasks are being used to great success in many application contexts. However, many of these systems are opaque, even to experts. This lack of understanding can be problematic for ethical, legal, or practical reasons. The research field Explainable AI (XAI) has therefore developed several approaches to explain image classifiers. The hope is to bring about understanding, e.g., regarding why certain images are classified as belonging to a particular target class. Most of these approaches use visual explanations. Drawing on Elgin’s work (True enough. MIT Press, Cambridge, 2017), I argue that analyzing what those explanations exemplify can help to assess their suitability for producing understanding. More specifically, I suggest to distinguish between two forms of examples according to their suitability for producing understanding. I call these forms samples and exemplars, respectively. Samples are prone to misinterpretation and thus carry the risk of leading to misunderstanding. Exemplars, by contrast, are intentionally designed or chosen to meet contextual requirements and to mitigate the risk of misinterpretation. They are thus preferable for bringing about understanding. By reviewing several XAI approaches directed at image classifiers, I show that most of them explain with samples. If my analysis is correct, it will be beneficial if such explainability methods use explanations that qualify as exemplars.","",""
"2025","AI-generated art and fiction: signifying everything, meaning nothing?","","",""
"2025","The sociotechnical entanglement of AI and values","Abstract           Scholarship on embedding values in AI is growing. In what follows, we distinguish two concepts of AI and argue that neither is amenable to values being ‘embedded’. If we think of AI as computational artifacts, then values and AI cannot be added together because they are ontologically distinct. If we think of AI as sociotechnical systems, then components of values and AI are in the same ontologic category—they are both social. However, even here thinking about the relationship as one of ‘embedding’ is a mischaracterization. The relationship between values and AI is best understood as a dimension of the relationship between technology and society, a relationship that can be theorized in multiple ways. The literature in this area is consistent in showing that technology and society are co-productive. Within the co-production framework, the relationship between values and AI is shown to be generative of new meaning. This stands in stark contrast to the framework of ‘embedding’ values which frames values as fixed things that can be inserted into technological artifacts.","",""
"2025","Artificial intelligence (AI)-poverty-economic growth nexus in selected BRICS-Plus countries: does the moderating role of governance matter?","Abstract           The BRICS nations (Brazil, Russia, India, China, and South Africa) aim to achieve Sustainable Development Goal (SDG) 1 (poverty eradication) and SDG 8 (sustainable economic growth), yet the moderating role of governance in artificial intelligence (AI)-poverty-growth nexus remains underexplored. Therefore, this study investigates the AI-poverty-economic growth nexus in selected BRICS-Plus countries (2012–2023), with governance as a moderating variable, using the Cross-Sectional Augmented Autoregressive Distributed Lag (CS-ARDL) technique. The results show a long-term equilibrium among variables, with unidirectional causality: (i) from growth to AI, and (ii) from AI to poverty and governance quality. The findings highlight AI’s transformative potential in tackling poverty and governance issues, with economic growth enabling AI advancements. This underscores the critical need to integrate AI within governance frameworks to address development challenges effectively. The short-run CS-ARDL results for the growth model indicate that AI and governance boost growth, though their interaction diminishes AI's impact. In the long-run, both sustain growth, with stricter governance moderating AI's potential. For the poverty model, AI increases poverty in the short-run, while governance reduces poverty by improving resource allocation and mitigating AI's impacts. The interaction between AI and governance highlights their role in moderating AI’s adverse effects. In the long-run, AI modestly worsens poverty, while governance alleviates poverty by promoting growth and redistributing AI-driven gains. The policy implications stress improving governance to balance AI’s economic benefits and mitigate poverty, emphasizing equitable resource allocation to harness AI’s potential for sustainable growth.","",""
"2025","Transparency and accountability: unpacking the real problems of explainable AI","","",""
"2025","Challenges of responsible AI in practice: scoping review and recommended actions","Abstract           Responsible AI (RAI) guidelines aim to ensure that AI systems respect democratic values. While a step in the right direction, they currently fail to impact practice. Our work discusses reasons for this lack of impact and clusters them into five areas: (1) the abstract nature of RAI guidelines, (2) the problem of selecting and reconciling values, (3) the difficulty of operationalising RAI success metrics, (4) the fragmentation of the AI pipeline, and (5) the lack of internal advocacy and accountability. Afterwards, we introduce a number of approaches to RAI from a range of disciplines, exploring their potential as solutions to the identified challenges. We anchor these solutions in practice through concrete examples, bridging the gap between the theoretical considerations of RAI and on-the-ground processes that currently shape how AI systems are built. Our work considers the socio-technical nature of RAI limitations and the resulting necessity of producing socio-technical solutions.","",""
"2025","Redefining intelligence: collaborative tinkering of healthcare professionals and algorithms as hybrid entity in public healthcare decision-making","AbstractThis paper analyzes the collaboration between healthcare professionals and algorithms in making decisions within the realm of public healthcare. By extending the concept of ‘tinkering’ from previous research conducted by philosopher Mol (Care in practice. On tinkering in clinics, homes and farms Verlag, Amsterdam, 2010) and anthropologist Pols (Health Care Anal 18: 374–388, 2009), who highlighted the improvisational and adaptive practices of healthcare professionals, this paper reveals that in the context of digitalizing healthcare, both professionals and algorithms engage in what I call ‘collaborative tinkering’ as they navigate the intricate and unpredictable nature of healthcare situations together. The paper draws upon an idea that is increasingly common in academic literature, namely that healthcare professionals and the algorithms they use can form a hybrid decision-making entity, challenging the conventional notion of agency and intelligence as being exclusively confined to individual humans or machines. Drawing upon an international, ethnographic study conducted in different hospitals around the world, the paper describes empirically how humans and algorithms come to decisions together, making explicit how, in the practice of daily work, agency and intelligence are distributed among a range of actors, including humans, technologies, knowledge resources, and the spaces where they interact. The concept of collaborative tinkering helps to make explicit how both healthcare professionals and algorithms engage in adaptive improvisation. This exploration not only enriches the understanding of collaborative dynamics between humans and AI but also problematizes the individualistic conception of AI that still exists in regulatory frameworks. By introducing empirical specificity through ethnographic insights and employing an anthropological perspective, the paper calls for a critical reassessment of current ethical and policy frameworks governing human–AI collaboration in healthcare, thereby illuminating direct implications for the future of AI ethics in medical practice.","",""
"2025","Mortality, belonging, and the paradox of immortality: reflections on the role of AI","","",""
"2025","How effective are depictions of AI? Reflections from an experimental study in science communication","","",""
"2025","Rethinking assessment: how AI is changing the way we measure student success?","","",""
"2025","Interplay of factors determining users’ intentions to adopt chatbots for airline tickets assistance. The moderating role of perceived waiting time","","",""
"2025","Curiosity killed the cat? From a masculinized ‘frontier mindset’ to ethical curiosity in AI engineering","Abstract           Curiosity is having its moment in AI engineering. Governments and Big Tech alike frame the trait as a key characteristic of data scientists. This paper offers a qualitative analysis of the perspectives of AI engineers on the importance of curiosity in their profession. The results of this study—which took place at a technology multinational of over 250 k + employees—warn that unless curiosity is carefully defined, detached from masculinized interpretations of what it entails, and linked to ethical development practices, the current emphasis on curiosity risks promoting harmful curiosity-based engineering practices. The paper begins by exposing the consequences of curiosity’s configuration within existing corporate and social inequalities. It then reviews theoretical literature on the gendering of skills and attributes in AI engineering. Having synthesized evidence of curiosity’s importance in corporate AI engineering, it draws on new qualitative data to draw out key themes that might be contributing to the gendering of curiosity, before exploring these themes in their wider context through the theoretical literature. It ends by outlining ways that curiosity-based engineering could promote rather than hinder ethical AI.","",""
"2025","Re-evaluating creative labor in the age of artificial intelligence: a qualitative case study of creative workers’ perspectives on technological transformation in creative industries","Abstract           This article explores how the emergence of creative AI technologies transforms creative workers’ self-apprehension in the context of critical theory and labor studies. The distinguishing contribution of this study resides in its focus on how CI laborers’ creativity perception and reception are affected by AI technologies’ intrusion into the creative domain. Creative AI technologies are expected to present new expressive capacities to creative workers and cost-cutting advantages for CIs’ production that  put a lot of creative jobs at risk. Findings show that creatives perceive the adaptation of AI technologies as both an opportunity for their creative process and a requirement of their active presence in the market survival as a matter of technocratic rule. We critically analyze creative labor’s novel mods engaged with updated technology and present reflections on the favorable co-creation conditions to flourish an understanding of socially intelligible technology and thereby a creative livelihood against technocracy.","",""
"2025","Simon Lindgren – A critical theory of AI","","",""
"2025","AI research assistants, intrinsic values, and the science we want","","",""
"2025","Synthetic media and computational capitalism: towards a critical theory of artificial intelligence","Abstract           This paper develops a critical theory of artificial intelligence, within a historical constellation where computational systems increasingly generate cultural content that destabilises traditional distinctions between human and machine production. Through this analysis, I introduce the concept of the algorithmic condition, a cultural moment when machine-generated work not only becomes indistinguishable from human creation but actively reshapes our understanding of ideas of authenticity. This transformation, I argue, moves beyond false consciousness towards what I call post-consciousness, where the boundaries between individual and synthetic consciousness become porous. Drawing on critical theory and extending recent work on computational ideology, I develop three key theoretical contributions, first, the concept of the Inversion to describe a new computational turn in algorithmic society; second, automimetric production as a framework for understanding emerging practices of automated value creation; and third, constellational analysis as a methodological approach for mapping the complex interplay of technical systems, cultural forms and political economic structures. Through these contributions, I argue that we need new critical methods capable of addressing both the technical specificity of AI systems and their role in restructuring forms of life under computational capitalism. The paper concludes by suggesting that critical reflexivity is needed to engage with the algorithmic condition without being subsumed by it and that it represents a growing challenge for contemporary critical theory.","",""
"2025","Think Miss Piggy, not Pinocchio: debunking the myth of ‘autonomous’ AI","","",""
"2025","The hidden threat of Argentina’s AI policing","","",""
"2025","Justice in the age of algorithms: can AI weigh morality?","","",""
"2025","Dual use research and artificial intelligence","","",""
"2025","History repeats itself, first as BPR, second as generative AI","","",""
"2025","Abstaining machine learning: philosophical considerations","Abstract           This paper establishes a connection between the fields of machine learning (ML) and philosophy concerning the phenomenon of behaving neutrally. It investigates a specific class of ML systems capable of delivering a neutral response to a given task, referred to as abstaining machine learning systems, that has not yet been studied from a philosophical perspective. The paper introduces and explains various abstaining machine learning systems, and categorizes them into distinct types. An examination is conducted on how abstention in the different machine learning system types aligns with the epistemological counterpart of suspended judgment, addressing both the nature of suspension and its normative profile. Additionally, a philosophical analysis is suggested on the autonomy and explainability of the abstaining response. It is argued, specifically, that one of the distinguished types of abstaining systems is preferable as it aligns more closely with our criteria for suspended judgment. Moreover, it is better equipped to autonomously generate abstaining outputs and offer explanations for abstaining outputs when compared to the other type.","",""
"2025","Gender differences in creative workers’ general attitudes toward artificial intelligence painting tools","","",""
"2025","How will the state think with ChatGPT? The challenges of generative artificial intelligence for public administrations","","",""
"2025","Framing the unframable: why AI art is a battle of metaphors","","",""
"2025","Ethical and epistemic implications of artificial intelligence in medicine: a stakeholder-based assessment","Abstract           As artificial intelligence (AI) technologies become increasingly embedded in high-stakes fields such as healthcare, ethical and epistemic considerations raise the need for evaluative frameworks to assess their societal impacts across multiple dimensions. This paper uses the ethical-epistemic matrix (EEM), a structured framework that integrates both ethical and epistemic principles, to evaluate medical AI applications more comprehensively. Building on the ethical principles of well-being, autonomy, justice, and explicability, the matrix introduces epistemic principles—accuracy, consistency, relevance, and instrumental efficacy—that assess AI’s role in knowledge production. This dual approach enables a nuanced assessment that reflects the diverse perspectives of stakeholders within the medical field—patients, clinicians, developers, the public, and health policy-makers—who assess AI systems differently based on distinct interests and epistemic goals. Although the EEM has been outlined conceptually before, no published research paper has yet used it explore the ethical and epistemic implications arising in its key intended application domain of AI in medicine. Through a systematic demonstration of the EEM as applied to medical AI, this paper argues that it encourages a broader understanding of AI’s implications and serves as a valuable methodological tool for evaluating future uses. This is illustrated with the case study of AI systems in sleep apnea detection, where the EEM highlights the ethical trade-offs and epistemic challenges that different stakeholders may perceive, which can be made more concrete if the tool is embedded in future technical projects.","",""
"2025","Beyond symbol processing: the embodied limits of LLMs and the gap between AI and human cognition","","",""
"2025","Trust influence on AI HR tools perceived usefulness in Swiss HRM: the mediating roles of perceived fairness and privacy concerns","Abstract           This study looks at factors influencing, first, trust in artificial intelligence (AI) systems in human resources management, second, perceived usefulness of these tools. Based on a survey experiment provided to 324 private and public Swiss HR professionals’, it first explores how some trust in automation framework’s predictors are related to trust in HR AI tools and, then, how this trust is in return related to UTAUT’s perceived usefulness of these AI-enhanced tools. To do this, the following article is based on a PLS-SEM structural equation model. Its main findings are that reliability, familiarity, intention of developers and propensity to trust are directly positively related to trust in the HR AI tools studied here. Nevertheless, public employees declare more negative feelings toward AI in HRM. Indeed, the latter systematically have less trust in HR AI than private employees. However, public sector employees do not find them any less useful or efficient than private sector employees, except when it comes to the HR AI tools used to assess employee performance and behavior. In addition to this, trust in these tools is systematically positively linked to their perceived usefulness. This influence is partly mediated by the perceived decision fairness of our tools, but not by the absence of privacy concerns associated with them. This said, this article makes a significant contribution to the literature about private and public actors’ perceptions of nascent HR AI-enhanced tools.","",""
"2025","Three tragedies that shape human life in age of AI and their antidotes","","",""
"2025","The prediction of non-ergodic humanity by artificial intelligence","Abstract           This article aims to affirm and instantiate the main accounts showing intrinsic limitations of artificial intelligence computing in a real world of organisms, people and speech. It is argued that these limits mainly concern non-ergodic (or non-repeating) phenomena. This paper aims to extend the debate on the limits of AI through a preliminary examination of the dispersion of both regularities and non-ergodic phenomena and processes in both society and human persons. It is argued that regularities and non-ergodic processes are deeply intertwined. Social regularity, for example from the built environment and conformity, is discussed. In society, non-ergodicity is especially found in the lifeworld of speech and intersubjectivity. The human person creates non-ergodicity through numerous routes. Individual regularities are seen in things such as habit and routine. This study asserts that human intersubjective life in the often nonergodic lifeworld and inbuilt non-repeating dimensions of an individual’s living out of the world, should be recognized as extensive areas where AI prediction will be weak. It is hypothesized that the intensity of non-ergodicity in phenomena is a firm indicator of weak AI prediction, and that most successful AI prediction of social phenomena predominantly reflects the sort of social regularities discussed in this article.","",""
"2025","Between fact and fairy: tracing the hallucination metaphor in AI discourse","Abstract           Large and powerful language models such as OpenAI’s GPT model family, Google’s LaMDA and BERT or Meta’s LlaMA are integral to many applications, such as translation, summarization or language generation. They have become an inherent part of current everyday activities and working practices. These models produce and process language in an impressively convincing human-like manner, but also repeatedly generate outputs that appear untrustworthy and factually incorrect. In computer science (Ji et al. 2023) and popular discourse alike this phenomenon is called hallucination. The term is used broadly to describe various forms of untruthfulness, from factual errors to inconsistencies between prompt and output. This article discusses the hallucination metaphor guided by STS perspectives and takes software documentation as its main corpus of analysis. We examine model papers and documentation from leading tech companies to trace the hallucination metaphor and the discursive work it does. We claim that tech companies anthropomorphize the models, relieving them and themselves from responsibility over non-factual outputs by normalizing the use of the metaphor. Models are relegated to two main positions: either a learning child that needs time to develop or an illogic agent, a position that we connect to cultural scripts of madness.","",""
"2025","Alexa, Google Assistant, and Siri’s language options: how voice assistants reproduce monoglossic language ideologies","","",""
"2025","Mapping out AI functions in intelligent disaster (mis)management and AI-caused disasters","","",""
"2025","From aura to semi-aura: reframing authenticity in AI-generated art—a systematic literature review","Abstract           The advent of AI-generated art necessitates a re-examination of the concept of “aura,” as originally posited by Walter Benjamin, and challenges prevailing perceptions of authenticity and originality in art. This systematic review addresses a critical gap in existing literature by exploring how AI reshapes these foundational concepts, positioning this study within an emergent and under-investigated field. While Benjamin’s aura historically conveys an irreplaceable quality inherent to unique artworks, AI-generated pieces blur the lines between original and reproduction, fundamentally questioning established aesthetic and ontological values. Through an interdisciplinary synthesis of ethical, legal, and philosophical perspectives, this review identifies polarized views: some scholars advocate AI’s democratizing effect on creativity, while others criticize its perceived lack of emotional depth and authenticity. Additionally, human-AI collaborations are highlighted as a fertile area for expanding traditional artistic practices, suggesting an emergent, hybridized form of aura that stems from the synergy of human intention and machine execution. By filling a gap in current scholarship, this study provides a robust foundation for future empirical research, inviting a reconceptualization of authorship, value, and aesthetic experience in the digital art landscape.","",""
"2025","Multimodal AI can teach you the word, but can it teach you the world?","","",""
"2025","All watched over by machines of loving grace—AI and control, symbolic violence, and the hyperreal","","",""
"2025","The future of tutoring: AI as an augmenting force","","",""
"2025","Science as a vocation redux: outsourcing the logic of discovery to AI","","",""
"2025","Human centered systems start with social dynamics and arrive at ontology","Abstract           The purpose of this research is to support and nurture tacit knowledge whilst simultaneously leading to the development of machine-based intelligent systems which incorporate machine readable knowledge for the benefit of society. This paper starts with an introduction to the persistent power struggle between human and technology and shines a light on Professor Michael Cooley’s involvement with the Lucas Plan in the 1970s and his PhD work which focused on the transition from manual draftsmanship to Computer Aided Design in engineering. A research lab is identified as a ‘complex adaptive system’ and forms the basis of a longitudinal case study on the Human Centered bottom-up approach to digitisation of cultural heritage. Components required to support and nurture the growth of a Participation Action Research lab are identified. The novel ‘ENRICHER’ method embodies human centeredness and is operationalized, tested, evaluated and findings discussed. Examples of emergence are also discussed. A metric of the ENRICHER method initially identified where the lab did not fully meet all the methods 8 points. Subsequent actions adjusted the holonic lens focus to metadata and the ongoing work on the creation of a cataloging tool for the librarians. The use of XML technologies integrates the work into a larger model of intelligence. It positions the work on the semantic web technology stack and opens up the pathway to ontology generation and development and management of large language models. The ENRICHER method is a way of developing human–machine symbiotics that also incorporate AI e.g. transcription, metadata generation.","",""
"2025","Another reason to call bullshit on AI “hallucinations”","","",""
"2025","A polycrisis threat model for AI","Abstract           A catastrophic AI threat model is a rigorous exploration of some particular mechanisms by which AI could potentially lead to catastrophic outcomes. In this article, I explore a polycrisis threat model. According to this model, AI will lead to a series of harms like disinformation and increased concentration of wealth and power. Interactions between these different harms will make things worse than they would have been had each harm operated in isolation. And the interacting harms will ultimately cause or constitute a catastrophe. My aim in this paper is not to defend the inevitability of such a polycrisis occurring. Instead, I aspire merely to establish that polycrisis-driven catastrophe is sufficiently plausible that it calls for further exploration. In doing so, I hope to emphasise that alongside worries about AI takeover, those concerned about catastrophic risk from AI should also take seriously worries about extreme power concentration and systemic disempowerment of humanity.","",""
"2025","Moral disagreement and the limits of AI value alignment: a dual challenge of epistemic justification and political legitimacy","Abstract           AI systems are increasingly in a position to have deep and systemic impacts on human wellbeing. Projects in value alignment, a critical area of AI safety research, must ultimately aim to ensure that all those who stand to be affected by such systems have good reason to accept their outputs. This is especially challenging where AI systems are involved in making morally controversial decisions. In this paper, we consider three current approaches to value alignment: crowdsourcing, reinforcement learning from human feedback, and constitutional AI. We argue that all three fail to accommodate reasonable moral disagreement, since they provide neither good epistemic reasons nor good political reasons for accepting AI systems’ morally controversial outputs. Since these appear to be the most promising approaches to value alignment currently on offer, we conclude that accommodating reasonable moral disagreement remains an open problem for AI safety, and we offer guidance for future research.","",""
"2025","The next media-fueled moral technology panic? News media’s and audience’s views on ChatGPT","Abstract           With the diffusion of new technologies, “moral panics” often tend to emerge, with news outlets frequently seen as catalysts. However, empirical evidence on their specific role still remains scarce. The rise of ChatGPT offers a unique chance to observe a new form of moral technology panic in real time. Using a longitudinal survey of a Dutch quota sample and computational content analysis of Dutch news coverage on ChatGPT, we examined whether both news attention and public perceptions of ChatGPT’s societal impact increased and became more negative over time. We linked survey-based exposure measures with automated content-analysis-based news characteristics to assess the news media’s role in moral panic creation. While the public quickly became aware of ChatGPT, we found no clear indication of a moral panic in the news or among respondents. Moreover, news exposure ultimately did not influence perceptions of ChatGPT’s societal impact. We discuss implications for moral panic theories.","",""
"2025","AI-driven hiring: a boon or a barrier to finding the right talent?","","",""
"2025","Welfare models and AI regulation: preferences of university students in the UK and Poland","Abstract           The growing uptake of artificial intelligence technologies in societies highlights the importance of understanding the regulatory preferences of different societal segments toward their regulation. We argue that a wider sociopolitical context shapes it and explore this argument using Esping-Andersen’s welfare state regime typology and a survey of university students from two distinct welfare regimes. Our findings reveal important differences and the effect of age on preference for the role of the state. Students in the UK favor decentralized, individualistic regulations, aligning with the liberal welfare model, while students in Poland support a balanced approach, consistent with a post-socialist welfare model.","",""
"2025","Testimony by LLMs","Abstract           Artificial testimony generated by large language models (LLMs) can be a source of knowledge. However, the requirement that artificial testifiers must satisfy for successful knowledge acquisition is different from the requirement that human testifiers must satisfy. Correspondingly, the epistemic ground of artificial testimonial knowledge is not the well-known and accepted ones suggested by renowned epistemological theories of (human) testimony. Based on Thomas Reid’s old teaching, we suggest a novel epistemological theory of artificial testimony that for receivers to justifiably believe artificially generated statements, testifiers of the statement should robustly perform the propensities of veracity and cautiousness. The theory transforms the weakness of Reid’s view to an advantage of its own. It sets an achievable standard for LLMs and clarifies the improvement that current LLMs should make for meeting the standard. Moreover, it indicates a pluralistic nature of testimonial justification pertaining to the pluralistic nature of possible testifiers for knowledge transmission.","",""
"2025","Ethical approval and informed consent in mental health research: a scoping review","Abstract           Although there is a wide range of scientific papers introducing artificial intelligence techniques in the mental health field, there is a lack of literature assessing the reporting of ethical concerns in such studies. In addition, it is not yet known whether the authors seek ethical approval or informed consent while performing such research. This study aimed to investigate the extent to which studies in the mental health domain that utilize chatbots either ignore or incompletely disclose patient consent and ethical approval from the responsible review boards. A scoping literature search was performed in PsychARTICLES, PubMed, and Web of Science using both MeSH terms and free-text keywords. Following PRISMA-ScR guidelines, we also contacted study authors to verify missing information about ethical approval or informed consent, enhancing the transparency and rigor of our analysis. Among the 27 studies reviewed, 13 reported obtaining ethical approval, and 16 reported collecting informed consent. The remaining studies did not provide such information. These findings underscore the ethical complexities surrounding AI in mental health, especially regarding the collection, storage, and use of sensitive patient data. There is a correlation between sample size and the acquisition of ethical approval, particularly in studies published in journals with low-impact factors. Future research should investigate the role of journal policies in influencing ethical practices. In addition, training programs could be developed to educate researchers on the importance of ethics, particularly in studies with smaller sample sizes.","",""
"2025","Bullshit universities: the future of automated education","Abstract           The advent of ChatGPT, and the subsequent rapid improvement in the performance of what has become known as Generative AI, has led to many pundits declaring that AI will revolutionize education, as well as work, in the future. In this paper, we argue that enthusiasm for the use of AI in tertiary education is misplaced. A proper understanding of the nature of the outputs of AI suggests that it would be profoundly misguided to replace human teachers with AI, while the history of automation in other settings suggests that it is naïve to think that AI can be developed to assist human teachers without replacing them. The dream that AI could teach students effectively neglects the importance of ‘learning how’ in order to ‘learn that’, that teachers are also role models, and the social nature of education. To the extent that students need to learn how to use AI, they should do so in specialized study skills units. Rather than creating a market for dodgy educational AI by lowering their ambitions about what they can offer, universities should invest in smaller class sizes and teachers who are passionate about their disciplines. To flourish in the future, just as much as they do today, societies will need people who have learned to think and not—or not just—intelligent machines.","",""
"2025","Can AI help make California police policy human centered?","","",""
"2025","AI, journalism, and critical AI literacy: exploring journalists’ perspectives on AI and responsible reporting","Abstract           This study explores the perspectives of media professionals on the concerns, needs, and responsibilities related to fostering AI literacy among journalists. We report on findings from two workshops with journalists (based in the USA, the UK, China, and India), as well as representatives of civil society organizations and academic specialists in media and AI literacy. Through a reflexive qualitative analysis of data collected during the workshops, we examine the obstacles to AI literacy development among journalists and the quality of resources currently available to them for learning about AI and AI ethics. We highlight the most pressing needs in AI-focused education for journalists and surface participants’ ideas for potential solutions, including an authoritative online compendium on AI and journalism and a database of diverse expert voices. We point to the areas where relevant stakeholders should direct their efforts to support journalists in navigating AI responsibly and critically.    ","",""
"2025","Bridging perspectives on artificial intelligence: a comparative analysis of hopes and concerns in developed and developing countries","","",""
"2025","Exploring acceptability of AI-enabled voice assistants and digital AI humans in healthcare: a cross-sectional survey","Abstract           Artificial intelligence, including Digital AI Humans (DHs) and Voice Assistants (VAs), offers new opportunities for healthcare delivery but may widen inequalities. This cross-sectional online survey examined factors influencing the acceptability of these technologies among 472 UK adults, considering demographics, digital literacy, healthcare access, familiarity with DHs and VAs, personality traits, and attitudes.           VA acceptability was assessed using logistic regression, with willingness to use VAs as the outcome variable. Lower acceptance was found among women, ethnic minorities, those with lower education levels, and individuals who infrequently searched for health information online. Conversely, higher acceptance was associated with engagement in online health discussions, greater awareness and use of VAs, perceived usefulness, fewer perceived barriers, and openness. DH acceptability was analysed through multiple regression, with attitudes toward DHs as the outcome variable. More positive attitudes were linked to White/Irish/European ethnicity, a greater perceived need for in-person care, participation in online health discussions, higher conscientiousness, and lower neuroticism, explaining 27.8% of the variance. Although 85.8% had used VAs and 82.2% owned one, only 25.8% reported daily use. Awareness of DHs was reported by 70.3% of participants, with attitudes generally positive (median score: 2.17/5, where lower scores indicate greater favourability). Institutional endorsement was a key factor, with 71.2% stating they would use VAs for healthcare if approved by the NHS. These findings support technology acceptance models, highlighting the roles of perceived usefulness, ease of use, and awareness. Culturally responsive design principles that address these factors may enhance adoption across diverse groups. Distinct personality traits influenced acceptance, with openness predicting VA acceptability, while conscientiousness and low neuroticism were associated with more positive attitudes toward DHs. While offering novel insights into human factors influencing AI adoption in healthcare, the study is limited by its reliance on proxy measures for acceptance.","",""
"2025","Deep learning as machine metis","Abstract           This article situates current deep learning (DL) artificial intelligence (AI) within Leroi-Gourhan’s deep history of the human species’ relation to technology. According to Leroi-Gourhan, technology is both a key element of anthropogenesis and a source of later tensions (or disentanglement) between the human species and its external and increasingly autonomous technics. Human organic (life-oriented) intelligence at first extends itself through technical (machine-oriented) intelligence, only to be later left behind by it. We propose a concept of machine intelligence that goes beyond technical intelligence, the latter a (still) hybrid human–machine intelligence. This new, emerging machine intelligence is DL AI. DL AI developed out of the failure of symbolic AI to instantiate a key generic component of intelligence: creativity. While symbolic AI was rigid and pre-programmed, DL is flexible and unpredictable, presenting an embryonic form of actual machine intelligence. Its creativity can be likened to the ancient Greek concept of metis, a cunning and polymorphous form of intelligence. Although often biased and problematic, DL exhibits a machine creativity that goes beyond the anthropocentric imaginings of AI as a (mechanistic) imitation of the human norm.","",""
"2025","The transformation of artistic creation: from Benjamin’s reproduction to AI generation","Abstract           This article examines the transformative impact of AI-based art generators by extending Walter Benjamin’s arguments on mechanical reproduction to the digital age. While Benjamin examined how mechanical reproduction affected works created with clear human intentionality, AI generated art introduces a fundamentally different dynamic through ‘distributed agency’ across human prompters, algorithmic interpretation mechanisms, and collective training datasets.           Through an analysis of four key examples that illustrate different aspects of AI’s influence on artistic practice—generative AI art platforms, the Portrait of Edmond de Belamy, Refik Anadol’s Archive Dreaming, and The 2023 Sony World Photography Awards controversy—the study advances four interconnected arguments: first, that generative AI reconfigures creative agency beyond traditional human-centered models; second, that AI establishes new dialogic relationships between creators, artworks, and audiences; third, that algorithmic generation differs fundamentally from mechanical reproduction by creating novel interpretative expressions rather than duplicating existing works; and fourth, that AI transforms the societal dimensions of artistic production through a dialectical relationship between democratization and proletarianization.           By critically extending Benjamin’s framework to address contemporary technological conditions, this study provides theoretical foundations for understanding art in an age of algorithmic creation. The findings reveal how AI both fulfills and challenges Benjamin’s predictions about technological art reproduction while creating new epistemic and sociotechnical configurations that require reconceptualizing traditional notions of artistic authenticity, creative agency, and cultural preservation in an era of increasing algorithmic mediation.","",""
"2025","A sociotechnological-system approach to AI ethics","Abstract           AI algorithms require human input to achieve technological aims. This fact is often overlooked in discussions of autonomous systems and AI safety, to the detriment of both philosophical discourse and practical progress. One potential remedy is to ground our theorizing more fundamentally in the idea that AI technologies are sociotechnological systems with human and artifactual components. In this article, I pursue this strategy, aiming to shift the focus in AI ethics from artifacts and their intrinsic properties—what I refer to as the robotic conception of AI—to the relationships among elements embedded in AI-involving sociotechnological systems. First, I defend the claim that the sociotechnological-system perspective provides an accurate description of some of our most advanced AI. Second, I argue that the dominance of the robotic conception has steered AI safety research down unproductive paths, while the sociotechnological perspective has the capacity to set us right. Specifically, the robotic conception encourages the development of artificial moral agents—whose creation we should avoid if possible—and distracts researchers with hypothetical trolley cases. In contrast, the sociotechnological approach coheres with actual progress being made on AI safety (e.g., networking, shared user-artifact control, and value alignment) and makes vivid solutions to the safety problem that do not require the creation of humnanlike moral decision-makers.","",""
"2025","Fear of artificial intelligence or fear of looking in the mirror? Revisiting the Western machine-takeover imaginary","Abstract           What do we fear when we fear AI? This paper presents the claim that robophobia is autophobia, the fear of AI is the fear of ourselves, in at least two fashions. First, I frame the question in relation to what I call the Western machine-takeover imaginary, and identify two historical tracks: (1) the fear of non-human autonomy and (2) the logic of the genie. The first track is rooted in the idea of the domination of creations and their possible revolt, a trope that reflects the anthropogenetic narratives that guide Western self-understanding. The second track establishes a link between cultural understanding of non-human, wish-fulfilling others, and modern AI drones and chatbots in terms of potential asymmetries between inputs and outputs, instructions and execution. When biased inputs inform system outputs that inform human inputs in a hermeneutical circle, the outputs we fear cannot be separated from the inputs we generate.","",""
"2025","Trust in artificial intelligence: a survey experiment to assess trust in algorithmic decision-making","Abstract           Artificial intelligence (AI) has seen rapid development over the past decade, leading to its integration into various aspects of human life. The ability to integrate AI systems hinges not solely on their technical efficacy but also on the perceptions held by users or decision-makers. Previous researches indicate that many people harbor concerns about AI, which can hinder the adoption of these technologies. This study uses a pre-registered survey experiment embedded in an online survey in Hungary (N = 2100) to assess trust in AI-based Automated Decision-Making (ADM). Participants were presented with hypothetical decisions in four domains (medical diagnoses, hiring, transportation, and financial investments). In a split-ballot design, participants were randomly assigned to a control group with human involvement and an experimental group where decision were supported by AI-based ADM. The main results show that decisions supported by human intervention are perceived as more trustworthy than those made by ADM (except for financial investment). However, our treatment heterogeneity analysis indicates that these effects are not consistent across all segments of society. A good understanding of AI, low privacy concerns, and an open personality can mitigate the negative impact of AI assistance on trust.","",""
"2025","Modern Prometheus: tracing the ill-defined path to AGI","Abstract           This article traces the conceptual lineage of AGI through three influential paradigms: strong AI, human-level AI, and AGI—highlighting the philosophical and operational tensions shaping contemporary debates. The article examines emerging reorientations toward operational benchmarks that decouple artificial intelligence from human-like cognition. It proposes a novel taxonomy to evaluate divergent developmental trajectories—Promethean and Noctuidean orientations—and argues that anthropocentric biases may limit recognition of diverse intelligences and amplify risks in AI deployment. Through careful analysis of historical contexts, philosophical arguments, recent technological advances, and emerging legal disputes, this article underscores an urgent need to clarify definitions and goals in the race to AGI.","",""
"2025","The politics within algorithms and the challenge of fairness","","",""
"2025","The impacts of companion AI on human relationships: risks, benefits, and design considerations","","",""
"2025","Ethical aspects of AI use in the circular economy","","",""
"2025","Are Turkish pre-service teachers worried about AI? A study on AI anxiety and digital literacy","Abstract           The primary objective of this study is to determine whether the level of digital literacy among pre-service teachers reliably correlates with their anxiety levels concerning artificial intelligence. The study was conducted as a correlational study, with a sample size of 221 pre-service teachers. The study’s population comprised 3922 pre-service teachers enrolled at Turkish state and private universities. To collect study data, the researchers used the “Personal Information Form,” “Digital Literacy Scale,” and “Artificial Intelligence Anxiety Scale.” The data were analyzed using stepwise regression analysis and descriptive statistics. The study’s results indicated that pre-service teachers exhibited high levels of digital literacy and moderate degrees of anxiety regarding artificial intelligence. Regression analysis revealed that 10.3% of pre-service teachers’ anxiety concerning artificial intelligence could be predicted by the technical sub-dimension of digital literacy. Consequently, it was demonstrated that pre-service teachers’ apprehensions regarding artificial intelligence decreased as their perception of technical digital fluency increased. Other sub-dimensions of digital literacy were deemed insignificant in predicting the anxiety levels of pre-service teachers regarding artificial intelligence. Based on these findings, suggestions for future study directions were proposed.","",""
"2025","AI for climate change adaptation: analyzing Machine Learning’s role in combating California’s wildfires","","",""
"2025","The digital erosion of intellectual integrity: why misuse of generative AI is worse than plagiarism","","",""
"2025","Correction: AI rule and a fundamental objection to epistocracy","","",""
"2025","What factors predict user acceptance of ChatGPT for mental and physical healthcare: an extended technology acceptance model framework","Abstract           The rise of ChatGPT has emphasized the need for an improved conceptual understanding of users’ agency when interacting with artificial intelligence (AI) systems for healthcare. Australian ChatGPT users (N = 216) completed a repeated measures online survey. Hierarchical regression analyses assessed the influence of demographic factors (age and gender), Technology Acceptance Model constructs (perceived usefulness and perceived ease of use), and extended variables (trust, privacy concerns) on users' behavioral intentions to use ChatGPT for physical and mental healthcare. The proposed model was partially supported: the findings emphasized the need to establish user trust in ChatGPT and its perceived usefulness in both areas of healthcare. Privacy concerns were a significant predictor of intentions to use ChatGPT for mental healthcare with perceived ease of use predicting intentions to use ChatGPT for physical healthcare. The findings indicate predictors of uses of AI cannot be generalized across healthcare types and unique drivers should be considered.","",""
"2025","Legal NLP in India: a comprehensive survey of tasks, challenges, and future directions","","",""
"2025","Mark Coeckelbergh: Why AI undermines democracy and what to do about it","","",""
"2025","The tyranny of algorithmic personification and why we must resist it","","",""
"2025","Democratic legitimacy of AI in judicial decision-making","","",""
"2025","Humanism strikes back? A posthumanist reckoning with ‘self-development’ and generative AI","Abstract           Since the release of OpenAI's ChatGPT in 2022, AI activity has reached a fever pitch. Calls for effective ethical responses to the pressurised AI environment have in turn abounded. Posthumanism, which seeks to build ethical futures by de-centring the ‘human’, is an obvious candidate to act as a lynchpin of theoretical intervention. In their responses, posthumanist scholars appear to have embraced AI’s potential to destabilise Humanist philosophical ideas. We critically interrogate this initial enthusiasm. Conceptually distinguishing ‘post-dualist self-development’ (PDSD) from ‘technical self-development’ (TSD), we show how AI prompts an urgent need to advance posthumanist engagement with how technical development unsupervised by humans is ontologically discrete from other forms of material agency. We argue that specific engagement with TSD as distinct from PDSD is a key to avoid ignoring or underestimating Humanist and anthropocentric aspects of current AI innovation, and the influence of anthropomorphism. Without a theoretical reckoning with these tensions, posthumanism in the AI-era runs the risk of potentially promoting technologies that reinvigorate Humanist and anthropocentric expansion. To conclude, we show how a posthumanist ethics of generative AI that pays requisite attention to both TSD and PDSD may enable more anticipatory and nuanced assessments of the risks and benefits of discrete AI technologies to inform public discourse, appropriate social, institutional, policy and governance responses, and direct AI research and development priorities.","",""
"2025","Endless forms most similar: the dearth of the author in AI-supported art","","",""
"2025","Debiasing AI: Rethinking the intersection of innovation and sustainability, Donghee Shin","","",""
"2025","Generative AI and the avant-garde: bridging historical innovation with contemporary art","Abstract           The adoption of generative AI technology in visual arts echoes the transformational process initiated by early 20th-century avant-garde movements such as Constructivism and Dadaism. By utilising technological advances of their time avant-garde artists redefine the role of an artist and what could be considered as artwork. Written from the perspective of an art practitioner and researcher, this paper explores how contemporary artists working with AI continue the radical and experimental spirit that characterised early avant-garde. The re-evaluation of artist roles from sole creators to engineers-collaborators and curators in an AI-mediated creative process underscores a shift in the artistic practice. Through detailed case studies of three contemporary artists, the paper illustrates how generative AI is not only used to create artwork but also to critique technological, cultural, and societal systems. Additionally, it addresses ethical concerns such as AI bias, data commodification, and the environmental impact of AI technologies, situating contemporary generative AI practices within the broader context of art's evolving societal role. Ultimately, the paper underscores the transformation of artistic practice in the digital age, where AI becomes both a creative tool and a subject of critical reflection.","",""
"2025","Context matters: why AI fails at lawmaking","","",""
"2025","Correction: Endless forms most similar: the death of the author in AI-supported art","","",""
"2025","Who authors AI art? (And why does it matter?)","","",""
"2025","The AI-extended professional self: user-centric AI integration into professional practice with exemplars from healthcare","Abstract           AI technologies are rapidly advancing and have shown potential for providing significant value across a variety of sectors, including healthcare. Much of research has focused on the technologies’ capabilities and pushing their boundaries, with many envisioning AI and AI-enabled robots replacing human labor and humans in the near future. However, in critical domains of professional practice such as healthcare, full replacement is neither realistic nor aimed for, and collaboration between AI and humans is a given for the foreseeable future. This article argues for a shift away from a sole focus on the efficiency and effectiveness of technology, proposing instead that AI-enabled technologies increasingly should learn to adapt to human users considering that healthcare professionals already are overburdened. Rather than contributing to this burden, AI might extend the professional self by anticipating and supporting human needs and intentions. Drawing on a selective meta-synthesis of recent reviews and studies, this article introduces the concept of the AI-extended professional self. This concept suggests a temporary, dynamic integration of human professionals with AI that extends their capabilities with minimal additional burdens regarding training and application. Through three exemplars from healthcare—healthcare consultations, breast cancer screening, and robotic surgery—this article explores how a perspective rooted in the AI-extended professional self might unlock the potential for deeper AI integration into professional practice. Beyond these exemplars, this article calls for interdisciplinary research into the associated potential and challenges, advocating that the burden of AI integration needs to shift from humans to AI-enabled technologies.","",""
"2025","What can AI learn from the ambiguity of Eastern ink art","","",""
"2025","Not the machine’s fault: taxonomising AI failure as computational (mis)use","Abstract           This paper proposes a re-examination of connectionist AI failures (controversial incidents) from the perspective of technological use. It advances four categories of failure: technically sound outputs inherent to connectionist programming; machine-world mis-configuration; motivational failure that deploys technology for illegitimate ends; and finally epistemic failure of misapplication where computing and AI are being used to solve for the wrong sets of social problems. Drawing on the history of computing, the paper argues that computational machines and its software (classical or connectionist) are numerical, procedural, and electronic in nature, and therefore, are geared to treat problems through the functions of numerical  calculation, tabulation, approximation, and extrapolation. On account of these limitations, failure ensues when computers meet the many problems that cannot be solved on these grounds. The paper proposes and calls for an ontological comparison between computers and the problems they are pressed to serve prior to any pragmatic deployment.","",""
"2025","Media representation of ethical and social issues inherent in autonomous vehicle technology","Abstract           Successful implementation of autonomous vehicle (AV) technology is not only an engineering challenge but also a social, political, and ethical one. As AVs become commonplace and begin affecting people’s daily lives in a more profound way, media coverage of the social and ethical considerations of these technologies will follow suit. We seek to analyze and categorize the media’s portrayal of the social and ethical issues surrounding AVs to better understand how these issues shape public debate. Our research employs a qualitative thematic analysis of existing public discourse, identifying six areas of interest: (1) social and ethical issues; (2) ethical frameworks; (3) recommendations; (4) tone; (5) type of AV technology; and (6) sensationalism. These themes were derived through a combination of inductive and deductive analysis, informed by existing literature on AV ethics and media framing. Through this approach, we aimed to identify salient considerations in AV technology development and inform future AV systems decisions. Our contributions to the study of public visibility seek to enhance product development and the implementation of AVs, which will largely be steered by public discourse.","",""
"2025","Move fast and break people? Ethics, companion apps, and the case of Character.ai","Abstract           Riffing off move fast and break things, the internal motto coined by Meta’s Mark Zuckerberg, this paper examines the ethical dimensions of human relationships with AI companions, focusing on Character.ai—a platform where users interact with AI-generated ‘characters’ ranging from fictional figures to representations of real people. Drawing on an assessment of the platform’s design, and the first civil lawsuit brought against Character.ai in the USA in 2024 following the suicide of a teenage user, this paper identifies unresolved ethical issues in companion-based AI technologies. These include risks from difficulty in separating AI-based roleplay from real life, unconstrained AI models performing edgy characters, reality detachment, and confusion by dishonest anthropomorphism and emulated empathy. All have implications for safety measures for vulnerable users. While acknowledging the potential benefits of AI companions, this paper argues for the urgent need for ethical frameworks that balance innovation with user safety. By proposing actionable recommendations for design and governance, the paper aims to guide industry, policymakers, and scholars in fostering safer and more responsible AI companion platforms.","",""
"2025","Transforming paperwork with AI: applications across healthcare and other industries","","",""
"2025","‘Python is ready to have a conversation with you’: the reproduction of an ontology of language in natural language processing educational material","","",""
"2025","The epistemological consequences of large language models: rethinking collective intelligence and institutional knowledge","","",""
"2025","Every wave carries a sense of déjà vu: revisiting the computerization movement perspective to understand the recent push towards artificial intelligence","Abstract           Analyzed through the lens of the “computerization movement” (CM), the development of revolutionary technologies has consistently followed a recurring trajectory in terms of the origin, momentum, diffusion, and societal impact. Building on the analysis of selected historical trajectories, similar dynamics are discernible for the recent push towards the adoption of artificial intelligence (AI), being enhanced by capabilities provided by Big Data infrastructure. This paper explores Big Data and AI within the framework of CMs, analyzing their driving visions, trajectories, interconnectedness, and the societal discourses formed around their adoption. By drawing parallels with selected past CMs and situating current events within such historical context, this study provides a novel perspective hopefully facilitating a better understanding of the current technological landscape, and aiding in the navigation of the complex interplay between innovation, social change, and human expectations. The study shows that even if technological innovations remain central for the recent push towards AI adoption, shared beliefs and visionary ideals underpinning adoption are equally influential. These beliefs and ideals have continually mobilized people around the relevance of AI—in the past and today—even as the supporting infrastructure, core technologies, and their relevance for society have evolved.      ","",""
"2025","Trust in AI","","",""
"2025","Are LLMs actually good for learning?","","",""
"2025","A scoping review of Arab journalists' perspectives and applications of artificial intelligence","","",""
"2025","The practices and politics of machine learning: a field guide for analyzing artificial intelligence","Abstract           This article develops an analytical and methodological field guide for studying the mundane practices that constitute machine learning systems. Drawing on science and technology studies (STS), I move beyond the opacity/transparency dichotomy that has dominated critical algorithm studies to examine how machine learning is assembled through everyday work. Rather than treating algorithms as black boxes or magical entities, I focus on four empirical moments of translation—feature extraction, vectorization, clustering, and data drift—where technical work becomes political choice. By ethnographically attending to practitioners' tinkering, negotiations, and valuation practices in these moments, we can trace how classification systems are constructed and stabilized. This approach allows us to ask: How are particular features of the world selected as relevant for prediction? Through what practices are people and phenomena translated into mathematical vector spaces? How are temporal assumptions encoded in data? By studying these mundane processes of construction, we can understand how machine learning systems enact particular ways of seeing, classifying, and predicting the world. This field guide thus contributes methodological tools for analyzing how the politics of machine learning is assembled in practice, opening analytical space for critical engagement beyond calls for transparency or fairness.","",""
"2025","Artificial intelligence through the eyes of Hannah Arendt: fear, alienation, and empowerment","","",""
"2025","Give them what they want: the impact of AI on theater audiences","","",""
"2025","Can generative AI reliably synthesise literature? exploring hallucination issues in ChatGPT","Abstract           This study evaluates the capabilities and limitations of generative AI, specifically ChatGPT, in conducting systematic literature reviews. Using the PRISMA methodology, we analysed 124 recent studies, focusing in-depth on a subset of 40 selected through strict inclusion criteria. Findings show that ChatGPT can enhance efficiency, with reported workload reductions averaging around 60–65%, though accuracy varies widely by task and context. In structured domains such as clinical research, title and abstract screening sensitivity ranged from 80.6% to 96.2%, while precision dropped as low as 4.6% in more interpretive tasks. Hallucination rates reached 91%, underscoring the need for careful oversight. Comparative analysis shows that AI matches or exceeds human performance in simple screening but underperforms in nuanced synthesis. To support more reliable integration, we introduce the Systematic Research Processing Framework (SRPF) as a guiding model for hybrid AI–human collaboration in research review workflows.","",""
"2025","A survey on moral foundation theory and pre-trained language models: current advances and challenges","Abstract           Moral values have deep roots in early civilizations, codified within norms and laws that regulated societal order and the common good. They play a crucial role in understanding the psychological basis of human behavior and cultural orientation. The moral foundation theory (MFT) is a well-established framework that identifies the core moral foundations underlying the manner in which different cultures shape individual and social lives. Recent advancements in natural language processing, particularly pre-trained language models (PLMs), have enabled the extraction and analysis of moral dimensions from textual data. This survey presents a comprehensive review of MFT-informed PLMs, providing an analysis of moral tendencies in PLMs and their application in the context of MFT. We also review relevant datasets and lexicons and discuss trends, limitations, and future directions. By providing a structured overview of the intersection between PLMs and MFT, this work bridges moral psychology insights within the realm of PLMs, paving the way for further research and development in creating morally aware AI systems.","",""
"2025","Is AI a subject that can live together with humans?","","",""
"2025","Navigating the new landscape of knowledge in the age of generative AI","","",""
"2025","Apples and oranges: AI’s incommensurability problem","","",""
"2025","Engineering equity: designing diversity-aware AI to reflect humanity","","",""
"2025","Deciphering authenticity in the age of AI: how AI-generated disinformation images and AI detection tools influence judgements of authenticity","Abstract           An ongoing surge of Artificial Intelligence (AI)-enabled false content has been spreading its way through the information ecosystem, including AI-generated images, which have been used as part of political disinformation campaigns. Thus, there remains a pressing need to understand which factors individuals rely upon when determining whether images are AI-generated, particularly when they can be used to spread disinformation. AI-generated images have been characterised by their aesthetic realism, which can be leveraged to deceive users, and those who use generative AI to create deceptive content also tend to exploit its ability to convey and elicit emotion. This experimental study explored how aesthetic realism and emotional salience, as key features of both AI-generated content and disinformation, may influence authenticity judgements of AI-generated disinformation images. In this study, 292 UK-based participants were presented with both AI-generated and non-AI-generated disinformation images which varied in aesthetic realism and emotional salience. Results showed that participants were more likely to judge realistic-looking AI-generated images as being authentic compared with less realistic-looking AI-generated images, but did so with less confidence in their decision. Emotional salience was not a significant predictor of judgements. When participants were presented with the correct verdict of an AI detection tool, their reliance on the tool to update their own judgements was predicted by the aesthetic realism of the image and their confidence levels. These findings may assist with the development of disinformation detection tools, as well as strategies that mitigate the spread of deceptive, synthesised visual content in the digital age.","",""
"2025","The phenomenology of human–AI aesthetics","","",""
"2025","The persistence of originality: will AI blur or brighten the line between inspiration and imitation?","","",""
"2025","Navigating fairness: introducing the multidimensional AIM-FAIR scale for evaluating AI decision-making","Abstract           People’s concerns regarding the fairness of algorithmic decision-making, coupled with its expanding utilization across various spheres of our lives underscores the need for robust measures to assess perceived fairness in standardized survey research. Existing fairness scales often suffer from inadequate content coverage, particularly in terms of Perceived Group Discrimination, and frequently employ suboptimal measurement methods, such as single-item assessments. This paper introduces the AIM-FAIR scale, a multidimensional tool grounded in classical test theory, employing Likert-scaled answering options and a reflective measurement model. Developed through four studies (n = 1777) and validated in both English and German, the scale includes 17 items across five subscales: Perceived Consistency, Perceived Equity, Perceived Group Bias, Perceived Manipulability, and Perceived (Explanatory) Transparency. Both language versions demonstrate excellent fit indices and consistent measurement invariance across diverse backgrounds, languages, and conditions. The AIM-FAIR scale offers higher ecological validity and a more comprehensive framework for evaluating fairness in ADM, enhancing cross-cultural and cross-linguistic research on AI fairness.","",""
"2025","The roles of cooperative attitude, personal innovativeness, and anxiety in AI adoption within the design community","","",""
"2025","Large language models and the problem of rhetorical debt","Abstract           This article offers broadly useful guidance for society’s adaptation to the omnipresence of generative AI, with implications for every profession and academic discipline that involves writing or coding (recognized by some as a form of writing). Offering an interdisciplinary perspective grounded in the digital humanities, software development and writing across the curriculum, and building on performance historian Christopher Grobe’s research on the role of arts and humanities expertise in AI development, I offer redefinitions of training data and prompt engineering. These essential yet misleading terms obscure the critical roles that humanities-based expertise has played in the development of GPTs and must play in guiding society’s adaptation to generative AI. I also briefly review scholarship on what constitutes “writing” and what it means to teach writing. Next, I reflect on long-terms trends, in professional software development, of code sharing and reliance on automation, and the likely impact of imposing similar practices in professional writing. After identifying the fundamental problem of rhetorical debt and outlining its consequences, I further motivate my argument, in relation to the new economic value of expert writing. This new economic value necessitates a revaluation of the humanities—not only by computer science, the tech industry, and schools and universities, but by humanists themselves.","",""
"2025","European reactions to AI in full and flawed democracies: an investigation of key factors","Abstract           This study examines the key factors that affect Europeans’ reactions to artificial intelligence (AI) in the context of both full and flawed democracies in Europe. AI applications have increasingly been integrated into democratic practices, ranging from micro-targeting of voters to election information campaigns and protests, as well as various administrative functions and services provided by governments. However, the impact of AI on democracy and democratic institutions has yielded mixed outcomes. Drawing upon a dataset of 4004 respondents, categorised into full democracies and flawed democracies based on The Democracy Index developed by the Economist Intelligence Unit (EIU), this research identifies crucial factors that shape Europeans’ attitudes toward AI in these two types of democracies. The analysis reveals noteworthy findings. First, flawed democracies tend to exhibit higher levels of trust in government entities compared to their counterparts in full democracies. Furthermore, individuals residing in flawed democracies demonstrate a more positive attitude toward AI when compared to respondents from full democracies. However, the study does not find significant differences in AI awareness between the two types of democracies, indicating a similar level of general knowledge about AI technologies amongst European citizens. Moreover, the study reveals that trust in AI measures, specifically “Trust AI Solution,” does not vary significantly between full and flawed democracies. This suggests that despite differences in democratic quality, both types of democracy have similar levels of confidence in AI solutions. Furthermore, employing regression models, the study uncovers the relative impact of these key factors and their correlations can reflect on policy implications. These findings contribute to a better understanding of the factors that shape the reactions of Europeans to AI in the democratic context, providing valuable information to policymakers and stakeholders in designing effective AI governance frameworks and strategies.","",""
"2025","Artificial intelligence in governance: recent trends, risks, challenges, innovative frameworks and future directions","","",""
"2025","Indic approach to ethical AI in automated decision making system: implications for social, cultural, and linguistic diversity in native population","","",""
"2025","Robot ethnography for culturally responsive human–robot interactions","Abstract           With recent major progress in AI, AI-driven robots are expected to operate outside of their original settings, such as factory floors, and be integrated into our daily lives. In this trajectory, integrating culture, with its broad and vague meaning, into human–robot interaction becomes a concern for the field of social robotics. In this article, we propose a culturally-responsive robot as a concept that encapsulates critical theoretical and experimental studies of culture in robotics without subscribing to the dominant view of the field that translates culture only to nationality. The main contribution of this article is to propose a methodology for implementing culturally-responsive robots, termed “robot-ethnographers.” Drawing from the long history of conventional ethnography, we define a robot-ethnographer as a robot situated within interaction contexts, shaping interactions, and learning culture through observation to adapt its behaviours accordingly. The paper analyses the properties of the robot-ethnographer in line with those of conventional ethnography in social science. Whilst acknowledging important technological limitations, this article concludes that robot-ethnographers can create culturally-responsive human–robot interactions.","",""
"2025","Unlocking Australia’s AI usage in law enforcement from human involvement perspective: a systematic literature review","Abstract           Exploring human trust in artificial intelligence (AI) in Law Enforcement domain is paramount for its ethical and effective deployment. As AI systems become increasingly integrated into society, trust ensures transparency, accountability, and fairness in their deployment. Despite the rapid increase in discussion about AI usage in law enforcement in various sectors globally, no secondary research explores that for OCEANIA, specifically in Australia. Moreover, how humans are considered in AI law enforcement is an uninvestigated area in this region. This paper aims to investigate this gap by systematically exploring various applications used in this region and analyzing how human perspectives are used, and to what extent. We conducted a systematic literature review (SLR) based on 56 qualitatively selected studies to (i) classify the types, frequency, and demography of published research; (ii) explore how AI is leveraged; (iii) identify how and to what extent humans are considered in development and applications of AI in law enforcement; (iv) understand the challenges and solutions reported in this domain and worldwide. In summary, our findings point to the need for solutions to address operational and ethical issues in integrating AI into law enforcement and for unified legislation or regulations to effectively design and utilize AI in this domain. Based on the findings that highlight the important concerns in this domain, we recommend a list of future research directions.","",""
"2025","A methodology for ethical decision-making in automated vehicles","Abstract           Despite significant advancements in AI and automated driving, a robust ethical framework for AV decision-making remains undeveloped. Such a framework requires clearly defined moral attributes to guide AVs in evaluating complex and ethically sensitive scenarios. Existing frameworks often rely on a single normative ethical theory, limiting their ability to address the nuanced nature of human decision-making and leading to conflicting outcomes. Augmented Utilitarianism (AU) offers a promising alternative by integrating elements of virtue ethics, deontology, and consequentialism into a non-normative framework. Grounded in moral psychology and neuroscience, AU employs mathematical ethical goal functions to capture societally aligned attributes. In this study, we propose and evaluate a method to elicit these attributes for AV decision-making. One hundred participants were presented with traffic scenarios, including critical and non-critical situations, and tasked with evaluating the relevance of an initial set of 11 attributes (e.g., physical harm, psychological harm, and moral responsibility) while suggesting additional relevant attributes. Results identified two new attributes—environmental harm and energy efficiency—and revealed that four attributes (physical harm, psychological harm, legality of the AV, and self-preservation) varied significantly between critical and non-critical scenarios. These findings suggest that the weight of attributes in ethical goal functions may need to adapt to situational criticality. The method was validated based on key evaluation criteria: it demonstrated sensitivity by producing varying relevance scores for attributes, was deemed relevant by participants for eliciting AV decision-making attributes, and allowed for the identification of additional attributes, enhancing the robustness of the framework. This work contributes to the development of a dynamic and context-sensitive ethical framework for AV decision-making.","",""
"2025","From risk to reward: AI’s role in shaping tomorrow’s economy and society","Abstract           This study investigates the impact of Artificial Intelligence (AI) on society, business, and management, using a qualitative approach centered on the analysis of interviews and a review of literature. Text mining techniques were applied through the KH Coder tool, allowing for a detailed exploration of how AI is transforming these three dimensions. The results reveal significant changes in management practices, deep economic impacts, and relevant social changes brought about by the rapid adoption of AI. The originality of this study lies in the combination of qualitative analysis with the exploration of textual data, providing a comprehensive view of the ethical and practical implications of AI. It also acknowledges limitations, such as the rapid pace of technological development and the potential bias in the perceptions collected. This work contributes to a better understanding of the challenges and opportunities presented by AI, and suggests pathways for ethical and effective integration.","",""
"2025","Monetization could corrupt algorithmic explanations","Abstract           Explainable artificial intelligence (XAI) aims to provide insights into the logic of automated decisions with the goal of promoting fairer, more transparent, and more trustworthy automated decision-making. Despite mounting regulatory pressure, changing consumer expectations, and a growing stream of XAI-related research, few consumer-facing applications of XAI exist. In anticipation of future XAI-enabled products and services, we use ethical foresight analysis to investigate the possible consequences of monetizing explanations. By developing a conceptual artifact we call an explanation platform, we analyze what could happen when digital advertising is fused with XAI. We explore the platform’s business and design logic, examine its potential social and ethical impact, and describe several plausible explanation manipulation scenarios and strategies. We find that while XAI monetization could incentivize industry adoption of XAI technology and expand algorithmic recourse across society, it could also lead to corrupted forms of explanations optimized for profit-driven objectives. Overall, our foresight analysis makes the case for the economic and technological feasibility of monetized XAI, but raises concerns about its desirability in liberal democratic societies.","",""
"2025","Technical insights into vision-based fall detection systems: performances, challenges, and constraints","","",""
"2025","The struggle is the lesson! Being an educator in the age of AI","","",""
"2025","Darwin in the machine: addressing algorithmic individuation through evolutionary narratives in computing","Abstract           This paper examines the application of evolutionary analogy in AI (artificial intelligence) research, focussing on narratives that perpetuate individuated and autonomous imaginaries of AI systems through biological diction. AI research has long drawn inspiration from evolution to design and predict algorithmic change. Occasionally, these narratives extend inspiration to reimagine AI as a non-human species subject to the same evolutionary pressures as biological organisms. As AI technologies embed more pervasively in public life and require critical perspectives on their social impacts, these comparisons in AI discourse raise critical questions about the limits of and responsibility in employing such analogies and their potential impact on how broader audiences consume and perceive AI systems. This paper examines the diverse ways and intentions behind how evolution is invoked in AI research narratives by analysing the adaptation of individuating evolutionary language and concepts across three fields of AI-related research: evolutionary computing, Artificial Life, and existential risk. It scrutinises the challenge of accurate scientific communication when drawing inspiration from biological evolution and assigning organismal attributes to digital technologies whilst decontextualising wider evolutionary scholarly discourses. I argue that the intertwined history between evolutionary theory and technological change paired with the potential risks to wider perceptions of AI and biological evolution, requires (1) strategic consideration about the limits of evolutionary analogies in categorising AI in relation to biological organisms, balancing creative inspiration with scientific caution and (2) active, collaborative multidisciplinary engagement with addressing potential misinformation, recognising that biological narratives have sociopolitical implications that influence human interaction with machines.","",""
"2025","The winds of culture: AI art generators and the Aeolian harp","","",""
"2025","Excuses, excuses: moral agency and the professional identity of AI developers","Abstract           Artificial intelligence developers, machine learning engineers, and data scientists occupy a contradictory role in the modern marketplace. While they are central to the business and science of AI, they are marginalized as moral agents. Consequently, the marketplace has cultivated environments in which developers can be unthinking in their own roles and responsibilities, while at the same time tasking them with creating “thinking machines.” The central aim of this article is to show that this state of affairs is morally unjustifiable. To accomplish this, we draw from Arthur Isak Applbaum’s work on adversary roles and Alasdair MacIntyre’s framework for professional moral agency to establish the context dependencies for a “good” AI developer. We then draw from available studies that have engaged developers in questions about their moral agency and place them in conversation with Dennis Thompson and Helen Nissenbaum about the excuses associated with “the problem of many hands,” a concept that has beguiled accountability in the AI community for decades. We then return to MacIntyre’s framework to provide evidence from the same set of studies that AI developers do understand themselves as being responsible for more than just the role, yet they lack a robust community to whom they can submit their choices for ethical scrutiny and work environments that are often non-conducive to their moral actualization. We conclude with specific recommendations for bringing developers’ moral agency more fully into the discourse about AI ethics.","",""
"2025","In generative artificial intelligence we trust: unpacking determinants and outcomes for cognitive trust","Abstract           Amid the pervasive integration of AI technologies across societal and industrial domains, understanding users’ trust in these systems becomes increasingly crucial. This study addresses the growing need to understand users’ trust in Generative Artificial Intelligence (GenAI) and explores the societal implications of this type of trust. Based on the socio-technical systems theory, this work employs the FAT (Fairness, Accountability, Transparency) framework and humanness factors of AI, anthropomorphism, social presence, and emotions, as antecedents of users’ human-like trust, which is proposed to influence users’ attitudes, perceived performance, and behavioral intentions. Structural equation modeling analysis (N = 244) reveals that fairness significantly enhances trust, while accountability and transparency do not. Social presence and emotions positively impact trust, whereas anthropomorphism shows no significant effect. Furthermore, trust shapes users’ attitudes, perceived performance, and behavioral intentions toward GenAI systems. This study contributes to the AI adoption and user trust literature by illuminating the main antecedents of human-like trust and showing its impact on user acceptance from a social-technical perspective. Beyond the academic contribution, this research highlights the broader societal relevance of user trust in GenAI, particularly regarding public concerns over black box issues and humanness features of GenAI systems.","",""
"2025","Impact of artificial intelligence usage on occupational stress: an exploratory study of risk factors and prevention strategies","","",""
"2025","Humansplaining: is it a thing? Is it bad?","","",""
"2025","Large language models: assessment for singularity","","",""
"2025","Bridging embodied cognition and AI: Agentive Cognitive Construction Grammar as a backing theory for neuro-symbolic AI","","",""
"2025","Legal frameworks for AI service business participants: a comparative analysis of liability protection across jurisdictions","","",""
"2025","The necessity of AI audit standards boards","Abstract           Auditing of AI systems is a promising way to understand and manage ethical problems and societal risks associated with contemporary AI systems, as well as some anticipated future risks. Efforts to develop standards for auditing artificial intelligence (AI) systems have therefore understandably gained momentum. However, current approaches are not just insufficient, but can be actively harmful. Transparency alone does not address concerns about risk. Internal auditing is insufficient, and easily becomes safety-washing. External audit is better, but requires credible standards. Industry-led approaches to building standards or to perform audits lack credibility and undermine other efforts. Regulation often is ill adapted and becomes a static barrier. Lastly, all of these limited technical, governance, and even ethical assessments fail to ensure continued stakeholder input and engagement. Instead, the paper proposes the establishment of an AI Audit Standards Board, in line with best practices in other fields, including safety-critical industries like aviation and nuclear energy, as well as more prosaic ones such as financial accounting and pharmaceuticals. This would address the evolving nature of AI technologies, help maintain public trust in AI, and promote a culture of safety and ethical responsibility within the AI industry. By ensuring audits remain relevant, robust, and responsive to the rapid advancements in AI, auditing AI will not devolve into safety washing and addresses risks and ethical concerns that will continue to arise as AI becomes increasingly important in society, and as human interaction with these systems changes over time.","",""
"2025","The Limits of Machine Learning Models of Misinformation","Abstract           Judgments of misinformation are made relative to the informational preferences of the communities making them. However, informational standards change over time, inducing distribution shifts that threaten the adequacy of machine learning models of misinformation. After articulating five kinds of distribution shifts, three solutions for enhancing success are discussed: larger static training sets, social engineering, and dynamic sampling. I argue that given the idiosyncratic ontology of misinformation, the first option is inadequate, the second is unethical, and thus the third is superior. However, I conclude that the prospects for machine learning models of misinformation are far weaker than most have presupposed, given that both epistemic and non-epistemic values are difficult to operationalize dynamically in machine code, rendering them surprisingly at most a species of recommender systems rather than literal truth detectors. ","",""
"2025","A counterintuitive approach to explainable AI in healthcare: balancing transparency, efficiency, and cost","","",""
"2025","Recursive InPainting (RIP): how much information is lost under recursive inferences?","Abstract           The rapid adoption of generative artificial intelligence (AI) is accelerating content creation and modification. For example, variations of a given content, be it text or images, can be created almost instantly and at a low cost. This will soon lead to the majority of text and images being created directly by AI models or by humans assisted by AI. This poses new risks; for example, AI-generated content may be used to train newer AI models and degrade their performance, or information may be lost in the transformations made by AI which could occur when the same content is processed over and over again by AI tools. An example of AI image modifications is inpainting in which an AI model completes missing fragments of an image. The incorporation of inpainting tools into photo editing programs promotes their adoption and encourages their recursive use to modify images. Inpainting can be applied recursively, starting from an image, removing some parts, applying inpainting to reconstruct the image, revising it, and then starting the inpainting process again on the reconstructed image, etc. This paper presents an empirical evaluation of recursive inpainting when using one of the most widely used image models: Stable Diffusion. The inpainting process is applied by randomly selecting a fragment of the image, reconstructing it, selecting another fragment, and repeating the process a predefined number of iterations. The images used in the experiments are taken from a publicly available art data set and correspond to different styles and historical periods. Additionally, photographs are also evaluated as a reference. The modified images are compared with the original ones by both using quantitative metrics and performing a qualitative analysis. The results show that recursive inpainting in some cases modifies the image so that it still resembles the original one while in others leads to image degeneration, so ending with a non-meaningful image. The outcome of the recursive inpainting process depends on several factors, such as the type of image, the size of the inpainting masks, and the number of iterations. The results of our evaluation illustrate how information can be lost due to successive AI transformations. The evaluation of additional models, images, and inpainting sequences is needed to confirm whether this observation is generally applicable or if it occurs only in some models and settings.","",""
"2025","Rethinking explainable AI in financial services","","",""
"2025","Technologies as “AI Companions”: a call for more inclusive emotional affordance for people with disabilities","","",""
"2025","When autonomy breaks: the hidden existential risk of AI","","",""
"2025","Why machine learning in the wild is a rare species","","",""
"2025","Communication experiment with moral-based chatbot: toward solution of divisions in democracy","","",""
"2025","The material making of language as practice of global domination and control: continuations from European colonialism to AI","Abstract           Although AI language technologies are typically presented as future-oriented technological innovation, none of the elements of machine learning technologies are unaffected by the cultural and historical contexts of their emergence. This is particularly true in the case of language constructions and the materialization of language in AI. Examination of computational language culture reveals striking continuities to concepts of language and their materialization in technology settings in the history of European colonialism. Based on an in-depth analysis of how languages were materially produced in colonialism and are treated in AI technologies, we show the strong colonial continuities in language materialization processes to this day. This also indicates the crucial role that language materializations play in the construction and maintenance of power and social order in a global realm.","",""
"2025","Trusting the (un)trustworthy? A new conceptual approach to the ethics of social care robots","Abstract           Social care robots (SCR) have come to the forefront of the ethical debate. While the possibility of robots helping us tackle the global care crisis is promising for some, others have raised concerns about the adequacy of AI-driven technologies for the ethically complex world of care. The robots do not seem able to provide the comprehensive care many people demand and deserve, at least they do not seem able to engage in humane, emotion-laden and significant care relationships. In this article, we will propose to focus the debate on a particularly relevant aspect of care: trust. We will argue that, to answer the question of whether SCR are ethically acceptable, we must first address another question, namely, whether they are trustworthy. To this end, we propose a three-level model of trust analysis: rational, motivational and personal or intimate. We will argue that some relevant forms of caregiving (especially care for highly dependent persons) require a very personal or intimate type of care that distinguishes it from other contexts. Nevertheless, this is not the only type of trust happening in care spaces. We will adduce that, while we cannot have intimate or highly personal relationships with robots, they are trustworthy at the rational and thin motivational level. The fact that robots cannot engage in some (personal) aspects of care does not mean that they cannot be useful in care contexts. We will defend that critical approaches to trusting SCR have been sustained by two misconceptions and propose a new model for analyzing their moral acceptability: sociotechnical trust in teams of humans and robots.","",""
"2025","Art Beyond Humanity: exploring the human through machine creation","","",""
"2025","Toward cultural interpretability: A linguistic anthropological framework for describing and evaluating large language models"," This article proposes a new integration of linguistic anthropology and machine learning (ML) around convergent interests in both the underpinnings of language and making language technologies more socially responsible. While linguistic anthropology focuses on interpreting the cultural basis for human language use, the ML field of interpretability is concerned with uncovering the patterns that Large Language Models (LLMs) learn from human verbal behavior. Through the analysis of a conversation between a human user and an LLM-powered chatbot, we demonstrate the theoretical feasibility of a new, conjoint field of inquiry, cultural interpretability (CI). By focusing attention on the communicative competence involved in the way human users and AI chatbots coproduce meaning in the articulatory interface of human-computer interaction, CI emphasizes how the dynamic relationship between language and culture makes contextually sensitive, open-ended conversation possible. We suggest that, by examining how LLMs internally “represent” relationships between language and culture, CI can: (1) provide insight into long-standing linguistic anthropological questions about the patterning of those relationships; and (2) aid model developers and interface designers in improving value alignment between language models and stylistically diverse speakers and culturally diverse speech communities. Our discussion proposes three critical research axes: relativity, variation, and indexicality. ","",""
"2025","Cooling down AI regulation controversies: Three closure processes in the Chilean legislative arena"," According to social studies of artificial intelligence (AI), public AI controversies tend to dissipate relatively quickly despite well-documented risks and harms. The reasons for this lack of controversiality are beginning to be studied. Drawing on the framework of sociotechnical controversies, we analyze the de-escalation of contentious discussions observed in the AI legislative process by Chile's National Congress. Utilizing a qualitative approach, we tracked the deliberations hosted by the Chamber of Deputies and the Senate of Chile across 51 sessions between 2023 and 2024. We describe three processes of cooling down in the AI debates: (1) deflection of technology liability, (2) instrumentalization of technology policy, and (3) moralization of technology use. However, constructive exchanges appear in some circumstances, which allow us to foresee some favorable conditions for participation in the debates on AI regulation. This paper contributes to AI controversy studies by outlining cooling-down processes and conditions that foster dialogue and providing a critical perspective on the formation of AI regulation. ","",""
"2025","Que(e)rying artificial intelligence use for infectious disease surveillance: The need for a reparative algorithmic praxis","The increasing likelihood of pandemics highlights the need for superior tools at our disposal. By robustly and efficiently analyzing vast datasets, artificial intelligence (AI) has the potential to help decision-makers better respond to, manage, and even avert infectious disease outbreaks. However, these systems could also stigmatize, discriminate, exclude, exploit, and/or otherwise oppress vulnerable populations. In doing so, they could amplify allocative and representational harms. Given the possible far-reaching consequences, critical ethical reflection and oversight are essential. Such reflection would be incomplete without considering the impacts on queer people. From HIV/AIDS to COVID-19, outbreaks have disproportionately affected sexual and gender minorities (SGMs), reflecting a long history of structural oppression and injustices. AI could further exacerbate inequalities—like anti-queer bias—particularly amid the omission of marginalized and minoritized perspectives from algorithmic fairness efforts. Adopting an Intersectional, reparative approach, this paper que(e)ries the use of AI for infectious disease surveillance purposes. Placing this technology within patterns of power, privilege, marginalization, and disadvantage, it interrogates how to achieve algorithmic justice for SGMs. It proposes concrete steps towards a reparative algorithmic praxis, including: (1) exploring how these systems reproduce inequalities, (2) centering sexual and gender diversity to disrupt problematic epistemic positions, and (3) combating opacity through participatory governance mechanisms. This work is necessary to understand how AI systems reproduce major health disparities and hold them accountable. By contemplating how to begin redressing harms, it offers a starting point for further deliberation and action towards inclusive, justice-oriented algorithmic systems in practice. I anticipate these lessons being deeply transferrable across contexts.","",""
"2025","The chat-chamber effect: Trusting the AI hallucination","This study investigates the potential for ChatGPT to trigger a media effect that sits at the intersection of echo-chamber communication and filter bubbles. We devised a two-phase, two-stage experimental design with ChatGPT 3.5 (treatment group) and Google search engine (control group) by asking participants to find out how many LGBTQIA+ individuals served as elected representatives in India (first phase) and Ireland (second phase). The similar trajectories of legal reforms observed in these countries, and their small number of LGBTQIA+ elected representatives, allowed us to identify the fault lines in ChatGPT's creation of knowledge and information around LGBTQ issues. We followed the experimental study with semi-structured interviews to identify whether the chatbot reinforced previously held beliefs and whether users cross-checked the information provided by ChatGPT. Our results show that Large Language Models may provide incorrect but proattitudinal information that remains unchecked and unverified by the users, an effect we refer to as Chat-Chamber. We conclude with a discussion of our findings and recommendations for future research in the area.","",""
"2025","The benefits of being between (many) fields: Mapping the high-dimensional space of AI research"," This article considers how the emerging Artificial Intelligence (AI) research field is constructed, primarily in university settings. AI research is a site of significant national funding, industry investment, and media interest. As such, for researchers working across the resource-constrained science system, their relationship to the field is significant; legitimisation as an AI researcher can bring material and symbolic rewards. Through interviews (n = 90) with academics affiliated with AI-branded research organisations in the US, UK, and Australia, the article develops an empirical account of the construction of AI research as a high-dimensional field – a field that moves between multiple disciplinary and sectoral boundaries across national and international hierarchies. The article draws on the sociology of expertise and studies of research infrastructures to develop the conceptual frame of dimensionality to explain the vertical and horizontal dynamics informing the AI field's development. The article's contributions are its description of the emerging AI field, which complements critical studies of how the figure of AI is mobilised in other settings, and its extension of field theory to fluid spaces that leverage the boundary zone between several overlapping field arrangements. ","",""
"2025","Why putting artificial intelligence ethics into practice is not enough: Towards a multi-level framework","Artificial intelligence (AI) ethics is undergoing a practical shift towards putting principles into design practices in developing responsible AI. While this practical turn is essential, this paper highlights its potential risk of overly focusing on addressing issues at the level of individual artifacts, which can neglect more profound structural challenges and the need for significant systemic change. Such oversight makes AI ethics lose its strength in addressing some hidden, long-term harms within broader contexts. In this paper, we propose that the reflection on structural issues should be an integral part of AI ethics. To achieve this, we develop a multi-level framework to analyze socio-ethical issues of AI at both the artifact and broader structural levels. This framework can serve as a potentially transformative approach to uncover some unspoken assumptions in current AI ethics discourses and expose some blind spots in AI guidelines, policies, and regulations. Our paper paves the way to develop a practical approach that can effectively integrate this multi-level framework into real-world AI design and policymaking, ultimately bringing about transformative change.","",""
"2025","Artificial intelligence for development (AI4D): A contested notion","Recently, the notion of artificial intelligence for development (AI4D) has been mobilized by various actors in the global South and North. We identify five analytical categories to help us understand the different and often contested perspectives on AI4D. They are (a) a developmentalist framework that emphasizes discourses around modernity and progress through a technoliberal lens of ‘catching up’; (b) an economic development framework taken up by African states, private sector and civil society, highlighting a positive and more future-looking outlook on AI's potential for development; (c) an international policy framework tied to globally agreed on policies such as the Sustainable Development Goals; (d) a colonial and extractivist framework that articulates how AI4D reinforces old processes of oppression in new ways; and (e) decolonial AI discourses grounded in Latin American, African and Indigenous approaches. Our critical review of literature on AI4D and related expressions shows that while the notion applies broadly to the global South, the majority of publications use the term in reference to AI development on the African continent. This commentary enriches our understanding of the plurality of meanings, where they come from, what they do, and what they leave unaddressed.","",""
"2025","Collectivism and individualism political bias in large language models: A two-step approach","             In this paper, we investigate the political biases of large language models concerning collectivism and individualism through a combined analysis of their value judgments and factual assessments. We propose a two-step approach to evaluate the patterns of bias in the outputs of large language models as well as a specific set of questions to examine large language model outputs’ political bias on collectivism and individualism. Our methodology involves two main phases. (a) Value assessment: we initiate the first phase by prompting large language models with questions from our set to identify patterns of political bias in their generated content. (b) Factual assessment: we refine the questions in our set and conduct a second round of prompting to verify the accuracy of the models’ responses regarding “             collectivism             ” and “             individualism             .” This step aims to assess whether the models can accurately discern these concepts in a factual context. Our experiments reveal varying degrees of political bias in the outputs of different large language models. While some models demonstrate proficiency in distinguishing between collectivism and individualism, they display outputs that are not neutral on political matters. Conversely, other models face challenges in accurately differentiating between these concepts and generating unbiased content. The latter indicates that many large language models not only fail to accurately distinguish between collectivism and individualism but also exhibit significant political biases in their outputs. We argue that a reliable large language model should achieve accuracy in factual assessments while generating unbiased content in value judgments, thus avoiding guiding users’ opinions.           ","",""
"2025","Indigenous peoples and artificial intelligence: A systematic review and future directions","This systematic literature review addresses the intersection of two rapidly evolving areas of knowledge and practice: Indigenous Knowledge Systems (IKS) and artificial intelligence (AI). There is growing scholarly recognition of the rich and diverse nature of IKS, which are unique intergenerational understandings of worldly relations from an Indigenous standpoint. There is now a vast literature on the promise and pitfalls of AI. However, there is a lack of systematic reviews showing how these two dynamic literatures are intersecting, and what the major themes are. AI has the potential to assist the promotion of IKS; however, there are also potential risks arising from AI for Indigenous peoples, such as the erosion of cultural knowledge, and data-grabbing that fails to respect the principles of Indigenous Data Sovereignty. These risks can exacerbate existing knowledge hierarchies and socio-economic inequalities. In this paper, we conducted a systematic review of articles published between 2012 and 2023 (January) on Indigenous peoples and AI. We shed light upon four unique overlapping categories into which existing literature can be classified and comprehensively discuss literature under each category. The first two categories discuss AI’s role in assisting the promotion of IKS and the third focuses on the pitfalls of using AI for Indigenous peoples. The final category discusses how IKS itself can enrich the development of AI. We further identify several gaps in the literature and highlight avenues requiring attention on AI’s role with Indigenous peoples and their knowledge systems.","",""
"2025","Global data empires: Analysing artificial intelligence data annotation in China and the USA","As the two leading countries in the development of artificial intelligence (AI) systems, China and the United States largely rely on separate AI infrastructure and data annotation ecosystems. Studies have focussed almost exclusively on data annotation associated with American and European companies, limiting our understanding of how this contrasts with the Chinese development of AI. This article provides a comparative analysis of the political economy of the Chinese and American/European AI data annotation ecosystems, focusing on the role of the state and the practice of outsourcing to data annotation institutions. It finds that while the US state plays a protectionist role concerning AI infrastructure such as semiconductors and data centres, it adopts a laissez-faire approach to data annotation. The Chinese state, however, understands it has a comparative advantage in data and invests heavily in its own data ecosystem while maintaining stringent regulations for Chinese tech companies. Secondly, many US companies outsource data annotation work to business process outsourcing centres and digital platforms, whereas Chinese companies maintain these activities in-house or, through a process of ‘inland-sourcing’, send this work to ‘third-tier’ cities in Chinese provinces to data labelling bases, often jointly managed by local government and private companies.","",""
"2025","Artificial intelligence and the state: Seeing like an artificial neural network","             The emergence of the modern state was closely intertwined with the advent of statistics and demographic data. Today, we are witnessing the ascent of artificial intelligence as a new technology of governance. This article seeks to lay the groundwork for a research agenda at the intersection of the state and artificial intelligence, unpacking the notion of “AI” and examining the consequences of the state transitioning from statistics to artificial intelligence as the means of “seeing” its subjects. The first part of the article argues that artificial intelligence represents a fundamental epistemic shift: from             variables to patterns             , from             rules to associations             , from             surveys to sensors             . This transition may transform governance and biopolitics, and with them, the very meanings of concepts such as citizenship, democracy, and population. In the second part of the article, the article draws on the literature on socio-technical transitions to conceptualize the integration of artificial intelligence into state practices, offering a framework to guide empirical research on how artificial intelligence is transforming governance.           ","",""
"2025","Networks, narratives and neocoloniality of             <i>AI for Climate Action</i>","This article results from a cross-disciplinary study which unfolds the neocolonial nature of the application of AI technologies for climate action by examining the actors’ network and discourses around this global practice. The study demonstrates that tech businesses from the Global North are currently leading the AI for Climate Action project, with certain non-profits and civil society organisations helping these actors to advance their core profit-driven ambitions. They facilitate discourses, such as ‘leveraging AI’, ‘AI innovation’, and ‘responsible AI’ to build techno-solutionist narratives and concurrently legitimise their networked relationships and actions. Drawing from our exploratory findings, we bring the question of geopolitical power imbalance into the forefront of this AI for Climate Action discussion. We highlight the deeply problematic realities around this practice that includes the minimal participation of the Global South actors in accessing the critical digital infrastructures to build AI technologies and their lack of control over the strategic decisions around the applications of such technologies in their own climate contexts. We contend that AI for Climate Action, due to the Global North-centric corporate interests, further exacerbates climate injustice by marginalising the populations who are already disproportionately impacted by climate change.","",""
"2025","Algorithmic accountabilities and health systems: A review and sociomaterial approach","The perceived importance and difficulty of accounting for algorithms in health systems continues to inform scholarship and practice across diverse fields. While accountability is often framed as a normative good, less clear is exactly what kind of normative work accountability is expected to do, and how it is expected to do it. Drawing on contributions from science and technology studies, and especially sociomaterial perspectives on governance, in this article I review how algorithmic accountability has been conceptualized in the academic and grey literature. I introduce five normative logics characterizing discussions of algorithmic accountability: (1) accountability as verification, (2) accountability as participation, (3) accountability as social licence, (4) accountability as fiduciary duty, and (5) accountability as compliance. I critically engage with the styles of valuation these are predicated upon, including how each configures the algorithm as an object of reference, and discuss the implications of this approach for understanding how health-related worlds are created and sustained, and how they might be otherwise.","",""
"2025","Better pay, clearer guidance: Investing in the working conditions of artificial intelligence data workers","The production of artificial intelligence (AI) requires human labour, with tasks ranging from well-paid engineering work to often-outsourced data work. This commentary explores the economic and policy implications of improving working conditions for AI data workers, specifically focusing on the impact of clearer task instructions and increased pay for data annotators. It contrasts rule-based and standard-based approaches to task instructions, revealing evidence-based practices for increasing accuracy in annotation and lowering task difficulty for annotators. AI developers have an economic incentive to invest in these areas as better annotation can lead to higher quality AI systems. The findings have broader implications for AI policy beyond the fairness of labour standards in the AI economy. Testing the design of annotation instructions is crucial for the development of annotation standards as a prerequisite for scientific review and effective human oversight of AI systems in protection of ethical values and fundamental rights.","",""
"2025","Ground-truth is law: The invisible conceptual work behind AI","This article challenges the idea that the turn from rule-based algorithm to machine learning systems leads to a decline in formal conceptualization. Through ethnographic research at two artificial intelligence (AI) production sites within the French justice system, the study shows that conceptual labor remains at the heart of machine learning, shifting from algorithm coding to the curation of training data. Revisiting Lawrence Lessig's claim “code is law,” the article argues that in AI, the influence of formal code has waned, but a new form of structured conceptual framing has emerged in the form of ground-truth datasets—where “ground-truth is law.” These datasets, shaped by a range of actors across the AI production chain, subtly guide algorithmic operations under the guise of neutrality. This study applies Anselm Strauss’ “arc of work” framework to identify five critical stages in AI production: goal setting, databasing, taxonomy construction, labeling, and monitoring—and demonstrates how conceptual understandings of the world are embedded within algorithmic systems in each phase of the process. The article then examines two key mechanisms that obscure this work: first, the fragmentation and distribution of AI tasks across a broad range of actors; and second, the shift of conceptual labor away from coding toward data preparation and algorithmic monitoring. This article lays the foundation for an investigative method aimed at tracking ethnographically the entire algorithmic production chain and the diverse actors involved, in order to better document how conceptual labor is integrated into machine learning systems.","",""
"2025","The ethics of AI value chains","             Researchers, practitioners, and policymakers with an interest in the ethics of artificial intelligence (AI) need more integrative approaches for studying and intervening in AI systems across many contexts and scales of activity. This paper presents             AI value chains             as an integrative concept that satisfies that need. To more clearly theorize AI value chains and conceptually distinguish them from supply chains, we review theories of value chains and AI value chains from strategic management, service science, economic geography, industry, government, and applied research literature. We then conduct an integrative review of a sample of 67 sources that cover the ethical concerns implicated in AI value chains. Building upon the findings of our integrative review, we recommend three future directions that researchers, practitioners, and policymakers can take to advance more ethical practices across AI value chains. We urge AI ethics researchers and practitioners to move toward value chain perspectives that situate actors in context, account for the many types of resources involved in cocreating AI systems, and integrate a wider range of ethical concerns across contexts and scales.           ","",""
"2025","Information, collaboration, regulation: Physician and AI researcher views on ethical considerations in clinical AI integration","             Due to the potential scope and impact of artificial intelligence’s (AI's) adoption in medicine, a comprehensive assessment of the potential ethical considerations arising during clinical integration is needed. Existing ethical frameworks and principles have aided in the identification of ethical considerations that may arise during the clinical integration of AI, however, our understanding of these considerations remains preliminary to the extent that it is not yet robustly informed by empirical research on key stakeholders’ experiences and perspectives. Utilizing a qualitative descriptive approach, we completed in-depth semi-structured interviews with physicians (             n              = 11) and AI researchers (             n              = 10) who had experience developing or using clinical AI regarding ethical considerations that they have perceived in relation to their work. An analysis of the interviews identified considerations related to information sharing and understanding, the risks and systemic impacts of clinical AI, and opportunities for safeguards. Physicians and AI researchers expressed questions relating to how much information and understanding is needed by both physicians and patients in order for AI to be ethically used in clinical practice, agreed that unintended impacts associated with clinical AI could pose threats to patient autonomy, and advocated for more diligent and thoughtful regulation of clinical AI innovation.           ","",""
"2025","Navigating ethical boundaries: Subtle agency and compliance among tech workers in China and the United States","Tech workers often experience ethical tensions arising from the misalignment of their values and the prevailing unethical or ethically ambiguous practices concerning data and algorithms in the workplace. Despite this, there is an insufficient understanding of how tech professionals address ethical tensions. Based on interviews with 98 tech workers in China and the United States, this study explores ethical tensions, the workers’ responses, and potential cross-national variations. It identifies three prevalent strategies by which tech workers navigate conflicts between their ethical principles and their companies’ practices: complying with market fundamentalism, compromising personal ethics, and upholding and critiquing ethical guidelines. Cross-national differences in strategy implementation highlight nuanced approaches by tech workers in diverse economic, political, and ethical contexts. The study positions these responses within a theoretical framework of ethical agency, revealing tech workers’ subtle ethical agency and the factors that constrain their decision-making processes. It also contributes data-driven insights to promote ethical practices in the global tech industry.","",""
"2025","Artificial Intelligence and the ethics of navigating ambiguity","             This paper examines ambiguity within AI practice, arguing for an ethics of AI which stays with fundamental ambiguities and accounts for their complex socio-material entanglements. However, common approaches to responsible governance of AI are often predicated upon notions of predictable pipelines and static outputs which are assumed to be easily describable and cleanly structured. Drawing upon empirical findings which challenge these notions, and conceptual tools from Simone de Beauvoir's             Ethics of Ambiguity             , I illustrate how AI [ethics] can be better understood as grounded in ambiguity and propose reframing ambiguity from a failure or risk to a core facet of the study and governance of AI. I report on interviews with 23 AI practitioners, combined with observations from an ethnography of an AI practitioner based in an industry AI lab, examining their motivations, aims and actions in developing and implementing AI models. Practitioners described the impact of local, epistemic and systemic constraints, employing heuristics, intuition and creative problem-solving to navigate embedded, inherent ambiguity and uncertainty across material and practice. Building on these analyses, I propose that engaging with ambiguity in the study and ethics of AI can provide productive sites for ethical reflection and governance.           ","",""
"2025","The dystopian imaginaries of ChatGPT: A designed cycle of fear","The advent of OpenAI’s ChatGPT in 2022 catalyzed a wave of excitement and apprehension, but especially fear. This article examines the dystopian narratives that emerged after ChatGPT’s release date. Through a critical analysis of media responses, we uncover how dystopian imaginaries discussing ChatGPT become rhetorically constructed in popular, journalistic discourse. The article locates prevalent anxieties surrounding ChatGPT’s unprecedented text-generation capabilities, and identifies recurrent fears regarding academic integrity, the proliferation of misinformation, ethical dilemmas in human-AI interaction, and the perpetuation of social biases. Moreover, the article introduces the concept of ‘fear cycles’ – recurring patterns of dystopian projections in response to emerging technologies. By documenting and dissecting these fear cycles, we offer insights into the underlying rhetorical features that drive societal reactions to technological advancements. The research ultimately contributes to a nuanced understanding of how ChatGPT dystopian imaginaries develop particular futures, while grounding the present in predictable anxieties related to technological innovation.","",""
"2025","Technology acceptance, moral panic, and perceived ease of use: Negotiating ChatGPT at research one universities","The perceived ease of use around artificial intelligence (AI) systems like ChatGPT has created an ostensible moral and technological panic around how AI technologies are transforming higher education. This study uses the Technology Acceptance Model to suggest ‘perceived ease of use’ is both a factor in acceptance and panic around emerging technologies. We provide a qualitative analysis of statements, guides, and policies about AI and ChatGPT from 148 U.S. universities to argue that there is not so much a panic happening in higher education as there is a concerted effort to negotiate and integrate artificial intelligence into classroom, research, and daily operations. Across four different categories of statements – university policies, teaching and learning resources, library guides, and professor statements – we find recurring values emphasizing usefulness as well as navigating ethical concerns. ChatGPT’s perceived ease of use has led higher education faculty and staff to avoid panic discourses and focus on problem-solving discourses as they negotiate its institutionalization.","",""
"2025","Sociotechnical imaginaries and public communication: Analytical framework and empirical illustration using the case of artificial intelligence","             The concept of sociotechnical imaginaries (SIs) has been widely used and proven fruitful to understand diverging trajectories of technologies. While scholars have acknowledged the multi-layered materialisation of SIs and highlighted the importance of the communicative layer therein, this aspect has remained under-conceptualised. Therefore, we propose an analytical framework to better understand Sociotechnical Imaginaries in Public Communication (SIPCs), defined as             publicly             constructed visions of (un)desirable sociotechnical futures that guide action, mobilise resources and lay out trajectories for the materialisation or prevention of those futures. In this article, we first discuss relevant strands of research on SIs and public communication. We then lay out the analytical framework of SIPCs that enables the rigorous reconstruction and comparison of sociotechnical imaginaries in public and mediated communication. Finally, we illustrate the framework with examples from public communication about artificial intelligence, an emerging key technology in contemporary societies.           ","",""
"2025","Review of Molnar’s The Walls Have Eyes: Surviving Migration in the Age of Artificial Intelligence","","",""
"2025","Detection of LLM-powered bots using image classification"," In the rapidly changing landscape of online social interactions, the presence of automated accounts, or bots, has always posed a significant challenge to maintaining platforms where the information posted is authentic and reliable. The emergence of large language models (LLMs) may exacerbate this problem, as researchers have recently found families of automated accounts that use generative artificial intelligence to produce their posts. This paper focuses on this new type of bot, particularly on detecting them. Using a new detection technique that relies on image classification to distinguish between human and automated accounts, we demonstrate remarkable efficiency in identifying bots whose posts are generated by large language models. Our research improves the results of previous work on the detection of bot accounts powered by generative artificial intelligence.          ","",""
"2025","Transfeminist AI governance","This paper re-imagines the governance of artificial intelligence (AI) through a transfeminist lens, focusing on challenges of power, participation, and injustice, and on opportunities for advancing equity, community-based resistance, and transformative change. AI governance is a field of research and practice seeking to maximize benefits and minimize harms caused by AI systems. Unfortunately, AI governance practices are frequently ineffective at preventing AI systems from harming people and the environment, with historically marginalized groups such as trans people being particularly vulnerable to harm. Building upon trans and feminist theories of ethics, I introduce an approach to transfeminist AI governance. Applying a transfeminist lens in combination with a critical self-reflexivity methodology, I retroactively reinterpret findings from three empirical studies of AI governance practices in Canada and globally. In three reflections on my findings, I show that large-scale AI governance systems structurally prioritize the needs of industry over marginalized communities. As a result, AI governance is limited by power imbalances and exclusionary norms. My reflections reveal that re-grounding AI governance in transfeminist ethical principles can support AI governance researchers, practitioners, and organizers in addressing those limitations.","",""
"2025","Brains, cogs, and square-headed robots: Anthropomorphisation of artificial intelligence in icons","Through quantitative semiotic analysis, the study investigates how icons bear the meaning of artificiality and synthetization. This research analyses a corpus of icons sampled from Web sites offering digital graphic assets to recognize and measure the prevalence of different motives. It reveals that some icons may inappropriately simplify or support a false representation of a given technology, such as excessive anthropomorphization of artificial intelligence.","",""
"2025","The rise of AI art: A look through digital artists' eyes","With the rapid development of artificial intelligence (AI), many tasks in the art field are transformed. A critical question emerges: how do artists perceive this change? This study investigates the perceptions of digital artists in the entertainment industry towards AI-generated art. Through 22 in-depth interviews, we find that artists considered AI art unethical. In addition, artists questioned the agency and the lack of human expression behind AI art. Furthermore, the creative process is a point of contention. Unlike traditional art forms, AI art offered them little control or room for experimentation. Artists could not iterate or make incremental changes. Regarding the final AI art output, although the digital artists were impressed with its aesthetics, they found AI art soulless. The study has important implications for emerging research on AI art.","",""
"2025","Charting the landscape of nefarious uses of generative artificial intelligence for online election interference","Generative artificial intelligence (GenAI) and large language models (LLMs) pose significant risks, particularly in the realm of online election interference. This paper explores the nefarious applications of GenAI, highlighting their potential to disrupt democratic processes through deepfakes, botnets, targeted misinformation campaigns, and synthetic identities. By examining recent case studies and public incidents, we illustrate how malicious actors exploit these technologies to try influencing voter behavior, spread disinformation, and undermine public trust in electoral systems. This paper also discusses the societal implications of these threats, emphasizing the urgent need for robust mitigation strategies and international cooperation to safeguard democratic integrity.","",""
"2025","Making sense of generative AI for creating art: An exploratory study","  This paper explores how people make sense of generative AI for creating art, and how various meanings that people associate with generative AI for creating art relate to their trust toward generative AI for creating art, general orientation toward technologies, and the level of creativity. Taking a perspective of communication and media studies, the present research reports the findings of an exploratory survey (n = 112) conducted in April 2024. The results suggest that positive orientation toward technologies appears to be most relevant for positive meanings and higher trust toward generative AI in the context of creating art, and more frequent use of AI for creating art was also related to higher level of trust. Proficiency was the factor found to be related to creativity, but positive meanings and trust were not. The present study, although exploratory, shows the relevance of these factors, along with a communication and media perspective, to researching computational creativity, as technology continues to be appropriated into our everyday life.  ","",""
"2025","Is artificial intelligence revolutionizing climate change education? An exploratory study of climate change ChatGPT output","Educators and students are increasingly using a subset of artificial intelligence (AI) large language models (LLMs) like ChatGPT to learn about climate change. Others speculate that learning will be revolutionized forever with the mainstreaming of AI-based sites like ChatGPT. Less is known about the quality of AI prompt responses about climate change. This study examines 100 ChatGPT climate change prompts to examine what exploratory themes emerge from climate response queries. The results show the presence of the following themes within ChatGPT responses: passive learning, lack of transparency, desensitization and low environmental concern language. Climate educators, students, and parents who are curious about ChatGPT’s functionality and the accuracy of its responses on climate change may find the results useful.","",""
"2025","Do I spy AI? The impact of AI-generated images on trust and donation behaviors","Artificial intelligence (AI) is changing the business landscape in countless ways. This research examines the role of AI-generated images in business applications, an area that has received less attention relative to other business-related AI technologies. The results of three experimental studies indicate that people can tell the difference between real and AI-generated images and that the former leads to increased levels of trust. Our findings further show that in charitable settings, real (versus AI-generated) images increase donation intentions and amount through the mediating influence of trust. This empirical and conceptual evidence offers valuable insights to businesses that are considering incorporating AI-generated images.","",""
"2025","Governing AI and the democratisation of governance","AI governance that is situated in data justice principles requires appropriate institutional governance arrangements as well as strategies addressing the socio-political-economic context of governance. In light of threats to civic agency and democracy in datafied societies, this shouldentail the systematic involvement of citizens and affected communities in debates and decisions over the governance of AI to make sure those who are affected by datafication have a voice in steering its future. From community action and social movement mobilisations to deliberative and participatory models such as citizen juries, civic initiatives have played an important role in influencing the norms, principles and policies of AI and data use. AI governance thus points us to the need for democratising governance itself and exploring a democratic perspective for life in the age of AI.","",""
"2025","Does governing artificial intelligence mean governing data? When looking at data (governance), what do we not see?","This commentary responds to Taylor et al.'s proposal for global data and artificial intelligence (AI) justice principles. While the proposed data governance framework is timely and important, I offer two provocations to invite critical reflection on the limits of the data-centric approaches to governing AI and digital societies more broadly. Drawing on the findings from the ERC INFO-LEG project, first, I argue that framing digital problems primarily in terms of data risks oversimplification of such problems, which are often complex socio-technical and political affairs. Second, I caution that data governance and the ensuing uncertainties of the boundaries of included data and groups can distract from the essence of the problems at hand, facilitating evasion strategies for powerful actors, for example, by navigating technical and legal complexity to their advantage. I advocate for more context-specific approaches to governing digital societies, where data governance may be part of the solution but not the starting point.","",""
"2025","Laying the foundations for AI justice in uncertain times","The race by governments and industry to exploit the potential of artificial intelligence (AI) raises concerns over prioritizing economic gains over community-based rights. This heightens the urgency of efforts to develop a justice-based framework for AI governance. Despite the European Union's AI Act, international AI governance remains fragmented, particularly as the U.S. pivots toward an innovation-first approach. Principles of data justice and data governance offer a solid foundation on which to build a framework for equitable AI governance, particularly as broader human rights principles become more fragmented and contested.","",""
"2025","What role can governance play in data justice? Reflections on the current AI moment","In translating data governance for the AI era, Taylor et al. have put forward a set of ambitious and considered benchmarks that can serve as an important vision for data justice. However, in doing so, we are also confronted with the challenges of the current ‘AI moment’. The open consolidation of powerful interests seeking to advance AI's unfettered development has also marginalised governance agendas, especially those seeking to assert publicness and avenues for resistance and refusal. In this sense, asserting benchmarks for just data governance can only garner real meaning if such benchmarks can account for the broader politics of the current AI moment and connect to the political mobilisation around data justice that might now be possible.","",""
"2025","Governing artificial intelligence means governing data: (re)setting the agenda for data justice"," The field of data justice has been evolving to take into account the role of data in powering the field of artificial intelligence (AI). In this paper we review the main conceptual bases for governing data and AI: the market-based approach, the personal–non-personal data distinction and strategic sovereignty. We then analyse how these are being operationalised into practical models for governance, including public data trusts, data cooperatives, personal data sovereignty, data collaboratives, data commons approaches and indigenous data sovereignty. We interrogate these models' potential for just governance based on four benchmarks which we propose as a reformulation of the Data Justice governance agenda identified by Taylor in her 2017 framework. Re-situating data justice at the intersection of data and AI, these benchmarks focus on preserving and strengthening public infrastructures and public goods; inclusiveness; contestability and accountability; and global responsibility. We demonstrate how they can be used to test whether a governance approach will succeed in redistributing power, engaging with public concerns and creating a plural politics of AI. ","",""
"2025","Epistemic coups and epistemic responsibility","","",""
"2025","Can we make a pause while racing toward the unknown future?","In the face of rapid technological development, this text asks whether we can take a moment to pause and reflect on where we are heading. Inspired by Minna Ruckenstein's original article, two initiatives are explored: a global petition to stop the development of advanced AI and a national campaign to restrict smartphone use in schools. While the first had little impact, the second gained a lot of public attention as it brought together scientists, experts, educators, families, schools, etc. The article argues that social scientists should go beyond criticism and help find better solutions by bringing together different initiatives. It calls for a more thoughtful use of technology, where people are not just users, but active participants in shaping a more balanced and people-centered future.","",""
"2025","Algorithmic gatewatching and its implications","This response to the article ‘Conceptualising the “Algorithmic Public Opinion”: Public Opinion Formation in the Digital Age’ by Alessandro Gandini, Silvia Keeling, and Urbano Reviglio considers the concept of algorithmic public opinion and highlights that while the algorithms of digital and social media platforms and associated information and communication infrastructures now exert a substantial influence on how public opinion formation unfolds (the process) and what public opinion patterns emerge as a result (the product), these algorithms are by no means the sole determinant of process and product. It argues instead that the ordinary users of these platforms – and they are active users of these communicative spaces, not just passive ‘audiences’ for the communicative efforts of others – retain significantly more agency than this article's discussion affords them.","",""
"2025","Algorithmic narrativity: Literary experiments that drive technology","This paper extends Raymond Williams’ insights on technology and society by asserting that not only scientific, social, and economic conditions but also aesthetic factors are crucial for technological adoption. The concept of ‘algorithmic narrativity’ is introduced to describe the combination of the human ability to understand experience through narrative with the power of the computer to process and generate data. Using examples ranging from the exhibition of John Clark's Latin Verse Machine in 1845 through generative poetry in the early 1950s to today's generative AI, the authors argue that aesthetic innovation precedes and is necessary to technological advances.","",""
"2025","Beyond ownership: Human–robot relationships between property and personhood"," As artificial intelligence (AI)-enabled social robots continue to enter our lifeworlds, we will need to grapple with challenges to assumptions about our relationship to and even with these technological objects. This essay works at the intersection of social robotics, legal studies, and human–machine communication to explore the concept of ownership in human–machine relations. In particular, we draw on more-than-human approaches to ask: To what extent should (need) we retire the concept of ownership in the context of AI-enabled social robots? With a particular emphasis on companion robots, we explore alternatives to the ownership modality by investigating concepts such as personhood, a degrees-of-relationship perspective, and a situational approach to understanding human–robot relationships. The goal is to re-imagine human–robot relationships beyond legal confinements by engaging a pragmatic perspective that supplements existing philosophical approaches. We conclude the paper by discussing practical implications of our proposed perspective. ","",""
"2025","What do we know about algorithmic literacy? The status quo and a research agenda for a growing field"," The increasing role of algorithms shaping our use of communication technology—particularly on social media—comes with a growth of empirical research attempting to assess how literate users are regarding these algorithms. This rapidly emerging field is marked by great diversity in terms of how it theorizes and measures our understanding of algorithms, due, in part, to the opaque “black box” nature of the algorithms themselves. In this review article, we summarize the state of knowledge on algorithmic literacy, including its definitions, development, measurement, and current theorizing on human–algorithm interaction. Drawing on this existing work, we propose an agenda including four different directions that future research could focus on: (1) balancing users’ expectations of algorithmic literacy with developers’ responsibility for algorithmic transparency, (2) methods for engaging users in increasing their literacy, (3) further developing the affective and behavioral facets of literacy, and (4) addressing the new algorithmic divide. ","",""
"2025","Machine visions: A corporate imaginary of artificial sight"," Machine vision is one the most consequential applications of artificial intelligence (AI) in contemporary society. This article analyzes how companies that produce machine vision technology articulate what it means for machines to “see.” Through a thematic analysis of more than 200 corporately produced documents, we examine the companies’ product offerings and identify three discursive techniques that entwine basic explanations of emerging technology with the ideologies of AI producers: dismantling sight into technical action, expanding the parameters of sight, and seeing through data. These recurring corporate narratives organize perceptions of automation, educating outsiders how to value computational outcomes and support them through rearranging the real-world conditions of labor. We argue that the social power of machine vision is not only in how it detects objects, but also in how it arbitrates what work is visible in visions of the industry’s future. ","",""
"2025","Mind ascribed to AI and the appreciation of AI-generated art"," Creative artificial intelligence (AI) has received a lot of attention in recent years. Artworks that are introduced to be generated by AI (rather than a human artist) are, however, often evaluated negatively. Integrating extant research, we suggest that AI is ascribed less mind (i.e. agency and experience) which is responsible for this effect. In two experiments ( N = 176 and N = 381) we observed negative indirect effects of artist information (AI vs human artist) on the appreciation of visual artworks. The AI is consistently ascribed less agency and less experience than a human artist. Higher levels of experience and agency ascribed to an artist are, in turn, associated with higher appreciation of a piece of art. In both experiments the total effect of artist information on appreciation was not significant. Artist information did not predict whether the artwork deviated positively from viewers’ expectations developed before the actual artwork was encountered. ","",""
"2025","Responsibility networks in media discourses on automation: A comparative analysis of social media algorithms and social companions"," The diffusion of algorithms, robots, and artificial intelligence has sparked public debates regarding opportunities, risks, and responsibility for addressing problems and developing solutions. Since media cover and shape sociotechnical imaginaries, this study investigates the Austrian media discourses on responsibility in two domains of automation: social media algorithms and social companions. Using a machine learning approach, relevant articles were identified, followed by a manual comparative content analysis. The findings indicate that media coverage of social media algorithms tends to be more critical compared to social companions. In the debate about social media algorithms, journalists emerge as the most common speakers raising responsibility issues and primarily attributing them to Internet platform providers. Conversely, responsibility for social robotics is predominantly articulated by experts, considering it as a responsibility shared by society, economy, and research. Furthermore, the media present different perspectives on the agency and responsibility of social media algorithms and social robots themselves. ","",""
"2025","I see a double-edged sword: How self-other perceptual gaps predict public attitudes toward ChatGPT regulations and literacy interventions"," The double-edged nature of generative artificial intelligence (AI) underscores the importance of understanding complex and paradoxical public views about this emerging technology. Heeding to this call, this study examined how the general public perceives and reacts to Chat GPT and the implications of these perceptions, drawing on the third-person and first-person effect. A national survey in the United States ( N = 1004) revealed that individuals tend to believe they would personally benefit from the positive influence of Chat GPT, while others will benefit relatively less. Also, results showed that people believe that self is more capable of using Chat GPT critically, ethically, and efficiently than others. Interestingly, the self-other gap in perceived efficacy was influenced by subjective knowledge but not by objective knowledge about Chat GPT. The self-other gap in perceived efficacy negatively predicted support for government regulation of Chat GPT, while the self-other gap in both perceived influence and efficacy positively predicted support for literacy interventions. ","",""
"2025","AI chatbot accountability in the age of algorithmic gatekeeping: Comparing generative search engine political information retrieval across five languages"," This study investigates the performance of search engine chatbots powered by large language models in generative political information retrieval. Applying algorithmic accountability as a central theme, this research (a) assesses the alignment of artificial intelligence (AI) chatbot responses with timely political information, (b) investigates the factual correctness and transparency of chatbot-sourced synopses, (c) examines the adherence of chatbots to democratic norms and impartiality ideals, (d) analyzes the sourcing and attribution behaviors of the chatbots, and (e) explores the universality of chatbot gatekeeping across different languages. Using the 2024 Taiwan presidential election as a case study and prompting as a method, the study audits responses from Microsoft Copilot in five languages. The findings reveal significant discrepancies in information readiness, content accuracy, norm adherence, source usage, and attribution behavior across languages. These results underscore the contextual awareness when applying accountability assessment that looks beyond transparency in AI-mediated communication, especially during politically sensitive events. ","",""
"2025","Rationalisation of the news: How AI reshapes and retools the gatekeeping processes of news organisations in the United Kingdom, United States and Germany","This article analyses how the use of artificial intelligence shapes the way news gets produced and distributed, based on 143 interviews with news workers at 34 leading publishers in the United States, United Kingdom and Germany. Drawing on gatekeeping theory and the concept of rationalisation, it describes and explains the use and effects of the technology in the news. Artificial intelligence (AI) is used in all parts of the gatekeeping process to drive efficiency gains, optimise processes and bring about greater effectiveness, with these effects being real but task-dependent and hard to quantify. Overall, AI reshapes and retools the production and distribution of news by providing publishers with new means in the service of achieving existing ends, rationalising the work of news organisations in the process, and pushing it more strongly towards logics of efficiency, predictability and calculability. I discuss these findings with respect to their impact on the public arena and the reconfiguration of power and control within the information ecosystem.","",""
"2025","Culture machine: How MetaCLIP codifies culture","             How is the cultural made computational? CLIP models are a recent artificial intelligence (AI) innovation which train on massive amounts of Internet data in order to align language and image, deploying this ‘grasp’ of cultural concepts to understand prompts, classify images and carry out tasks. To critically investigate this cultural codification, we explore MetaCLIP, a recent variation developed by Meta. We analyse the model’s metadata, a single file of 500,000 terms that aims to achieve a ‘balanced distribution’ or sufficiently broad understanding of concepts. We show how this model assembles histories, languages, ideologies and media artefacts into a kind of cultural knowledge. We argue this codification fuses the ancient technique of the             list             with a more recent technique of             latent space             . We conclude by framing these technologies as             cultural machines             that exert power in defining and operationalising a particular understanding of ‘culture’ invisibly and at scale.           ","",""
"2025","Silicon Valley revisited: On Californian ideologies and the differences they make","             While research has highlighted the ties between algorithms and culture, this article focuses on how cross-cultural encounters shape developers’ perceptions of their algorithmic work. I ask: How do cultural transitions and intercultural encounters influence people’s perceptions of their algorithms? How do key issues regarding algorithmic production get translated and reinterpreted? Based on 50 semi-structured interviews with Israelis who immigrated to Silicon Valley, I show that their interpretation of the cultural differences between Israeli and Silicon Valley cultures—and their culturally specific             logic of difference             —informs how they perceive the algorithms they develop and their ethical implications. I argue that these perceptions stem from deep-seated cultural tropes but are also potentially ameliorated by their encounter with Silicon Valley’s culture. Thus, this article calls for a more diverse view of Silicon Valley’s ideologies, showing that attention to cross-cultural encounters can both reveal and complicate the performative logics sustaining Silicon Valley’s liberal image.           ","",""
"2025","Creative Underspheres and democratic challenges: Exploring the implications of generative AI misuse","This article introduces the concept of the Undersphere – a networked community brought together via creative exchange – to highlight how the increased proliferation of Generative AI poses risks not yet acknowledged by policymakers within emerging AI regulatory frameworks. Employing a single case study methodology – namely, exploring exchanges made on r/StableDiffusion, a known subgroup on Reddit – it illustrates the conceptual parameters of the Undersphere, outlines the potential for creative harm within the GenAI space, and counters these elements against the AI regulatory frameworks found within the EU AI Act. It concludes that a risk management framework that provides a more fluid approach to addressing risks, such as those found in governance frameworks aimed at eradicating climate change, could be better positioned to address insecurities manifesting from the GenAI space.","",""
"2025","The impact of exposure to generative AI art on aesthetic appreciation, perceptions of AI mind, and evaluations of AI and of art careers","Visual art is increasingly created by generative artificial intelligence (generative AI). This study, conducted online with 470 US participants, investigated whether exposure to art attributed to AI may influence aesthetic appreciation, perceptions of AI mind, and evaluations of AI (acceptance of AI as an artist, evaluation of AI as an identity and realistic threat) and of art careers. Exposure to art introduced as generated by AI (vs a human artist) reduced appreciation. No significant impact was observed on the other dependent variables. For ostensibly AI-generated art, higher appreciation was associated with more acceptance toward AI as an artist and lower levels of AI realistic threat. This suggests that mere exposure to art attributed to AI may not be sufficient to induce a change in perceptions of AI mind, evaluations of AI and of art careers, but these effects might occur if AI-attributed art is appreciated aesthetically.","",""
"2025","Public sector chatbots: AI frictions and data infrastructures at the interface of the digital welfare state","Chatbots have become a mundane experience for Internet users. Public sector institutions have recently been introducing more advanced chatbots. In this article, we consider two cases of public sector chatbots, one in Estonia and one in Sweden, seeking to challenge the seemingly coherent understanding of artificial intelligence (AI) in the public sector. The aim is to both question the “thingness” of AI and show AI chatbots can be very different things. The material in this article is based on in-depth interviews and observations at public sector institutions that have relatively recently implemented chatbots. We employ the notion of AI frictions as a sensitizing concept to engage with the material and the diverging character of the public sector chatbots in the two countries. In the analysis, we identify AI frictions related to expectations of AI, organizational logics, as well as values connected with the digitalization of the public sector.","",""
"2025","Journalists, Emotions, and the Introduction of Generative AI Chatbots: A Large-Scale Analysis of Tweets Before and After the Launch of ChatGPT","As part of a broader look at the impact of generative AI, this study investigated the emotional responses of journalists to the release of ChatGPT at the time of its launch. By analyzing nearly 1 million Tweets from journalists at major US news outlets, we tracked changes in emotion, tone, and sentiment before and after the introduction of ChatGPT in November 2022. Using various computational and natural language processing techniques to measure emotional shifts in response to ChatGPT’s release, we found an increase in positive emotion and a more favorable tone post-launch, suggesting initial optimism toward AI’s potential. This research underscores the pivotal role of journalists as interpreters of technological innovation and disruption, highlighting how their emotional reactions may shape public narratives around emerging technologies. The study contributes to understanding the intersection of journalism, emotion, and AI, offering insights into the broader societal impact of generative AI tools.","",""
"2025","Who Is Spreading AI-Generated Health Rumors? A Study on the Association Between AIGC Interaction Types and the Willingness to Share Health Rumors"," Generative chatbots based on artificial intelligence technology have become an essential channel for people to obtain health information. They provide not only comprehensive health information but also real-time virtual companionship. However, the health information provided by AI may not be completely accurate. Employing a 3 × 2 × 2 experimental design, the research examines the effects of interaction types with AI-generated content (AIGC), specifically under virtual companionship and knowledge acquisition scenarios, on the willingness to share health-related rumors. In addition, it explores the impact of the nature of the rumors (fear vs hope) and the role of altruistic tendencies in this context. The results show that people are more willing to share rumors in a knowledge acquisition situation. Fear-type rumors can stimulate people’s willingness to share more than hope-type rumors. Altruism plays a moderating role, increasing the willingness to share health rumors in the scenario of virtual companionship, while decreasing the willingness to share health rumors in the scenario of knowledge acquisition. These findings support Kelley’s three-dimensional attribution theory and negativity bias theory, and extend these results to the field of human–computer interaction. The results of this study help to understand the rumor spreading mechanism in the context of human–computer interaction and provide theoretical support for the improvement of health chatbots. ","",""
"2025","Autocompleting inequality","The latest wave of AI hype has been driven by ‘generative AI’ systems exemplified by ChatGPT, which was created by OpenAI’s ‘fine-tuning’ of a large language model (LLM). This process involves using human labor to provide feedback on generative outputs in order to bring these into greater ‘alignment’ with ‘safety’. This article analyzes the fine-tuning of generative AI as a process of social ordering, beginning with the encoding of cultural dispositions into LLMs, their containment and redirection into vectors of ‘safety’, and the subsequent challenge of these ‘guard rails’ by users. Fine-tuning becomes a means by which some social hierarchies are reproduced, reshaped, and flattened. By analyzing documentation provided by generative AI developers, I show how fine-tuning makes use of human judgement to reshape the algorithmic reproduction of inequality, while also arguing that the most important values driving AI alignment are commercial imperatives and aligning with political economy.","",""
"2025","Imagining communities  with ‘intelligent’ machines","  Shared perceptions of the world are imagined with and within available media technological environments. In other words our communication environment conditions our social imagination and the ways in which we can see the world. The essay, based on the inaugural lecture of the author, discusses how this conditioning takes place and with what consequences in the contemporary digital societies. The essay draws on the research by the author on innovationism and discusses the concepts of reversed tools, content confusion and attention factory. Utilizing the study by Berg &amp; Valaskivi (2023) on commercial image recognition services and their performance in recognizing religion in images as an example, the essay illustrates failures and imperfections of AI technologies which are often considered more neutral than human beings. The essay calls for critical thinking on digitalization and expansion of AI technologies and encourages prioritization of humane interests as well as social and cultural welbeing over commerciality in technological development.   ","",""
"2025","AI Snake Oil: What AI Can Do, What It Can't, and How to Tell the Difference","","",""
"2025","Humanist in the Loop: Teaching Critical AI Literacies, Episode 1","","",""
"2025","The Eye of the Master: A Social History of Artificial Intelligence","","",""
"2025","Endangered Judgment: Joseph Weizenbaum, Artificial Intelligence, and the Imperialism of Instrumental Reason","Abstract                This article surveys Joseph Weizenbaum's insights on judgment and the academic and computational practices that he thought were threatening it in the 1960s and 1970s. It also details how the writing of Hannah Arendt, the events of the Vietnam War, and Weizenbaum's identity as a Jewish émigré from Nazi Germany shaped his views on judgment. The essay then describes Weizenbaum's attraction to the humanities and less technically oriented education as a way of mitigating these threats. The conclusion reflects on how Weizenbaum's writings relate to present-day concerns about judgment, computational thinking, and the humanities. Reading Weizenbaum helps highlight computer science's problematic faith in “instrumental reason,” computational practices that promote technological solutionism, and the eugenic qualities inherent in the machine metaphor. The essay also underscores the capacity, and continued value, of human judgment in a world imperiled by large socio-technical systems and AI.","",""
"2025","Rethinking Error: “Hallucinations” and Epistemological Indifference","Abstract                In our current generative AI paradigm, so-called hallucinations are typically seen as a kind of nuisance that will eventually be swept away as the technology improves. There are several reasons to question this assumption. One of them is that the very phenomenon is the result of deliberate business decisions by corporations invested in delivering diverse sentence structures through deep learning and generative pretrained transformers (GPTs). This article urges a fresh view on hallucinations by arguing that, rather than being errors in any conventional sense, they are evidence of a probabilistic system incapable of dealing with questions of knowledge. These systems are epistemologically indifferent. Yet, by presenting as errors to users of generative AI, hallucinations can function as practical reminders of and indexes to the limits of this kind of machine learning. Viewed this way, hallucinations remind us that every time one gets something reasonable-seeming from a system such as OpenAI's ChatGPT, one might as well have been given something quite outrageous; from the machine's perspective, it's all the same.","",""
"2025","The Worlds I See: Curiosity, Exploration, and Discovery at the Dawn of AI","","",""
"2025","The Birth of Computer Vision","","",""
"2025","Generative AI, Everyday Aesthetic Production, and the Imperial Mode of Living","Abstract                It is becoming increasingly clear that generative AI technologies come with significant environmental costs. Regardless, commercial generative AI services suggest they will facilitate a form of everyday life in which individual users can produce aesthetic content without limits. This mode of everyday aesthetic production is sold to users through fantasies of frictionlessness and immediacy that simultaneously sanitize and expand generative AI's environmental impact. This essay argues that commercial generative AI services thereby solidify what Ulrich Brand and Markus Wissen call the “imperial mode of living,” in which consumerist comforts are directly sustained by environmental destruction and extractivism. Moreover, the essay suggests that the forms of everyday aesthetic production that generative AI services encourage surreptitiously increase what Bernard Stiegler calls “informational entropy,” describing a dependency on technological infrastructures and processes of aesthetic homogenization that entrench and thus make it more difficult to challenge AI's imperial tendencies.","",""
"2025","Cultural Red Teaming: ARRG! and Creative Misuse of AI Systems","Abstract                Artists have been using and misusing tools for decades. But the development and commercialization of diffusion-based image generation tools have challenged definitions of reappropriation, complicating the work of artists who use technology to provoke ideological critiques of technological power. In this article, three artists, operating as the Algorithmic Resistance Research Group (ARRG!), present a framework for the creative misuse of generative AI systems. By situating the lineage of technology-based art as “hacking” (and making as a form of inquiry), this article argues that misusing systems as a form of exploration within art making is a useful tool for critical AI discourse. Focusing on an exhibition of work presented at the hacker conference DEFCON 31 in 2023, the article examines creative misuse aligned with specific critiques. By using generative AI tools while rejecting their premises, artists can redefine creativity as a human endeavor, creating work that engages and shapes debate.","",""
"2025","Another kind of authenticity: the visual simulacra of artificial intelligence","","",""
"2025","A knotwork for alternative AI: pursuing community-centred and co-creative technological practices","","",""
"2025","Beyond replication: enhancing glove puppetry learning experience through multiple forms of virtual puppetry approaches","","",""
"2025","The reservoir effect: the role of GenAI based conversational agents in participatory futures","","",""
"2025","Parametric pattern design for manually knitted textile panels","","",""
"2025","Spectrum of creative agencies in AI-based art: analysis of art reviews","","",""
